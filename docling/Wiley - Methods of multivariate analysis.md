## Methods of Multivariate Analysis

Second Edition

Alvin C Rencher

<!-- image -->

<!-- image -->

WILEY SERIES IN PROBABILITY AND STATISTICS

## Methods of Multivariate Analysis

## Second Edition

## Methods of Multivariate Analysis

Second Edition

ALVIN C. RENCHER

Brigham Young University

<!-- image -->

This book is printed on acid-free paper.

<!-- image -->

Copyright c © 2002 by John Wiley &amp; Sons, Inc. All rights reserved.

Published simultaneously in Canada.

No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording, scanning or otherwise, except as permitted under Sections 107 or 108 of the 1976 United States Copyright Act, without either the prior written permission of the Publisher, or authorization through payment of the appropriate per-copy fee to the Copyright Clearance Center, 222 Rosewood Drive, Danvers, MA 01923, (978) 750-8400, fax (978) 750-4744. Requests to the Publisher for permission should be addressed to the Permissions Department, John Wiley &amp; Sons, Inc., 605 Third Avenue, New York, NY 10158-0012, (212) 850-6011, fax (212) 850-6008. E-Mail: PERMREQ@WILEY.COM.

For ordering and customer service, call 1-800-CALL-WILEY.

## Library of Congress Cataloging-in-Publication Data

Rencher, Alvin C., 1934-

Methods of multivariate analysis / Alvin C. Rencher.-2nd ed.

p.

cm. - (Wiley series in probability and mathematical statistics)

'A Wiley-Interscience publication.'

Includes bibliographical references and index.

ISBN 0-471-41889-7 (cloth)

1.

Multivariate analysis.

QA278 .R45 2001

519.5 ′ 35-dc21

Printed in the United States of America

10 9 8 7 6 5 4 3 2 1

I.

Title.

II.

Series.

2001046735

## Contents

| 1. Introduction   | 1. Introduction                     | 1. Introduction                                         | 1   |
|-------------------|-------------------------------------|---------------------------------------------------------|-----|
|                   | 1.1 Why Multivariate Analysis?, 1   | 1.1 Why Multivariate Analysis?, 1                       |     |
| 1.2               | Prerequisites, 3                    | Prerequisites, 3                                        |     |
| 1.3               | Objectives, 3                       | Objectives, 3                                           |     |
| 1.4               | Basic Types of Data and Analysis, 3 | Basic Types of Data and Analysis, 3                     |     |
| 2.                | Matrix Algebra                      | Matrix Algebra                                          | 5   |
|                   | 2.1                                 | Introduction, 5                                         |     |
|                   | 2.2                                 | Notation and Basic Definitions, 5                       |     |
|                   |                                     | 2.2.1 Matrices, Vectors, and Scalars, 5                 |     |
|                   |                                     | 2.2.2 Equality of Vectors and Matrices, 7               |     |
|                   |                                     | 2.2.3 Transpose and Symmetric Matrices, 7               |     |
|                   | 2.2.4                               | Special Matrices, 8                                     |     |
|                   | 2.3                                 | Operations, 9                                           |     |
|                   |                                     | 2.3.1 Summation and Product Notation, 9                 |     |
|                   |                                     | 2.3.2 Addition of Matrices and Vectors, 10              |     |
|                   |                                     | 2.3.3 Multiplication of Matrices and Vectors, 11        |     |
| 2.4               | Partitioned Matrices, 20            | Partitioned Matrices, 20                                |     |
| 2.5               | Rank, 22                            | Rank, 22                                                |     |
| 2.6               | Inverse, 23                         | Inverse, 23                                             |     |
| 2.7               | Positive Definite Matrices, 25      | Positive Definite Matrices, 25                          |     |
| 2.8               | Determinants, 26                    | Determinants, 26                                        |     |
| 2.9               | Trace, 30                           | Trace, 30                                               |     |
| 2.10              | Orthogonal Vectors and Matrices, 31 | Orthogonal Vectors and Matrices, 31                     |     |
| 2.11              | Eigenvectors, 32                    | Eigenvectors, 32                                        |     |
|                   | Eigenvalues and 2.11.1              | Eigenvalues and 2.11.1                                  |     |
|                   |                                     | Definition, 32                                          |     |
|                   | 2.11.2                              | I + A and I - A , 33 tr ( A ) and A , 34                |     |
|                   | 2.11.3                              | | | 2.11.4 Positive Definite and Semidefinite Matrices, |     |
|                   |                                     | The Product AB , 35                                     |     |
|                   | 2.11.5                              |                                                         |     |

|    |                                                 | 2.11.7                                                       | Spectral Decomposition, 35                               |
|----|-------------------------------------------------|--------------------------------------------------------------|----------------------------------------------------------|
|    |                                                 | 2.11.8                                                       | Square Root Matrix, 36                                   |
|    |                                                 | 2.11.9                                                       | Square Matrices and Inverse Matrices, 36                 |
|    |                                                 | 2.11.10                                                      | Singular Value Decomposition, 36                         |
| 3. | Characterizing and Displaying Multivariate Data | Characterizing and Displaying Multivariate Data              | Characterizing and Displaying Multivariate Data          |
|    | 3.1                                             | Mean and Variance of a Univariate Random Variable, 43        | 43                                                       |
|    | 3.2                                             | Covariance and Correlation of Bivariate Random Variables, 45 |                                                          |
|    |                                                 | 3.2.1                                                        | Covariance, 45                                           |
|    |                                                 | 3.2.2                                                        | Correlation, 49                                          |
|    | 3.3                                             | Scatter Plots of Bivariate Samples,                          | 50                                                       |
|    | 3.4                                             | Graphical                                                    | Displays for Multivariate Samples, 52                    |
|    | 3.5                                             | Mean                                                         | Vectors, 53                                              |
|    | 3.6                                             | Covariance                                                   | Matrices, 57                                             |
|    | 3.7                                             | Correlation                                                  | Matrices, 60                                             |
|    | 3.8                                             | Mean Vectors and Covariance Matrices for Variables, 62       | Subsets of                                               |
|    |                                                 | 3.8.1                                                        | Two Subsets, 62                                          |
|    |                                                 | 3.8.2                                                        | Three or More Subsets, 64                                |
|    | 3.9                                             | Linear Combinations of Variables, 66                         |                                                          |
|    |                                                 | 3.9.1                                                        | Sample Properties, 66                                    |
|    |                                                 | 3.9.2                                                        | Population Properties, 72                                |
|    | 3.10                                            | Measures of Overall                                          | Variability, 73                                          |
|    | 3.11                                            | Estimation of Missing Values, 74                             |                                                          |
|    | 3.12                                            | Distance between                                             | Vectors, 76                                              |
| 4. | The Multivariate 4.1 4.1.1                      | Normal Distribution Multivariate Normal Density Function,    | 82 82 Univariate Normal Density, 82                      |
|    | 4.1.2                                           |                                                              | Multivariate Normal Density, 83                          |
|    |                                                 | 4.1.3                                                        | Generalized Population Variance, 83                      |
|    |                                                 | 4.1.4                                                        | Diversity of Applications of the Multivariate Normal, 85 |
|    | 4.2                                             | Properties of Multivariate Normal Random Variables, 85       |                                                          |
|    |                                                 | in the Multivariate Normal, 90                               |                                                          |
|    | 4.3                                             | Estimation 4.3.1 Maximum Likelihood Estimation,              | 90                                                       |
|    |                                                 | 4.3.2                                                        | Distribution of y and S , 91                             |
|    | 4.4                                             | Assessing                                                    | Multivariate Normality, 92                               |
|    | Investigating                                   | 4.4.1 Univariate                                             | Normality, 92                                            |
|    |                                                 | 4.4.2                                                        | Investigating Multivariate Normality, 96                 |

| CONTENTS   | CONTENTS                          | CONTENTS                                                                            | CONTENTS                                                                            | vii   |
|------------|-----------------------------------|-------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------|-------|
|            | 4.5                               | Outliers, 99                                                                        | Outliers, 99                                                                        |       |
|            |                                   | 4.5.1                                                                               | Outliers in Univariate Samples, 100                                                 |       |
|            |                                   | 4.5.2                                                                               | Outliers in Multivariate Samples, 101                                               |       |
| 5.         | Tests on One or Two Mean Vectors  | Tests on One or Two Mean Vectors                                                    | Tests on One or Two Mean Vectors                                                    | 112   |
|            | 5.1                               | Multivariate versus Univariate Tests, 112                                           | Multivariate versus Univariate Tests, 112                                           |       |
|            | 5.2                               | Tests on 𝛍 with 𝚺 Known, 113                                                        | Tests on 𝛍 with 𝚺 Known, 113                                                        |       |
|            |                                   | 5.2.1                                                                               | Review of Univariate Test for H 0 : µ = µ 0 with σ Known, 113                       |       |
|            |                                   | 5.2.2                                                                               | Multivariate Test for H 0 : 𝛍 = 𝛍 0 with 𝚺 Known, 114                               |       |
|            | 5.3                               | Tests on 𝛍 When 𝚺 Is Unknown, 117                                                   | Tests on 𝛍 When 𝚺 Is Unknown, 117                                                   |       |
|            |                                   | 5.3.1                                                                               | Review of Univariate t -Test for H 0 : µ = µ 0 with σ Unknown, 117                  |       |
|            |                                   | 5.3.2                                                                               | Hotelling's T 2 -Test for H 0 : 𝛍 = 𝛍 0 with 𝚺 Unknown,                             | 117   |
|            | 5.4                               | Comparing Two Mean Vectors, 121                                                     | Comparing Two Mean Vectors, 121                                                     |       |
|            |                                   | 5.4.1                                                                               | Review of Univariate Two-Sample t -Test, 121                                        |       |
|            |                                   | 5.4.2                                                                               | Multivariate Two-Sample T 2 -Test, 122                                              |       |
|            |                                   | 5.4.3                                                                               | Likelihood Ratio Tests, 126                                                         |       |
|            | 5.5                               | Tests on Individual Variables Conditional on Rejection of H 0 by the T 2 -Test, 126 | Tests on Individual Variables Conditional on Rejection of H 0 by the T 2 -Test, 126 |       |
|            | 5.6                               | Computation of T 2 , 130                                                            | Computation of T 2 , 130                                                            |       |
|            |                                   | 5.6.1                                                                               | Obtaining T 2 from a MANOVA Program, 130                                            |       |
|            |                                   | 5.6.2                                                                               | Obtaining T 2 from Multiple Regression, 130                                         |       |
|            | 5.7                               | Paired Observations Test, 132                                                       | Paired Observations Test, 132                                                       |       |
|            |                                   | 5.7.1                                                                               | Univariate Case, 132                                                                |       |
|            |                                   | 5.7.2                                                                               | Multivariate Case, 134                                                              |       |
|            | 5.8                               | Test for Additional Information, 136                                                | Test for Additional Information, 136                                                |       |
|            | 5.9                               | Profile Analysis, 139                                                               | Profile Analysis, 139                                                               |       |
|            |                                   | 5.9.1                                                                               | One-Sample Profile Analysis, 139                                                    |       |
|            |                                   | 5.9.2                                                                               | Two-Sample Profile Analysis, 141                                                    |       |
| 6.         | Multivariate Analysis of Variance | Multivariate Analysis of Variance                                                   | Multivariate Analysis of Variance                                                   | 156   |
|            | 6.1                               | One-Way Models, 156                                                                 | One-Way Models, 156                                                                 |       |
|            |                                   | 6.1.1                                                                               | Univariate One-Way Analysis of Variance (ANOVA),                                    | 156   |
|            |                                   | 6.1.2                                                                               | Multivariate One-Way Analysis of Variance Model (MANOVA), 158                       |       |
|            |                                   | 6.1.3                                                                               | Wilks' Test Statistic, 161                                                          |       |
|            |                                   | 6.1.4                                                                               | Roy's Test, 164                                                                     |       |
|            |                                   | 6.1.5                                                                               | Pillai and Lawley-Hotelling Tests, 166                                              |       |

|      |                                                                                          | 6.1.6                                                                                    | Unbalanced One-Way MANOVA, 168                                                           |
|------|------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------|
|      |                                                                                          | 6.1.7                                                                                    | Summary of the Four Tests and Relationship to T 2 , 168                                  |
|      |                                                                                          | 6.1.8                                                                                    | Measures of Multivariate Association, 173                                                |
|      | 6.2 Comparison of the Four Manova Test Statistics, 176                                   | 6.2 Comparison of the Four Manova Test Statistics, 176                                   | 6.2 Comparison of the Four Manova Test Statistics, 176                                   |
| 6.3  | Contrasts, 178                                                                           | Contrasts, 178                                                                           | Contrasts, 178                                                                           |
|      |                                                                                          | 6.3.1                                                                                    | Univariate Contrasts, 178                                                                |
|      |                                                                                          | 6.3.2                                                                                    | Multivariate Contrasts, 180                                                              |
| 6.4  | Tests on Individual Variables Following Rejection of H 0 by the Overall MANOVA Test, 183 | Tests on Individual Variables Following Rejection of H 0 by the Overall MANOVA Test, 183 | Tests on Individual Variables Following Rejection of H 0 by the Overall MANOVA Test, 183 |
| 6.5  | Two-Way                                                                                  | Classification, 186                                                                      | Classification, 186                                                                      |
|      |                                                                                          | 6.5.1                                                                                    | Review of Univariate Two-Way ANOVA, 186                                                  |
|      |                                                                                          | 6.5.2                                                                                    | Multivariate Two-Way MANOVA, 188                                                         |
| 6.6  | Other Models, 195                                                                        | Other Models, 195                                                                        | Other Models, 195                                                                        |
|      |                                                                                          | 6.6.1                                                                                    | Higher Order Fixed Effects, 195                                                          |
|      |                                                                                          | 6.6.2                                                                                    | Mixed Models, 196                                                                        |
| 6.7  | Checking on the Assumptions, 198                                                         | Checking on the Assumptions, 198                                                         | Checking on the Assumptions, 198                                                         |
| 6.8  | Profile Analysis, 199                                                                    | Profile Analysis, 199                                                                    | Profile Analysis, 199                                                                    |
| 6.9  | Repeated Measures Designs, 204                                                           | Repeated Measures Designs, 204                                                           | Repeated Measures Designs, 204                                                           |
|      |                                                                                          | 6.9.1                                                                                    | Multivariate vs. Univariate Approach, 204                                                |
|      |                                                                                          | 6.9.2                                                                                    | One-Sample Repeated Measures Model, 208                                                  |
|      |                                                                                          | 6.9.3                                                                                    | k -Sample Repeated Measures Model, 211                                                   |
|      |                                                                                          | 6.9.4                                                                                    | Computation of Repeated Measures Tests, 212                                              |
|      |                                                                                          | 6.9.5                                                                                    | Repeated Measures with Two Within-Subjects Factors and One Between-Subjects Factor, 213  |
|      |                                                                                          | 6.9.6                                                                                    | Repeated Measures with Two Within-Subjects Factors, 219                                  |
|      |                                                                                          | 6.9.7                                                                                    | Factors and Two Between-Subjects Additional Topics, 221                                  |
| 6.10 | Growth Curves, 221                                                                       | Growth Curves, 221                                                                       | Growth Curves, 221                                                                       |
|      |                                                                                          | 6.10.1                                                                                   | Growth Curve for One Sample, 221                                                         |
|      |                                                                                          | 6.10.2                                                                                   | Growth Curves for Several Samples, 229                                                   |
|      |                                                                                          | 6.10.3                                                                                   | Additional Topics, 230                                                                   |
| 6.11 | Tests on a Subvector, 231                                                                | Tests on a Subvector, 231                                                                | Tests on a Subvector, 231                                                                |
|      |                                                                                          | 6.11.1                                                                                   | Test for Additional Information, 231                                                     |
|      |                                                                                          | 6.11.2                                                                                   | Stepwise Selection of Variables, 233                                                     |
| 7.   | Tests on Covariance Matrices                                                             | Tests on Covariance Matrices                                                             | Tests on Covariance Matrices                                                             |
|      | 7.1                                                                                      | Introduction, 248                                                                        | Introduction, 248                                                                        |
|      | 7.2                                                                                      | Testing a Specified Pattern for 𝚺 , 248 7.2.1 Testing H 0 𝚺 𝚺 0 , 248                    | Testing a Specified Pattern for 𝚺 , 248 7.2.1 Testing H 0 𝚺 𝚺 0 , 248                    |

- 7.2.1 Testing H 0 : 𝚺 = 𝚺 0, 248

|    |                                                               | 7.2.2                                                                             | Testing Sphericity, 250                                                           |
|----|---------------------------------------------------------------|-----------------------------------------------------------------------------------|-----------------------------------------------------------------------------------|
|    | : = [ - + ] 7.3 Tests Comparing Covariance Matrices, 254      | : = [ - + ] 7.3 Tests Comparing Covariance Matrices, 254                          | : = [ - + ] 7.3 Tests Comparing Covariance Matrices, 254                          |
|    |                                                               | 7.3.1 Univariate Tests of Equality of Variances, 254                              |                                                                                   |
|    |                                                               | 7.3.2                                                                             | Multivariate Tests of Equality of Covariance Matrices, 255                        |
|    | 7.4                                                           | Tests of Independence, 259                                                        | Tests of Independence, 259                                                        |
|    |                                                               | 7.4.1                                                                             | Independence of Two Subvectors, 259                                               |
|    |                                                               | 7.4.2                                                                             | Independence of Several Subvectors, 261                                           |
|    |                                                               | 7.4.3                                                                             | Test for Independence of All Variables, 265                                       |
| 8. | Discriminant Analysis: Description of Group Separation        | Discriminant Analysis: Description of Group Separation                            | 270                                                                               |
|    | 8.1                                                           | Introduction, 270                                                                 | Introduction, 270                                                                 |
|    | 8.2                                                           | The Discriminant Function for Two Groups, 271                                     | The Discriminant Function for Two Groups, 271                                     |
|    | 8.3                                                           | Relationship between Two-Group Discriminant Analysis and Multiple Regression, 275 | Relationship between Two-Group Discriminant Analysis and Multiple Regression, 275 |
|    | 8.4                                                           | Discriminant Analysis for Several Groups, 277                                     | Discriminant Analysis for Several Groups, 277                                     |
|    |                                                               | 8.4.1                                                                             | Discriminant Functions, 277                                                       |
|    | 8.5                                                           | Standardized Discriminant Functions, 282                                          | Standardized Discriminant Functions, 282                                          |
|    | 8.6                                                           | Tests of Significance, 284                                                        | Tests of Significance, 284                                                        |
|    |                                                               | 8.6.1                                                                             | Tests for the Two-Group Case, 284                                                 |
|    |                                                               | 8.6.2                                                                             | Tests for the Several-Group Case, 285                                             |
|    | 8.7                                                           | Interpretation of Discriminant Functions, 288                                     | Interpretation of Discriminant Functions, 288                                     |
|    |                                                               | 8.7.1                                                                             | Standardized Coefficients, 289                                                    |
|    |                                                               | 8.7.2                                                                             | Partial F -Values, 290                                                            |
|    |                                                               | 8.7.3                                                                             | Correlations between Variables and Discriminant Functions, 291                    |
|    |                                                               | 8.7.4                                                                             | Rotation, 291                                                                     |
|    | 8.8                                                           | Scatter Plots, 291                                                                | Scatter Plots, 291                                                                |
|    | 8.9                                                           | Stepwise Selection of Variables, 293                                              | Stepwise Selection of Variables, 293                                              |
| 9. | Classification Analysis: Allocation of Observations to Groups | Classification Analysis: Allocation of Observations to Groups                     | 299                                                                               |
|    | 9.1                                                           | Introduction, 299 Classification into Two Groups, 300                             | Introduction, 299 Classification into Two Groups, 300                             |
|    |                                                               | Classification into Several Groups, 304                                           | Classification into Several Groups, 304                                           |
|    | 9.3                                                           | 9.3.1 Equal Population Covariance Matrices: Linear                                | 9.3.1 Equal Population Covariance Matrices: Linear                                |
|    |                                                               | 9.3.2                                                                             | Classification Functions, 304 Unequal Population Covariance Matrices: Quadratic   |

|     | 9.4                   | Estimating Misclassification Rates, 307                    | Estimating Misclassification Rates, 307                    | Estimating Misclassification Rates, 307   |
|-----|-----------------------|------------------------------------------------------------|------------------------------------------------------------|-------------------------------------------|
|     | 9.5                   | Improved Estimates of Error Rates, 309                     | Improved Estimates of Error Rates, 309                     | Improved Estimates of Error Rates, 309    |
|     |                       | 9.5.1                                                      | Partitioning the Sample, 310                               |                                           |
|     |                       | 9.5.2                                                      | Holdout Method, 310                                        |                                           |
|     | 9.6                   | Subset Selection, 311                                      | Subset Selection, 311                                      |                                           |
|     | 9.7                   | Nonparametric Procedures, 314                              | Nonparametric Procedures, 314                              |                                           |
|     |                       | 9.7.1                                                      | Multinomial Data, 314                                      |                                           |
|     |                       | 9.7.2                                                      | Classification Based on Density Estimators, 315            |                                           |
|     |                       | 9.7.3                                                      | Nearest Neighbor Classification Rule, 318                  |                                           |
| 10. | Multivariate          | Regression                                                 |                                                            | 322                                       |
|     | 10.1                  | Introduction, 322                                          | Introduction, 322                                          |                                           |
|     | 10.2                  | Multiple Regression: Fixed x 's, 323                       | Multiple Regression: Fixed x 's, 323                       |                                           |
|     |                       | 10.2.1                                                     | Model for Fixed x 's, 323                                  |                                           |
|     |                       | 10.2.2                                                     | Least Squares Estimation in the Fixed- x Model, 324        |                                           |
|     |                       | 10.2.3                                                     | An Estimator for σ 2 , 326                                 |                                           |
|     |                       | 10.2.4                                                     | The Model Corrected for Means, 327                         |                                           |
|     |                       | 10.2.5                                                     | Hypothesis Tests, 329                                      |                                           |
|     |                       | 10.2.6                                                     | R 2 in Fixed- x Regression, 332                            |                                           |
|     |                       | 10.2.7                                                     | Subset Selection, 333                                      |                                           |
|     | 10.3                  | Multiple Regression: Random x 's, 337                      | Multiple Regression: Random x 's, 337                      |                                           |
|     | 10.4                  | Multivariate Multiple Regression: Estimation, 337          | Multivariate Multiple Regression: Estimation, 337          |                                           |
|     |                       | 10.4.1                                                     | The Multivariate Linear Model, 337                         |                                           |
|     |                       | 10.4.2                                                     | Least Squares Estimation in the Multivariate Model,        | 339                                       |
|     |                       | 10.4.3                                                     | Properties of Least Squares Estimators ˆ B , 341           |                                           |
|     |                       | 10.4.4                                                     | An Estimator for 𝚺 , 342                                   |                                           |
|     |                       | 10.4.5                                                     | Model Corrected for Means, 342                             |                                           |
|     | 10.5                  | Multivariate Multiple Regression: Hypothesis Tests, 343    | Multivariate Multiple Regression: Hypothesis Tests, 343    |                                           |
|     |                       | 10.5.1                                                     | Test of Overall Regression, 343                            |                                           |
|     |                       | 10.5.2                                                     | Test on a Subset of the x 's, 347                          |                                           |
|     | 10.6                  | Measures of Association between the y 's and the x 's, 349 | Measures of Association between the y 's and the x 's, 349 |                                           |
|     | 10.7                  | Subset Selection, 351                                      | Subset Selection, 351                                      |                                           |
|     |                       | 10.7.1                                                     | Stepwise Procedures, 351                                   |                                           |
|     |                       | 10.7.2                                                     | All Possible Subsets, 355                                  |                                           |
|     | 10.8                  | Multivariate Regression: Random x 's, 358                  | Multivariate Regression: Random x 's, 358                  |                                           |
| 11. | Canonical Correlation | Canonical Correlation                                      | Canonical Correlation                                      | 361                                       |
|     | 11.1                  | Introduction, 361                                          | Introduction, 361                                          |                                           |
|     | 11.2                  | Canonical Correlations and Canonical Variates, 361         | Canonical Correlations and Canonical Variates, 361         |                                           |

| 11.3   | Properties of Canonical Correlations, 366   | Properties of Canonical Correlations, 366                                                 | Properties of Canonical Correlations, 366   |
|--------|---------------------------------------------|-------------------------------------------------------------------------------------------|---------------------------------------------|
|        | Tests of Significance, 367                  | Tests of Significance, 367                                                                | Tests of Significance, 367                  |
|        | 11.4.1                                      | Tests of No Relationship between the y 's and the x 's, 367                               |                                             |
|        | 11.4.2                                      | Test of Significance of Succeeding Canonical                                              |                                             |
|        |                                             | Correlations after the First, 369                                                         |                                             |
|        | 11.5 Interpretation, 371                    | 11.5 Interpretation, 371                                                                  |                                             |
|        | 11.5.1                                      | Standardized Coefficients, 371                                                            |                                             |
|        | 11.5.2                                      | Correlations between Variables and Canonical Variates,                                    | 373                                         |
|        | 11.5.3                                      | Rotation, 373                                                                             |                                             |
|        | 11.5.4                                      | Redundancy Analysis, 373                                                                  |                                             |
|        | 11.6                                        | Relationships of Canonical Correlation Analysis to Other                                  |                                             |
|        | Multivariate Techniques, 374                | Multivariate Techniques, 374                                                              |                                             |
|        | 11.6.1                                      | Regression, 374                                                                           |                                             |
|        | 11.6.2                                      | MANOVA and Discriminant Analysis, 376                                                     |                                             |
| 12.    | Principal Component Analysis                | Principal Component Analysis                                                              | 380                                         |
|        | 12.1                                        | Introduction, 380                                                                         |                                             |
|        | 12.2                                        | Geometric and Algebraic Bases of Principal Components, 381 12.2.1 Geometric Approach, 381 |                                             |
|        | 12.2.2                                      | Algebraic Approach, 385                                                                   |                                             |
|        | 12.3                                        | Principal Components and Perpendicular Regression, 387                                    |                                             |
|        | 12.4                                        | Plotting of Principal Components, 389                                                     |                                             |
|        | 12.5                                        | Principal Components from the Correlation Matrix, 393                                     |                                             |
|        | 12.6                                        | Deciding How Many Components to Retain, 397                                               |                                             |
|        | 12.7                                        | Information in the Last Few Principal Components, 401                                     |                                             |
|        | 12.8                                        | Interpretation of Principal Components, 401                                               |                                             |
|        | 12.8.1                                      | Special Patterns in S or R , 402                                                          |                                             |
|        | 12.8.2                                      | Rotation, 403                                                                             |                                             |
|        | 12.8.3                                      | Correlations between Variables and Principal Components, 403                              |                                             |
|        | 12.9                                        | Selection of Variables, 404                                                               |                                             |
| 13.    | Factor Analysis                             | Factor Analysis                                                                           | 408                                         |
|        | 13.1                                        | Introduction, 408                                                                         |                                             |
|        | 13.2                                        | Orthogonal Factor Model, 409                                                              |                                             |
|        |                                             | 13.2.2 Nonuniqueness of Factor Loadings, 414                                              |                                             |
|        | 13.3                                        | Estimation of Loadings and Communalities, 415                                             |                                             |
|        |                                             | 13.3.2 Principal Factor Method, 421                                                       |                                             |

|     |                      | 13.3.3                                                                   | Iterated Principal Factor Method, 424                                    |
|-----|----------------------|--------------------------------------------------------------------------|--------------------------------------------------------------------------|
|     |                      | 13.3.4                                                                   | Maximum Likelihood Method, 425                                           |
|     | 13.4                 | Choosing the Number of Factors, m , 426                                  | Choosing the Number of Factors, m , 426                                  |
|     | 13.5                 | Rotation, 430                                                            | Rotation, 430                                                            |
|     |                      | 13.5.1                                                                   | Introduction, 430                                                        |
|     |                      | 13.5.2                                                                   | Orthogonal Rotation, 431                                                 |
|     |                      | 13.5.3                                                                   | Oblique Rotation, 435                                                    |
|     |                      | 13.5.4                                                                   | Interpretation, 438                                                      |
|     | 13.6                 | Factor Scores, 438                                                       | Factor Scores, 438                                                       |
|     | 13.7                 | Validity of the Factor Analysis Model, 443                               | Validity of the Factor Analysis Model, 443                               |
|     | 13.8                 | The Relationship of Factor Analysis to Principal Component Analysis, 447 | The Relationship of Factor Analysis to Principal Component Analysis, 447 |
| 14. | Cluster Analysis     | Cluster Analysis                                                         | 451                                                                      |
|     | 14.1                 | Introduction, 451                                                        | Introduction, 451                                                        |
|     | 14.2                 | Measures of Similarity or Dissimilarity, 452                             | Measures of Similarity or Dissimilarity, 452                             |
|     | 14.3                 | Hierarchical Clustering, 455                                             | Hierarchical Clustering, 455                                             |
|     |                      | 14.3.1                                                                   | Introduction, 455                                                        |
|     |                      | 14.3.2                                                                   | Single Linkage (Nearest Neighbor), 456                                   |
|     |                      | 14.3.3                                                                   | Complete Linkage (Farthest Neighbor), 459 Average Linkage, 463           |
|     |                      | 14.3.4 14.3.5                                                            | Centroid, 463                                                            |
|     |                      | 14.3.6                                                                   | Median, 466                                                              |
|     |                      | 14.3.7                                                                   | Ward's Method, 466                                                       |
|     |                      | 14.3.8                                                                   | Flexible Beta Method, 468                                                |
|     |                      | 14.3.9                                                                   | Properties of Hierarchical Methods, 471                                  |
|     |                      | 14.3.10                                                                  | Divisive Methods, 479                                                    |
|     | 14.4                 | Nonhierarchical Methods, 481                                             | Nonhierarchical Methods, 481                                             |
|     |                      | 14.4.1                                                                   | Partitioning, 481                                                        |
|     |                      | 14.4.2                                                                   | Other Methods, 490                                                       |
|     | 14.5                 | Choosing the Number of Clusters, 494                                     | Choosing the Number of Clusters, 494                                     |
|     | 14.6                 | Cluster Validity, 496                                                    | Cluster Validity, 496                                                    |
|     | 14.7                 | Clustering Variables, 497                                                | Clustering Variables, 497                                                |
| 15. | Graphical Procedures | Graphical Procedures                                                     | 504                                                                      |
|     | 15.1                 | Multidimensional Scaling, 504                                            | Multidimensional Scaling, 504                                            |
|     |                      | 15.1.1                                                                   | Introduction, 504                                                        |
|     |                      | 15.1.2                                                                   | Metric Multidimensional Scaling, 505                                     |
|     |                      | 15.1.3                                                                   | Nonmetric Multidimensional Scaling, 508                                  |

| 15.2   | Correspondence Analysis, 514   | Correspondence Analysis, 514                          | Correspondence Analysis, 514   |
|--------|--------------------------------|-------------------------------------------------------|--------------------------------|
|        | 15.2.1                         | Introduction, 514                                     |                                |
|        | 15.2.2                         | Row and Column Profiles, 515                          |                                |
|        | 15.2.3                         | Testing Independence, 519                             |                                |
|        | 15.2.4                         | Coordinates for Plotting Row and Column Profiles, 521 |                                |
|        | 15.2.5                         | Multiple Correspondence Analysis, 526                 |                                |
| 15.3   | Biplots, 531                   | Biplots, 531                                          |                                |
|        | 15.3.1                         | Introduction, 531                                     |                                |
|        | 15.3.2                         | Principal Component Plots, 531                        |                                |
|        | 15.3.3                         | Singular Value Decomposition Plots, 532               |                                |
|        | 15.3.4                         | Coordinates, 533                                      |                                |
|        | 15.3.5                         | Other Methods, 535                                    |                                |
| A.     | Tables                         | Tables                                                | 549                            |
| B.     | Answers and Hints to Problems  | Answers and Hints to Problems                         | 591                            |
| C.     | Data Sets and SAS Files        | Data Sets and SAS Files                               | 679                            |
|        | References                     | References                                            | 681                            |
|        | Index                          | Index                                                 | 695                            |

## Preface

I have long been fascinated by the interplay of variables in multivariate data and by the challenge of unraveling the effect of each variable. My continuing objective in the second edition has been to present the power and utility of multivariate analysis in a highly readable format.

Practitioners and researchers in all applied disciplines often measure several variables on each subject or experimental unit. In some cases, it may be productive to isolate each variable in a system and study it separately. Typically, however, the variables are not only correlated with each other, but each variable is influenced by the other variables as it affects a test statistic or descriptive statistic. Thus, in many ually they yield little information about the system. Using multivariate analysis, the variables can be examined simultaneously in order to access the key features of the process that produced them. The multivariate approach enables us to (1) explore the joint performance of the variables and (2) determine the effect of each variable in the presence of the others.

Multivariate analysis provides both descriptive and inferential procedures-we can search for patterns in the data or test hypotheses about patterns of a priori interest. With multivariate descriptive techniques, we can peer beneath the tangled web of variables on the surface and extract the essence of the system. Multivariate inferential procedures include hypothesis tests that (1) process any number of variables without inflating the Type I error rate and (2) allow for whatever intercorrelations the variables possess. A wide variety of multivariate descriptive and inferential procedures is readily accessible in statistical software packages.

My selection of topics for this volume reflects many years of consulting with researchers in many fields of inquiry. A brief overview of multivariate analysis is given in Chapter 1. Chapter 2 reviews the fundamentals of matrix algebra. Chapters 3 and 4 give an introduction to sampling from multivariate populations. Chapters 5, 6, 7, 10, and 11 extend univariate procedures with one dependent variable (including t -tests, analysis of variance, tests on variances, multiple regression, and multiple correlation) to analogous multivariate techniques involving several dependent variables. A review of each univariate procedure is presented before covering the multivariate counterpart. These reviews may provide key insights the student missed in previous courses.

Chapters 8, 9, 12, 13, 14, and 15 describe multivariate techniques that are not extensions of univariate procedures. In Chapters 8 and 9, we find functions of the variables that discriminate among groups in the data. In Chapters 12 and 13, we

find functions of the variables that reveal the basic dimensionality and characteristic patterns of the data, and we discuss procedures for finding the underlying latent variables of a system. In Chapters 14 and 15 (new in the second edition), we give methods for searching for groups in the data, and we provide plotting techniques that show relationships in a reduced dimensionality for various kinds of data.

In Appendix A, tables are provided for many multivariate distributions and tests. These enable the reader to conduct an exact test in many cases for which software packages provide only approximate tests. Appendix B gives answers and hints for most of the problems in the book.

Appendix C describes an ftp site that contains (1) all data sets and (2) SAS command files for all examples in the text. These command files can be adapted for use in working problems or in analyzing data sets encountered in applications.

To illustrate multivariate applications, I have provided many examples and exercises based on 59 real data sets from a wide variety of disciplines. A practitioner or consultant in multivariate analysis gains insights and acumen from long experience in working with data. It is not expected that a student can achieve this kind of seasoning in a one-semester class. However, the examples provide a good start, and further development is gained by working problems with the data sets. For example, in Chapters 12 and 13, the exercises cover several typical patterns in the covariance or correlation matrix. The student's intuition is expanded by associating these covariance patterns with the resulting configuration of the principal components or factors.

Although this is a methods book, I have included a few derivations. For some readers, an occasional proof provides insights obtainable in no other way. I hope that instructors who do not wish to use proofs will not be deterred by their presence. The proofs can be disregarded easily when reading the book.

My objective has been to make the book accessible to readers who have taken as few as two statistical methods courses. The students in my classes in multivariate analysis include majors in statistics and majors from other departments. With the applied researcher in mind, I have provided careful intuitive explanations of the concepts and have included many insights typically available only in journal articles or in the minds of practitioners.

My overriding goal in preparation of this book has been clarity of exposition. I hope that students and instructors alike will find this multivariate text more comfortable than most. In the final stages of development of both the first and second editions, I asked my students for written reports on their initial reaction as they read each day's assignment. They made many comments that led to improvements in the manuscript. I will be very grateful if readers will take the time to notify me of errors or of other suggestions they might have for improvements.

I have tried to use standard mathematical and statistical notation as far as possible and to maintain consistency of notation throughout the book. I have refrained from the use of abbreviations and mnemonic devices. These save space when one is reading a book page by page, but they are annoying to those using a book as a reference.

Equations are numbered sequentially throughout a chapter; for example, (3.75) indicates the 75th numbered equation in Chapter 3. Tables and figures are also num-

bered sequentially throughout a chapter in the form 'Table 3.8' or 'Figure 3.1.' Examples are not numbered sequentially; each example is identified by the same number as the section in which it appears and is placed at the end of the section.

When citing references in the text, I have used the standard format involving the year of publication. For a journal article, the year alone suffices, for example, Fisher (1936). But for books, I have usually included a page number, as in Seber (1984, p. 216).

This is the first volume of a two-volume set on multivariate analysis. The second volume is entitled Multivariate Statistical Inference and Applications (Wiley, 1998). The two volumes are not necessarily sequential; they can be read independently. I adopted the two-volume format in order to (1) provide broader coverage than would be possible in a single volume and (2) offer the reader a choice of approach.

The second volume includes proofs of many techniques covered in the first 13 chapters of the present volume and also introduces additional topics. The present volume includes many examples and problems using actual data sets, and there are fewer algebraic problems. The second volume emphasizes derivations of the results and contains fewer examples and problems with real data. The present volume has fewer references to the literature than the other volume, which includes a careful review of the latest developments and a more comprehensive bibliography. In this second edition, I have occasionally referred the reader to Rencher (1998) to note that added coverage of a certain subject is available in the second volume.

I am indebted to many individuals in the preparation of the first edition. My initial exposure to multivariate analysis came in courses taught by Rolf Bargmann at the University of Georgia and D. R. Jensen at Virginia Tech. Additional impetus to probe the subtleties of this field came from research conducted with Bruce Brown at BYU. I wish to thank Bruce Brown, Deane Branstetter, Del Scott, Robert Smidt, and Ingram Olkin for reading various versions of the manuscript and making valuable suggestions. I am grateful to the following students at BYU who helped with computations and typing: Mitchell Tolland, Tawnia Newton, Marianne Matis Mohr, Gregg Littlefield, Suzanne Kimball, Wendy Nielsen, Tiffany Nordgren, David Whiting, Karla Wasden, and Rachel Jones.

## SECOND EDITION

For the second edition, I have added Chapters 14 and 15, covering cluster analysis, multidimensional scaling, correspondence analysis, and biplots. I also made numerous corrections and revisions (almost every page) in the first 13 chapters, in an effort to improve composition, readability, and clarity. Many of the first 13 chapters now have additional problems.

I have listed the data sets and SAS files on the Wiley ftp site rather than on a diskette, as in the first edition. I have made improvements in labeling of these files.

I am grateful to the many readers who have pointed out errors or made suggestions for improvements. The book is better for their caring and their efforts.

I thank Lonette Stoddard and Candace B. McNaughton for typing and J. D. Williams for computer support. As with my other books, I dedicate this volume to my wife, LaRue, who has supplied much needed support and encouragement.

ALVIN C. RENCHER

## Acknowledgments

I thank the authors, editors, and owners of copyrights for permission to reproduce the following materials:

- Figure 3.8 and Table 3.2, Kleiner and Hartigan (1981), Reprinted by permission of Journal of the American Statistical Association
- Table 3.3, Kramer and Jensen (1969a), Reprinted by permission of Journal of Quality Technology
- Table 3.4, Reaven and Miller (1979), Reprinted by permission of Diabetologia
- Table 3.5, Timm (1975), Reprinted by permission of Elsevier North-Holland Publishing Company
- Table 3.6, Elston and Grizzle (1962), Reprinted by permission of Biometrics
- Table 3.7, Frets (1921), Reprinted by permission of Genetica
- Table 3.8, O'Sullivan and Mahan (1966), Reprinted by permission of American Journal of Clinical Nutrition
- Table 4.3, Royston (1983), Reprinted by permission of Applied Statistics
- Table 5.1, Beall (1945), Reprinted by permission of Psychometrika
- Table 5.2, Hummel and Sligo (1971), Reprinted by permission of Psychological Bulletin
- Table 5.3, Kramer and Jensen (1969b), Reprinted by permission of Journal of Quality Technology
- Table 5.5, Lubischew (1962), Reprinted by permission of Biometrics
- Table 5.6, Travers (1939), Reprinted by permission of Psychometrika
- Table 5.7, Andrews and Herzberg (1985), Reprinted by permission of SpringerVerlag
- Table 5.8, Tintner (1946), Reprinted by permission of Journal of the American Statistical Association
- Table 5.9, Kramer (1972), Reprinted by permission of the author
- Table 5.10, Cameron and Pauling (1978), Reprinted by permission of National Academy of Science

- Table 6.2, Andrews and Herzberg (1985), Reprinted by permission of SpringerVerlag
- Table 6.3, Rencher and Scott (1990), Reprinted by permission of Communications in Statistics: Simulation and Computation
- Table 6.6, Posten (1962), Reprinted by permission of the author
- Table 6.8, Crowder and Hand (1990, pp. 21-29), Reprinted by permission of Routledge Chapman and Hall
- Table 6.12, Cochran and Cox (1957), Timm (1980), Reprinted by permission of John Wiley and Sons and Elsevier North-Holland Publishing Company
- Table 6.14, Timm (1980), Reprinted by permission of Elsevier North-Holland Publishing Company
- Table 6.16, Potthoff and Roy (1964), Reprinted by permission of Biometrika Trustees
- Table 6.17, Baten, Tack, and Baeder (1958), Reprinted by permission of Quality Progress
- Table 6.18, Keuls et al. (1984), Reprinted by permission of Scientia Horticulturae
- Table 6.19, Burdick (1979), Reprinted by permission of the author
- Table 6.20, Box (1950), Reprinted by permission of Biometrics
- Table 6.21, Rao (1948), Reprinted by permission of Biometrika Trustees
- Table 6.22, Cameron and Pauling (1978), Reprinted by permission of National Academy of Science
- Table 6.23, Williams and Izenman (1989), Reprinted by permission of Colorado State University
- Table 6.24, Beauchamp and Hoel (1974), Reprinted by permission of Journal of Statistical Computation and Simulation
- Table 6.25, Box (1950), Reprinted by permission of Biometrics
- Table 6.26, Grizzle and Allen (1969), Reprinted by permission of Biometrics
- Table 6.27, Crepeau et al. (1985), Reprinted by permission of Biometrics
- Table 6.28, Zerbe (1979a), Reprinted by permission of Journal of the American Statistical Association
- Table 6.29, Timm (1980), Reprinted by permission of Elsevier North-Holland Publishing Company
- Table 7.1, Siotani et al. (1963), Reprinted by permission of the Institute of Statistical Mathematics

- Table 7.2, Reprinted by permission of R. J. Freund
- Table 8.1, Kramer and Jensen (1969a), Reprinted by permission of Journal of Quality Technology
- Table 8.3, Reprinted by permission of G. R. Bryce and R. M. Barker
- Table 10.1, Box and Youle (1955), Reprinted by permission of Biometrics
- Tables 12.2, 12.3, and 12.4, Jeffers (1967), Reprinted by permission of Applied Statistics
- Table 13.1, Brown et al. (1984), Reprinted by permission of the Journal of Pascal, Ada, and Modula
- Correlation matrix in Example 13.6, Brown, Strong, and Rencher (1973), Reprinted by permission of The Journal of the Acoustical Society of America
- Table 14.1, Hartigan (1975), Reprinted by permission of John Wiley and Sons
- Table 14.3, Dawkins (1989), Reprinted by permission of The American Statistician
- Table 14.7, Hand et al. (1994), Reprinted by permission of D. J. Hand
- Table 14.12, Sokol and Rohlf (1981), Reprinted by permission of W. H. Freeman and Co.
- Table 14.13, Hand et al. (1994), Reprinted by permission of D. J. Hand
- Table 15.1, Kruskal and Wish (1978), Reprinted by permission of Sage Publications
- Tables 15.2 and 15.5, Hand et al. (1994), Reprinted by permission of D. J. Hand
- Table 15.13, Edwards and Kreiner (1983), Reprinted by permission of Biometrika
- Table 15.15, Hand et al. (1994), Reprinted by permission of D. J. Hand
- Table 15.16, Everitt (1987), Reprinted by permission of the author
- Table 15.17, Andrews and Herzberg (1985), Reprinted by permission of Springer Verlag
- Table 15.18, Clausen (1988), Reprinted by permission of Sage Publications
- Table 15.19, Andrews and Herzberg (1985), Reprinted by permission of Springer Verlag
- Table A.1, Mulholland (1977), Reprinted by permission of Biometrika Trustees
- Table A.2, D'Agostino and Pearson (1973), Reprinted by permission of Biometrika Trustees
- Table A.3, D'Agostino and Tietjen (1971), Reprinted by permission of Biometrika Trustees

- Table A.4, D'Agostino (1972), Reprinted by permission of Biometrika Trustees
- Table A.5, Mardia (1970, 1974), Reprinted by permission of Biometrika Trustees
- Table A.6, Barnett and Lewis (1978), Reprinted by permission of John Wiley and Sons
- Table A.7, Kramer and Jensen (1969a), Reprinted by permission of Journal of Quality Technology
- Table A.8, Bailey (1977), Reprinted by permission of Journal of the American Statistical Association
- Table A.9, Wall (1967), Reprinted by permission of the author, Albuquerque, NM
- Table A.10, Pearson and Hartley (1972) and Pillai (1964, 1965), Reprinted by permission of Biometrika Trustees
- Table A.11, Schuurmann et al. (1975), Reprinted by permission of Journal of Statistical Computation and Simulation
- Table A.12, Davis (1970a,b, 1980), Reprinted by permission of Biometrika Trustees
- Table A.13, Kleinbaum, Kupper, and Muller (1988), Reprinted by permission of PWS-KENT Publishing Company
- Table A.14, Lee et al. (1977), Reprinted by permission of Elsevier NorthHolland Publishing Company
- Table A.15, Mathai and Katiyar (1979), Reprinted by permission of Biometrika Trustees

## C H A P T E R 1

## Introduction

## 1.1 WHYMULTIVARIATE ANALYSIS?

Multivariate analysis consists of a collection of methods that can be used when several measurements are made on each individual or object in one or more samples. We will refer to the measurements as variables and to the individuals or objects as units (research units, sampling units, or experimental units) or observations . In practice, multivariate data sets are common, although they are not always analyzed as such. But the exclusive use of univariate procedures with such data is no longer excusable, given the availability of multivariate techniques and inexpensive computing power to carry them out.

Historically, the bulk of applications of multivariate techniques have been in the behavioral and biological sciences. However, interest in multivariate methods has now spread to numerous other fields of investigation. For example, I have collaborated on multivariate problems with researchers in education, chemistry, physics, geology, engineering, law, business, literature, religion, public broadcasting, nursing, mining, linguistics, biology, psychology, and many other fields. Table 1.1 shows some examples of multivariate observations.

The reader will notice that in some cases all the variables are measured in the same scale (see 1 and 2 in Table 1.1). In other cases, measurements are in different scales (see 3 in Table 1.1). In a few techniques, such as profile analysis (Sections 5.9 and 6.8), the variables must be commensurate, that is, similar in scale of measurement; however, most multivariate methods do not require this.

Ordinarily the variables are measured simultaneously on each sampling unit. Typically, these variables are correlated. If this were not so, there would be little use for many of the techniques of multivariate analysis. We need to untangle the overlapping information provided by correlated variables and peer beneath the surface to see the underlying structure. Thus the goal of many multivariate approaches is simplification . We seek to express what is going on in terms of a reduced set of dimensions. Such multivariate techniques are exploratory ; they essentially generate hypotheses rather than test them.

On the other hand, if our goal is a formal hypothesis test, we need a technique that will (1) allow several variables to be tested and still preserve the significance level

Table 1.1. Examples of Multivariate Data

| Units                        | Variables                                                                         |
|------------------------------|-----------------------------------------------------------------------------------|
| 1. Students                  | Several exam scores in a single course                                            |
| 2. Students                  | Grades in mathematics, history, music, art, physics                               |
| 3. People                    | Height, weight, percentage of body fat, resting heart rate                        |
| 4. Skulls                    | Length, width, cranial capacity                                                   |
| 5. Companies                 | Expenditures for advertising, labor, raw materials                                |
| 6. Manufactured items        | Various measurements to check on compliance with specifications                   |
| 7. Applicants for bank loans | Income, education level, length of residence, savings account, current debt load  |
| 8. Segments of literature    | Sentence length, frequency of usage of certain words and of style characteristics |
| 9. Human hairs               | Composition of various elements                                                   |
| 10. Birds                    | Lengths of various bones                                                          |

and (2) do this for any intercorrelation structure of the variables. Many such tests are available.

As the two preceding paragraphs imply, multivariate analysis is concerned generally with two areas, descriptive and inferential statistics. In the descriptive realm, we often obtain optimal linear combinations of variables. The optimality criterion varies from one technique to another, depending on the goal in each case. Although linear combinations may seem too simple to reveal the underlying structure, we use them for two obvious reasons: (1) they have mathematical tractability (linear approximations are used throughout all science for the same reason) and (2) they often perform well in practice. These linear functions may also be useful as a follow-up to inferential procedures. When we have a statistically significant test result that compares several groups, for example, we can find the linear combination (or combinations) of variables that led to rejection of the hypothesis. Then the contribution of each variable to these linear combinations is of interest.

In the inferential area, many multivariate techniques are extensions of univariate procedures. In such cases, we review the univariate procedure before presenting the analogous multivariate approach.

Multivariate inference is especially useful in curbing the researcher's natural tendency to read too much into the data. Total control is provided for experimentwise error rate; that is, no matter how many variables are tested simultaneously, the value of α (the significance level) remains at the level set by the researcher.

Some authors warn against applying the common multivariate techniques to data for which the measurement scale is not interval or ratio. It has been found, however, that many multivariate techniques give reliable results when applied to ordinal data.

For many years the applications lagged behind the theory because the computations were beyond the power of the available desktop calculators. However, with modern computers, virtually any analysis one desires, no matter how many variables

or observations are involved, can be quickly and easily carried out. Perhaps it is not premature to say that multivariate analysis has come of age.

## 1.2 PREREQUISITES

The mathematical prerequisite for reading this book is matrix algebra. Calculus is not used [with a brief exception in equation (4.29)]. But the basic tools of matrix algebra are essential, and the presentation in Chapter 2 is intended to be sufficiently complete so that the reader with no previous experience can master matrix manipulation up to the level required in this book.

The statistical prerequisites are basic familiarity with the normal distribution, t -tests, confidence intervals, multiple regression, and analysis of variance. These techniques are reviewed as each is extended to the analogous multivariate procedure.

This is a multivariate methods text. Most of the results are given without proof. In a few cases proofs are provided, but the major emphasis is on heuristic explanations. Our goal is an intuitive grasp of multivariate analysis, in the same mode as other statistical methods courses. Some problems are algebraic in nature, but the majority involve data sets to be analyzed.

## 1.3 OBJECTIVES

I have formulated three objectives that I hope this book will achieve for the reader. These objectives are based on long experience teaching a course in multivariate methods, consulting on multivariate problems with researchers in many fields, and guiding statistics graduate students as they consulted with similar clients.

The first objective is to gain a thorough understanding of the details of various multivariate techniques, their purposes, their assumptions, their limitations, and so on. Many of these techniques are related; yet they differ in some essential ways. We emphasize these similarities and differences.

The second objective is to be able to select one or more appropriate techniques for a given multivariate data set. Recognizing the essential nature of a multivariate data set is the first step in a meaningful analysis. We introduce basic types of multivariate data in Section 1.4.

The third objective is to be able to interpret the results of a computer analysis of a multivariate data set. Reading the manual for a particular program package is not enough to make an intelligent appraisal of the output. Achievement of the first objective and practice on data sets in the text should help achieve the third objective.

## 1.4 BASIC TYPES OF DATA AND ANALYSIS

We will list four basic types of (continuous) multivariate data and then briefly describe some possible analyses. Some writers would consider this an oversimpli-

fication and might prefer elaborate tree diagrams of data structure. However, many data sets can fit into one of these categories, and the simplicity of this structure makes it easier to remember. The four basic data types are as follows:

1. A single sample with several variables measured on each sampling unit (subject or object);
2. A single sample with two sets of variables measured on each unit;
3. Two samples with several variables measured on each unit;
4. Three or more samples with several variables measured on each unit.

Each data type has extensions, and various combinations of the four are possible. A few examples of analyses for each case are as follows:

1. A single sample with several variables measured on each sampling unit:
2. (a) Test the hypothesis that the means of the variables have specified values.
3. (b) Test the hypothesis that the variables are uncorrelated and have a common variance.
4. (c) Find a small set of linear combinations of the original variables that summarizes most of the variation in the data (principal components).
5. (d) Express the original variables as linear functions of a smaller set of underlying variables that account for the original variables and their intercorrelations (factor analysis).
2. A single sample with two sets of variables measured on each unit:
7. (a) Determine the number, the size, and the nature of relationships between the two sets of variables (canonical correlation). For example, you may wish to relate a set of interest variables to a set of achievement variables. How much overall correlation is there between these two sets?
8. (b) Find a model to predict one set of variables from the other set (multivariate multiple regression).
3. Two samples with several variables measured on each unit:
10. (a) Compare the means of the variables across the two samples (Hotelling's T 2 -test).
11. (b) Find a linear combination of the variables that best separates the two samples (discriminant analysis).
12. (c) Find a function of the variables that accurately allocates the units into the two groups (classification analysis).
4. Three or more samples with several variables measured on each unit:
14. (a) Compare the means of the variables across the groups (multivariate analysis of variance).
15. (b) Extension of 3(b) to more than two groups.
16. (c) Extension of 3(c) to more than two groups.

## C H A P T E R 2

## Matrix Algebra

## 2.1 INTRODUCTION

This chapter introduces the basic elements of matrix algebra used in the remainder of this book. It is essentially a review of the requisite matrix tools and is not intended to be a complete development. However, it is sufficiently self-contained so that those with no previous exposure to the subject should need no other reference. Anyone unfamiliar with matrix algebra should plan to work most of the problems entailing numerical illustrations. It would also be helpful to explore some of the problems involving general matrix manipulation.

With the exception of a few derivations that seemed instructive, most of the results are given without proof. Some additional proofs are requested in the problems. For the remaining proofs, see any general text on matrix theory or one of the specialized matrix texts oriented to statistics, such as Graybill (1969), Searle (1982), or Harville (1997).

## 2.2 NOTATION AND BASIC DEFINITIONS

## 2.2.1 Matrices, Vectors, and Scalars

A matrix is a rectangular or square array of numbers or variables arranged in rows and columns. We use uppercase boldface letters to represent matrices. All entries in matrices will be real numbers or variables representing real numbers. The elements of a matrix are displayed in brackets. For example, the ACT score and GPA for three students can be conveniently listed in the following matrix:

<!-- formula-not-decoded -->

The elements of A can also be variables, representing possible values of ACT and GPA for three students:

where ai j is a general element.

With three rows and two columns, the matrix A in (2.1) or (2.2) is said to be 3 × 2. In general, if a matrix A has n rows and p columns, it is said to be n × p . Alternatively, we say the size of A is n × p .

A vector is a matrix with a single column or row. The following could be the test scores of a student in a course in multivariate analysis:

<!-- formula-not-decoded -->

Variable elements in a vector can be identified by a single subscript:

<!-- formula-not-decoded -->

We use lowercase boldface letters for column vectors. Row vectors are expressed as

<!-- formula-not-decoded -->

where x ′ indicates the transpose of x . The transpose operation is defined in Section 2.2.3.

Geometrically, a vector with p elements identifies a point in a p -dimensional space. The elements in the vector are the coordinates of the point. In (2.35) in Section 2.3.3, we define the distance from the origin to the point. In Section 3.12, we define the distance between two vectors. In some cases, we will be interested in a directed line segment or arrow from the origin to the point.

A single real number is called a scalar , to distinguish it from a vector or matrix. Thus 2, -4, and 125 are scalars. A variable representing a scalar is usually denoted by a lowercase nonbolded letter, such as a = 5. A product involving vectors and matrices may reduce to a matrix of size 1 × 1, which then becomes a scalar.

<!-- formula-not-decoded -->

In this double-subscript notation for the elements of a matrix, the first subscript indicates the row; the second identifies the column. The matrix A in (2.2) can also be expressed as

<!-- formula-not-decoded -->

## 2.2.2 Equality of Vectors and Matrices

Two matrices are equal if they are the same size and the elements in corresponding positions are equal. Thus if A = ( ai j ) and B = ( bi j ) , then A = B if ai j = bi j for all i and j . For example, let

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Then A = C . But even though A and B have the same elements, A /negationslash= B because the two matrices are not the same size. Likewise, A /negationslash= D because a 23 /negationslash= d 23. Thus two matrices of the same size are unequal if they differ in a single position.

## 2.2.3 Transpose and Symmetric Matrices

The transpose of a matrix A , denoted by A ′ , is obtained from A by interchanging rows and columns. Thus the columns of A ′ are the rows of A , and the rows of A ′ are the columns of A . The following examples illustrate the transpose of a matrix or vector:

<!-- formula-not-decoded -->

The transpose operation does not change a scalar, since it has only one row and one column.

If the transpose operator is applied twice to any matrix, the result is the original matrix:

<!-- formula-not-decoded -->

If the transpose of a matrix is the same as the original matrix, the matrix is said to be symmetric ; that is, A is symmetric if A = A ′ . For example,

<!-- formula-not-decoded -->

Clearly, all symmetric matrices are square.

## 2.2.4 Special Matrices

The diagonal of a p × p square matrix A consists of the elements a 11, a 22 , . . . , app . For example, in the matrix

<!-- formula-not-decoded -->

the elements 5, 9, and 1 lie on the diagonal. If a matrix contains zeros in all offdiagonal positions, it is said to be a diagonal matrix . An example of a diagonal matrix is

<!-- formula-not-decoded -->

This matrix can also be denoted as

<!-- formula-not-decoded -->

A diagonal matrix can be formed from any square matrix by replacing offdiagonal elements by 0's. This is denoted by diag ( A ) . Thus for the preceding matrix A , we have

<!-- formula-not-decoded -->

A diagonal matrix with a 1 in each diagonal position is called an identity matrix and is denoted by I . For example, a 3 × 3 identity matrix is given by

<!-- formula-not-decoded -->

An upper triangular matrix is a square matrix with zeros below the diagonal, such as

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

A lower triangular matrix is defined similarly.

A vector of 1's is denoted by j :

<!-- formula-not-decoded -->

A square matrix of 1's is denoted by J . For example, a 3 × 3 matrix J is given by

<!-- formula-not-decoded -->

Finally, we denote a vector of zeros by 0 and a matrix of zeros by O . For example,

<!-- formula-not-decoded -->

## 2.3 OPERATIONS

## 2.3.1 Summation and Product Notation

For completeness, we review the standard mathematical notation for sums and products. The sum of a sequence of numbers a 1, a 2 , . . . , an is indicated by

<!-- formula-not-decoded -->

If the n numbers are all the same, then ∑ n i = 1 a = a + a +··· + a = na . The sum of all the numbers in an array with double subscripts, such as

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

This is sometimes abbreviated to

<!-- formula-not-decoded -->

is indicated by

The product of a sequence of numbers a 1, a 2 , . . . , an is indicated by

<!-- formula-not-decoded -->

If the n numbers are all equal, the product becomes ∏ n i = 1 a = ( a )( a ) · · · ( a ) = a n .

## 2.3.2 Addition of Matrices and Vectors

If two matrices (or two vectors) are the same size, their sum is found by adding corresponding elements; that is, if A is n × p and B is n × p , then C = A + B is also n × p and is found as ( ci j ) = ( ai j + bi j ) . For example,

<!-- formula-not-decoded -->

Similarly, the difference between two matrices or two vectors of the same size is found by subtracting corresponding elements. Thus C = A -B is found as ( ci j ) = ( ai j -bi j ) . For example,

<!-- formula-not-decoded -->

If two matrices are identical, their difference is a zero matrix; that is, A = B implies A -B = O . For example,

<!-- formula-not-decoded -->

Matrix addition is commutative:

<!-- formula-not-decoded -->

The transpose of the sum (difference) of two matrices is the sum (difference) of the transposes:

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

## 2.3.3 Multiplication of Matrices and Vectors

In order for the product AB to be defined, the number of columns in A must be the same as the number of rows in B , in which case A and B are said to be conformable . Then the ( i j )th element of C = AB is

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Thus ci j is the sum of products of the i th row of A and the j th column of B . We therefore multiply each row of A by each column of B , and the size of AB consists of the number of rows of A and the number of columns of B . Thus, if A is n × m and B is m × p , then C = AB is n × p . For example, if

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

then

<!-- formula-not-decoded -->

Note that A is 4 3, B is 3 2, and AB is 4 2. In this case, AB is of a different

× × × size than either A or B .

If A and B are both n × n , then AB is also n × n . Clearly, A 2 is defined only if A is a square matrix.

In some cases AB is defined, but BA is not defined. In the preceding example, BA cannot be found because B is 3 × 2 and A is 4 × 3 and a row of B cannot be multiplied by a column of A . Sometimes AB and BA are both defined but are different in size. For example, if A is 2 × 4 and B is 4 × 2, then AB is 2 × 2 and BA is 4 × 4. If A and B are square and the same size, then AB and BA are both defined. However,

<!-- formula-not-decoded -->

except for a few special cases. For example, let

<!-- formula-not-decoded -->

Then

<!-- formula-not-decoded -->

Thus we must be careful to specify the order of multiplication. If we wish to multiply both sides of a matrix equation by a matrix, we must multiply on the left or on the right and be consistent on both sides of the equation.

Multiplication is distributive over addition or subtraction:

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Note that, in general, because of (2.20),

<!-- formula-not-decoded -->

Using the distributive law, we can expand products such as ( A -B )( C -D ) to obtain

<!-- formula-not-decoded -->

The transpose of a product is the product of the transposes in reverse order:

<!-- formula-not-decoded -->

Note that (2.27) holds as long as A and B are conformable. They need not be square. Multiplication involving vectors follows the same rules as for matrices. Suppose A is n × p , a is p × 1, b is p × 1, and c is n × 1. Then some possible products are Ab , c ′ A , a ′ b , b ′ a , and ab ′ . For example, let

<!-- formula-not-decoded -->

Then

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Note that Ab is a column vector, c ′ A is a row vector, c ′ Ab is a scalar, and a ′ b = b ′ a . The triple product c ′ Ab was obtained as c ′ ( Ab ) . The same result would be obtained if we multiplied in the order ( c ′ A ) b :

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

This is true in general for a triple product:

<!-- formula-not-decoded -->

Thus multiplication of three matrices can be defined in terms of the product of two matrices, since (fortunately) it does not matter which two are multiplied first. Note that A and B must be conformable for multiplication, and B and C must be conformable. For example, if A is n × p , B is p × q , and C is q × m , then both multiplications are possible and the product ABC is n × m .

We can sometimes factor a sum of triple products on both the right and left sides. For example,

<!-- formula-not-decoded -->

As another illustration, let X be n × p and A be n × n . Then

<!-- formula-not-decoded -->

If a and b are both n × 1, then

<!-- formula-not-decoded -->

is a sum of products and is a scalar. On the other hand, ab ′ is defined for any size a and b and is a matrix, either rectangular or square:

<!-- formula-not-decoded -->

Similarly,

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Thus a ′ a is a sum of squares, and aa ′ is a square (symmetric) matrix. The products a ′ a and aa ′ are sometimes referred to as the dot product and matrix product , respectively. The square root of the sum of squares of the elements of a is the distance from the origin to the point a and is also referred to as the length of a :

<!-- formula-not-decoded -->

As special cases of (2.33) and (2.34), note that if j is n × 1, then

<!-- formula-not-decoded -->

where j and J were defined in (2.11) and (2.12). If a is n × 1 and A is n × p , then

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Since a ′ b is a scalar, it is equal to its transpose:

Thus a ′ j is the sum of the elements in a , j ′ A contains the column sums of A , and Aj contains the row sums of A . In a ′ j , the vector j is n × 1; in j ′ A , the vector j is n × 1; and in Aj , the vector j is p × 1.

<!-- formula-not-decoded -->

This allows us to write ( a ′ b ) 2 in the form

<!-- formula-not-decoded -->

From (2.18), (2.26), and (2.39) we obtain

<!-- formula-not-decoded -->

Note that in analogous expressions with matrices, however, the two middle terms cannot be combined:

<!-- formula-not-decoded -->

If a and x 1 , x 2 , . . . , x n are all p × 1 and A is p × p , we obtain the following factoring results as extensions of (2.21) and (2.29):

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

We can express matrix multiplication in terms of row vectors and column vectors. If a ′ i is the i th row of A and b j is the j th column of B , then the ( i j )th element of AB

is a ′ i b j . For example, if A has three rows and B has two columns,

<!-- formula-not-decoded -->

then the product AB can be written as

<!-- formula-not-decoded -->

This can be expressed in terms of the rows of A :

<!-- formula-not-decoded -->

Note that the first column of AB in (2.46) is

<!-- formula-not-decoded -->

and likewise the second column is Ab 2. Thus AB can be written in the form

<!-- formula-not-decoded -->

This result holds in general:

<!-- formula-not-decoded -->

To further illustrate matrix multiplication in terms of rows and columns, let A = ( a ′ 1 a ′ 2 ) be a 2 × p matrix, x be a p × 1 vector, and S be a p × p matrix. Then

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Any matrix can be multiplied by its transpose. If A is n × p , then

AA ′ is n × n and is obtained as products of rows of A [see (2.52)] .

Similarly,

A ′ A is p × p and is obtained as products of columns of A [see (2.54)] .

From (2.6) and (2.27), it is clear that both AA ′ and A ′ A are symmetric.

In the preceding illustration for AB in terms of row and column vectors, the rows of A were denoted by a ′ i and the columns of B , by b j . If both rows and columns of a matrix A are under discussion, as in AA ′ and A ′ A , we will use the notation a ′ i for rows and a ( j ) for columns. To illustrate, if A is 3 × 4, we have

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

With this notation for rows and columns of A , we can express the elements of A ′ A or of AA ′ as products of the rows of A or of the columns of A . Thus if we write A in terms of its rows as

<!-- formula-not-decoded -->

where, for example, then we have

<!-- formula-not-decoded -->

Similarly, if we express A in terms of its columns as

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

then

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Let A = ( ai j ) be an n × n matrix and D be a diagonal matrix, D = diag ( d 1 , d 2 , . . . , dn ) . Then, in the product DA , the i th row of A is multiplied by di , and in AD , the j th column of A is multiplied by dj . For example, if n = 3, we have

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

In the special case where the diagonal matrix is the identity, we have

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

If A is rectangular, (2.58) still holds, but the two identities are of different sizes.

The product of a scalar and a matrix is obtained by multiplying each element of the matrix by the scalar:

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Since caij = ai j c , the product of a scalar and a matrix is commutative:

<!-- formula-not-decoded -->

Multiplication of vectors or matrices by scalars permits the use of linear combinations, such as

<!-- formula-not-decoded -->

If A is a symmetric matrix and x and y are vectors, the product

<!-- formula-not-decoded -->

is called a quadratic form , whereas

<!-- formula-not-decoded -->

For example,

is called a bilinear form . Either of these is, of course, a scalar and can be treated as such. Expressions such as x ′ Ay / √ x ′ Ax are permissible (assuming A is positive definite; see Section 2.7).

## 2.4 PARTITIONED MATRICES

It is sometimes convenient to partition a matrix into submatrices. For example, a partitioning of a matrix A into four submatrices could be indicated symbolically as follows:

<!-- formula-not-decoded -->

For example, a 4 × 5 matrix A can be partitioned as

<!-- formula-not-decoded -->

where

<!-- formula-not-decoded -->

If two matrices A and B are conformable and A and B are partitioned so that the submatrices are appropriately conformable, then the product AB can be found by following the usual row-by-column pattern of multiplication on the submatrices as if they were single elements; for example,

<!-- formula-not-decoded -->

It can be seen that this formulation is equivalent to the usual row-by-column definition of matrix multiplication. For example, the ( 1 , 1 ) element of AB is the product of the first row of A and the first column of B . In the ( 1 , 1 ) element of A 11 B 11 we have the sum of products of part of the first row of A and part of the first column of B . In the ( 1 , 1 ) element of A 12 B 21 we have the sum of products of the rest of the first row of A and the remainder of the first column of B .

Multiplication of a matrix and a vector can also be carried out in partitioned form. For example,

<!-- formula-not-decoded -->

where the partitioning of the columns of A corresponds to the partitioning of the elements of b . Note that the partitioning of A into two sets of columns is indicated by a comma, A = ( A 1 , A 2 ) .

The partitioned multiplication in (2.66) can be extended to individual columns of A and individual elements of b :

<!-- formula-not-decoded -->

Thus Ab is expressible as a linear combination of the columns of A , the coefficients being elements of b . For example, let

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Using a linear combination of columns of A as in (2.67), we obtain

<!-- formula-not-decoded -->

Then

We note that if A is partitioned as in (2.66), A = ( A 2 , A 2 ) , the transpose is not equal to ( A ′ 1 , A ′ 2 ) , but rather

<!-- formula-not-decoded -->

## 2.5 RANK

Before defining the rank of a matrix, we first introduce the notion of linear independence and dependence. A set of vectors a 1, a 2 , . . . , a n is said to be linearly dependent if constants c 1, c 2 , . . . , cn (not all zero) can be found such that

<!-- formula-not-decoded -->

If no constants c 1, c 2 , . . . , cn can be found satisfying (2.69), the set of vectors is said to be linearly independent .

If (2.69) holds, then at least one of the vectors a i can be expressed as a linear combination of the other vectors in the set. Thus linear dependence of a set of vectors implies redundancy in the set. Among linearly independent vectors there is no redundancy of this type.

The rank of any square or rectangular matrix A is defined as rank ( A ) = number of linearly independent rows of A = number of linearly independent columns of A .

It can be shown that the number of linearly independent rows of a matrix is always equal to the number of linearly independent columns.

If A is n × p , the maximum possible rank of A is the smaller of n and p , in which case A is said to be of full rank (sometimes said full row rank or full column rank ). For example,

<!-- formula-not-decoded -->

has rank 2 because the two rows are linearly independent (neither row is a multiple of the other). However, even though A is full rank, the columns are linearly dependent because rank 2 implies there are only two linearly independent columns. Thus, by (2.69), there exist constants c 1, c 2, and c 3 such that

<!-- formula-not-decoded -->

By (2.67), we can write (2.70) in the form or

<!-- formula-not-decoded -->

Asolution vector to (2.70) or (2.71) is given by any multiple of c = ( 14 , -11 , -12 ) ′ . Hence we have the interesting result that a product of a matrix A and a vector c is equal to 0 , even though A /negationslash= O and c /negationslash= 0 . This is a direct consequence of the linear dependence of the column vectors of A .

Another consequence of the linear dependence of rows or columns of a matrix is the possibility of expressions such as AB = CB , where A /negationslash= C . For example, let

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

All three matrices A , B , and C are full rank; but being rectangular, they have a rank deficiency in either rows or columns, which permits us to construct AB = CB with A /negationslash= C . Thus in a matrix equation, we cannot, in general, cancel matrices from both sides of the equation.

There are two exceptions to this rule. One exception involves a nonsingular matrix to be defined in Section 2.6. The other special case occurs when the expression holds for all possible values of the matrix common to both sides of the equation. For example,

<!-- formula-not-decoded -->

To see this, let x = ( 1 , 0 , . . . , 0 ) ′ . Then the first column of A equals the first column of B . Now let x = ( 0 , 1 , 0 , . . . , 0 ) ′ , and the second column of A equals the second column of B . Continuing in this fashion, we obtain A = B .

Suppose a rectangular matrix A is n × p of rank p , where p &lt; n . We typically shorten this statement to ' A is n × p of rank p &lt; n .'

## 2.6 INVERSE

If a matrix A is square and of full rank, then A is said to be nonsingular , and A has a unique inverse , denoted by A -1 , with the property that

<!-- formula-not-decoded -->

Then

For example, let

Then

<!-- formula-not-decoded -->

If A is square and of less than full rank, then an inverse does not exist, and A is said to be singular . Note that rectangular matrices do not have inverses as in (2.73), even if they are full rank.

If A and B are the same size and nonsingular, then the inverse of their product is the product of their inverses in reverse order,

<!-- formula-not-decoded -->

Note that (2.74) holds only for nonsingular matrices. Thus, for example, if A is n × p of rank p &lt; n , then A ′ A has an inverse, but ( A ′ A ) -1 is not equal to A -1 ( A ′ ) -1 because A is rectangular and does not have an inverse.

If a matrix is nonsingular, it can be canceled from both sides of an equation, provided it appears on the left (or right) on both sides. For example, if B is nonsingular, then

<!-- formula-not-decoded -->

since we can multiply on the right by B -1 to obtain

<!-- formula-not-decoded -->

Otherwise, if A , B , and C are rectangular or square and singular, it is easy to construct AB = CB , with A /negationslash= C , as illustrated near the end of Section 2.5.

The inverse of the transpose of a nonsingular matrix is given by the transpose of the inverse:

<!-- formula-not-decoded -->

If the symmetric nonsingular matrix A is partitioned in the form

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

then the inverse is given by

<!-- formula-not-decoded -->

where b = a 22 -a ′ 12 A -1 11 a 12. A nonsingular matrix of the form B + cc ′ , where B is nonsingular, has as its inverse

<!-- formula-not-decoded -->

## 2.7 POSITIVE DEFINITE MATRICES

The symmetric matrix A is said to be positive definite if x ′ Ax &gt; 0 for all possible vectors x (except x = 0 ). Similarly, A is positive semidefinite if x ′ Ax ≥ 0 for all x /negationslash= 0 . [A quadratic form x ′ Ax was defined in (2.63).] The diagonal elements aii of a positive definite matrix are positive. To see this, let x ′ = ( 0 , . . . , 0 , 1 , 0 , . . . , 0 ) with a 1 in the i th position. Then x ′ Ax = aii &gt; 0. Similarly, for a positive semidefinite matrix A , aii ≥ 0 for all i .

One way to obtain a positive definite matrix is as follows:

If A = B ′ B , where B is n × p of rank p &lt; n , then B ′ B is positive definite. (2.78)

This is easily shown:

<!-- formula-not-decoded -->

where z = Bx . Thus x ′ Ax = ∑ n i = 1 z 2 i , which is positive ( Bx cannot be 0 unless x = 0 , because B is full rank). If B is less than full rank, then by a similar argument, B ′ B is positive semidefinite.

Note that A = B ′ B is analogous to a = b 2 in real numbers, where the square of any number (including negative numbers) is positive.

In another analogy to positive real numbers, a positive definite matrix can be factored into a 'square root' in two ways. We give one method in (2.79) and the other in Section 2.11.8.

A positive definite matrix A can be factored into

<!-- formula-not-decoded -->

where T is a nonsingular upper triangular matrix. One way to obtain T is the Cholesky decomposition , which can be carried out in the following steps.

Let A = ( ai j ) and T = ( t i j ) be n × n . Then the elements of T are found as follows:

For example, let

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Then by the Cholesky method, we obtain

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

## 2.8 DETERMINANTS

The determinant of an n × n matrix A is defined as the sum of all n ! possible products of n elements such that

1. each product contains one element from every row and every column, and
2. the factors in each product are written so that the column subscripts appear in order of magnitude and each product is then preceded by a plus or minus sign according to whether the number of inversions in the row subscripts is even or odd.

An inversion occurs whenever a larger number precedes a smaller one. The symbol n ! is defined as

<!-- formula-not-decoded -->

The determinant of A is a scalar denoted by | A | or by det ( A ) . The preceding definition is not useful in evaluating determinants, except in the case of 2 × 2 or 3 × 3 matrices. For larger matrices, other methods are available for manual computation, but determinants are typically evaluated by computer. For a 2 × 2 matrix, the determinant is found by

<!-- formula-not-decoded -->

For a 3 × 3 matrix, the determinant is given by

<!-- formula-not-decoded -->

This can be found by the following scheme. The three positive terms are obtained by

<!-- image -->

and the three negative terms, by

<!-- image -->

The determinant of a diagonal matrix is the product of the diagonal elements; that is, if D = diag ( d 1 , d 2 , . . . , dn ) , then

<!-- formula-not-decoded -->

As a special case of (2.83), suppose all diagonal elements are equal, say,

<!-- formula-not-decoded -->

Then

For example, let

Then

<!-- formula-not-decoded -->

The extension of (2.84) to any square matrix A is

<!-- formula-not-decoded -->

Since the determinant is a scalar, we can carry out operations such as

<!-- formula-not-decoded -->

provided that | A | &gt; 0 for | A | 1 / 2 and that | A | /negationslash = 0 for 1 / | A | .

If the square matrix A is singular, its determinant is 0:

<!-- formula-not-decoded -->

If A is near singular , then there exists a linear combination of the columns that is close to 0 , and | A | is also close to 0. If A is nonsingular, its determinant is nonzero:

<!-- formula-not-decoded -->

If A is positive definite, its determinant is positive:

<!-- formula-not-decoded -->

If A and B are square and the same size, the determinant of the product is the product of the determinants:

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

The determinant of the transpose of a matrix is the same as the determinant of the matrix, and the determinant of the the inverse of a matrix is the reciprocal of the

determinant:

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

If a partitioned matrix has the form

<!-- formula-not-decoded -->

where A 11 and A 22 are square but not necessarily the same size, then

<!-- formula-not-decoded -->

∣ For a general partitioned matrix,

<!-- formula-not-decoded -->

where A 11 and A 22 are square and nonsingular (not necessarily the same size), the determinant is given by either of the following two expressions:

<!-- formula-not-decoded -->

Note the analogy of (2.93) and (2.94) to the case of the determinant of a 2 × 2 matrix as given by (2.81):

<!-- formula-not-decoded -->

If B is nonsingular and c is a vector, then

<!-- formula-not-decoded -->

## 2.9 TRACE

A simple function of an n × n matrix A is the trace , denoted by tr ( A ) and defined as the sum of the diagonal elements of A ; that is, tr ( A ) = ∑ n i = 1 aii . The trace is, of course, a scalar. For example, suppose

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

The trace of the sum of two square matrices is the sum of the traces of the two matrices:

<!-- formula-not-decoded -->

An important result for the product of two matrices is

<!-- formula-not-decoded -->

This result holds for any matrices A and B where AB and BA are both defined. It is not necessary that A and B be square or that AB equal BA . For example, let

<!-- formula-not-decoded -->

Then

Then

<!-- formula-not-decoded -->

From (2.52) and (2.54), we obtain

<!-- formula-not-decoded -->

where the ai j 's are elements of the n × p matrix A .

## 2.10 ORTHOGONALVECTORS AND MATRICES

Two vectors a and b of the same size are said to be orthogonal if

<!-- formula-not-decoded -->

Geometrically, orthogonal vectors are perpendicular [see (3.14) and the comments following (3.14)]. If a ′ a = 1, the vector a is said to be normalized . The vector a can always be normalized by dividing by its length, √ a ′ a . Thus

<!-- formula-not-decoded -->

A matrix C = ( c 1 , c 2 , . . . , c p ) whose columns are normalized and mutually orthogonal is called an orthogonal matrix. Since the elements of C ′ C are products of columns of C [see (2.54)], which have the properties c ′ i c i = 1 for all i and c ′ i c j = 0 for all i /negationslash= j , we have

<!-- formula-not-decoded -->

If C satisfies (2.101), it necessarily follows that

<!-- formula-not-decoded -->

from which we see that the rows of C are also normalized and mutually orthogonal. It is clear from (2.101) and (2.102) that C -1 = C ′ for an orthogonal matrix C .

We illustrate the creation of an orthogonal matrix by starting with

<!-- formula-not-decoded -->

whose columns are mutually orthogonal. To normalize the three columns, we divide by the respective lengths, √ 3, √ 6, and √ 2, to obtain

<!-- formula-not-decoded -->

Note that the rows also became normalized and mutually orthogonal so that C satisfies both (2.101) and (2.102).

Multiplication by an orthogonal matrix has the effect of rotating axes; that is, if a point x is transformed to z = Cx , where C is orthogonal, then is normalized so that c ′ c = 1.

<!-- formula-not-decoded -->

and the distance from the origin to z is the same as the distance to x .

## 2.11 EIGENVALUES AND EIGENVECTORS

## 2.11.1 Definition

For every square matrix A , a scalar λ and a nonzero vector x can be found such that

<!-- formula-not-decoded -->

In (2.104), λ is called an eigenvalue of A , and x is an eigenvector of A corresponding to λ . To find λ and x , we write (2.104) as

<!-- formula-not-decoded -->

If | A -λ I | /negationslash = 0, then ( A -λ I ) has an inverse and x = 0 is the only solution. Hence, in order to obtain nontrivial solutions, we set | A -λ I | = 0 to find values of λ that can be substituted into (2.105) to find corresponding values of x . Alternatively, (2.69) and (2.71) require that the columns of A -λ I be linearly dependent. Thus in ( A -λ I ) x = 0 , the matrix A -λ I must be singular in order to find a solution vector x that is not 0 .

The equation | A -λ I | = 0 is called the characteristic equation . If A is n × n , the characteristic equation will have n roots; that is, A will have n eigenvalues λ 1, λ 2 , . . . , λ n . The λ 's will not necessarily all be distinct or all nonzero. However, if A arises from computations on real (continuous) data and is nonsingular, the λ 's will all be distinct (with probability 1). After finding λ 1, λ 2 , . . . , λ n , the accompanying eigenvectors x 1, x 2 , . . . , x n can be found using (2.105).

If we multiply both sides of (2.105) by a scalar k and note by (2.62) that k and A -λ I commute, we obtain

<!-- formula-not-decoded -->

Thus if x is an eigenvector of A , k x is also an eigenvector, and eigenvectors are unique only up to multiplication by a scalar. Hence we can adjust the length of x , but the direction from the origin is unique; that is, the relative values of (ratios of) the components of x = ( x 1 , x 2 , . . . , xn ) ′ are unique. Typically, the eigenvector x is scaled so that x ′ x = 1.

To illustrate, we will find the eigenvalues and eigenvectors for the matrix

<!-- formula-not-decoded -->

The characteristic equation is

<!-- formula-not-decoded -->

from which λ 1 = 3 and λ 2 = 2. To find the eigenvector corresponding to λ 1 = 3, we use (2.105),

<!-- formula-not-decoded -->

As expected, either equation is redundant in the presence of the other, and there remains a single equation with two unknowns, x 1 = x 2. The solution vector can be written with an arbitrary constant,

<!-- formula-not-decoded -->

If c is set equal to 1 / √ 2 to normalize the eigenvector, we obtain

<!-- formula-not-decoded -->

Similarly, corresponding to λ 2 = 2, we have

## 2.11.2 I + A and I -A

<!-- formula-not-decoded -->

If λ is an eigenvalue of A and x is the corresponding eigenvector, then 1 + λ is an eigenvalue of I + A and 1 -λ is an eigenvalue of I -A . In either case, x is the corresponding eigenvector.

We demonstrate this for I + A :

<!-- formula-not-decoded -->

## 2.11.3 tr(A) and | A |

For any square matrix A with eigenvalues λ 1, λ 2 , . . . , λ n , we have

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Note that by the definition in Section 2.9, tr ( A ) is also equal to ∑ n i = 1 aii , but aii /negationslash= λ i .

We illustrate (2.107) and (2.108) using the matrix

<!-- formula-not-decoded -->

from the illustration in Section 2.11.1, for which λ 1 = 3 and λ 2 = 2. Using (2.107), we obtain

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

and from (2.108), we have

By definition, we obtain

<!-- formula-not-decoded -->

## 2.11.4 Positive Definite and Semidefinite Matrices

The eigenvalues and eigenvectors of positive definite and positive semidefinite matrices have the following properties:

1. The eigenvalues of a positive definite matrix are all positive.
2. The eigenvalues of a positive semidefinite matrix are positive or zero, with the number of positive eigenvalues equal to the rank of the matrix.

It is customary to list the eigenvalues of a positive definite matrix in descending order: λ 1 &gt; λ 2 &gt; · · · &gt; λ p . The eigenvectors x 1, x 2 , . . . , x n are listed in the same order; x 1 corresponds to λ 1, x 2 corresponds to λ 2, and so on.

The following result, known as the Perron-Frobenius theorem, is of interest in Chapter 12: If all elements of the positive definite matrix A are positive, then all elements of the first eigenvector are positive. (The first eigenvector is the one associated with the first eigenvalue, λ 1.)

## 2.11.5 The Product AB

If A and B are square and the same size, the eigenvalues of AB are the same as those of BA , although the eigenvectors are usually different. This result also holds if AB and BA are both square but of different sizes, as when A is n × p and B is p × n . (In this case, the nonzero eigenvalues of AB and BA will be the same.)

## 2.11.6 Symmetric Matrix

The eigenvectors of an n × n symmetric matrix A are mutually orthogonal. It follows that if the n eigenvectors of A are normalized and inserted as columns of a matrix C = ( x 1 , x 2 , . . . , x n ) , then C is orthogonal.

## 2.11.7 Spectral Decomposition

It was noted in Section 2.11.6 that if the matrix C = ( x 1 , x 2 , . . . , x n ) contains the normalized eigenvectors of an n × n symmetric matrix A , then C is orthogonal. Therefore, by (2.102), I = CC ′ , which we can multiply by A to obtain

<!-- formula-not-decoded -->

We now substitute C = ( x 1 , x 2 , . . . , x n ) :

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Since C is orthogonal and C ′ C = CC ′ = I , we can multiply (2.109) on the left by C ′ and on the right by C to obtain where

The expression A = CDC ′ in (2.109) for a symmetric matrix A in terms of its eigenvalues and eigenvectors is known as the spectral decomposition of A .

<!-- formula-not-decoded -->

Thus a symmetric matrix A can be diagonalized by an orthogonal matrix containing normalized eigenvectors of A , and by (2.110) the resulting diagonal matrix contains eigenvalues of A .

## 2.11.8 Square Root Matrix

If A is positive definite, the spectral decomposition of A in (2.109) can be modified by taking the square roots of the eigenvalues to produce a square root matrix ,

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

The square root matrix A 1 / 2 is symmetric and serves as the square root of A :

<!-- formula-not-decoded -->

## 2.11.9 Square Matrices and Inverse Matrices

Other functions of A have spectral decompositions analogous to (2.112). Two of these are the square and inverse of A . If the square matrix A has eigenvalues λ 1, λ 2 , . . . , λ n and accompanying eigenvectors x 1, x 2 , . . . , x n , then A 2 has eigenvalues λ 2 1 , λ 2 2 , . . . , λ 2 n and eigenvectors x 1, x 2 , . . . , x n . If A is nonsingular, then A -1 has eigenvalues 1 /λ 1, 1 /λ 2 , . . . , 1 /λ n and eigenvectors x 1, x 2 , . . . , x n . If A is also symmetric, then

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

where C = ( x 1 , x 2 , . . . , x n ) has as columns the normalized eigenvectors of A (and of A 2 and A -1 ), D 2 = diag (λ 2 1 , λ 2 2 , . . . , λ 2 n ) , and D -1 = diag ( 1 /λ 1 , 1 /λ 2 , . . . , 1 /λ n ) .

## 2.11.10 Singular Value Decomposition

In (2.109) in Section 2.11.7, we expressed a symmetric matrix A in terms of its eigenvalues and eigenvectors in the spectral decomposition A = CDC ′ . In a similar manner, we can express any (real) matrix A in terms of eigenvalues and eigenvectors of A ′ A and AA ′ . Let A be an n × p matrix of rank k . Then the singular value decomposition of A can be expressed as

<!-- formula-not-decoded -->

where U is n × k , D is k × k , and V is p × k . The diagonal elements of the nonsingular diagonal matrix D = diag (λ 1 , λ 2 , . . . , λ k ) are the positive square roots of where

λ 2 1 , λ 2 2 , . . . , λ 2 k , which are the nonzero eigenvalues of A ′ A or of AA ′ . The values λ 1, λ 2 , . . . , λ k are called the singular values of A . The k columns of U are the normalized eigenvectors of AA ′ corresponding to the eigenvalues λ 2 1 , λ 2 2 , . . . , λ 2 k . The k columns of V are the normalized eigenvectors of A ′ A corresponding to the eigenvalues λ 2 1 , λ 2 2 , . . . , λ 2 k . Since the columns of U and of V are (normalized) eigenvectors of symmetric matrices, they are mutually orthogonal (see Section 2.11.6), and we have U ′ U = V ′ V = I .

## PROBLEMS

## 2.1 Let

<!-- formula-not-decoded -->

- (a) Find A + B and A -B .
- (b) Find A ′ A and AA ′
- .
- 2.2 Use the matrices A and B in Problem 2.1:
- (a) Find ( A B ) and A B
- (b) Show that ( A ) A , thus illustrating (2.6).

## 2.3 Let

- ′ ′ =
+ ′ ′ + ′ and compare them, thus illustrating (2.15).

<!-- formula-not-decoded -->

- (a) Find AB and BA .
- (b) Find AB , A , and B and verify that (2.89) holds in this case.
- | | | | | |
- 2.4 Use the matrices A and B in Problem 2.3:
- (a) Find A + B and tr ( A + B )
- .
- (b) Find tr ( A ) and tr ( B ) and show that (2.96) holds for these matrices.

## 2.5 Let

<!-- formula-not-decoded -->

- (a) Find AB and BA .

<!-- formula-not-decoded -->

- (b) Compare tr ( AB ) and tr ( BA )

## 2.6 Let

- and confirm that (2.97) holds here.

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

- (a) Show that AB = O .
- (c) Show that | A | = 0.
- (b) Find a vector x such that Ax = 0 .

## 2.7 Let

<!-- formula-not-decoded -->

Find the following:

- (a) Bx

- (d) x ′ Ay

- (g) xx ′

- (b) y ′ B

- (e) x ′ x

- (h) xy ′

- (c) x ′ Ax

- (f) x ′ y

- (i) B ′ B

- 2.8 Use x , y , and A as defined in Problem 2.7:
- (a) Find x + y and x -y .
- (b) Find ( x -y ) ′ A ( x -y ) .
- 2.9 Using B and x in Problem 2.7, find Bx as a linear combination of columns of B as in (2.67) and compare with Bx found in Problem 2.7(a).

## 2.10 Let

<!-- formula-not-decoded -->

- (a) Show that ( AB ) ′ = B ′ A ′ as in (2.27).
- (c) Find | A | .
- (b) Show that AI = A and that IB = B .

## 2.11 Let

<!-- formula-not-decoded -->

- (a) Find a ′ b and ( a ′ b ) 2 .
- (b) Find bb ′ and a ′ ( bb ′ ) a .
- (c) Compare ( a ′ b ) 2 with a ′ ( bb ′ ) a and thus illustrate (2.40).

## 2.12 Let

<!-- formula-not-decoded -->

Find DA , AD , and DAD .

- 2.13 Let the matrices A and B be partitioned as follows:

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

- (a) Find AB as in (2.65) using the indicated partitioning.
- (b) Check by finding AB in the usual way, ignoring the partitioning.

## 2.14 Let

<!-- formula-not-decoded -->

Find AB and CB . Are they equal? What is the rank of A , B , and C ? 2.15 Let

<!-- formula-not-decoded -->

- (a) Find tr ( A ) and tr ( B ) .
- (b) Find A + B and tr ( A + B ) . Is tr ( A + B ) = tr ( A ) + tr ( B ) ?
- (d) Find AB and AB . Is AB A B ?
- (c) Find | A | and | B | .

## 2.16 Let

- (a) Show that | A | &gt; 0.
- | | | | = | || |

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

- (b) Using the Cholesky decomposition in Section 2.7, find an upper triangular matrix T such that A = T ′ T .

<!-- formula-not-decoded -->

## 2.17 Let

- (a) Show that | A | &gt; 0.

<!-- formula-not-decoded -->

- (b) Using the Cholesky decomposition in Section 2.7, find an upper triangular matrix T such that A = T ′ T .
- 2.18 The columns of the following matrix are mutually orthogonal:

<!-- formula-not-decoded -->

- (a) Normalize the columns of A by dividing each column by its length; denote the resulting matrix by C .
- (b) Show that C is an orthogonal matrix, that is, C ′ C = CC ′ = I .

## 2.19 Let

- (a) Find the eigenvalues and associated normalized eigenvectors.

## 2.20 Let

<!-- formula-not-decoded -->

- (b) Find tr ( A ) and | A | and show that tr ( A ) = ∑ 3 i = 1 λ i and | A | = ∏ 3 i = 1 λ i .

<!-- formula-not-decoded -->

- (a) The eigenvalues of A are 1, 4, -2. Find the normalized eigenvectors and use them as columns in an orthogonal matrix C .
- (b) Show that C ′ AC = D as in (2.111), where D is diagonal with the eigenvalues of A on the diagonal.
- (c) Show that A = CDC ′ as in (2.109).
- 2.21 For the positive definite matrix

<!-- formula-not-decoded -->

calculate the eigenvalues and eigenvectors and find the square root matrix A 1 / 2 as in (2.112). Check by showing that ( A 1 / 2 ) 2 = A .

## 2.22 Let

as in (2.49).

<!-- formula-not-decoded -->

- (a) Find the spectral decomposition of A as in (2.109).
- (b) Find the spectral decomposition of A 2 and show that the diagonal matrix of eigenvalues is equal to the square of the matrix D found in part (a), thus illustrating (2.115).
- (c) Find the spectral decomposition of A -1 and show that the diagonal matrix of eigenvalues is equal to the inverse of the matrix D found in part (a), thus illustrating (2.116).
- 2.23 Find the singular value decomposition of A as in (2.117), where

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

- 2.24 If j is a vector of 1's, as defined in (2.11), show that the following hold:
- (b) j ′ A is a row vector whose elements are the column sums of A as in (2.38).
- (c) Aj is a column vector whose elements are the row sums of A
- (a) j ′ a = a ′ j = ∑ i ai as in (2.37).
- as in (2.38).
- 2.25 Verify (2.41); that is, show that ( x y ) ( x y ) x x 2 x y y y .
- 2.26 Show that A ′ A is symmetric, where A is n × p .
- -′ -= ′ -′ + ′
- 2.27 If a and x 1, x 2 , . . . , x n are all p × 1 and A is p × p , show that (2.42)-(2.45) hold:
- (a) ∑ n i = 1 a ′ x i = a ′ ∑ n i = 1 x i .
- ∑ ∑ 2.28 Assume that A = ( a ′ 1 a ′ 2 ) is 2 × p , x is p × 1, and S is p × p . (a) Show that
- (b) ∑ n i = 1 Ax i = A ∑ n i = 1 x i . (c) n i = 1 ( a ′ x i ) 2 = a ′ ( n i = 1 x i x ′ i ) a .
- ∑ ∑ (d) n i = 1 Ax i ( Ax i ) ′ = A ( n i = 1 x i x ′ i ) A ′ .

<!-- formula-not-decoded -->

- (b) Show that

as in (2.50).

- 2.29 (a) If the rows of A are denoted by a ′ i , show that A ′ A = ∑ n i = 1 a i a ′ i as in (2.51).
- 2.30 Show that ( A ′ ) -1 = ( A -1 ) ′ as in (2.75).
- (b) If the columns of A are denoted by a ( j ) , show that AA ′ = ∑ p j = 1 a ( j ) a ′ ( j ) as in (2.53).
- 2.31 Show that the inverse of the partitioned matrix given in (2.76) is correct by multiplying by

<!-- formula-not-decoded -->

to obtain an identity.

- 2.32 Show that the inverse of B + cc ′ given in (2.77) is correct by multiplying by B + cc ′ to obtain an identity.
- 2.33 Show that c A A
- 2.34 Show that A -1 1 A
- | | = c n | | as in (2.85).
- 2.35 If B is nonsingular and c is a vector, show that | B + cc ′ | = | B | ( 1 + c ′ B -1 c ) as in (2.95).
- | | = / | | as in (2.91).
- ′ ′ as in (2.98).
- 2.37 Show that CC I in (2.102) follows from C C I in (2.101).
- 2.36 Show that tr ( A A ) = tr ( AA ) = ∑ i j a 2 i j
- ′ = ′ =
- 2.38 Show that the eigenvalues of AB are the same as those of BA , as noted in Section 2.11.5.
- 2.39 If A 1 / 2 is the square root matrix defined in (2.112), show that
- (a) ( A 1 / 2 ) 2 A as in (2.114),
- (b) | A 1 / 2 | 2 = | A | ,
- =
- (c) | A 1 / 2 | = | A | 1 / 2 .

<!-- formula-not-decoded -->

## C H A P T E R 3

## Characterizing and Displaying Multivariate Data

We review some univariate and bivariate procedures in Sections 3.1, 3.2, and 3.3 and then extend them to vectors of higher dimension in the remainder of the chapter.

## 3.1 MEANANDVARIANCE OF A UNIVARIATE RANDOM VARIABLE

Informally, a random variable may be defined as a variable whose value depends on the outcome of a chance experiment. Generally, we will consider only continuous random variables. Some types of multivariate data are only approximations to this ideal, such as test scores or a seven-point semantic differential (Likert) scale consisting of ordered responses ranging from strongly disagree to strongly agree. Special techniques have been developed for such data, but in many cases, the usual methods designed for continuous data work almost as well.

The density function f ( y ) indicates the relative frequency of occurrence of the random variable y . (We do not use Y to denote the random variable for reasons given at the beginning of Section 3.5.) Thus, if f ( y 1 ) &gt; f ( y 2 ) , then points in the neighborhood of y 1 are more likely to occur than points in the neighborhood of y 2.

The population mean of a random variable y is defined (informally) as the mean of all possible values of y and is denoted by µ . The mean is also referred to as the expected value of y , or E ( y ) . If the density f ( y ) is known, the mean can sometimes be found using methods of calculus, but we will not use these techniques in this text.

If f ( y ) is unknown, the population mean µ will ordinarily remain unknown unless it has been established from extensive past experience with a stable population. If a large random sample from the population represented by f ( y ) is available, it is highly probable that the mean of the sample is close to µ .

The sample mean of a random sample of n observations y 1, y 2 , . . . , yn is given by the ordinary arithmetic average

<!-- formula-not-decoded -->

Generally, y will never be equal to µ ; by this we mean that the probability is zero that a sample will ever arise in which y is exactly equal to µ . However, y is considered a good estimator for µ because E ( y ) = µ and var ( y ) = σ 2 / n , where σ 2 is the variance of y . In other words, y is an unbiased estimator of µ and has a smaller variance than a single observation y . The variance σ 2 is defined shortly. The notation E ( y ) indicates the mean of all possible values of y ; that is, conceptually, every possible sample is obtained from the population, the mean of each is found, and the average of all these sample means is calculated.

If every y in the population is multiplied by a constant a , the expected value is also multiplied by a :

<!-- formula-not-decoded -->

The sample mean has a similar property. If zi = ayi for i = 1, 2 , . . . , n , then

<!-- formula-not-decoded -->

The variance of the population is defined as var ( y ) = σ 2 = E ( y -µ) 2 . This is the average squared deviation from the mean and is thus an indication of the extent to which the values of y are spread or scattered. It can be shown that σ 2 = E ( y 2 ) -µ 2 .

The sample variance is defined as

<!-- formula-not-decoded -->

which can be shown to be equal to

<!-- formula-not-decoded -->

The sample variance s 2 is generally never equal to the population variance σ 2 (the probability of such an occurrence is zero), but it is an unbiased estimator for σ 2 ; that is, E ( s 2 ) = σ 2 . Again the notation E ( s 2 ) indicates the mean of all possible sample variances. The square root of either the population variance or sample variance is called the standard deviation .

If each y is multiplied by a constant a , the population variance is multiplied by a 2 , that is, var ( ay ) = a 2 σ 2 . Similarly, if zi = ayi , i = 1, 2 , . . . , n , then the sample variance of z is given by

<!-- formula-not-decoded -->

## 3.2 COVARIANCE AND CORRELATION OF BIVARIATE RANDOMVARIABLES

## 3.2.1 Covariance

If two variables x and y are measured on each research unit (object or subject), we have a bivariate random variable ( x , y ) . Often x and y will tend to covary; if one is above its mean, the other is more likely to be above its mean, and vice versa. For example, height and weight were observed for a sample of 20 college-age males. The data are given in Table 3.1.

The values of height x and weight y from Table 3.1 are both plotted in the vertical direction in Figure 3.1. The tendency for x and y to stay on the same side of the mean

Table 3.1. Height and Weight for a Sample of 20 College-age Males

|   Person |   Height x |   Weight y |   Person |   Height x |   Weight y |
|----------|------------|------------|----------|------------|------------|
|        1 |         69 |        153 |       11 |         72 |        140 |
|        2 |         74 |        175 |       12 |         79 |        265 |
|        3 |         68 |        155 |       13 |         74 |        185 |
|        4 |         70 |        135 |       14 |         67 |        112 |
|        5 |         72 |        172 |       15 |         66 |        140 |
|        6 |         67 |        150 |       16 |         71 |        150 |
|        7 |         66 |        115 |       17 |         74 |        165 |
|        8 |         70 |        137 |       18 |         75 |        185 |
|        9 |         76 |        200 |       19 |         75 |        210 |
|       10 |         68 |        130 |       20 |         76 |        220 |

Figure 3.1. Two variables with a tendency to covary.

<!-- image -->

is clear in Figure 3.1. This illustrates positive covariance. With negative covariance the points would tend to deviate simultaneously to opposite sides of the mean.

The population covariance is defined as cov ( x , y ) = σ xy = E [ ( x -µ x )( y -µ y ) ] , where µ x and µ y are the means of x and y , respectively. Thus if x and y are usually both above their means or both below their means, the product ( x -µ x )( y -µ y ) will typically be positive, and the average value of the product will be positive. Conversely, if x and y tend to fall on opposite sides of their respective means, the product will usually be negative and the average product will be negative. It can be shown that σ xy = E ( xy ) -µ x µ y .

If the two random variables x and y in a bivariate random variable are added or multiplied, a new random variable is obtained. The mean of x + y or of xy is as follows:

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Formally, x and y are independent if their joint density factors into the product of their individual densities: f ( x , y ) = g ( x ) h ( y ) . Informally, x and y are independent if the random behavior of either of the variables is not affected by the behavior of the other. Note that (3.7) is true whether or not x and y are independent, but (3.8) holds only for x and y independently distributed.

The notion of independence of x and y is more general than that of zero covariance. The covariance σ xy measures linear relationship only, whereas if two random variables are independent, they are not related either linearly or nonlinearly. Independence implies σ xy = 0, but σ xy = 0 does not imply independence. It is easy to show that if x and y are independent, then σ xy = 0:

<!-- formula-not-decoded -->

One way to demonstrate that the converse is not true is to construct examples of bivariate x and y that have zero covariance and yet are related in a nonlinear way (the relationship will have zero slope). This is illustrated in Figure 3.2.

If x and y have a bivariate normal distribution (see Chapter 4), then zero covariance implies independence. This is because (1) the covariance measures only linear relationships and (2) in the bivariate normal case, the mean of y given x (or x given y ) is a straight line.

The sample covariance is defined as

<!-- formula-not-decoded -->

Figure 3.2. A sample from a population where x and y have zero covariance and yet are dependent.

<!-- image -->

It can be shown that

<!-- formula-not-decoded -->

Note that sxy is essentially never equal to σ xy (for continuous data); that is, the probability is zero that sxy will equal σ xy . It is true, however, that sxy is an unbiased estimator for σ xy , that is, E ( sxy ) = σ xy .

Since sxy /negationslash= σ xy in any given sample, this is also true when σ xy = 0. Thus when the population covariance is zero, no random sample from the population will have zero covariance. The only way a sample from a continuous bivariate distribution will have zero covariance is for the experimenter to choose the values of x and y so that sxy = 0. (Such a sample would not be a random sample.) One way to achieve this is to place the values in the form of a grid. This is illustrated in Figure 3.3.

The sample covariance measures only linear relationships. If the points in a bivariate sample follow a curved trend, as, for example, in Figure 3.2, the sample covariance will not measure the strength of the relationship. To see that sxy measures only linear relationships, note that the slope of a simple linear regression line is

<!-- formula-not-decoded -->

Thus sxy is proportional to the slope, which shows only the linear relationship between y and x .

Variables with zero sample covariance can be said to be orthogonal . By (2.99), two sets of numbers a 1, a 2 , . . . , an and b 1, b 2 , . . . , bn are orthogonal if ∑ n i = 1 ai bi =

Figure 3.3. A sample of ( x , y ) values with zero covariance.

<!-- image -->

0. This is true for the centered variables xi -x and yi -y when the sample covariance is zero, that is, ∑ n i = 1 ( xi -x )( yi -y ) = 0.

Example 3.2.1. To obtain the sample covariance for the height and weight data in Table 3.1, we first calculate x , y , and ∑ i xi yi , where x is height and y is weight:

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Now, by (3.10), we have

<!-- formula-not-decoded -->

By itself, the sample covariance 128.88 is not very meaningful. We are not sure if this represents a small, moderate, or large amount of relationship between y and x . A method of standardizing the covariance is given in the next section.

## 3.2.2 Correlation

Since the covariance depends on the scale of measurement of x and y , it is difficult to compare covariances between different pairs of variables. For example, if we change a measurement from inches to centimeters, the covariance will change. To find a measure of linear relationship that is invariant to changes of scale, we can standardize the covariance by dividing by the standard deviations of the two variables. This standardized covariance is called a correlation . The population correlation of two random variables x and y is

<!-- formula-not-decoded -->

and the sample correlation is

<!-- formula-not-decoded -->

The sample correlation rxy is related to the cosine of the angle between two vectors. Let θ be the angle between vectors a and b in Figure 3.4. The vector from the terminal point of a to the terminal point of b can be represented as c = b -a . Then the law of cosines can be stated in vector form as

Either of these correlations will range between -1 and 1.

<!-- formula-not-decoded -->

Figure 3.4. Vectors a and b in 3-space.

<!-- image -->

<!-- formula-not-decoded -->

Since cos ( 90 ◦ ) = 0, we see from (3.14) that a ′ b = 0 when θ = 90 ◦ . Thus a and b are perpendicular when a ′ b = 0. By (2.99), two vectors a and b , such that a ′ b = 0, are also said to be orthogonal . Hence orthogonal vectors are perpendicular in a geometric sense.

To express the correlation in the form given in (3.14), let the n observation vectors ( x 1 , y 1 ) , ( x 2 , y 2 ), . . . , ( xn , yn ) in two dimensions be represented as two vectors x ′ = ( x 1 , x 2 , . . . , xn ) and y ′ = ( y 1 , y 2 , . . . , yn ) in n dimensions, and let x and y be centered as x -x j and y -y j . Then the cosine of the angle θ between them [see (3.14)] is equal to the sample correlation between x and y :

<!-- formula-not-decoded -->

Thus if the angle θ between the two centered vectors x -x j and y -y j is small so that cos θ is near 1, rxy will be close to 1. If the two vectors are perpendicular, cos θ and rxy will be zero. If the two vectors have nearly opposite directions, rxy will be close to -1.

Example 3.2.2. To obtain the correlation for the height and weight data of Table 3.1, we first calculate the sample variance of x :

<!-- formula-not-decoded -->

Then sx = √ 14 . 576 = 3 . 8179 and, similarly, sy = 37 . 964. By (3.13), we have

<!-- formula-not-decoded -->

## 3.3 SCATTER PLOTS OF BIVARIATE SAMPLES

Figures 3.2 and 3.3 are examples of scatter plots of bivariate samples. In Figure 3.1, the two variables x and y were plotted separately for the data in Table 3.1. Figure 3.5 shows a bivariate scatter plot of the same data.

Figure 3.5. Bivariate scatter plot of the data in Figure 3.1.

<!-- image -->

If the origin is shifted to ( x , y ) , as indicated by the dashed lines, then the first and third quadrants contain most of the points. Scatter plots for correlated data typically show a substantial positive or negative slope.

A hypothetical sample of the uncorrelated variables height and IQ is shown in Figure 3.6. We could change the shape of the swarm of points by altering the scale on either axis. But because of the independence assumed for these variables, each quadrant is likely to have as many points as any other quadrant. A tall person is as likely to have a high IQ as a low IQ. A person of low IQ is as likely to be short as to be tall.

Figure 3.6. A sample of data from a population where x and y are uncorrelated.

<!-- image -->

## 3.4 GRAPHICAL DISPLAYS FOR MULTIVARIATE SAMPLES

It is a relatively simple procedure to plot bivariate samples as in Section 3.3. The position of a point shows at once the value of both variables. However, for three or more variables it is a challenge to show graphically the values of all the variables in an observation vector y . On a two-dimensional plot, the value of a third variable could be indicated by color or intensity or size of the plotted point. Four dimensions might be represented by starting with a two-dimensional scatter plot and adding two additional dimensions as line segments at right angles, as in Figure 3.7. The 'corner point' represents y 1 and y 2, whereas y 3 and y 4 are given by the lengths of the two line segments.

We will now describe various methods proposed for representing p dimensions in a plot of an observation vector, where p &gt; 2.

- Profiles represent each point by p vertical bars, with the heights of the bars depicting the values of the variables. Sometimes the profile is outlined by a polygonal line rather than bars.
- Stars portray the value of each (normalized) variable as a point along a ray from the center to the outside of a circle. The points on the rays are usually joined to form a polygon.
- Glyphs (Anderson 1960) are circles of fixed size with rays whose lengths represent the values of the variables. Anderson suggested using only three lengths of rays, thus rounding the variable values to three levels.
- Faces (Chernoff 1973) depict each variable as a feature on a face, such as length of nose, size of eyes, shape of eyes, and so on. Flury and Riedwyl (1981) suggested using asymmetric faces, thus increasing the number of representable variables.

Figure 3.7. Four-dimensional plot.

<!-- image -->

Boxes (Hartigan 1975) show each variable as the length of a dimension of a box. For more than three variables, the dimensions are partitioned into segments.

Among these five methods, Chambers and Kleiner (1982) prefer the star plots because they 'combine a reasonably distinctive appearance with computational simplicity and ease of interpretation.' Commenting on the other methods, they state, 'Profiles are not so easy to compare as a general shape. Faces are memorable, but they are more complex to draw, and one must be careful in assigning variables to parameters and in choosing parameter ranges. Faces to some extent disguise the data in the sense that individual data values may not be directly comparable from the plot.'

Table 3.2. Percentage of Republican Votes in Residential Elections in Six Southern States for Selected Years

| State          |   1932 |   1936 |   1940 |   1960 |   1964 |   1968 |
|----------------|--------|--------|--------|--------|--------|--------|
| Missouri       |     35 |     38 |     48 |     50 |     36 |     45 |
| Maryland       |     36 |     37 |     41 |     46 |     35 |     42 |
| Kentucky       |     40 |     40 |     42 |     54 |     36 |     44 |
| Louisiana      |      7 |     11 |     14 |     29 |     57 |     23 |
| Mississippi    |      4 |      3 |      4 |     25 |     87 |     14 |
| South Carolina |      2 |      1 |      4 |     49 |     59 |     39 |

Example 3.4. The data in Table 3.2 are from Kleiner and Hartigan (1981). For these data, the preceding five graphical devices are illustrated in Figure 3.8. The relative magnitudes of the variables can be compared more readily using stars or profiles than faces.

## 3.5 MEANVECTORS

It is a common practice in many texts to use an uppercase letter for a variable name and the corresponding lowercase letter for a particular value or observed value of the random variable, for example, P ( Y &gt; y ) . This notation is convenient in some univariate contexts, but it is often confusing in multivariate analysis, where we use uppercase letters for matrices. In the belief that it is easier to distinguish between a random vector and an observed value than between a vector and a matrix, throughout this text we follow the notation established in Chapter 2. Uppercase boldface letters are used for matrices of random variables or constants, lowercase boldface letters represent vectors of random variables or constants, and univariate random variables or constants are usually represented by lowercase nonbolded letters.

Let y represent a random vector of p variables measured on a sampling unit (subject or object). If there are n individuals in the sample, the n observation vectors are

Figure 3.8. Profiles, stars, glyphs, faces, and boxes of percentage of Republican votes in six presidential elections in six southern states. The radius of the circles in the stars is 50%. Assignments of variables to facial features are 1932, shape of face; 1936, length of nose; 1940, curvature of mouth; 1960, width of mouth; 1964, slant of eyes; and 1968, length of eyebrows. (From the Journal of the American Statistical Association , 1981, p. 262.)

<!-- image -->

denoted by y 1, y 2 , . . . , y n , where

<!-- formula-not-decoded -->

The sample mean vector y can be found either as the average of the n observation vectors or by calculating the average of each of the p variables separately:

<!-- formula-not-decoded -->

where, for example, y 2 = ∑ n i = 1 yi 2 / n . Thus y 1 is the mean of the n observations on the first variable, y 2 is the mean of the second variable, and so on.

All n observation vectors y 1, y 2 , . . . , y n can be transposed to row vectors and listed in the data matrix Y as follows:

<!-- formula-not-decoded -->

Since n is usually greater than p , the data can be more conveniently tabulated by entering the observation vectors as rows rather than columns. Note that the first subscript i corresponds to units (subjects or objects) and the second subscript j refers to variables. This convention will be followed whenever possible.

In addition to the two ways of calculating y given in (3.16), we can obtain y from Y . We sum the n entries in each column of Y and divide by n , which gives y ′ . This can be indicated in matrix notation using (2.38),

<!-- formula-not-decoded -->

where j ′ is a vector of 1's, as defined in (2.11). For example, the second element of j ′ Y is

<!-- formula-not-decoded -->

We can transpose (3.18) to obtain

<!-- formula-not-decoded -->

We now turn to populations. The mean of y over all possible values in the population is called the population mean vector or expected value of y . It is defined as a vector of expected values of each variable,

<!-- formula-not-decoded -->

where µ j is the population mean of the j th variable.

It can be shown that the expected value of each y j in y is µ j , that is, E ( y j ) = µ j . Thus the expected value of y (over all possible samples) is

<!-- formula-not-decoded -->

Therefore, y is an unbiased estimator of 𝛍 . We emphasize again that y is never equal to 𝛍 .

Example 3.5. Table 3.3 gives partial data from Kramer and Jensen (1969a). Three variables were measured (in milliequivalents per 100 g) at 10 different locations in the South. The variables are

y 1 = available soil calcium,

y 2 = exchangeable soil calcium,

y 3 = turnip green calcium.

To find the mean vector y , we simply calculate the average of each column and obtain

<!-- formula-not-decoded -->

Table 3.3. Calcium in Soil and Turnip Greens

|   Location Number |   y 1 |   y 2 |   y 3 |
|-------------------|-------|-------|-------|
|                 1 |    35 |   3.5 |  2.8  |
|                 2 |    35 |   4.9 |  2.7  |
|                 3 |    40 |  30   |  4.38 |
|                 4 |    10 |   2.8 |  3.21 |
|                 5 |     6 |   2.7 |  2.73 |
|                 6 |    20 |   2.8 |  2.81 |
|                 7 |    35 |   4.6 |  2.88 |
|                 8 |    35 |  10.9 |  2.9  |
|                 9 |    35 |   8   |  3.28 |
|                10 |    30 |   1.6 |  3.2  |

## 3.6 COVARIANCE MATRICES

The sample covariance matrix S = ( s jk ) is the matrix of sample variances and covariances of the p variables:

<!-- formula-not-decoded -->

In S the sample variances of the p variables are on the diagonal, and all possible pairwise sample covariances appear off the diagonal. The j th row (column) contains the covariances of y j with the other p -1 variables.

We give three approaches to obtaining S . The first of these is to simply calculate the individual elements s jk . The sample variance of the j th variable, s j j = s 2 j , is calculated as in (3.4) or (3.5), using the j th column of Y :

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

where y j is the mean of the j th variable, as in (3.16). The sample covariance of the j th and k th variables, s jk , is calculated as in (3.9) or (3.10), using the j th and k th columns of Y :

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Note that in (3.23) the variance s j j is expressed as s 2 j , the square of the standard deviation s j , and that S is symmetric because s jk = skj in (3.25). Other names used for the covariance matrix are variance matrix, variance-covariance matrix , and dispersion matrix .

By way of notational clarification, we note that in the univariate case, the sample variance is denoted by s 2 . But in the multivariate case, we denote the sample covariance matrix as S , not as S 2 .

The sample covariance matrix S can also be expressed in terms of the observation vectors:

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Since ( y i -y ) ′ = ( yi 1 -y 1 , yi 2 -y 2 , . . . , yip -y p ) , the element in the ( 1 , 1 ) position of ( y i -y )( y i -y ) ′ is ( yi 1 -y 1 ) 2 , and when this is summed over i as in (3.27), the result is the numerator of s 11 in (3.23). Similarly, the ( 1 , 2 ) element of ( y i -y )( y i -y ) ′ is ( yi 1 -y 1 )( yi 2 -y 2 ) , which sums to the numerator of s 12 in (3.25). Thus (3.27) is equivalent to (3.23) and (3.25), and likewise (3.28) produces (3.24) and (3.26).

We can also obtain S directly from the data matrix Y in (3.17), which provides a third approach. The first term in the right side of (3.26), ∑ i yi j yik , is the product of the j th and k th columns of Y , whereas the second term, ny j y k , is the ( j k ) th element of n yy ′ . It was noted in (2.54) that Y ′ Y is obtained as products of columns of Y . By (3.18) and (3.19), y = Y ′ j / n and y ′ = j ′ Y / n ; and using (2.36), we have n yy ′ = Y ′ ( J / n ) Y . Thus S can be written as

<!-- formula-not-decoded -->

Expression (3.29) is a convenient representation of S , since it makes direct use of the data matrix Y . However, the matrix I -J / n is n × n and may be unwieldy in computation if n is large.

If y is a random vector taking on any possible value in a multivariate population, the population covariance matrix is defined as

<!-- formula-not-decoded -->

The diagonal elements σ j j = σ 2 j are the population variances of the y 's, and the off-diagonal elements σ j k are the population covariances of all possible pairs of y 's.

The notation 𝚺 for the covariance matrix is widely used and seems natural because 𝚺 is the uppercase version of σ . It should not be confused with the same symbol used for summation of a series. The difference should always be apparent from the context. To help further distinguish the two uses, the covariance matrix 𝚺 will differ

in typeface and in size from the summation symbol ∑ . Also, whenever they appear together, the summation symbol will have an index of summation, such as ∑ n i = 1 . The population covariance matrix in (3.30) can also be found as

<!-- formula-not-decoded -->

which is analogous to (3.27) for the sample covariance matrix. The p × p matrix ( y -𝛍 )( y -𝛍 ) ′ is a random matrix. The expected value of a random matrix is defined as the matrix of expected values of the corresponding elements. To see that (3.31) produces population variances and covariances of the p variables as in (3.30), note that

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

It can be easily shown that 𝚺 can be expressed in a form analogous to (3.28):

<!-- formula-not-decoded -->

Since E ( s jk ) = σ j k for all j , k , the sample covariance matrix S is an unbiased estimator for 𝚺 :

<!-- formula-not-decoded -->

As in the univariate case, we note that it is the average of all possible values of S that is equal to 𝚺 . Generally, S will never be equal to 𝚺 .

Example 3.6. To calculate the sample covariance matrix for the calcium data of Table 3.3 using the computational forms (3.24) and (3.26), we need the sum of squares of each column and the sum of products of each pair of columns. We illustrate the computation of s 13.

<!-- formula-not-decoded -->

From Example 3.5 we have y 1 = 28 . 1 and y 3 = 3 . 089. By (3.26), we obtain

<!-- formula-not-decoded -->

Continuing in this fashion, we obtain

<!-- formula-not-decoded -->

## 3.7 CORRELATION MATRICES

The sample correlation between the j th and k th variables is defined in (3.13) as

<!-- formula-not-decoded -->

The sample correlation matrix is analogous to the covariance matrix with correlations in place of covariances:

<!-- formula-not-decoded -->

The second row, for example, contains the correlation of y 2 with each of the y 's (including the correlation of y 2 with itself, which is 1). Of course, the matrix R is symmetric, since r jk = rkj .

The correlation matrix can be obtained from the covariance matrix, and vice versa. Define

<!-- formula-not-decoded -->

Then by (2.57)

where as in (3.12).

Example 3.7. In Example 3.6, we obtained the sample covariance matrix S for the calcium data in Table 3.3. To obtain the sample correlation matrix for the same data, we can calculate the individual elements using (3.34) or use the direct matrix operation in (3.37). The diagonal matrix D s can be found by taking the square roots of the diagonal elements of S ,

<!-- formula-not-decoded -->

(note that we have used the unrounded version of S for computation). Then by (3.37),

<!-- formula-not-decoded -->

Note that . 865 &gt; . 493 &gt; . 327, which is a different order than that of the covariances in S in Example 3.6. Thus we cannot compare covariances, even within the same matrix S .

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

The population correlation matrix analogous to (3.35) is defined as

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

## 3.8 MEANVECTORS AND COVARIANCE MATRICES FOR SUBSETS OF VARIABLES

## 3.8.1 Two Subsets

Sometimes a researcher is interested in two different kinds of variables, both measured on the same sampling unit. This corresponds to type 2 data in Section 1.4. For example, several classroom behaviors are observed for students, and during the same time period (the basic experimental unit) several teacher behaviors are also observed. The researcher wishes to study the relationships between the pupil variables and the teacher variables.

Wewill denote the two subvectors by y and x , with p variables in y and q variables in x . Thus each observation vector in a sample is partitioned as

<!-- formula-not-decoded -->

Hence there are p + q variables in each of n observation vectors. In Chapter 10 we will discuss regression of the y 's on the x 's, and in Chapter 11 we will define a measure of correlation between the y 's and the x 's.

For the sample of n observation vectors, the mean vector and covariance matrix have the form

<!-- formula-not-decoded -->

where S yy is p × p , S yx is p × q , S xy is q × p , and S xx is q × q . Note that because of the symmetry of S ,

<!-- formula-not-decoded -->

Thus (3.42) could be written

<!-- formula-not-decoded -->

To illustrate (3.41) and (3.42), let p = 2 and q = 3. Then

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

The pattern in each of S yy , S yx , S xy , and S xx is clearly seen in this illustration. For example, the first row of S yx has the covariance of y 1 with each of x 1, x 2, x 3; the second row exhibits covariances of y 2 with the three x 's. On the other hand, S xy has as its first row the covariances of x 1 with y 1 and y 2, and so on. Thus S xy = S ′ yx , as noted in (3.43).

The analogous population results for a partitioned random vector are

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

where 𝚺 xy = 𝚺 ′ yx . The submatrix 𝚺 yy is a p × p covariance matrix containing the variances of y 1, y 2 , . . . , yp on the diagonal and the covariance of each y j with each yk off the diagonal. Similarly, 𝚺 xx is the q × q covariance matrix of x 1, x 2 , . . . , xq . The matrix 𝚺 yx is p × q and contains the covariance of each y j with each xk . The covariance matrix 𝚺 yx is also denoted by cov ( y , x ) , that is,

<!-- formula-not-decoded -->

Note the difference in meaning between cov ( y x ) = 𝚺 in (3.46) and cov ( y , x ) = 𝚺 yx in (3.47); cov ( y x ) involves a single vector containing p + q variables, and cov ( y , x ) involves two vectors.

If x and y are independent, then 𝚺 yx = O . This means that each y j is uncorrelated with each xk so that σ y j xk = 0 for j = 1, 2 , . . . , p ; k = 1, 2 , . . . , q .

Example 3.8.1. Reaven and Miller (1979; see also Andrews and Herzberg 1985, pp. 215-219) measured five variables in a comparison of normal patients and diabetics. In Table 3.4 we give partial data for normal patients only. The three variables of major interest were x 1 = glucose intolerance,

x 3 = insulin resistance.

x 2 = insulin response to oral glucose,

The two additional variables of minor interest were y 1 = relative weight,

y 2 = fasting plasma glucose .

The mean vector, partitioned as in (3.41), is

<!-- formula-not-decoded -->

The covariance matrix, partitioned as in the illustration following (3.44), is

<!-- formula-not-decoded -->

Notice that S yy and S xx are symmetric and that S xy is the transpose of S yx .

## 3.8.2 Three or More Subsets

In some cases, three or more subsets of variables are of interest. If the observation vector y is partitioned as

<!-- formula-not-decoded -->

Table 3.4. Relative Weight, Blood Glucose, and Insulin Levels

| Patient Number   |   y 1 | y 2   | x 1     | x 2    |   x 3 |
|------------------|-------|-------|---------|--------|-------|
| 1                |  0.81 | 80    | 356     | 124    |    55 |
| 2                |  0.95 | 97    | 289     | 117    |    76 |
| 3                |  0.94 | 105   | 319     | 143    |   105 |
| 4                |  1.04 | 90    | 356     | 199    |   108 |
| 5                |  1    | 90    | 323     | 240    |   143 |
| 6                |  0.76 | 86    | 381     | 157    |   165 |
| 7                |  0.91 | 100   | 350     | 221    |   119 |
| 8                |  1.1  | 85    | 301     | 186    |   105 |
| 9                |  0.99 | 97    | 379     | 142    |    98 |
| 10               |  0.78 | 97    | 296     | 131    |    94 |
| 11               |  0.9  | 91    | 353     | 221    |    53 |
| 12               |  0.73 | 87    | 306     | 178    |    66 |
| 13               |  0.96 | 78    | 290     | 136    |   142 |
| 14               |  0.84 | 90    | 371     | 200    |    93 |
| 15               |  0.74 | 86    | 312     | 208    |    68 |
| 16               |  0.98 | 80    | 393     | 202    |   102 |
| 17               |  1.1  | 90    | 364     | 152    |    76 |
| 18               |  0.85 | 99    | 359     | 185    |    37 |
| 19               |  0.83 | 85    | 296     | 116    |    60 |
| 20               |  0.93 | 90    | 345     | 123    |    50 |
| 21               |  0.95 | 90    | 378     | 136    |    47 |
| 22               |  0.74 | 88    | 304     | 134    |    50 |
| 23               |  0.95 | 95    | 347     | 184    |    91 |
| 24               |  0.97 | 90    | 327     | 192    |   124 |
| 25               |  0.72 | 92    | 386     | 279    |    74 |
| 26               |  1.11 | 74    | 365     | 228    |   235 |
| 27               |  1.2  | 98    | 365     | 145    |   158 |
| 28               |  1.13 | 100   | 352     | 172    |   140 |
| 29               |  1    | 86    | 325     | 179    |   145 |
| 30               |  0.78 | 98    | 321     | 222    |    99 |
| 31               |  1    | 70    | 360     | 134    |    90 |
| 32               |  1    | 99    | 336     | 143    |   105 |
| 33               |  0.71 | 75    | 352     | 169    |    32 |
| 34               |  0.76 | 90    | 353     | 263    |   165 |
| 35               |  0.89 | 85    | 373     | 174    |    78 |
| 36               |  0.88 | 99    | 376     | 134    |    80 |
| 37               |  1.17 | 100   | 367     | 182    |    54 |
| 38               |  0.85 | 78    | 335     | 241    |   175 |
| 39               |  0.97 | 106   | 396     | 128    |    80 |
| 40               |  1    | 98    | 277     | 222    |   186 |
| 41               |  1    | 102   | 378     | 165    |   117 |
| 42               |  0.89 | 90    | 360     | 282    |   160 |
| 43               |  0.98 | 94    | 291     | 94     |    71 |
|                  |  0.74 |       |         |        |    42 |
| 45 46            |  0.91 | 93 86 | 318 328 | 73 106 |    56 |

where y 1 has p 1 variables, y 2 has p 2 , . . . , y k has pk , with p = p 1 + p 2 +··· + pk , then the sample mean vector and covariance matrix are given by

<!-- formula-not-decoded -->

The p 2 × pk submatrix S 2 k , for example, contains the covariances of the variables in y 2 with the variables in y k .

<!-- formula-not-decoded -->

The corresponding population results are

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

## 3.9 LINEAR COMBINATIONS OF VARIABLES

## 3.9.1 Sample Properties

We are frequently interested in linear combinations of the variables y 1, y 2 , . . . , yp . For example, two of the types of linear functions we use in later chapters are (1) linear combinations that maximize some function and (2) linear combinations that compare variables, for example, y 1 -y 3. In this section, we investigate the means, variances, and covariances of linear combinations.

Let a 1, a 2 , . . . , ap be constants and consider the linear combination of the elements of the vector y ,

<!-- formula-not-decoded -->

where a ′ = ( a 1 , a 2 , . . . , ap ) . If the same coefficient vector a is applied to each y i in a sample, we have

<!-- formula-not-decoded -->

The sample mean of z can be found either by averaging the n values z 1 = a ′ y 1 , z 2 = a ′ y 2 , . . . , zn = a ′ y n or as a linear combination of y , the sample mean vector of y 1, y 2 , . . . , y n :

<!-- formula-not-decoded -->

The result in (3.54) is analogous to the univariate result (3.3), z = ay , where zi = ayi , i = 1, 2 , . . . , n .

Similarly, the sample variance of zi = a ′ y i , i = 1, 2 , . . . , n , can be found as the sample variance of z 1, z 2 , . . . , zn or directly from a and S , where S is the sample covariance matrix of y 1, y 2 , . . . , y n :

<!-- formula-not-decoded -->

Note that s 2 z = a ′ Sa is the multivariate analogue of the univariate result in (3.6), s 2 z = a 2 s 2 , where zi = ayi , i = 1, 2 , . . . , n , and s 2 is the variance of y 1, y 2 , . . . , yn .

If we define another linear combination w = b ′ y = b 1 y 1 + b 2 y 2 +··· + bp y p , where b ′ = ( b 1 , b 2 , . . . , bp ) is a vector of constants different from a ′ , then the sample covariance of z and w is given by

Since a variance is always nonnegative, we have s 2 z ≥ 0, and therefore a ′ Sa ≥ 0, for every a . Hence S is at least positive semidefinite (see Section 2.7) . If the variables are continuous and are not linearly related, and if n -1 &gt; p (so that S is full rank), then S is positive definite (with probability 1).

<!-- formula-not-decoded -->

The sample correlation between z and w is readily obtained as

<!-- formula-not-decoded -->

We now denote the two constant vectors a and b as a 1 and a 2 to facilitate later expansion to more than two such vectors. Let

<!-- formula-not-decoded -->

and define

By (2.50), this factors into

<!-- formula-not-decoded -->

The bivariate results in (3.59) and (3.61) can be readily extended to more than two linear combinations. (See principal components in Chapter 12, for instance, where we attempt to transform the y 's to a few dimensions that capture most of the information in the y 's.) If we have k linear transformations, they can be expressed as

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Then we can factor y from this expression by (2.49):

<!-- formula-not-decoded -->

If we evaluate the bivariate z i for each p -variate y i in the sample, we obtain z i = Ay i , i = 1, 2 , . . . , n , and the average of z over the sample can be found from y :

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

We can use (3.55) and (3.56) to construct the sample covariance matrix for z :

<!-- formula-not-decoded -->

or in matrix notation,

<!-- formula-not-decoded -->

where z is k × 1, A is k × p , and y is p × 1 (we typically have k ≤ p ). If z i = Ay i is evaluated for all y i , i = 1, 2 , . . . , n , then by (3.54) and (2.49), the sample mean vector of the z 's is

<!-- formula-not-decoded -->

By an extension of (3.60), the sample covariance matrix of the z 's becomes

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Note that by (3.63) and (3.64), we have

<!-- formula-not-decoded -->

A slightly more general linear transformation is

<!-- formula-not-decoded -->

The sample mean vector and covariance matrix of z are given by

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Example 3.9.1. Timm (1975, p. 233; 1980, p. 47) reported the results of an experiment where subjects responded to 'probe words' at five positions in a sentence. The variables are response times for the j th probe word, y j , j = 1, 2 , . . . , 5. The data are given in Table 3.5.

Table 3.5. Response Times for Five Probe Word Positions

|   Subject Number |   y 1 |   y 2 |   y 3 |   y 4 |   y 5 |
|------------------|-------|-------|-------|-------|-------|
|                1 |    51 |    36 |    50 |    35 |    42 |
|                2 |    27 |    20 |    26 |    17 |    27 |
|                3 |    37 |    22 |    41 |    37 |    30 |
|                4 |    42 |    36 |    32 |    34 |    27 |
|                5 |    27 |    18 |    33 |    14 |    29 |
|                6 |    43 |    32 |    43 |    35 |    40 |
|                7 |    41 |    22 |    36 |    25 |    38 |
|                8 |    38 |    21 |    31 |    20 |    16 |
|                9 |    36 |    23 |    27 |    25 |    28 |
|               10 |    26 |    31 |    31 |    32 |    36 |
|               11 |    29 |    20 |    25 |    26 |    25 |

These variables are commensurate (same measurement units and similar means and variances), and the researcher may wish to examine some simple linear combinations. Consider the following linear combination for illustrative purposes:

<!-- formula-not-decoded -->

If z is calculated for each of the 11 observations, we obtain z 1 = 288, z 2 = 155 , . . . , z 11 = 146 with mean z = 197 . 0 and variance s 2 z = 2084 . 0. These same results can be obtained using (3.54) and (3.55). The sample mean vector and covariance matrix for the data are

or

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Then, by (3.54),

<!-- formula-not-decoded -->

and by (3.55), s 2 z = a ′ Sa = 2084 . 0.

<!-- formula-not-decoded -->

We now define a second linear combination:

<!-- formula-not-decoded -->

The sample mean and variance of w are w = b ′ y = 44 . 45 and s 2 w = b ′ Sb = 605 . The sample covariance of z and w is, by (3.56), sz w = a ′ Sb = 40 . 2.

Using (3.57), we find the sample correlation between z and w to be

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

We now define the three linear functions

<!-- formula-not-decoded -->

which can be written in matrix form as

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

67.

The sample mean vector for z is given by (3.62) as

<!-- formula-not-decoded -->

and the sample covariance matrix of z is given by (3.64) as

<!-- formula-not-decoded -->

The covariance matrix S z can be converted to a correlation matrix by use of (3.37):

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

is obtained from the square roots of the diagonal elements of S z .

## 3.9.2 Population Properties

The sample results in Section 3.9.1 for linear combinations have population counterparts. Let z = a ′ y , where a is a vector of constants. Then the population mean of z is

<!-- formula-not-decoded -->

and the population variance is

<!-- formula-not-decoded -->

Let w = b ′ y , where b is a vector of constants different from a . The population covariance of z = a ′ y and w = b ′ y is

<!-- formula-not-decoded -->

By (3.12) the population correlation of z and w is where

<!-- formula-not-decoded -->

If Ay represents several linear combinations, the population mean vector and covariance matrix are given by

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

The more general linear transformation z = Ay + b has population mean vector and covariance matrix

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

## 3.10 MEASURES OF OVERALL VARIABILITY

The covariance matrix contains the variances of the p variables and the covariances between all pairs of variables and is thus a multifaceted picture of the overall variation in the data. Sometimes it is desirable to have a single numerical value for the overall multivariate scatter. One such measure is the generalized sample variance , defined as the determinant of the covariance matrix:

<!-- formula-not-decoded -->

The generalized sample variance has a geometric interpretation. The extension of an ellipse to more than two dimensions is called a hyperellipsoid . A p -dimensional hyperellipsoid ( y -y ) ′ S -1 ( y -y ) = a 2 , centered at y and based on S -1 to standardize the distance to the center, contains a proportion of the observations y 1, y 2 , . . . , y n in the sample (if S were replaced by 𝚺 , the value of a 2 could be determined by tables of the chi-square distribution; see property 3 in Section 4.2). The ellipsoid ( y -y ) ′ S -1 ( y -y ) = a 2 has axes proportional to the square roots of the eigenvalues of S . It can be shown that the volume of the ellipsoid is proportional to | S | 1 / 2 . If the smallest eigenvalue λ p is zero, there is no axis in that direction, and the ellipsoid lies wholly in a ( p -1 ) -dimensional subspace of p -space. Consequently, the volume in p -space is zero. This can also be seen by (2.108), | S | = λ 1 λ 2 · · · λ p . Hence, if λ p = 0, | S | = 0. A zero eigenvalue indicates a redundancy in the form of a linear relationship among the variables. (As will be seen in Section 12.7, the eigenvector corresponding to the zero eigenvalue reveals the form of the linear dependency.) One solution to the dilemma when λ p = 0 is to remove one or more variables.

Another measure of overall variability, the total sample variance , is simply the trace of S :

<!-- formula-not-decoded -->

This measure of overall variation ignores covariance structure altogether but is found useful for comparison purposes in techniques such as principal components (Chapter 12).

In general, for both | S | and tr ( S ) , relatively large values reflect a broad scatter of y 1, y 2 , . . . , y p about y , whereas lower values indicate closer concentration about y . In the case of | S | , however, as noted previously, an extremely small value of | S | or | R | may indicate either small scatter or multicollinearity , a term indicating near linear relationships in a set of variables. Multicollinearity may be due to high pairwise correlations or to a high multiple correlation between one variable and several of the other variables. For other measures of intercorrelation, see Rencher (1998, Section 1.7).

## 3.11 ESTIMATION OF MISSING VALUES

It is not uncommon to find missing measurements in an observation vector, that is, missing values for one or more variables. A small number of rows with missing entries in the data matrix Y [see (3.17)] does not constitute a serious problem; we can simply discard each row that has a missing value. However, with this procedure, a small portion of missing data, if widely distributed, would lead to a substantial loss of data. For example, in a large data set with n = 550 and p = 85, only about 1.5% of the 550 × 85 = 46 , 750 measurements were missing. However, nearly half of the observation vectors (rows of Y ) turned out to be incomplete.

The distribution of missing values in a data set is an important consideration. Randomly missing variable values scattered throughout a data matrix are less serious than a pattern of missing values that depends to some extent on the values of the missing variables.

We discuss two methods of estimating the missing values, or 'filling the holes,' in the data matrix, also called imputation . Both procedures presume that the missing values occur at random. If the occurrence or nonoccurrence of missing values is related to the values of some of the variables, then the techniques may not estimate the missing responses very well.

The first method is very simple: substitute a mean for each missing value, specifically the average of the available data in the column of the data matrix in which the unknown value lies. Replacing an observation by its mean reduces the variance and the absolute value of the covariance. Therefore, the sample covariance matrix S computed from the data matrix Y in (3.17) with means imputed for missing values is biased. However, it is positive definite.

The second technique is a regression approach. The data matrix Y is partitioned into two parts, one containing all rows with missing entries and the other comprising

all the complete rows. Suppose yi j is the only missing entry in the i th row of Y . Then using the data in the submatrix with complete rows, y j is regressed on the other variables to obtain a prediction equation ˆ y j = b 0 + b 1 y 1 +··· + bj -1 y j -1 + bj + 1 y j + 1 +··· + bp y p . Then the nonmissing entries in the i th row are entered as independent variables in the regression equation to obtain the predicted value, ˆ yi j . The regression method was first proposed by Buck (1960) and is a special case of the EM algorithm (Dempster, Laird, and Rubin 1977).

The regression method can be improved by iteration, carried out, for example, in the following way. Estimate all missing entries in the data matrix using regression. After filling in the missing entries, use the full data matrix to obtain new prediction equations. Use these prediction equations to calculate new predicted values ˆ yi j for missing entries. Use the new data matrix to obtain revised prediction equations and new predicted values ˆ yi j . Continue this process until the predicted values stabilize.

A modification may be needed if the missing entries are so pervasive that it is difficult to find data to estimate the initial regression equations. In this case, the process could be started by using means as in the first method and then beginning the iteration.

The regression approach will ordinarily yield better results than the method of inserting means. However, if the other variables are not very highly correlated with the one to be predicted, the regression technique is essentially equivalent to imputing means. The regression method underestimates the variances and covariances, though to a lesser extent than the method based on means.

Example 3.11. We illustrate the iterated regression method of estimating missing values. Consider the calcium data of Table 3.3 as reproduced here and suppose the entries in parentheses are missing:

|   Location Number |   y 1 | y 2   | y 3    |
|-------------------|-------|-------|--------|
|                 1 |    35 | (3.5) | 2.80   |
|                 2 |    35 | 4.9   | (2.70) |
|                 3 |    40 | 30.0  | 4.38   |
|                 4 |    10 | 2.8   | 3.21   |
|                 5 |     6 | 2.7   | 2.73   |
|                 6 |    20 | 2.8   | 2.81   |
|                 7 |    35 | 4.6   | 2.88   |
|                 8 |    35 | 10.9  | 2.90   |
|                 9 |    35 | 8.0   | 3.28   |
|                10 |    30 | 1.6   | 3.20   |

We first regress y 2 on y 1 and y 3 for observations 3-10 and obtain ˆ y 2 = b 0 + b 1 y 1 + b 3 y 3. When this is evaluated for the two nonmissing entries in the first row ( y 1 = 35 and y 3 = 2 . 80), we obtain ˆ y 2 = 4 . 097. Similarly, we regress y 3 on y 1 and y 2 for observations 3-10 to obtain ˆ y 3 = c 0 + c 1 y 1 + c 2 y 2. Evaluating this for the two nonmissing entries in the second row yields ˆ y 3 = 3 . 011. We now insert these

estimates for the missing values and calculate the regression equations based on all 10 observations. Using the revised equation ˆ y 2 = b 0 + b 1 y 1 + b 3 y 3, we obtain a new predicted value, ˆ y 2 = 3 . 698. Similarly, we obtain a revised regression equation for y 3 that gives a new predicted value, ˆ y 3 = 2 . 981. With these values inserted, we calculate new equations and obtain new predicted values, ˆ y 2 = 3 . 672 and ˆ y 3 = 2 . 976. At the third iteration we obtain ˆ y 2 = 3 . 679 and ˆ y 3 = 2 . 975. There is very little change in subsequent iterations. These values are closer to the actual values, y 2 = 3 . 5 and y 3 = 2 . 70, than the initial regression estimates, ˆ y 2 = 4 . 097 and ˆ y 3 = 3 . 011. They are also much better estimates than the means of the second and third columns, y 2 = 7 . 589 and y 3 = 3 . 132.

## 3.12 DISTANCE BETWEEN VECTORS

In a univariate setting, the distance between two points is simply the difference (or absolute difference) between their values. For statistical purposes, this difference may not be very informative. For example, we do not want to know how many centimeters apart two means are, but rather how many standard deviations apart they are. Thus we examine the standardized or statistical distances, such as

<!-- formula-not-decoded -->

To obtain a useful distance measure in a multivariate setting, we must consider not only the variances of the variables but also their covariances or correlations. The simple (squared) Euclidean distance between two vectors, ( y 1 -y 2 ) ′ ( y 1 -y 2 ) , is not useful in some situations because there is no adjustment for the variances or the covariances. For a statistical distance, we standardize by inserting the inverse of the covariance matrix:

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

These (squared) distances between two vectors were first proposed by Mahalanobis (1936) and are often referred to as Mahalanobis distances . If a random variable has a larger variance than another, it receives relatively less weight in a Mahalanobis distance. Similarly, two highly correlated variables do not contribute as much as two variables that are less correlated. In essence, then, the use of the inverse of the covariance matrix in a Mahalanobis distance has the effect of (1) standardizing all variables to the same variance and (2) eliminating correlations. To illustrate this,

Other examples are

we use the square root matrix defined in (2.112) to rewrite (3.81) as

<!-- formula-not-decoded -->

where z = ( 𝚺 1 / 2 ) -1 ( y -𝛍 ) = ( 𝚺 1 / 2 ) -1 y -( 𝚺 1 / 2 ) -1 𝛍 . Now, by (3.76) it can be shown that

<!-- formula-not-decoded -->

Hence the transformed variables z 1, z 2 , . . . , z p are uncorrelated, and each has variance equal to 1 / n . If the appropriate covariance matrix for the random vector were used in a Mahalanobis distance, the variances would reduce to 1. For example, if cov ( y ) = 𝚺 / n were used above in place of 𝚺 , we would obtain cov ( z ) = I .

## PROBLEMS

- 3.1 If zi = ayi for i = 1, 2 , . . . , n , show that z = ay as in (3.3).
- 3.3 For the data in Figure 3.3, show that ∑ i ( xi -x )( yi -y ) = 0. 3.4 Show that ( x -x j ) ′ ( y -y j ) = ∑ i ( xi -x )( yi -y ) , thus verifying (3.15).
- 3.2 If zi = ayi for i = 1, 2 , . . . , n , show that s 2 z = a 2 s 2 as in (3.6).
- 3.5 For p = 3 show that

<!-- formula-not-decoded -->

which illustrates (3.27).

- 3.6 Show that z = a ′ y as in (3.54), where zi = a ′ y i , i = 1, 2 , . . . , n .
- 3.8 Show that tr ( ASA ′ ) = ∑ k i = 1 a ′ i Sa i as in (3.65). 3.9 Use (3.76) to verify (3.83), cov ( z ) = I / n , where z = ( 𝚺 1 / 2 ) -1 ( y -𝛍 ) .
- 3.7 Show that s 2 z = a ′ Sa as in (3.55), where zi = a ′ y i , i = 1, 2 , . . . , n .
- 3.10 Use the calcium data in Table 3.3:
- (a) Calculate S using the data matrix Y
- as in (3.29).
- (b) Obtain R by calculating r r r
- (c) R
- Find using (3.37).
- 3.11 Use the calcium data in Table 3.3:
- (a) Find the generalized sample variance S as in (3.77).
- (b) Find the total sample variance tr ( S )
- , as in (3.78).
- 12, 13, and 23, as in (3.34) and (3.35).
- | |

- 3.12 Use the probe word data of Table 3.5:
- (a) Find the generalized sample variance | S | as in (3.77).
- (b) Find the total sample variance tr ( S ) as in (3.78).
- 3.13 For the probe word data in Table 3.5, find R using (3.37).
- 3.14 For the variables in Table 3.3, define z = 3 y 1 -y 2 + 2 y 3 = ( 3 , -1 , 2 ) y . Find z and s 2 z in two ways:
- (a) Evaluate z for each row of Table 3.3 and find z and s 2 z directly from z 1, z 2 , . . . , z 10 using (3.1) and (3.5).
- (b) Use z = a ′ y and s 2 z = a ′ Sa , as in (3.54) and (3.55).
- 3.15 For the variables in Table 3.3, define w = -2 y 1 + 3 y 2 + y 3 and define z as in Problem 3.14. Find rz w in two ways:
- (a) Evaluate z and w for each row of Table 3.3 and find rz w from the 10 pairs ( zi , w i ) , i = 1, 2 , . . . , 10, using (3.10) and (3.13).
- (b) Find rz w using (3.57).
- 3.16 For the variables in Table 3.3, find the correlation between y 1 and 1 2 ( y 2 + y 3 ) using (3.57).

Table 3.6. Ramus Bone Length at Four Ages for 20 Boys

|              | Age          | Age          | Age          | Age          |
|--------------|--------------|--------------|--------------|--------------|
|              | 8 yr         | 8 1 2 yr     | 9 yr         | 9 1 2 yr     |
| Individual 1 | ( y 1 ) 47.8 | ( y 2 ) 48.8 | ( y 3 ) 49.0 | ( y 4 ) 49.7 |
| 2            | 46.4         | 47.3         | 47.7         | 48.4         |
| 3            | 46.3         | 46.8         | 47.8         | 48.5         |
| 4            | 45.1         | 45.3         | 46.1         | 47.2         |
| 5            | 47.6         | 48.5         | 48.9         | 49.3         |
| 6            | 52.5         | 53.2         | 53.3         | 53.7         |
| 7            | 51.2         | 53.0         | 54.3         | 54.5         |
| 8            | 49.8         | 50.0         | 50.3         | 52.7         |
| 9            | 48.1         | 50.8         | 52.3         | 54.4         |
| 10           | 45.0         | 47.0         | 47.3         | 48.3         |
| 11           | 51.2         | 51.4         | 51.6         | 51.9         |
| 12           | 48.5         | 49.2         | 53.0         | 55.5         |
| 13           | 52.1         | 52.8         | 53.7         | 55.0         |
| 14           | 48.2         | 48.9         | 49.3         | 49.8         |
| 15           | 49.6         | 50.4         | 51.2         | 51.8         |
| 16           | 50.7         | 51.7         | 52.7         | 53.3         |
| 17           | 47.2         | 47.7         | 48.4         | 49.5         |
| 18           | 53.3         | 54.6         | 55.1         | 55.3         |
| 19           | 46.2         | 47.5         | 48.1         | 48.4         |
| 20           | 46.3         | 47.6         | 51.3         | 51.8         |

- 3.17 Define the following linear combinations for the variables in Table 3.3:

<!-- formula-not-decoded -->

- (a) Find z and S z using (3.62) and (3.64).
- (b) Find R z from S z using (3.37).

3.18 The data in Table 3.6 (Elston and Grizzle 1962) consist of measurements y 1, y 2, y 3, and y 4 of the ramus bone at four different ages on each of 20 boys.

- (a) Find y , S , and R
- (b) Find | S | and tr ( S ) .

Table 3.7. Measurements on the First and Second Adult Sons in a Sample of 25 Families

| First Son       | First Son        | Second Son      | Second Son       |
|-----------------|------------------|-----------------|------------------|
| Head Length y 1 | Head Breadth y 2 | Head Length x 1 | Head Breadth x 2 |
| 191             | 155              | 179             | 145              |
| 195             | 149              | 201             | 152              |
| 181             | 148              | 185             | 149              |
| 183             | 153              | 188             | 149              |
| 176             | 144              | 171             | 142              |
| 208             | 157              | 192             | 152              |
| 189             | 150              | 190             | 149              |
| 197             | 159              | 189             | 152              |
| 188             | 152              | 197             | 159              |
| 192             | 150              | 187             | 151              |
| 179             | 158              | 186             | 148              |
| 183             | 147              | 174             | 147              |
| 174             | 150              | 185             | 152              |
| 190             | 159              | 195             | 157              |
| 188             | 151              | 187             | 158              |
| 163             | 137              | 161             | 130              |
| 195             | 155              | 183             | 158              |
| 186             | 153              | 173             | 148              |
| 181             | 145              | 182             | 146              |
| 175             | 140              | 165             | 137              |
| 192             | 154              | 185             | 152              |
| 174             | 143              | 178             | 147              |
| 176             | 139              | 176             | 143              |
| 197             | 167              | 200             | 158              |
| 190             | 163              | 187             | 150              |

- 3.19 For the data in Table 3.6, define z = y 1 + 2 y 2 + y 3 -3 y 4 and w = -2 y 1 + 3 y 2 -y 3 + 2 y 4.
- (a) Find z , w , s 2 z , and s 2 w using (3.54) and (3.55).
- (b) Find sz w and rz w using (3.56) and (3.57).
- 3.20 For the data in Table 3.6 define

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Find z , S z , and R z using (3.62), (3.64), and (3.37), respectively.

- 3.21 The data in Table 3.7 consist of head measurements on first and second sons (Frets 1921). Define y 1 and y 2 as the measurements on the first son and x 1 and x 2 for the second son.
- (a) Find the mean vector for all four variables and partition it into ( y x ) as in (3.41).
- (b) Find the covariance matrix for all four variables and partition it into

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

as in (3.42).

- 3.22 Table 3.8 contains data from O'Sullivan and Mahan (1966; see also Andrews and Herzberg 1985, p. 214) with measurements of blood glucose levels on three occasions for 50 women. The y 's represent fasting glucose measurements on the three occasions; the x 's are glucose measurements 1 hour after sugar intake. Find the mean vector and covariance matrix for all six variables and partition them into ( y x ) , as in (3.41), and

as in (3.42).

Table 3.8. Blood Glucose Measurements on Three Occasions

| Fasting   | Fasting   | Fasting   | One Hour after Sugar Intake   | One Hour after Sugar Intake   | One Hour after Sugar Intake   |
|-----------|-----------|-----------|-------------------------------|-------------------------------|-------------------------------|
| y 1       | y 2       | y 3       | x 1                           | x 2                           | x 3                           |
| 60        | 69        | 62        | 97                            | 69                            | 98                            |
| 56        | 53        | 84        | 103                           | 78                            | 107                           |
| 80        | 69        | 76        | 66                            | 99                            | 130                           |
| 55        | 80        | 90        | 80                            | 85                            | 114                           |

(continued)

Table 3.8. (Continued)

| Fasting   | Fasting   | Fasting   | One Hour after Sugar Intake   | One Hour after Sugar Intake   | One Hour after Sugar Intake   |
|-----------|-----------|-----------|-------------------------------|-------------------------------|-------------------------------|
| y 1       | y 2       | y 3       | x 1                           | x 2                           | x 3                           |
| 62        | 75        | 68        | 116                           | 130                           | 91                            |
| 74        | 64        | 70        | 109                           | 101                           | 103                           |
| 64        | 71        | 66        | 77                            | 102                           | 130                           |
| 73        | 70        | 64        | 115                           | 110                           | 109                           |
| 68        | 67        | 75        | 76                            | 85                            | 119                           |
| 69        | 82        | 74        | 72                            | 133                           | 127                           |
| 60        | 67        | 61        | 130                           | 134                           | 121                           |
| 70        | 74        | 78        | 150                           | 158                           | 100                           |
| 66        | 74        | 78        | 150                           | 131                           | 142                           |
| 83        | 70        | 74        | 99                            | 98                            | 105                           |
| 68        | 66        | 90        | 119                           | 85                            | 109                           |
| 78        | 63        | 75        | 164                           | 98                            | 138                           |
| 103       | 77        | 77        | 160                           | 117                           | 121                           |
| 77        | 68        | 74        | 144                           | 71                            | 153                           |
| 66        | 77        | 68        | 77                            | 82                            | 89                            |
| 70        | 70        | 72        | 114                           | 93                            | 122                           |
| 75        | 65        | 71        | 77                            | 70                            | 109                           |
| 91        | 74        | 93        | 118                           | 115                           | 150                           |
| 66        | 75        | 73        | 170                           | 147                           | 121                           |
| 75        | 82        | 76        | 153                           | 132                           | 115                           |
| 74        | 71        | 66        | 143                           | 105                           | 100                           |
| 76        | 70        | 64        | 114                           | 113                           | 129                           |
| 74        | 90        | 86        | 73                            | 106                           | 116                           |
| 74        | 77        | 80        | 116                           | 81                            | 77                            |
| 67        | 71        | 69        | 63                            | 87                            | 70                            |
| 78        | 75        | 80        | 105                           | 132                           | 80                            |
| 64        | 66        | 71        | 83                            | 94                            | 133                           |
| 71        | 80        | 76        | 81                            | 87                            | 86                            |
| 63        | 75        | 73        | 120                           | 89                            | 59                            |
| 90        | 103       | 74        | 107                           | 109                           | 101                           |
| 60        | 76        | 61        | 99                            | 111                           | 98                            |
| 48        | 77        | 75        | 113                           | 124                           | 97                            |
| 66        | 93        | 97        | 136                           | 112                           | 122                           |
| 74        | 70        | 76        | 109                           | 88                            | 105                           |
| 60        | 74        | 71        | 72                            | 90                            | 71                            |
| 63        | 75        | 66        | 130                           | 101                           | 90                            |
| 66        | 80        | 86        | 130                           | 117                           | 144                           |
| 77        | 67        | 74        | 83                            | 92                            | 107                           |
| 70        | 67        | 100       | 150                           | 142                           | 146                           |
| 73        | 76        | 81        | 119                           | 120                           | 119                           |
| 78        | 90        | 77        | 122                           | 155                           | 149                           |
| 73        | 68        | 80        | 102                           | 90                            | 122                           |
| 65        | 60        | 70        | 119                           | 94                            | 89                            |
| 52        | 70        | 76        | 92                            | 94                            | 100                           |

Note

: Measurements are in mg/100 ml.

## C H A P T E R 4

## The Multivariate Normal Distribution

## 4.1 MULTIVARIATE NORMAL DENSITY FUNCTION

Many univariate tests and confidence intervals are based on the univariate normal distribution. Similarly, the majority of multivariate procedures have the multivariate normal distribution as their underpinning.

The following are some of the useful features of the multivariate normal distribution (see Section 4.2): (1) the distribution can be completely described using only means, variances, and covariances; (2) bivariate plots of multivariate data show linear trends; (3) if the variables are uncorrelated, they are independent; (4) linear functions of multivariate normal variables are also normal; (5) as in the univariate case, the convenient form of the density function lends itself to derivation of many properties and test statistics; and (6) even when the data are not multivariate normal, the multivariate normal may serve as a useful approximation, especially in inferences involving sample mean vectors, which are approximately multivariate normal by the central limit theorem (see Section 4.3.2).

Since the multivariate normal density is an extension of the univariate normal density and shares many of its features, we review the univariate normal density function in Section 4.1.1. We then describe the multivariate normal density in Sections 4.1.24.1.4.

## 4.1.1 Univariate Normal Density

If a random variable y , with mean µ and variance σ 2 , is normally distributed, its density is given by

<!-- formula-not-decoded -->

When y has the density (4.1), we say that y is distributed as N (µ, σ 2 ) , or simply y is N (µ, σ 2 ) . This function is represented by the familiar bell-shaped curve illustrated in Figure 4.1 for µ = 10 and σ = 2 . 5.

Figure 4.1. The normal density curve.

<!-- image -->

## 4.1.2 Multivariate Normal Density

If y has a multivariate normal distribution with mean vector 𝛍 and covariance matrix 𝚺 , the density is given by

<!-- formula-not-decoded -->

where p is the number of variables. When y has the density (4.2), we say that y is distributed as Np ( 𝛍 , 𝚺 ) , or simply y is Np ( 𝛍 , 𝚺 ) .

The term ( y -µ) 2 /σ 2 = ( y -µ)(σ 2 ) -1 ( y -µ) in the exponent of the univariate normal density (4.1) measures the squared distance from y to µ in standard deviation units. Similarly, the term ( y -𝛍 ) ′ 𝚺 -1 ( y -𝛍 ) in the exponent of the multivariate normal density (4.2) is the squared generalized distance from y to 𝛍 , or the Mahalanobis distance,

<!-- formula-not-decoded -->

The characteristics of this distance between y and 𝛍 were discussed in Section 3.12. Note that /Delta1 , the square root of (4.3), is not in standard deviation units as is ( y -µ)/σ . The distance /Delta1 increases with p , the number of variables (see Problem 4.4).

In the coefficient of the exponential function in (4.2), | 𝚺 | 1 / 2 appears as the analogue of √ σ 2 in (4.1). In the next section, we discuss the effect of | 𝚺 | on the density.

## 4.1.3 Generalized Population Variance

In Section 3.10, we referred to | S | as a generalized sample variance. Analogously, | 𝚺 | is a generalized population variance . If σ 2 is small in the univariate normal, the y values are concentrated near the mean. Similarly, a small value of | 𝚺 | in the

Figure 4.2. Bivariate normal densities.

<!-- image -->

multivariate case indicates that the y 's are concentrated close to 𝛍 in p -space or that there is multicollinearity among the variables. The term multicollinearity indicates that the variables are highly intercorrelated, in which case the effective dimensionality is less than p . (See Chapter 12 for a method of finding a reduced number of new dimensions that represent the data.) In the presence of multicollinearity, one or more eigenvalues of 𝚺 will be near zero and | 𝚺 | will be small, since | 𝚺 | is the product of the eigenvalues [see (2.108)].

Figure 4.2 shows, for the bivariate case, a comparison of a distribution with small | 𝚺 | and a distribution with larger | 𝚺 | . An alternative way to portray the concentration of points in the bivariate normal distribution is with contour plots. Figure 4.3 shows contour plots for the two distributions in Figure 4.2. Each ellipse contains a different proportion of observation vectors y . The contours in Figure 4.3 can be found by setting the density function equal to a constant and solving for y , as illustrated in Figure 4.4. The bivariate normal density surface sliced at a constant height traces an ellipse, which contains a given proportion of the observations (Rencher 1998, Section 2.1.3).

In both Figures 4.2 and 4.3, small | 𝚺 | appears on the left and large | 𝚺 | appears on the right. In Figure 4.3 a , there is a larger correlation between y 1 and y 2. In Figure 4.3 b , the variances are larger (in the natural directions). In general, for any num-

Figure 4.3. Contour plots for the distributions in Figure 4.2.

<!-- image -->

Figure 4.4. Constant density contour for bivariate normal.

<!-- image -->

ber of variables p , a decrease in intercorrelations among the variables or an increase in the variances will lead to a larger | 𝚺 | .

## 4.1.4 Diversity of Applications of the Multivariate Normal

Nearly all the inferential procedures we discuss in this book are based on the multivariate normal distribution. We acknowledge that a major motivation for the widespread use of the multivariate normal is its mathematical tractability. From the multivariate normal assumption, a host of useful procedures can be derived, and many of these are available in software packages. Practical alternatives to the multivariate normal are fewer than in the univariate case. Because it is not as simple to order (or rank) multivariate observation vectors as it is for univariate observations, there are not as many nonparametric procedures available for multivariate data.

Although real data may not often be exactly multivariate normal, the multivariate normal will frequently serve as a useful approximation to the true distribution. Tests and graphical procedures are available for assessing normality (see Sections 4.4 and 4.5). Fortunately, many of the procedures based on multivariate normality are robust to departures from normality.

## 4.2 PROPERTIES OF MULTIVARIATE NORMAL RANDOMVARIABLES

We list some of the properties of a random p × 1 vector y from a multivariate normal distribution Np ( 𝛍 , 𝚺 ) :

1. Normality of linear combinations of the variables in y :
2. (a) If a is a vector of constants, the linear function a ′ y = a 1 y 1 + a 2 y 2 +··· + ap y p is univariate normal:

<!-- formula-not-decoded -->

The mean and variance of a ′ y were given in (3.69) and (3.70) as E ( a ′ y ) = a ′ 𝛍 and var ( a ′ y ) = a ′ 𝚺 a for any random vector y . We now have the additional attribute that a ′ y has a (univariate) normal distribution if y is Np ( 𝛍 , 𝚺 ) .

- (b) If A is a constant q × p matrix of rank q , where q ≤ p , the q linear combinations in Ay have a multivariate normal distribution:

<!-- formula-not-decoded -->

Here, again, E ( Ay ) = A 𝛍 and cov ( Ay ) = A 𝚺 A ′ , in general, as given in (3.73) and (3.74). But we now have the additional feature that the q variables in Ay have a multivariate normal distribution.

## 2. Standardized variables:

A standardized vector z can be obtained in two ways:

<!-- formula-not-decoded -->

where 𝚺 = T ′ T is factored using the Cholesky procedure in Section 2.7, or

<!-- formula-not-decoded -->

where 𝚺 1 / 2 is the symmetric square root matrix of 𝚺 defined in (2.112) such that 𝚺 = 𝚺 1 / 2 𝚺 1 / 2 . In either (4.4) or (4.5), the standardized vector of random variables has all means equal to 0, all variances equal to 1, and all correlations equal to 0. In either case, it follows from property 1b that z is multivariate normal:

<!-- formula-not-decoded -->

## 3. Chi-square distribution:

A chi-square random variable with p degrees of freedom is defined as the sum of squares of p independent standard normal random variables. Thus, if z is the standardized vector defined in (4.4) or (4.5), then ∑ p j = 1 z 2 j = z ′ z has the χ 2 -distribution with p degrees of freedom, denoted as χ 2 p or χ 2 ( p ) . From either (4.4) or (4.5) we obtain z ′ z = ( y -𝛍 ) ′ 𝚺 -1 ( y -𝛍 ) . Hence,

<!-- formula-not-decoded -->

4. Normality of marginal distributions:
2. (a) Any subset of the y's in y has a multivariate normal distribution, with mean vector consisting of the corresponding subvector of 𝛍 and covariance matrix composed of the corresponding submatrix of 𝚺 . To illustrate, let y 1 = ( y 1 , y 2 , . . . , yr ) ′ denote the subvector containing the first r elements of y and y 2 = ( yr + 1 , . . . , yp ) ′ consist of the remaining p -r elements. Thus y , 𝛍 , and 𝚺 are partitioned as

<!-- formula-not-decoded -->

where y 1 and 𝛍 1 are r × 1 and 𝚺 11 is r × r . Then y 1 is multivariate normal:

<!-- formula-not-decoded -->

Here, again, E ( y 1 ) = 𝛍 1 and cov ( y 1 ) = 𝚺 11 hold for any random vector partitioned in this way. But if y is p -variate normal, then y 1 is r -variate normal.

- (b) As a special case of the preceding result, each y j in y has the univariate normal distribution:

<!-- formula-not-decoded -->

The converse of this is not true. If the density of each y j in y is normal, it does not necessarily follow that y is multivariate normal.

In the next three properties, let the observation vector be partitioned into two subvectors denoted by y and x , where y is p × 1 and x is q × 1. Or, alternatively, let x represent some additional variables to be considered along with those in y . Then, as in (3.45) and (3.46),

<!-- formula-not-decoded -->

In properties 5, 6, and 7, we assume that

<!-- formula-not-decoded -->

5. Independence:
2. (a) The subvectors y and x are independent if 𝚺 yx = O .
3. (b) Two individual variables y j and yk are independent if σ j k = 0. Note that this is not true for many nonnormal random variables, as illustrated in Section 3.2.1.

## 6. Conditional distribution:

If y and x are not independent, then 𝚺 yx /negationslash= O , and the conditional distribution of y given x , f ( y | x ) , is multivariate normal with

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Note that E ( y | x ) is a vector of linear functions of x , whereas cov ( y | x ) is a matrix that does not depend on x . The linear trend in (4.7) holds for any pair of variables. Thus to use (4.7) as a check on normality, one can examine bivariate scatter plots of all pairs of variables and look for any nonlinear trends. In (4.7), we have the justification for using the covariance or correlation to measure the relationship between two bivariate normal random variables. As noted in Section 3.2.1, the covariance and correlation are good measures of relationship only for variables with linear trends and are generally unsuitable for nonnormal random variables with a curvilinear relationship. The matrix 𝚺 yx 𝚺 -1 xx in (4.7) is called the matrix of regression coefficients because it relates E ( y | x ) to x . The sample counterpart of this matrix appears in (10.52).

## 7. Distribution of the sum of two subvectors:

If y and x are the same size (both p × 1) and independent, then

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

In the remainder of this section, we illustrate property 6 for the special case of the bivariate normal. Let

<!-- formula-not-decoded -->

have a bivariate normal distribution with

<!-- formula-not-decoded -->

By definition f ( y | x ) = g ( y , x )/ h ( x ) , where h ( x ) is the density of x and g ( y , x ) is the joint density of y and x . Hence

<!-- formula-not-decoded -->

and because the right side is a product, we seek a function of y and x that is independent of x and whose density can serve as f ( y | x ) . Since linear functions of y and x are normal by property 1a, we consider y -β x and seek the value of β so that y -β x and x are independent.

Since z = y -β x and x are normal and independent, cov ( x , z ) = 0. To find cov ( x , z ) , we express x and z as functions of u ,

<!-- formula-not-decoded -->

Now

<!-- formula-not-decoded -->

Since cov ( x , z ) = 0, we obtain β = σ yx /σ 2 x , and z = y -β x becomes

<!-- formula-not-decoded -->

By property 1a, the density of y -(σ yx /σ 2 x ) x is normal with

<!-- formula-not-decoded -->

For a given value of x , we can express y as y = β x + ( y -β x ) , where β x is a fixed quantity corresponding to the given value of x and y -β x is a random deviation. Then f ( y | x ) is normal, with

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

## 4.3 ESTIMATION IN THE MULTIVARIATE NORMAL

## 4.3.1 Maximum Likelihood Estimation

When a distribution such as the multivariate normal is assumed to hold for a population, estimates of the parameters are often found by the method of maximum likelihood . This technique is conceptually simple: The observation vectors y 1, y 2 , . . . , y n are considered to be known and values of 𝛍 and 𝚺 are sought that maximize the joint density of the y 's, called the likelihood function . For the multivariate normal, the maximum likelihood estimates of 𝛍 and 𝚺 are

<!-- formula-not-decoded -->

where W = ∑ n i = 1 ( y i -y )( y i -y ) ′ and S is the sample covariance matrix defined in (3.22) and (3.27). Since ˆ 𝚺 has divisor n instead of n -1, it is biased [see (3.33)], and we usually use S in place of ˆ 𝚺 .

We now give a justification of y as the maximum likelihood estimator of 𝛍 . Because the y i 's constitute a random sample, they are independent, and the joint density is the product of the densities of the y 's. The likelihood function is, therefore,

<!-- formula-not-decoded -->

To see that ˆ 𝛍 = y maximizes the likelihood function, we begin by adding and subtracting y in the exponent in (4.13),

<!-- formula-not-decoded -->

When this is expanded in terms of y i -y and y -𝛍 , two of the four resulting terms vanish because ∑ i ( y i -y ) = 0 , and (4.13) becomes

<!-- formula-not-decoded -->

Since 𝚺 -1 is positive definite, we have -n ( y -𝛍 ) ′ 𝚺 -1 ( y -𝛍 )/ 2 ≤ 0 and 0 &lt; e -n ( y -𝛍 ) ′ 𝚺 -1 ( y -𝛍 )/ 2 ≤ 1, with the maximum occurring when the exponent is 0. Therefore, L is maximized when ˆ 𝛍 = y .

The maximum likelihood estimator of the population correlation matrix P ρ [see (3.39)] is the sample correlation matrix, that is,

<!-- formula-not-decoded -->

Relationships among multinormal variables are linear, as can be seen in (4.7). Thus the estimators S and R serve well for the multivariate normal because they measure only linear relationships (see Sections 3.2.1 and 4.2). These estimators are not as useful for some nonnormal distributions.

## 4.3.2 Distribution of y and S

For the distribution of y = ∑ n i = 1 y i / n , we can distinguish two cases:

1. When y is based on a random sample y 1 , y 2 , . . . , y n from a multivariate normal distribution Np ( 𝛍 , 𝚺 ) , then y is Np ( 𝛍 , 𝚺 / n ) .
2. When y is based on a random sample y 1, y 2 , . . . , y n from a nonnormal multivariate population with mean vector 𝛍 and covariance matrix 𝚺 , then for large n , y is approximately Np ( 𝛍 , 𝚺 / n ) . More formally, this result is known as the multivariate central limit theorem : If y is the mean vector of a random sample y 1, y 2 , . . . , y n from a population with mean vector 𝛍 and covariance matrix 𝚺 , then as n →∞ , the distribution of √ n ( y -𝛍 ) approaches Np ( 0 , 𝚺 ) .

There are p variances in S and ( p 2 ) covariances, for a total of

<!-- formula-not-decoded -->

distinct entries. The joint distribution of these p ( p + 1 )/ 2 distinct variables in W = ( n -1 ) S = ∑ i ( y i -y )( y i -y ) ′ is the Wishart distribution, denoted by Wp ( n -1 , 𝚺 ) , where n -1 is the degrees of freedom.

The Wishart distribution is the multivariate analogue of the χ 2 -distribution, and it has similar uses. As noted in property 3 of Section 4.2, a χ 2 random variable is defined formally as the sum of squares of independent standard normal (univariate) random variables:

<!-- formula-not-decoded -->

If y is substituted for µ , then ∑ i ( yi -y ) 2 /σ 2 = ( n -1 ) s 2 /σ 2 is χ 2 ( n -1 ) . Similarly, the formal definition of a Wishart random variable is

<!-- formula-not-decoded -->

where y 1, y 2 , . . . , y n are independently distributed as Np ( 𝛍 , 𝚺 ) . When y is substituted for 𝛍 , the distribution remains Wishart with one less degree of freedom:

<!-- formula-not-decoded -->

Finally, we note that when sampling from a multivariate normal distribution, y and S are independent.

## 4.4 ASSESSING MULTIVARIATE NORMALITY

Many tests and graphical procedures have been suggested for evaluating whether a data set likely originated from a multivariate normal population. One possibility is to check each variable separately for univariate normality. Excellent reviews for both the univariate and multivariate cases have been given by Gnanadesikan (1997, pp. 178-220) and Seber (1984, pp. 141-155). We give a representative sample of univariate and multivariate methods in Sections 4.4.1 and 4.4.2, respectively.

## 4.4.1 Investigating Univariate Normality

When we have several variables, checking each for univariate normality should not be the sole approach, because (1) the variables are correlated and (2) normality of the individual variables does not guarantee joint normality. On the other hand, multivariate normality implies individual normality. Hence, if even one of the separate variables is not normal, the vector is not multivariate normal. An initial check on the individual variables may therefore be useful.

A basic graphical approach for checking normality is the Q -Q plot comparing quantiles of a sample against the population quantiles of the univariate normal. If the points are close to a straight line, there is no indication of departure from normality. Deviation from a straight line indicates nonnormality (at least for a large sample). In fact, the type of nonlinear pattern may reveal the type of departure from normality. Some possibilities are illustrated in Figure 4.5.

Quantiles are similar to the more familiar percentiles, which are expressed in terms of percent; a test score at the 90th percentile, for example, is above 90% of the test scores and below 10% of them. Quantiles are expressed in terms of fractions or proportions. Thus the 90th percentile score becomes the .9 quantile score.

The sample quantiles for the Q -Q plot are obtained as follows. First we rank the observations y 1, y 2 , . . . , yn and denote the ordered values by y ( 1 ) , y ( 2 ) , . . . , y ( n ) ;

Figure 4.5. Typical Q -Q plots for nonnormal data.

<!-- image -->

thus y ( 1 ) ≤ y ( 2 ) ≤ · · · ≤ y ( n ) . Then the point y ( i ) is the i / n sample quantile. For example, if n = 20, y ( 7 ) is the 7 20 = . 35 quantile, because .35 of the sample is less than or equal to y ( 7 ) . The fraction i / n is often changed to ( i -1 2 )/ n as a continuity correction. If n = 20, ( i -1 2 )/ n ranges from .025 to .975 and more evenly covers the interval from 0 to 1. With this convention, y ( i ) is designated as the ( i -1 2 )/ n sample quantile.

The population quantiles for the Q -Q plot are similarly defined corresponding to ( i -1 2 )/ n . If we denote these by q 1 , q 2 , . . . , qn , then qi is the value below which a

proportion ( i -1 2 )/ n of the observations in the population lie; that is, ( i -1 2 )/ n is the probability of getting an observation less than or equal to qi . Formally, qi can be found for the standard normal random variable y with distribution N ( 0 , 1 ) by solving

<!-- formula-not-decoded -->

which would require numerical integration or tables of the cumulative standard normal distribution, /Phi1( x ) . Another benefit of using ( i -1 2 )/ n instead of i / n is that n / n = 1 would make qn = ∞ .

The population need not have the same mean and variance as the sample, since changes in mean and variance merely change the slope and intercept of the plotted line in the Q -Q plot. Therefore, we use the standard normal distribution, and the qi values can easily be found from a table of cumulative standard normal probabilities. We then plot the pairs ( qi , y ( i )) and examine the resulting Q -Q plot for linearity.

Special graph paper, called normal probability paper, is available that eliminates the need to look up the qi values. We need only plot ( i -1 2 )/ n in place of qi , that is, plot the pairs [ ( i -1 2 )/ n , y ( i ) ] and look for linearity as before. As an even easier alternative, most general-purpose statistical software programs provide normal probability plots of the pairs ( qi , y ( i )) .

The Q -Q plots provide a good visual check on normality and are considered to be adequate for this purpose by many researchers. For those who desire a more objective procedure, several hypothesis tests are available. We give three of these that have good properties and are computationally tractable.

Wediscuss first a classical approach based on the following measures of skewness and kurtosis:

<!-- formula-not-decoded -->

These are sample estimates of the population skewness and kurtosis parameters √ β 1 and β 2, respectively. When the population is normal, √ β 1 = 0 and β 2 = 3. If √ β 1 &lt; 0, we have negative skewness; if √ β 1 &gt; 0, the skewness is positive. Positive skewness is illustrated in Figure 4.6. If β 2 &lt; 3, we have negative kurtosis, and if β 2 &gt; 3, there is positive kurtosis. A distribution with negative kurtosis is characterized by being flatter than the normal distribution, that is, less peaked, with heavier flanks and thinner tails. A distribution with positive kurtosis has a higher peak than the normal, with an excess of values near the mean and in the tails but with thinner flanks. Positive and negative kurtosis are illustrated in Figure 4.7.

<!-- formula-not-decoded -->

The test of normality can be carried out using the exact percentage points for √ b 1 in Table A.1 for 4 ≤ n ≤ 25, as given by Mulholland (1977). Alternatively, for n ≥ 8

Figure 4.6. A distribution with positive skewness.

<!-- image -->

the function g , as defined by

<!-- formula-not-decoded -->

is approximately N ( 0 , 1 ) , where

<!-- formula-not-decoded -->

Table A.2, from D'Agostino and Pearson (1973), gives values for δ and 1 /λ . To use b 2 as a test of normality, we can use Table A.3, from D'Agostino and Tietjen (1971), which gives simulated percentiles of b 2 for selected values of n in the range 7 ≤ n ≤ 50. Charts of percentiles of b 2 for 20 ≤ n ≤ 200 can be found in D'Agostino and Pearson (1973).

Figure 4.7. Distributions with positive and negative kurtosis compared to the normal.

<!-- image -->

Our second test for normality was given by D'Agostino (1971). The observations y 1, y 2 , . . . , yn are ordered as y ( 1 ) ≤ y ( 2 ) ≤ · · · ≤ y ( n ) , and we calculate

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

A table of percentiles for Y , given by D'Agostino (1972) for 10 ≤ n ≤ 250, is provided in Table A.4.

The final test we report is by Lin and Mudholkar (1980). The test statistic is

<!-- formula-not-decoded -->

where r is the sample correlation of the n pairs ( yi , xi ) , i = 1, 2 , . . . , n , with xi defined as

<!-- formula-not-decoded -->

If the y 's are normal, z is approximately N ( 0 , 3 / n ) . A more accurate upper 100 α percentile is given by with

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

where /Phi1 is the distribution function of the N ( 0 , 1 ) distribution; that is, /Phi1( x ) is the probability of an observation less than or equal to x , as in (4.17). The inverse function /Phi1 -1 is essentially a quantile. For example, u . 05 = -1 . 645 and u . 95 = 1 . 645.

## 4.4.2 Investigating Multivariate Normality

Checking for multivariate normality is conceptually not as straightforward as assessing univariate normality, and consequently the state of the art is not as well developed. The complexity of this issue can be illustrated in the context of a goodness-of-fit test for normality. For a goodness-of-fit test in the univariate case,

the range covered by a sample y 1, y 2 , . . . , yn is divided into several intervals, and we count how many y 's fall into each interval. These observed frequencies (counts) are compared to the expected frequencies under the assumption that the sample came from a normal distribution with the same mean and variance as the sample. If the n observations y 1, y 2 , . . . , y n are multivariate, however, the procedure is not so simple. We now have a p -dimensional region that would have to be divided into many more subregions than in the univariate case, and the expected frequencies for these subregions would be less easily obtained. With so many subregions, relatively few would contain observations.

Thus because of the inherent 'sparseness' of multivariate data, a goodness-of-fit test would be impractical. The points y 1, y 2 , . . . , y n are more distant from each other in p -space than in any one of the p individual dimensions. Unless n is very large, a multivariate sample may not provide a very complete picture of the distribution from which it was taken.

As a consequence of the sparseness of the data in p -space, the tests for multivariate normality may not be very powerful. However, some check on the distribution is often desirable. Numerous procedures have been proposed for assessing multivariate normality. We now discuss three of these.

The first procedure is based on the standardized distance from each y i to y ,

<!-- formula-not-decoded -->

Gnanadesikan and Kettenring (1972) showed that if the y i 's are multivariate normal, then

<!-- formula-not-decoded -->

has a beta distribution, which is related to the F distribution. To obtain a Q -Q plot, the values u 1, u 2 , . . . , un are ranked to give u ( 1 ) ≤ u ( 2 ) ≤ · · · ≤ u ( n ) , and we plot ( u ( i ), υ i ) , where the quantiles υ i of the beta are given by the solution to

<!-- formula-not-decoded -->

where a = 1 2 p , b = 1 2 ( n -p -1 ) ,

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Anonlinear pattern in the plot would indicate a departure from normality. The quantiles of the beta as defined in (4.29) are easily obtained in many software packages.

A formal significance test is also available for D 2 ( n ) = max i D 2 i . Table A.6 gives the upper 5% and 1% critical values for p = 2, 3, 4, 5 from Barnett and Lewis (1978).

Some writers have suggested that the distribution of D 2 i in (4.27) can be adequately approximated by a χ 2 p since ( y -𝛍 ) ′ 𝚺 -1 ( y -𝛍 ) is χ 2 p [see (4.6)]. However, in Section 5.3.2, it is shown that this approximation is very poor for even moderate values of p . Small (1978) showed that plots of D 2 i vs. χ 2 quantiles are misleading.

The second procedure involves scatter plots in two dimensions. If p is not too high, the bivariate plots of each pair of variables are often reduced in size and shown on one page, arranged to correspond to the entries in a correlation matrix. In this visual matrix, the eye readily picks out those pairs of variables that show a curved trend, outliers, or other nonnormal appearance. This plot is illustrated in Example 4.5.2 in Section 4.5.2. The procedure is based on properties 4 and 6 of Section 4.2, from which we infer that (1) each pair of variables has a bivariate normal distribution and (2) bivariate normal variables follow a straight-line trend.

A popular option in many graphical programs is the ability to dynamically rotate a plot of three variables. While the points are rotating on the screen, a threedimensional effect is created. The shape of the three-dimensional cloud of points is readily perceived, and we can detect various features of the data. The only drawbacks to this technique are that (1) it is a dynamic display and cannot be printed and (2) if p is very large, the number of subsets of three variables becomes unwieldy, although the number of pairs may still be tractable for plotting. These numbers are compared in Table 4.1, where ( p 2 ) and ( p 3 ) represent the number of subsets of sizes 2 and 3, respectively. Thus in many cases, the scatter plots for pairs of variables will continue to be used, even though three-dimensional plotting techniques are available.

The third procedure for assessing multivariate normality is a generalization of the univariate test based on the skewness and kurtosis measures √ b 1 and b 2 as given by (4.18) and (4.19). The test is due to Mardia (1970). Let y and x be independent and identically distributed with mean vector 𝛍 and covariance matrix 𝚺 . Then skewness and kurtosis for multivariate populations are defined by Mardia as

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Table 4.1. Comparison of Number of Subsets of Sizes 2 and 3

|   p |   ( p 2 ) |   ( p 3 ) |
|-----|-----------|-----------|
|   6 |        15 |        20 |
|   8 |        28 |        56 |
|  10 |        45 |       120 |
|  12 |        66 |       220 |
|  15 |       105 |       455 |

Since third-order central moments for the multivariate normal distribution are zero, β 1 , p = 0 when y is Np ( 𝛍 , 𝚺 ) . It can also be shown that if y is Np ( 𝛍 , 𝚺 ) , then

<!-- formula-not-decoded -->

To estimate β 1 , p and β 2 , p using a sample y 1, y 2 , . . . , y p , we first define

<!-- formula-not-decoded -->

where ˆ 𝚺 = ∑ n i = 1 ( y i -y )( y i -y ) ′ / n is the maximum likelihood estimator (4.12). Then estimates of β 1 , p and β 2 , p are given by

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Table A.5 (Mardia 1970, 1974) gives percentage points of b 1 , p and b 2 , p for p = 2 , 3 , 4, which can be used in testing for multivariate normality. For other values of p or when n ≥ 50, the following approximate tests are available. For b 1 , p , the statistic

<!-- formula-not-decoded -->

is approximately χ 2 with 1 6 p ( p + 1 )( p + 2 ) degrees of freedom. We reject the hypothesis of multivariate normality if z 1 ≥ χ 2 . 05 . With b 2 , p , on the other hand, we wish to reject for large values (distribution too peaked) or small values (distribution too flat). For the upper 2.5% points of b 2 , p use

<!-- formula-not-decoded -->

which is approximately N ( 0 , 1 ) . For the lower 2.5% points we have two cases: (1) when 50 ≤ n ≤ 400, use

<!-- formula-not-decoded -->

which is approximately N ( 0 , 1 ) ; (2) when n ≥ 400, use z 2 as given by (4.39).

## 4.5 OUTLIERS

The detection of outliers has been of concern to statisticians and other scientists for over a century. Some authors have claimed that the researcher can typically expect

up to 10% of the observations to have errors in measurement or recording. Occasional stray observations from a different population than the target population are also fairly common. We review some major concepts and suggested procedures for univariate outliers in Section 4.5.1 before moving to the multivariate case in Section 4.5.2. An alternative to detection of outliers is to use robust estimators of 𝛍 and 𝚺 (see Rencher 1998, Section 1.10) that are less sensitive to extreme observations than are the standard estimators y and S .

## 4.5.1 Outliers in Univariate Samples

Excellent treatments of outliers have been given by Beckman and Cook (1983), Hawkins (1980), and Barnett and Lewis (1978). We abstract a few highlights from Beckman and Cook. Many techniques have been proposed for detecting outliers in the residuals from regression or designed experiments, but we will be concerned only with simple random samples from the normal distribution.

There are two principal approaches for dealing with outliers. The first is identification , which usually involves deletion of the outlier(s) but may also provide important information about the model or the data. The second method is accommodation , in which the method of analysis or the model is modified. Robust methods, in which the influence of outliers is reduced, provide an example of modification of the analysis. An example of a correction to the model is a mixture model that combines two normals with different variances. For example, Marks and Rao (1978) accommodated a particular type of outlier by a mixture of two normal distributions.

In small or moderate-sized univariate samples, visual methods of identifying outliers are the most frequently used. Tests are also available if a less subjective approach is desired.

Two types of slippage models have been proposed to account for outliers. Under the mean slippage model, all observations have the same variance, but one or more of the observations arise from a distribution with a different (population) mean. In the variance slippage model, one or more of the observations arise from a model with larger (population) variance but the same mean. Thus in the mean slippage model, the bulk of the observations arise from N (µ, σ 2 ) , whereas the outliers originate from N (µ + θ, σ 2 ) . For the variance slippage model, the main distribution would again be N (µ, σ 2 ) , with the outliers coming from N (µ, a σ 2 ) where a &gt; 1. These models have led to the development of tests for rejection of outliers. We now briefly discuss some of these tests.

For a single outlier in a sample y 1, y 2 , . . . , yn , most tests are based on the maximum studentized residual,

<!-- formula-not-decoded -->

∣ ∣ If the largest or smallest observation is rejected, one could then examine the n -1 remaining observations for another possible outlier, and so on. This procedure is

called a consecutive test . However, if there are two or more outliers, the less extreme ones will often make it difficult to detect the most extreme one, due to inflation of both mean and variance. This effect is called masking .

Ferguson (1961) showed that the maximum studentized residual (4.41) is more powerful than most other techniques for detecting intermediate or large shifts in the mean and gave the following guidelines for small shifts:

1. For outliers with small positive shifts in the mean, tests based on sample skewness are best.
2. For outliers with small shifts in the mean in either direction, tests based on the sample kurtosis are best.
3. For outliers with small positive shifts in the variance, tests based on the sample kurtosis are best.

Because of the masking problem in consecutive tests, block tests have been proposed for simultaneous rejection of k &gt; 1 outliers. These tests work well if k is known, but in practice, k is usually not known. If the value we conjecture for k is too small, we incur the risk of failing to detect any outliers because of masking. If we set k too large, there is a high risk of rejecting more outliers than there really are, an effect known as swamping .

## 4.5.2 Outliers in Multivariate Samples

In the case of multivariate data, the problems in detecting outliers are intensified for several reasons:

1. For p &gt; 2 the data cannot be readily plotted to pinpoint the outliers.
2. Multivariate data cannot be ordered as can a univariate sample, where extremes show up readily on either end.
3. An observation vector may have a large recording error in one of its components or smaller errors in several components.
4. A multivariate outlier may reflect slippage in mean, variance, or correlation. This is illustrated in Figure 4.8. Observation 1 causes a small shift in means and variances of both y 1 and y 2 but has little effect on the correlation. Observation 2 has little effect on means and variances, but it reduces the correlation somewhat. Observation 3 has a major effect on means, variances, and correlation.

One approach to multivariate outlier identification or accommodation is to use robust methods of estimation. Such methods minimize the influence of outliers in estimation or model fitting. However, an outlier sometimes furnishes valuable information, and the specific pursuit of outliers can be very worthwhile.

We present two methods of multivariate outlier identification, both of which are related to methods of assessing multivariate normality. (A third approach based

Figure 4.8. Bivariate sample showing three types of outliers.

<!-- image -->

on principal components is given in Section 12.4.) The first method, due to Wilks (1963), is designed for detection of a single outlier. Wilks' statistic is

<!-- formula-not-decoded -->

where S is the usual sample covariance matrix and S -i is obtained from the same sample with the i th observation deleted. The statistic w can also be expressed in terms of D 2 ( n ) = max i ( y i -y ) ′ S -1 ( y i -y ) as

<!-- formula-not-decoded -->

thus basing a test for an outlier on the distances D 2 i used in Section 4.4.2 in a graphical procedure for checking multivariate normality. Table A.6 gives the upper 5% and 1% critical values for D 2 ( n ) from Barnett and Lewis (1978).

Yang and Lee (1987) provide an F -test of w as given by (4.43). Define

<!-- formula-not-decoded -->

Then the Fi 's are independently and identically distributed as Fp , n -p -1, and a test can be constructed in terms of max i Fi :

<!-- formula-not-decoded -->

Therefore, the test can be carried out using an F -table. Note that

<!-- formula-not-decoded -->

where w is given in (4.43).

The second test we discuss is designed for detection of several outliers. Schwager and Margolin (1982) showed that the locally best invariant test for mean slippage is based on Mardia's (1970) sample kurtosis b 2 , p as defined by (4.35) and (4.37). To be more specific, among all tests invariant to a class of transformations of the type z = Ay + b , where A is nonsingular (see Problem 4.8), the test using b 2 , p is most powerful for small shifts in the mean vector. This result holds if the proportion of outliers is no more than 21.13%. With some restrictions on the pattern of the outliers, the permissible fraction of outliers can go as high as 33 1 3 %. The hypothesis is H 0: no outliers are present. This hypothesis is rejected for large values of b 2 , p .

A table of critical values of b 2 , p and some approximate tests were described in Section 4.4.2 following (4.37). Thus the test doubles as a check for multivariate normality and for the presence of outliers. One advantage of this test for outliers is that we do not have to specify the number of outliers and run the attendant risk of masking or swamping. Schwager and Margolin (1982) pointed out that this feature 'increases the importance of performing an overall test that is sensitive to a broad range of outlier configurations. There is also empirical evidence that the kurtosis test performs well in situations of practical interest when compared with other inferential outlier procedures.'

Sinha (1984) extended the result of Schwager and Margolin to cover the general case of elliptically symmetric distributions. An elliptically symmetric distribution is one in which f ( y ) = | 𝚺 | -1 / 2 g [ ( y -𝛍 ) ′ 𝚺 -1 ( y -𝛍 ) ] . By varying the function g , distributions with shorter or longer tails than the normal can be obtained. The critical value of b 2 , p must be adjusted to correspond to the distribution, but rejection for large values would be a locally best invariant test.

Example 4.5.2. We use the ramus bone data set of Table 3.6 to illustrate a search for multivariate outliers, while at the same time checking for multivariate normality. An examination of each column of Table 3.6 does not reveal any apparent univariate outliers. To check for multivariate outliers, we first calculate D 2 i in (4.27) for each observation vector. The results are given in Table 4.2. We see that D 2 9 , D 2 12 , and D 2 20 seem to stand out as possible outliers. In Table A.6, the upper 5% critical value for the maximum value, D 2 ( 20 ) , is given as 11.63. In our case, the largest D 2 i is D 2 9 = 11 . 03, which does not exceed the critical value. This does not surprise us, since the test was designed to detect a single outlier, and we may have as many as three.

We compute ui and υ i in (4.28) and (4.29) and plot them in Figure 4.9. The figure shows a departure from linearity due to three values and possibly a fourth.

Table 4.2. Values of D 2 i for the Ramus Bone Data in Table 3.6

|   Observation Number |   D 2 i |   Observation Number |   D 2 i |
|----------------------|---------|----------------------|---------|
|                    1 |  0.7588 |                   11 |  2.8301 |
|                    2 |  1.298  |                   12 | 10.5718 |
|                    3 |  1.7591 |                   13 |  2.5941 |
|                    4 |  3.8539 |                   14 |  0.6594 |
|                    5 |  0.8706 |                   15 |  0.3246 |
|                    6 |  2.8106 |                   16 |  0.8321 |
|                    7 |  4.2915 |                   17 |  1.1083 |
|                    8 |  7.9897 |                   18 |  4.3633 |
|                    9 | 11.0301 |                   19 |  2.1088 |
|                   10 |  5.3519 |                   20 | 10.0931 |

We next calculate b 1 , p and b 2 , p , as given by (4.36) and (4.37):

<!-- formula-not-decoded -->

In Table A.5, the upper .01 critical value for b 1 , p is 9.9; the upper .005 critical value for b 2 , p is 27.1. Thus both b 1 , p and b 2 , p exceed their critical values, and we have significant skewness and kurtosis, apparently caused by the three observations with large values of D 2 i .

Figure 4.9. Q -Q plot of ui and υ i for the ramus bone data of Table 3.6.

<!-- image -->

Figure 4.10. Scatter plots for the ramus bone data in Table 3.6.

<!-- image -->

The bivariate scatter plots are given in Figure 4.10. Three values are clearly separate from the other observations in the plot of y 1 versus y 4. In Table 3.6, the 9th, 12th, and 20th values of y 4 are not unusual, nor are the 9th, 12th, and 20th values of y 1. However, the increase from y 1 to y 4 is exceptional in each case. If these values are not due to errors in recording the data and if this sample is representative, then we appear to have a mixture of two populations. This should be taken into account in making inferences.

## PROBLEMS

## 4.1 Consider the two covariance matrices

<!-- formula-not-decoded -->

Show that | 𝚺 2 | &gt; | 𝚺 1 | and that tr ( 𝚺 2 ) &lt; tr ( 𝚺 1 ) . Thus the generalized variance of population 2 is greater than the generalized variance of population 1, even though the total variance is less. Comment on why this is true in terms of the variances and correlations.

- 4.2 For z = ( T ′ ) -1 ( y -𝛍 ) in (4.4), show that E ( z ) = 0 and cov ( z ) = I .
- 4.3 Show that the form of the likelihood function in (4.13) follows from the previous expression.
- 4.4 For ( y -𝛍 ) 𝚺 1 ( y -𝛍 ) in (4.3) and (4.6), show that E [ ( y -𝛍 ) 𝚺 1 ( y -𝛍 ) ] = p . Assume E ( y ) 𝛍 and cov ( y ) 𝚺 . Normality is not required.
- ′ -′ -= =
- 4.5 Show that by adding and subtracting y , the exponent of (4.13) has the form given in (4.14), that is,

<!-- formula-not-decoded -->

- 4.6 Show that b 1 and b 2, as given in (4.18) and (4.19), are invariant to the transformation zi = ayi + b .
- √
- 4.7 Show that if y is Np ( 𝛍 , 𝚺 ) , then β 2 , p p ( p 2 ) as in (4.34).
- = +
- 4.8 Show that b 1 , p and b 2 , p , as given by (4.36) and (4.37), are invariant under the transformation z i = Ay i + b , where A is nonsingular. Thus b 1 , p and b 2 , p do not depend on the units of measurement.
- 4.9 Show that F ( n ) = [ ( n -p -1 )/ p ] ( 1 /w -1 ) as in (4.45).
- 4.10 Suppose y is N 3 ( 𝛍 , 𝚺 ) , where

<!-- formula-not-decoded -->

- (a) Find the distribution of z 2 y y 3 y

<!-- formula-not-decoded -->

- (b) Find the joint distribution of z y y y z y y 2 y
- = 1 -2 + 3.
- (c) Find the distribution of y
- 1 = 1 + 2 + 3 and 2 = 1 -2 + 3.
- 2.
- (d) Find the joint distribution of y y
- 1 and 3.
- (e) Find the joint distribution of y 1, y 3, and 1 ( y 1 y 2 )
- 4.11 Suppose y is N 3 ( 𝛍 , 𝚺 ) , with 𝛍 and 𝚺 given in the previous problem.
- 2 + .
- (a) Find a vector z such that z ( T ) ( y 𝛍 ) is N 3 ( 0 , I )
- (b) Find a vector z such that z = ( 𝚺 1 / 2 ) -1 ( y -𝛍 ) is N 3 ( 0 , I ) as in (4.5).
- 4.12 Suppose y is N 4 ( 𝛍 , 𝚺 ) , where
- = ′ -1 -as in (4.4).
- (c) What is the distribution of ( y -𝛍 ) ′ 𝚺 -1 ( y -𝛍 ) ?

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

.

- (a) Find the distribution of z = 4 y 1 -2 y 2 + y 3 -3 y 4.
- (c) Find the joint distribution of z 1 = 3 y 1 + y 2 -4 y 3 -y 4, z 2 = -y 1 -3 y 2 + y 3 -2 y 4, and z 3 = 2 y 1 + 2 y 2 + 4 y 3 -5 y 4.
- (b) Find the joint distribution of z 1 = y 1 + y 2 + y 3 + y 4 and z 2 = -2 y 1 + 3 y 2 + y 3 -2 y 4.
- (d) What is the distribution of y 3?
- (e) What is the joint distribution of y 2 and y 4?
- (f) Find the joint distribution of y 1, 1 2 ( y 1 + y 2 ) , 1 3 ( y 1 + y 2 + y 3 ) , and 1 4 ( y 1 + y 2 + y 3 + y 4 ) .
- 4.13 Suppose y is N 4 ( 𝛍 , 𝚺 ) with 𝛍 and 𝚺 given in the previous problem.
- (a) Find a vector z such that z = ( T ′ ) -1 ( y -𝛍 ) is N 4 ( 0 , I ) as in (4.4).
- (c) What is the distribution of ( y -𝛍 ) ′ 𝚺 -1 ( y -𝛍 ) ?
- (b) Find a vector z such that z = ( 𝚺 1 / 2 ) -1 ( y -𝛍 ) is N 4 ( 0 , I ) as in (4.5).
- 4.14 Suppose y is N 3 ( 𝛍 , 𝚺 ) , with

<!-- formula-not-decoded -->

Which of the following random variables are independent?

- (a) y 1 and y 2
- (b) y 1 and y 3
- (c) y 2 and y 3
- (d) ( y 1 , y 2 ) and y 3
- (e) ( y 1 , y 3 ) and y 2
- 4.15 Suppose y is N 4 ( 𝛍 , 𝚺 ) , with

<!-- formula-not-decoded -->

Which of the following random variables are independent?

- (a) y 1 and y 2 (f) y 3 and y 4
- (b) y 1 and y 3 (g) ( y 1 , y 2 ) and y 3
- (k) y 1 and y 2 and y 3
- (l) y 1 and y 2 and y 4
- (c) y 1 and y 4 (h) ( y 1 , y 2 ) and y 4 (m) ( y 2 , y 2 ) and ( y 3 , y 4 )
- (d) y 2 and y 3 (i) ( y 1 , y 3 ) and y 4
- (e) y 2 and y 4 (j) y 1 and ( y 2 , y 4 )
- (n) ( y 1 , y 3 ) and ( y 2 , y 4 )

- 4.16 Assume y and x are subvectors, each 2 × 1, where ( y x ) is N 4 ( 𝛍 , 𝚺 ) with

<!-- formula-not-decoded -->

- (a) Find E ( y | x ) by (4.7).
- 4.17 Suppose y and x are subvectors, such that y is 2 × 1 and x is 3 × 1, with 𝛍 and 𝚺 partitioned accordingly:
- (b) Find cov ( y | x ) by (4.8).

<!-- formula-not-decoded -->

Assume that ( y x ) is distributed as N 5 ( 𝛍 , 𝚺 ) . (a) Find E ( y | x ) by (4.7).

- (b) Find cov ( y | x ) by (4.8).
- 4.18 Suppose that y 1, y 2 , . . . , y n is a random sample from a nonnormal multivariate population with mean 𝛍 and covariance matrix 𝚺 . If n is large, what is the approximate distribution of each of the following?
- (b) y
- (a) √ n ( y -𝛍 )
- 4.19 For the ramus bone data treated in Example 4.5.2, check each of the four variables for univariate normality using the following techniques:
- (a) Q -Q
- plots;
- (b) b 1 and b 2 as given by (4.18) and (4.19);
- √
- (c) D'Agostino's test using D and Y given in (4.22) and (4.23);
- (d) The test by Lin and Mudholkar using z
- defined in (4.24).
- 4.20 For the calcium data in Table 3.3, check for multivariate normality and outliers using the following tests:
- (a) Calculate D 2 i as in (4.27) for each observation.
- (b) Compare the largest value of D 2 i with the critical value in Table A.6.
- (c) Compute ui and υ i in (4.28) and (4.29) and plot them. Is there an indication of nonlinearity or outliers?
- (d) Calculate b 1 , p and b 2 , p in (4.36) and (4.37) and compare them with critical values in Table A.5.

- 4.21 For the probe word data in Table 3.5, check each of the five variables for univariate normality and outliers using the following tests:
- (a) Q -Q plots;
- (b) √ b 1 and b 2 as given by (4.18) and (4.19);
- (c) D'Agostino's test using D and Y given in (4.22) and (4.23);
- (d) The test by Lin and Mudholkar using z defined in (4.24).
- 4.22 For the probe word data in Table 3.5, check for multivariate normality and outliers using the following tests:
- (a) Calculate D 2 i as in (4.27) for each observation.
- (b) Compare the largest value of D 2 i with the critical value in Table A.6.
- (c) Compute ui and υ i in (4.28) and (4.29) and plot them. Is there an indication of nonlinearity or outliers?
- (d) Calculate b 1 , p and b 2 , p in (4.36) and (4.37) and compare them with critical values in Table A.5.
- 4.23 Six hematology variables were measured on 51 workers (Royston 1983):

- y 1 = hemoglobin concentration

y 4 = lymphocyte count

y 2 = packed cell volume

y 5 = neutrophil count

y 3 = white blood cell count y 6 = serum lead concentration

The data are given in Table 4.3. Check each of the six variables for univariate normality using the following tests:

- (a) Q -Q plots;
- (b) √ b 1 and b 2 as given by (4.18) and (4.19);
- (c) D'Agostino's test using D and Y given in (4.22) and (4.23);
- (d) The test by Lin and Mudholkar using z defined in (4.24).

Table 4.3. Hematology Data

|   Observation Number |   y 1 |   y 2 |   y 3 |   y 4 |   y 5 |   y 6 |
|----------------------|-------|-------|-------|-------|-------|-------|
|                    1 |  13.4 |    39 |  4100 |    14 |    25 |    17 |
|                    2 |  14.6 |    46 |  5000 |    15 |    30 |    20 |
|                    3 |  13.5 |    42 |  4500 |    19 |    21 |    18 |
|                    4 |  15   |    46 |  4600 |    23 |    16 |    18 |
|                    5 |  14.6 |    44 |  5100 |    17 |    31 |    19 |
|                    6 |  14   |    44 |  4900 |    20 |    24 |    19 |
|                    7 |  16.4 |    49 |  4300 |    21 |    17 |    18 |
|                    8 |  14.8 |    44 |  4400 |    16 |    26 |    29 |
|                    9 |  15.2 |    46 |  4100 |    27 |    13 |    27 |
|                   10 |  15.5 |    48 |  8400 |    34 |    42 |    36 |

(continued)

110 Table 4.3. (Continued)

|   Observation Number |   y 1 |   y 2 |   y 3 |   y 4 |   y 5 |   y 6 |
|----------------------|-------|-------|-------|-------|-------|-------|
|                   11 |  15.2 |    47 |  5600 |    26 |    27 |    22 |
|                   12 |  16.9 |    50 |  5100 |    28 |    17 |    23 |
|                   13 |  14.8 |    44 |  4700 |    24 |    20 |    23 |
|                   14 |  16.2 |    45 |  5600 |    26 |    25 |    19 |
|                   15 |  14.7 |    43 |  4000 |    23 |    13 |    17 |
|                   16 |  14.7 |    42 |  3400 |     9 |    22 |    13 |
|                   17 |  16.5 |    45 |  5400 |    18 |    32 |    17 |
|                   18 |  15.4 |    45 |  6900 |    28 |    36 |    24 |
|                   19 |  15.1 |    45 |  4600 |    17 |    29 |    17 |
|                   20 |  14.2 |    46 |  4200 |    14 |    25 |    28 |
|                   21 |  15.9 |    46 |  5200 |     8 |    34 |    16 |
|                   22 |  16   |    47 |  4700 |    25 |    14 |    18 |
|                   23 |  17.4 |    50 |  8600 |    37 |    39 |    17 |
|                   24 |  14.3 |    43 |  5500 |    20 |    31 |    19 |
|                   25 |  14.8 |    44 |  4200 |    15 |    24 |    29 |
|                   26 |  14.9 |    43 |  4300 |     9 |    32 |    17 |
|                   27 |  15.5 |    45 |  5200 |    16 |    30 |    20 |
|                   28 |  14.5 |    43 |  3900 |    18 |    18 |    25 |
|                   29 |  14.4 |    45 |  6000 |    17 |    37 |    23 |
|                   30 |  14.6 |    44 |  4700 |    23 |    21 |    27 |
|                   31 |  15.3 |    45 |  7900 |    43 |    23 |    23 |
|                   32 |  14.9 |    45 |  3400 |    17 |    15 |    24 |
|                   33 |  15.8 |    47 |  6000 |    23 |    32 |    21 |
|                   34 |  14.4 |    44 |  7700 |    31 |    39 |    23 |
|                   35 |  14.7 |    46 |  3700 |    11 |    23 |    23 |
|                   36 |  14.8 |    43 |  5200 |    25 |    19 |    22 |
|                   37 |  15.4 |    45 |  6000 |    30 |    25 |    18 |
|                   38 |  16.2 |    50 |  8100 |    32 |    38 |    18 |
|                   39 |  15   |    45 |  4900 |    17 |    26 |    24 |
|                   40 |  15.1 |    47 |  6000 |    22 |    33 |    16 |
|                   41 |  16   |    46 |  4600 |    20 |    22 |    22 |
|                   42 |  15.3 |    48 |  5500 |    20 |    23 |    23 |
|                   43 |  14.5 |    41 |  6200 |    20 |    36 |    21 |
|                   44 |  14.2 |    41 |  4900 |    26 |    20 |    20 |
|                   45 |  15   |    45 |  7200 |    40 |    25 |    25 |
|                   46 |  14.2 |    46 |  5800 |    22 |    31 |    22 |
|                   47 |  14.9 |    45 |  8400 |    61 |    17 |    17 |
|                   48 |  16.2 |    48 |  3100 |    12 |    15 |    18 |
|                   49 |  14.5 |    45 |  4000 |    20 |    18 |    20 |
|                   50 |  16.4 |    49 |  6900 |    35 |    22 |    24 |
|                   51 |  14.7 |    44 |  7800 |    38 |    34 |    16 |

- 4.24 For the hematology data in Table 4.3, check for multivariate normality using the following techniques:
- (a) Calculate D 2 i as in (4.27) for each observation.
- (b) Compare the largest value of D 2 i with the critical value in Table A.6 (extrapolate).
- (c) Compute ui and υ i in (4.28) and (4.29) and plot them. Is there an indication of nonlinearity or outliers?
- (d) Calculate b 1 , p and b 2 , p in (4.36) and (4.37) and compare them with critical values in Table A.5.

## C H A P T E R 5

## Tests on One or Two Mean Vectors

## 5.1 MULTIVARIATE VERSUS UNIVARIATE TESTS

Hypothesis testing in a multivariate context is more complex than in a univariate setting. The number of parameters may be staggering. The p -variate normal distribution, for example, has p means, p variances, and ( p 2 ) covariances, where ( p 2 ) represents the number of pairs among the p variables. The total number of parameters is

<!-- formula-not-decoded -->

For p = 10, for example, the number of parameters is 65, for each of which, a hypothesis could be formulated. Additionally, we might be interested in testing hypotheses about subsets of these parameters or about functions of them. In some cases, we have the added dilemma of choosing among competing test statistics (see Chapter 6).

We first discuss the motivation for testing p variables multivariately rather than (or in addition to) univariately, as, for example, in hypotheses about µ 1, µ 2 , . . . , µ p in 𝛍 . There are at least four arguments for a multivariate approach to hypothesis testing:

1. The use of p univariate tests inflates the Type I error rate, α , whereas the multivariate test preserves the exact α level. For example, if we do p = 10 separate univariate tests at the .05 level, the probability of at least one false rejection is greater than .05. If the variables were independent (they rarely are), we would have (under H 0)

P ( at least one rejection ) 1 P ( all 10 tests accept H )

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

The resulting overall α of .40 is not an acceptable error rate. Typically, the 10 variables are correlated, and the overall α would lie somewhere between .05 and .40.

2. The univariate tests completely ignore the correlations among the variables, whereas the multivariate tests make direct use of the correlations.
3. The multivariate test is more powerful in many cases. The power of a test is the probability of rejecting H 0 when it is false. In some cases, all p of the univariate tests fail to reach significance, but the multivariate test is significant because small effects on some of the variables combine to jointly indicate significance. However, for a given sample size, there is a limit to the number of variables a multivariate test can handle without losing power. This is discussed further in Section 5.3.2.
4. Many multivariate tests involving means have as a byproduct the construction of a linear combination of variables that reveals more about how the variables unite to reject the hypothesis.

## 5.2 TESTS ON 𝛍 WITH 𝚺 KNOWN

The test on a mean vector assuming a known 𝚺 is introduced to illustrate the issues involved in multivariate testing and to serve as a foundation for the unknown 𝚺 case. We first review the univariate case, in which we work with a single variable y that is distributed as N (µ, σ 2 ) .

## 5.2.1 Review of Univariate Test for H 0 : µ = µ 0 with σ Known

The hypothesis of interest is that the mean of y is equal to a given value, µ 0, versus the alternative that it is not equal to µ 0:

<!-- formula-not-decoded -->

We do not consider one-sided alternative hypotheses because they do not readily generalize to multivariate tests. We assume a random sample of n observations y 1, y 2 , . . . , yn from N (µ, σ 2 ) with σ 2 known. We calculate y = ∑ n i = 1 yi / n and compare it to µ 0 using the test statistic

<!-- formula-not-decoded -->

which is distributed as N ( 0 , 1 ) if H 0 is true. For α = . 05, we reject H 0 if | z | ≥ 1 . 96. Equivalently, we can use z 2 , which is distributed as χ 2 with one degree of freedom, and reject H 0 if z 2 ≥ ( 1 . 96 ) 2 = 3 . 84. If n is large, we are assured by the central limit theorem that z is approximately normal, even if the observations are not from a normal distribution.

## 5.2.2 Multivariate Test for H 0 : 𝛍 = 𝛍 0 with 𝚺 Known

In the multivariate case we have several variables measured on each sampling unit, and we wish to hypothesize a value for the mean of each variable, H 0 : 𝛍 = 𝛍 0 vs. H 1 : 𝛍 /negationslash= 𝛍 0. More explicitly, we have

<!-- formula-not-decoded -->

To test H 0, we use a random sample of n observation vectors y 1, y 2 , . . . , y n from Np ( 𝛍 , 𝚺 ) , with 𝚺 known, and calculate y = ∑ n i = 1 y i / n . The test statistic is where each µ 0 j is specified from previous experience or is a target value. The vector equality in H 0 implies µ j = µ 0 j for all j = 1 , 2 , . . . , p . The vector inequality in H 1 implies at least one µ j /negationslash= µ 0 j . Thus, for example, if µ j = µ 0 j for all j except 2, for which µ 2 /negationslash= µ 02, then we wish to reject H 0.

<!-- formula-not-decoded -->

If H 0 is true, Z 2 is distributed as χ 2 p by (4.6), and we therefore reject H 0 if Z 2 &gt; χ 2 α, p . Note that for one variable, z 2 [the square of (5.1)] has a chi-square distribution with 1 degree of freedom, whereas, for p variables, Z 2 in (5.2) is distributed as a chi-square with p degrees of freedom.

If 𝚺 is unknown, we could use S in its place in (5.2), and Z 2 would have an approximate χ 2 -distribution. But n would have to be larger than in the analogous univariate situation, in which t = ( y -µ 0 )/( s / √ n ) is approximately N ( 0 , 1 ) for n &gt; 30. The value of n needed for n ( y -𝛍 0 ) ′ S -1 ( y -𝛍 0 ) to have an approximate χ 2 -distribution depends on p . This is clarified further in Section 5.3.2.

Example 5.2.2. In Table 3.1, height and weight were given for a sample of 20 college-age males. Let us assume that this sample originated from the bivariate normal N 2 ( 𝛍 , 𝚺 ) , where

<!-- formula-not-decoded -->

Suppose we wish to test H 0 : 𝛍 = ( 70 , 170 ) ′ . From Example 3.2.1, y 1 = 71 . 45 and y 2 = 164 . 7. We thus have

<!-- formula-not-decoded -->

Using α = . 05, χ 2 . 05 , 2 = 5 . 99, and we therefore reject H 0 : 𝛍 = ( 70 , 170 ) ′ because Z 2 = 8 . 4026 &gt; 5 . 99.

The rejection region for y = ( y 1 , y 2 ) ′ is on or outside the ellipse in Figure 5.1; that is, the test statistic Z 2 is greater than 5.99 if and only if y is outside the ellipse. If y falls inside the ellipse, H 0 is accepted. Thus, distance from 𝛍 0 as well as direction must be taken into account. When the distance is standardized by 𝚺 -1 , all points on the curve are 'statistically equidistant' from the center.

Note that the test is sensitive to the covariance structure. If cov ( y 1 , y 2 ) were negative, y 2 would tend to decrease as y 1 increases, and the ellipse would be tilted in the other direction. In this case, y would be in the acceptance region.

Let us now investigate the consequence of testing each variable separately. Using z α/ 2 = 1 . 96 for α = . 05, we have

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Thus both tests accept the hypothesis. In this case neither of the y 's is far enough from the hypothesized value to cause rejection. But when the positive correlation between y 1 and y 2 is taken into account in the multivariate test, the two evidences

Figure 5.1. Elliptical acceptance region.

<!-- image -->

Figure 5.2. Acceptance and rejection regions for univariate and multivariate tests.

<!-- image -->

against 𝛍 0 combine to cause rejection. This illustrates the third advantage of multivariate tests given in Section 5.1.

Figure 5.2 shows the rectangular acceptance region for the univariate tests superimposed on the elliptical multivariate acceptance region. The rectangle was obtained by calculating the two acceptance regions

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Points inside the ellipse but outside the rectangle will be rejected in at least one univariate dimension but will be accepted multivariately. This illustrates the inflation of α resulting from univariate tests, as discussed in the first motive for multivariate testing in Section 5.1. This phenomenon has been referred to as Rao's paradox. For further discussion see Rao (1966), Healy (1969), and Morrison (1990, p. 174). Points outside the ellipse but inside the rectangle will be rejected multivariately but accepted univariately in both dimensions. This illustrates the third motive for multivariate testing given in Section 5.1, namely, that the multivariate test is more powerful in some situations.

Thus in either case represented by the shaded areas, we should use the multivariate test result, not the univariate results. In the one case, the multivariate test is more powerful than the univariate tests; in the other case, the multivariate test preserves α

whereas the univariate tests inflate α . Consequently, when the multivariate and univariate results disagree, our tendency is to trust the multivariate result. In Section 5.5, we discuss various procedures for ascertaining the contribution of the individual variables after the multivariate test has rejected the hypothesis.

## 5.3 TESTS ON 𝛍 WHEN 𝚺 IS UNKNOWN

In Section 5.2, we said little about properties of the tests, because the tests discussed were of slight practical consequence due to the assumption that 𝚺 is known. We will be more concerned with test properties in Sections 5.3 and 5.4, first in the one-sample case and then in the two-sample case. The reader may wonder why we include onesample tests, since we seldom, if ever, have need of a test for H 0 : 𝛍 = 𝛍 0. However, we will cover this case for two reasons:

1. Many general principles are more easily illustrated in the one-sample framework than in the two-sample case.
2. Somevery useful tests can be cast in the one-sample framework. Two examples are (1) H 0 : 𝛍 d = 0 used in the paired comparison test covered in Section 5.7 and (2) H 0 : C 𝛍 = 0 used in profile analysis in Section 5.9, in analysis of repeated measures in Section 6.9, and in growth curves in Section 6.10.

## 5.3.1 Review of Univariate t -Test for H 0 : µ = µ 0 with σ Unknown

We first review the familiar one-sample t -test in the univariate case, with only one variable measured on each sampling unit. We assume that a random sample y 1, y 2 , . . . , yn is available from N (µ, σ 2 ) . We estimate µ by y and σ 2 by s 2 , where y and s 2 are given by (3.1) and (3.4). To test H 0 : µ = µ 0 vs. H 1 : µ /negationslash= µ 0, we use

<!-- formula-not-decoded -->

If H 0 is true, t is distributed as tn -1, where n -1 is the degrees of freedom. We reject H 0 if | √ n ( y -µ 0 )/ s | ≥ t α/ 2 , n -1, where t α/ 2 , n -1 is a critical value from the t -table.

The first expression in (5.3), t = ( y -µ 0 )/( s / √ n ) , is the characteristic form of the t -statistic, which represents a sample standardized distance between y and µ 0. In this form, the hypothesized mean is subtracted from y and the difference is divided by sy = s / √ n . Since y 1, y 2 , . . . , yn is a random sample from N (µ, σ 2 ) , the random variables y and s are independent. We will see an analogous characteristic form for the T 2 -statistic in the multivariate case in Section 5.3.2.

## 5.3.2 Hotelling's T 2 -Test for H 0 : µ = µ 0 with Σ Unknown

We now move to the multivariate case in which p variables are measured on each sampling unit. We assume that a random sample y 1, y 2 , . . . , y n is available from Np ( 𝛍 , 𝚺 ) , where y i contains the p measurements on the i th sampling unit (subject

or object). We estimate 𝛍 by y and 𝚺 by S , where y and S are given by (3.16), (3.19), (3.22), (3.27), and (3.29). In order to test H 0 : 𝛍 = 𝛍 0 versus H 1 : 𝛍 /negationslash= 𝛍 0, we use an extension of the univariate t -statistic in (5.3). In squared form, the univariate t can be rewritten as

<!-- formula-not-decoded -->

When y -µ 0 and s 2 are replaced by y -𝛍 0 and S , we obtain the test statistic

<!-- formula-not-decoded -->

Alternatively, T 2 can be obtained from Z 2 in (5.2) by replacing 𝚺 with S .

The distribution of T 2 was obtained by Hotelling (1931), assuming H 0 is true and sampling is from Np ( 𝛍 , 𝚺 ) . The distribution is indexed by two parameters, the dimension p and the degrees of freedom ν = n -1. We reject H 0 if T 2 &gt; T 2 α, p , n -1 and accept H 0 otherwise. Critical values of the T 2 -distribution are found in Table A.7, taken from Kramer and Jensen (1969a).

Note that the terminology 'accept H 0' is used for expositional convenience to describe our decision when we do not reject the hypothesis. Strictly speaking, we do not accept H 0 in the sense of actually believing it is true. If the sample size were extremely large and we accepted H 0, we could be reasonably certain that the true 𝛍 is close to the hypothesized value 𝛍 0. Otherwise, accepting H 0 means only that we have failed to reject H 0.

The T 2 -statistic can be viewed as the sample standardized distance between the observed sample mean vector and the hypothetical mean vector. If the sample mean vector is notably distant from the hypothetical mean vector, we become suspicious of the hypothetical mean vector and wish to reject H 0.

The test statistic is a scalar quantity, since T 2 = n ( y -𝛍 0 ) ′ S -1 ( y -𝛍 0 ) is a quadratic form. As with the χ 2 -distribution of Z 2 , the density of T 2 is skewed because the lower limit is zero and there is no upper limit.

The characteristic form of the T 2 -statistic (5.5) is

<!-- formula-not-decoded -->

The characteristic form has two features:

1. S / n is the sample covariance matrix of y and serves as a standardizing matrix in the distance function.
2. Since y 1, y 2 , . . . , y n are distributed as Np ( 𝛍 , 𝚺 ) , it follows that y is Np ( 𝛍 , 1 n 𝚺 ) , ( n -1 ) S is W ( n -1 , 𝚺 ) , and y and S are independent (see Section 4.3.2).

In (5.3), the univariate t -statistic represents the number of standard deviations y is separated from µ 0. In appearance, the T 2 -statistic (5.6) is similar, but no such simple interpretation is possible. If we add a variable, the distance in (5.6) increases. (By analogy, the hypotenuse of a right triangle is longer than either of the legs.) Thus we need a test statistic that indicates the significance of the distance from y to 𝛍 0, while allowing for the number of dimensions (see comment 3 at the end of this section about the T 2 -table). Since the resulting T 2 -statistic cannot be readily interpreted in terms of the number of standard deviations y is from 𝛍 0, we do not have an intuitive feel for its significance as we do with the univariate t . We must compare the calculated value of T 2 with the table value. In addition, the T 2 -table provides some insights into the behavior of the T 2 -distribution. Four of these insights are noted at the end of this section.

If a test leads to rejection of H 0 : 𝛍 = 𝛍 0, the question arises as to which variable or variables contributed most to the rejection. This issue is discussed in Section 5.5 for the two-sample T 2 -test of H 0 : 𝛍 1 = 𝛍 2, and the results there can be easily adapted to the one-sample test of H 0 : 𝛍 = 𝛍 0. For confidence intervals on the individual µ j 's in 𝛍 , see Rencher (1998, Section 3.4).

The following are some key properties of the T 2 -test:

1. Wemust have n -1 &gt; p . Otherwise, S is singular and T 2 cannot be computed.
3. The alternative hypothesis is two-sided. Because the space is multidimensional, we do not consider one-sided alternative hypotheses, such as 𝛍 &gt; 𝛍 0. However, even though the alternative hypothesis H 1 : 𝛍 /negationslash= 𝛍 0 is essentially two-sided, the critical region is one-tailed (we reject H 0 for large values of T 2 ). This is typical of many multivariate tests.
2. In both the one-sample and two-sample cases, the degrees of freedom for the T 2 -statistic will be the same as for the analogous univariate t -test; that is, ν = n -1 for one sample and ν = n 1 + n 2 -2 for two samples (see Section 5.4.2).
4. In the univariate case, t 2 n -1 = F 1 , n -1. The statistic T 2 can also be converted to an F -statistic as follows:

<!-- formula-not-decoded -->

Note that the dimension p (number of variables) of the T 2 -statistic becomes the first of the two degrees-of-freedom parameters of the F . The number of degrees of freedom for T 2 is denoted by ν , and the F transformation is given in terms of a general ν , since other applications of T 2 will have ν different from n -1 (see, for example, Sections 5.4.2 and 6.3.2).

Equation (5.7) gives an easy way to find critical values for the T 2 -test. However, we have provided critical values of T 2 in Table A.7 because of the insights they provide into the behavior of the T 2 -distribution in particular and multivariate tests in general. The following are some insights that can readily be gleaned from the T 2 -tables:

1. The first column of Table A.7 contains squares of t -table values; that is, T 2 α, 1 ,ν = t 2 α/ 2 ,ν . (We use t 2 α/ 2 because the univariate test of H 0 : µ = µ 0 vs. H 1 : µ /negationslash= µ 0 is two-tailed.) Thus for p = 1 , T 2 reduces to t 2 . This can easily be seen by comparing (5.5) with (5.4).
2. The last row of each page of Table A.7 contains χ 2 critical values, that is, T 2 p , ∞ = χ 2 p . Thus as n increases, S approaches 𝚺 , and

<!-- formula-not-decoded -->

approaches Z 2 = n ( y -𝛍 0 ) ′ 𝚺 -1 ( y -𝛍 0 ) in (5.2), which is distributed as χ 2 p .

3. The values increase along each row of Table A.7; that is, for a fixed ν , the critical value T 2 α, p ,ν increases with p . It was noted above that in any given sample, the calculated value of T 2 increases if a variable is added. However, since the critical value also increases, a variable should not be added unless it adds a significant amount to T 2 .
4. As p increases, larger values of ν are required for the distribution of T 2 to approach χ 2 . In the univariate case, t in (5.3) is considered a good approximation to the standard normal z in (5.1) when ν = n -1 is at least 30. In the first column ( p = 1) of Table A.7, we see T 2 . 05 , 1 , 30 = 4 . 171 and T 2 . 05 , 1 , ∞ = 3 . 841, with a ratio of 4 . 171 / 3 . 841 = 1 . 086. For p = 5 , ν must be 100 to obtain the same ratio: T 2 . 05 , 5 , 100 / T 2 . 05 , 5 , ∞ = 1 . 086. For p = 10, we need ν = 200 to obtain a similar value of the ratio: T 2 . 05 , 10 , 200 / T 2 . 05 , 10 , ∞ = 1 . 076. Thus one must be very cautious in stating that T 2 has an approximate χ 2 -distribution for large n . The α level (Type I error rate) could be substantially inflated. For example, suppose p = 10 and we assume that n = 30 is sufficiently large for a χ 2 -approximation to hold. Then we would reject H 0 for T 2 ≥ 18 . 307 with a target α -level of .05. However, the correct critical value is 34.044, and the misuse of 18.307 would yield an actual α of P ( T 2 18 . 307 ) . 314.
3. 10 , 29 ≥ =

Example 5.3.2. In Table 3.3 we have n = 10 observations on p = 3 variables. Desirable levels for y 1 and y 2 are 15.0 and 6.0, respectively, and the expected level of y 3 is 2.85. We can, therefore, test the hypothesis

<!-- formula-not-decoded -->

In Examples 3.5 and 3.6, y and S were obtained as

<!-- formula-not-decoded -->

To test H 0, we use (5.5):

<!-- formula-not-decoded -->

From Table A.7, we obtain the critical value T 2 . 05 , 3 , 9 = 16 . 766. Since the observed value of T 2 exceeds the critical value, we reject the hypothesis.

## 5.4 COMPARING TWO MEAN VECTORS

We first review the univariate two-sample t -test and then proceed with the analogous multivariate test.

## 5.4.1 Review of Univariate Two-Sample t -Test

In the one-variable case we obtain a random sample y 11, y 12 , . . . , y 1 n 1 from N (µ 1 , σ 2 1 ) and a second random sample y 21, y 22 , . . . , y 2 n 2 from N (µ 2 , σ 2 2 ) . We assume that the two samples are independent and that σ 2 1 = σ 2 2 = σ 2 , say, with σ 2 unknown. [The assumptions of independence and equal variances are necessary in order for the t -statistic in (5.8) to have a t -distribution.] From the two samples we calculate y 1 , y 2 , SS1 = ∑ n 1 i = 1 ( y 1 i -y 1 ) 2 = ( n 1 -1 ) s 2 1 , SS2 = ∑ n 2 i = 1 ( y 2 i -y 2 ) 2 = ( n 2 -1 ) s 2 2 , and the pooled variance

<!-- formula-not-decoded -->

where n 1 + n 2 -2 is the sum of the weights n 1 -1 and n 2 -1 in the numerator. With this denominator, s 2 pl is an unbiased estimator for the common variance, σ 2 , that is, E ( s 2 pl ) = σ 2 .

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

To test we use

which has a t -distribution with n 1 + n 2 -2 degrees of freedom when H 0 is true. We therefore reject H 0 if | t | ≥ t α/ 2 , n 1 + n 2 -2.

Note that (5.8) exhibits the characteristic form of a t -statistic. In this form, the denominator is the sample standard deviation of the numerator; that is, is an estimate of

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

## 5.4.2 Multivariate Two-Sample T 2 -Test

We now consider the case where p variables are measured on each sampling unit in two samples. We wish to test

<!-- formula-not-decoded -->

We obtain a random sample y 11, y 12 , . . . , y 1 n 1 from Np ( 𝛍 1 , 𝚺 1 ) and a second random sample y 21, y 22 , . . . , y 2 n 2 from Np ( 𝛍 2 , 𝚺 2 ) . We assume that the two samples are independent and that 𝚺 1 = 𝚺 2 = 𝚺 , say, with 𝚺 unknown. These assumptions are necessary in order for the T 2 -statistic in (5.9) to have a T 2 -distribution. A test of H 0 : 𝚺 1 = 𝚺 2 is given in Section 7.3.2. For an approximate test of H 0 : 𝛍 1 = 𝛍 2 that can be used when 𝚺 1 /negationslash= 𝚺 2, see Rencher (1998, Section 3.9).

The sample mean vectors are y 1 = ∑ n 1 i = 1 y 1 i / n 1 and y 2 = ∑ n 2 i = 1 y 2 i / n 2. Define W 1 and W 2 to be the matrices of sums of squares and cross products for the two samples:

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Since ( n 1 -1 ) S 1 is an unbiased estimator of ( n 1 -1 ) 𝚺 and ( n 2 -1 ) S 2 is an unbiased estimator of ( n 2 -1 ) 𝚺 , we can pool them to obtain an unbiased estimator of the common population covariance matrix, 𝚺 :

<!-- formula-not-decoded -->

Thus E ( S pl ) = 𝚺 .

<!-- formula-not-decoded -->

The square of the univariate t -statistic (5.8) can be expressed as

<!-- formula-not-decoded -->

This can be generalized to p variables by substituting y 1 -y 2 for y 1 -y 2 and S pl for s 2 pl to obtain

<!-- formula-not-decoded -->

which is distributed as T 2 p , n 1 + n 2 -2 when H 0 : 𝛍 1 = 𝛍 2 is true. To carry out the test, we collect the two samples, calculate T 2 by (5.9), and reject H 0 if T 2 ≥ T 2 α, p , n 1 + n 2 -2 . Critical values of T 2 are found in Table A.7. For tables of the power of the T 2 -test (probability of rejecting H 0 when it is false) and illustrations of their use, see Rencher (1998, Section 3.10).

The T 2 -statistic (5.9) can be expressed in characteristic form as the standardized distance between y 1 and y 2 :

<!-- formula-not-decoded -->

where ( 1 / n 1 + 1 / n 2 ) S pl is the sample covariance matrix for y 1 -y 2 and S pl is independent of y 1 -y 2 because of sampling from the multivariate normal. For a discussion of robustness of T 2 to departures from the assumptions of multivariate normality and homogeneity of covariance matrices ( 𝚺 1 = 𝚺 2 ) , see Rencher (1998, Section 3.7).

Some key properties of the two-sample T 2 -test are given in the following list:

1. It is necessary that n 1 + n 2 -2 &gt; p for S pl to be nonsingular.
2. The statistic T 2 is, of course, a scalar. The 3 p + p ( p -1 )/ 2 quantities in y 1 , y 2 , and S pl have been reduced to a single scale on which T 2 is large if the sample evidence favors H 1 : 𝛍 1 /negationslash= 𝛍 2 and small if the evidence supports H 0 : 𝛍 1 = 𝛍 2; we reject H 0 if the standardized distance between y 1 and y 2 is large.
3. Since the lower limit of T 2 is zero and there is no upper limit, the density is skewed. In fact, as noted in (5.11), T 2 is directly related to F , which is a well-known skewed distribution.
4. For degrees of freedom of T 2 we have n 1 + n 2 -2, which is the same as for the corresponding univariate t -statistic (5.8).

5. The alternative hypothesis H 1 : 𝛍 1 /negationslash= 𝛍 2 is two sided. The critical region T 2 &gt; T 2 α is one-tailed, however, as is typical of many multivariate tests.
6. The T 2 -statistic can be readily transformed to an F -statistic using (5.7):

<!-- formula-not-decoded -->

where again the dimension p of the T 2 -statistic becomes the first degree-offreedom parameter for the F -statistic.

Example 5.4.2. Four psychological tests were given to 32 men and 32 women. The data are recorded in Table 5.1 (Beall 1945). The variables are

<!-- formula-not-decoded -->

The mean vectors and covariance matrices of the two samples are

<!-- formula-not-decoded -->

The sample covariance matrices do not appear to indicate a disparity in the population covariance matrices. (A significance test to check this assumption is carried out in Example 7.3.2, and the hypothesis H 0 : 𝚺 1 = 𝚺 2 is not rejected.) The pooled covariance matrix is

<!-- formula-not-decoded -->

Table 5.1. Four Psychological Test Scores on 32 Males and 32 Females

| Males   | Males   | Males   | Males   | Females   | Females   | Females   | Females   |
|---------|---------|---------|---------|-----------|-----------|-----------|-----------|
| y 1     | y 2     | y 3     | y 4     | y 1       | y 2       | y 3       | y 4       |
| 15      | 17      | 24      | 14      | 13        | 14        | 12        | 21        |
| 17      | 15      | 32      | 26      | 14        | 12        | 14        | 26        |
| 15      | 14      | 29      | 23      | 12        | 19        | 21        | 21        |
| 13      | 12      | 10      | 16      | 12        | 13        | 10        | 16        |
| 20      | 17      | 26      | 28      | 11        | 20        | 16        | 16        |
| 15      | 21      | 26      | 21      | 12        | 9         | 14        | 18        |
| 15      | 13      | 26      | 22      | 10        | 13        | 18        | 24        |
| 13      | 5       | 22      | 22      | 10        | 8         | 13        | 23        |
| 14      | 7       | 30      | 17      | 12        | 20        | 19        | 23        |
| 17      | 15      | 30      | 27      | 11        | 10        | 11        | 27        |
| 17      | 17      | 26      | 20      | 12        | 18        | 25        | 25        |
| 17      | 20      | 28      | 24      | 14        | 18        | 13        | 26        |
| 15      | 15      | 29      | 24      | 14        | 10        | 25        | 28        |
| 18      | 19      | 32      | 28      | 13        | 16        | 8         | 14        |
| 18      | 18      | 31      | 27      | 14        | 8         | 13        | 25        |
| 15      | 14      | 26      | 21      | 13        | 16        | 23        | 28        |
| 18      | 17      | 33      | 26      | 16        | 21        | 26        | 26        |
| 10      | 14      | 19      | 17      | 14        | 17        | 14        | 14        |
| 18      | 21      | 30      | 29      | 16        | 16        | 15        | 23        |
| 18      | 21      | 34      | 26      | 13        | 16        | 23        | 24        |
| 13      | 17      | 30      | 24      | 2         | 6         | 16        | 21        |
| 16      | 16      | 16      | 16      | 14        | 16        | 22        | 26        |
| 11      | 15      | 25      | 23      | 17        | 17        | 22        | 28        |
| 16      | 13      | 26      | 16      | 16        | 13        | 16        | 14        |
| 16      | 13      | 23      | 21      | 15        | 14        | 20        | 26        |
| 18      | 18      | 34      | 24      | 12        | 10        | 12        | 9         |
| 16      | 15      | 28      | 27      | 14        | 17        | 24        | 23        |
| 15      | 16      | 29      | 24      | 13        | 15        | 18        | 20        |
| 18      | 19      | 32      | 23      | 11        | 16        | 18        | 28        |
| 18      | 16      | 33      | 23      | 7         | 7         | 19        | 18        |
| 17      | 20      | 21      | 21      | 12        | 15        | 7         | 28        |
| 19      | 19      | 30      | 28      | 6         | 5         | 6         | 13        |

By (5.9), we obtain

<!-- formula-not-decoded -->

From interpolation in Table A.7, we obtain T 2 . 01 , 4 , 62 = 15 . 373, and we therefore reject H 0 : 𝛍 1 = 𝛍 2. See Example 5.5 for a discussion of which variables contribute most to separation of the two groups.

## 5.4.3 Likelihood Ratio Tests

The maximum likelihood approach to estimation was introduced in Section 4.3.1. As noted there, the likelihood function is the joint density of y 1, y 2 , . . . , y n . The values of the parameters that maximize the likelihood function are the maximum likelihood estimators.

The likelihood ratio method of test construction uses the ratio of the maximum value of the likelihood function assuming H 0 is true to the maximum under H 1, which is essentially unrestricted. Likelihood ratio tests usually have good power and sometimes have optimum power over a wide class of alternatives.

When applied to multivariate normal samples and H 0 : 𝛍 1 = 𝛍 2, the likelihood ratio approach leads directly to Hotelling's T 2 -test in (5.9). Similarly, in the onesample case, the T 2 -statistic in (5.5) is the likelihood ratio test. Thus the T 2 -test, which we introduced rather informally, is the best test according to certain criteria.

## 5.5 TESTS ON INDIVIDUAL VARIABLES CONDITIONAL ON REJECTION OF H 0 BY THE T 2 -TEST

If the hypothesis H 0 : 𝛍 1 = 𝛍 2 is rejected, the implication is that µ 1 j /negationslash= µ 2 j for at least one j = 1 , 2 , . . . , p . But there is no guarantee that H 0 : µ 1 j = µ 2 j will be rejected for some j by a univariate test. However, if we consider a linear combination of the variables, z = a ′ y , then there is at least one coefficient vector a for which

<!-- formula-not-decoded -->

will reject the corresponding hypothesis H 0 : µ z 1 = µ z 2 or H 0 : a ′ 𝛍 1 = a ′ 𝛍 2. By (3.54), z 1 = a ′ y 1 and z 2 = a ′ y 2 , and from (3.55) the variance estimator s 2 z is the pooled estimator a ′ S pl a . Thus (5.12) can be written as

<!-- formula-not-decoded -->

Since t ( a ) can be negative, we work with t 2 ( a ) . The linear function z = a ′ y is a projection of y onto a line through the origin. We seek the line (direction) on which the difference y 1 -y 2 is maximized when projected. The projected difference a ′ ( y 1 -y 2 ) [standardized by a ′ S pl a as in (5.13)] will be less in any other direction than that parallel to the line joining y 1 and y 2 . The value of a that projects onto this line, or, equivalently, maximizes t 2 ( a ) in (5.13), is (any multiple of)

<!-- formula-not-decoded -->

Since a in (5.14) projects y 1 -y 2 onto a line parallel to the line joining y 1 and y 2 , we would expect that t 2 ( a ) = T 2 , and this is indeed the case (see Problem 5.3).

When a = S -1 pl ( y 1 -y 2 ) , then z = a ′ y is called the discriminant function . Sometimes the vector a itself in (5.14) is loosely referred to as the discriminant function.

If H 0 : 𝛍 1 = 𝛍 2 is rejected by T 2 in (5.9), the discriminant function a ′ y will lead to rejection of H 0 : a ′ 𝛍 1 = a ′ 𝛍 2 using (5.13), with a = S -1 pl ( y 1 -y 2 ) . We can then examine each aj in a for an indication of the contribution of the corresponding y j to rejection of H 0. This follow-up examination of each aj should be done only if H 0 : 𝛍 1 = 𝛍 2 is rejected by T 2 . The discriminant function will appear again in Section 5.6.2 and in Chapters 8 and 9.

We list these and other procedures that could be used to check each variable following rejection of H 0 by a two-sample T 2 -test:

1. Univariate t -tests, one for each variable,

<!-- formula-not-decoded -->

where s j j is the j th diagonal element of S pl. Reject H 0 : µ 1 j = µ 2 j if | t j | &gt; t α/ 2 , n 1 + n 2 -2. For confidence intervals on µ 1 j -µ 2 j , see Rencher (1998, Section 3.6).

2. To adjust the α -level resulting from performing the p tests in (5.15), we could use a Bonferroni critical value t α/ 2 p , n 1 + n 2 -2 for (5.15) (Bonferroni 1936). A critical value t α/ 2 p is much greater than the corresponding t α/ 2, and the resulting overall α -level is conservative. Bonferroni critical values t α/ 2 p ,ν are given in Table A.8, from Bailey (1977).
3. Another critical value that could be used with (5.15) is T α, p , n 1 + n 2 -2, where T α is the square root of T 2 α from Table A.7; that is, T α, p , n 1 + n 2 -2 = √ T 2 α, p , n 1 + n 2 -2 . This allows for all p variables to be tested as well as all possible linear combinations, as in (5.13), even linear combinations chosen after seeing the data. Consequently, the use of T α is even more conservative than using t α/ 2 p ; that is, T α, p , n 1 + n 2 -2 &gt; t α/ 2 p , n 1 + n 2 -2.
5. Standardized discriminant function coefficients (see Section 8.5)
4. Partial F - or t -tests [ test of each variable adjusted for the other variables; see (5.32) in Section 5.8 ]
6. Correlations between the variables and the discriminant function (see Section 8.7.3)
7. Stepwise discriminant analysis (see Section 8.9)

The first three methods are univariate approaches that do not use covariances or correlations among the variables in the computation of the test statistic. The last four methods are multivariate in the sense that the correlation structure is explicitly taken into account in the computation.

Method 6, involving the correlation between each variable and the discriminant function, is recommended in many texts and software packages. However, Rencher

(1988) has shown that these correlations are proportional to individual t - or F -tests (see Section 8.7.3). Thus this method is equivalent to method 1 and is a univariate rather than a multivariate approach. Method 7 is often used to identify a subset of important variables or even to rank the variables according to order of entry. But Rencher and Larson (1980) have shown that stepwise methods have a high risk of selecting spurious variables, unless the sample size is very large.

Wenowconsider the univariate procedures 1, 2, and 3. The probability of rejecting one or more of the p univariate tests when H 0 is true is called the overall α or experimentwise error rate . If we do univariate tests only, with no T 2 -test, then the tests based on t α/ 2 p and T α in procedures 2 and 3 are conservative (overall α too low), and tests based on t α/ 2 in procedure 1 are liberal (overall α too high). However, when these tests are carried out only after rejection by the T 2 -test (such tests are sometimes called protected tests), the experimentwise error rates change. Obviously the tests will reject less often (under H 0) if they are carried out only if T 2 rejects. Thus the tests using t α/ 2 p and T α become even more conservative, and the test using t α/ 2 becomes more acceptable.

Hummel and Sligo (1971) studied the experimentwise error rate for univariate t -tests following rejection of H 0 by the T 2 -test (protected tests). Using α = . 05, they found that using t α/ 2 for a critical value yields an overall α acceptably close to the nominal .05. In fact, it is slightly conservative, making this the preferred univariate test (within the limits of their study). They also compared this procedure with that of performing univariate tests without a prior T 2 -test (unprotected tests). For this case, the overall α is too high, as expected. Table 5.2 gives an excerpt of Hummel and Sligo's results. The sample size is for each of the two samples; the r 2 in common is for every pair of variables.

Hummel and Sligo therefore recommended performing the multivariate T 2 -test followed by univariate t -tests. This procedure appears to have the desired overall α level and will clearly have better power than tests using T α or t α/ 2 p as a critical value. Table 5.2 also highlights the importance of using univariate t -tests only if the multivariate T 2 -test is significant. The inflated α 's resulting if t -tests are used without regard to the outcome of the T 2 -test are clearly evident. Thus among the three univariate procedures (procedures 1, 2, and 3), the first appears to be preferred.

Among the multivariate approaches (procedures 4, 5, and 7), we prefer the fifth procedure, which compares the (absolute value of) coefficients in the discriminant function to find the effect of each variable in separating the two groups of observations. These coefficients will often tell a different story from the univariate tests, because the univariate tests do not take into account the correlations among the variables or the effect of each variable on T 2 in the presence of the other variables. A variable will typically have a different effect in the presence of other variables than it has by itself. In the discriminant function z = a ′ y = a 1 y 1 + a 2 y 2 +··· + ap y p , where a = S -1 pl ( y 1 -y 2 ) , the coefficients a 1 , a 2 , . . . , ap indicate the relative importance of the variables in a multivariate context, something the univariate t -tests cannot do. If the variables are not commensurate (similar in scale and variance), the coefficients should be standardized, as in Section 8.5; this allows for more valid comparisons among the variables. Rencher and Scott (1990) provided a decomposi-

Table 5.2. Comparison of Experimentwise Error Rates (Nominal α = . 05)

|                            |                            | Common r 2                 | Common r 2                 | Common r 2                 | Common r 2                 |
|----------------------------|----------------------------|----------------------------|----------------------------|----------------------------|----------------------------|
| Sample Size                | Number of Variables        | .10                        | .30                        | .50                        | .70                        |
|                            | Univariate Tests Only a    | Univariate Tests Only a    | Univariate Tests Only a    | Univariate Tests Only a    | Univariate Tests Only a    |
| 10                         | 3                          | .145                       | .112                       | .114                       | .077                       |
| 10                         | 6                          | .267                       | .190                       | .178                       | .111                       |
| 10                         | 9                          | .348                       | .247                       | .209                       | .129                       |
| 30                         | 3                          | .115                       | .119                       | .117                       | .085                       |
| 30                         | 6                          | .225                       | .200                       | .176                       | .115                       |
| 30                         | 9                          | .296                       | .263                       | .223                       | .140                       |
| 50                         | 3                          | .138                       | .124                       | .102                       | .083                       |
| 50                         | 6                          | .230                       | .190                       | .160                       | .115                       |
| 50                         | 9                          | .324                       | .258                       | .208                       | .146                       |
| Multivariate Test Followed | Multivariate Test Followed | Multivariate Test Followed | Multivariate Test Followed | Multivariate Test Followed | Multivariate Test Followed |
| 10                         | 3                          | .044                       | .029                       | .035                       | .022                       |
| 10                         | 6                          | .046                       | .029                       | .030                       | .017                       |
| 10                         | 9                          | .050                       | .026                       | .025                       | .018                       |
| 30                         | 3                          | .037                       | .044                       | .029                       | .025                       |
| 30                         | 6                          | .037                       | .037                       | .032                       | .021                       |
| 30                         | 9                          | .042                       | .042                       | .030                       | .021                       |
| 50                         | 3                          | .038                       | .041                       | .033                       | .028                       |
| 50                         | 6                          | .037                       | .039                       | .028                       | .027                       |
| 50                         | 9                          | .036                       | .038                       | .026                       | .020                       |

a Ignoring multivariate tests.

b Carried out only if multivariate test rejects.

tion of the information in the standardized discriminant function coefficients. For a detailed analysis of the effect of each variable in the presence of the other variables, see Rencher (1993; 1998, Sections 3.3.5 and 3.5.3).

Example 5.5. For the psychological data in Table 5.1, we obtained y 1 , y 2 , and S pl in Example 5.4.2. The discriminant function coefficient vector is obtained from (5.14) as

<!-- formula-not-decoded -->

Thus the linear combination that best separates the two groups is

<!-- formula-not-decoded -->

in which y 1 and y 3 appear to contribute most to separation of the two groups. (After standardization, the relative contribution of the variables changes somewhat; see the answer to Problem 8.7 in Appendix B.)

## 5.6 COMPUTATION OF T 2

If one has a program available with matrix manipulation capability, it is a simple matter to compute T 2 using (5.9). However, this approach is somewhat cumbersome for those not accustomed to the use of such a programming language, and many would prefer a more automated procedure. But very few general-purpose statistical programs provide for direct calculation of the two-sample T 2 -statistic, perhaps because it is so easy to obtain from other procedures. We will discuss two types of widely available procedures that can be used to compute T 2 .

## 5.6.1 Obtaining T 2 from a MANOVA Program

Multivariate analysis of variance (MANOVA) is discussed in Chapter 6, and the reader may wish to return to the present section after becoming familiar with that material. One-way MANOVA involves a comparison of mean vectors from several samples. Typically, the number of samples is three or more, but the procedure will also accommodate two samples. The two-sample T 2 test is thus a special case of MANOVA.

Four common test statistics are defined in Section 6.1: Wilks' /Lambda1 , the LawleyHotelling U ( s ) , Pillai's V ( s ) , and Roy's largest root θ . Without concerning ourselves here with how these are defined or calculated, we show how to use each to obtain the two-sample T 2 :

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

(For the special case of two groups, V ( s ) = θ .) These relationships are demonstrated in Section 6.1.7. If the MANOVA program gives eigenvectors of E -1 H ( E and H are defined in Section 6.1.2), the eigenvector corresponding to the largest eigenvalue will be equal to (a constant multiple of) the discriminant function S -1 pl ( y 1 -y 2 ) .

## 5.6.2 Obtaining T 2 from Multiple Regression

In this section, the y 's become independent variables in a regression model. For each observation vector y 1 i and y 2 i in a two-sample T 2 , define a 'dummy' group variable

as

<!-- formula-not-decoded -->

Then w = 0 for all n 1 + n 2 observations. The prediction equation for the regression of w on the y 's can be written as

<!-- formula-not-decoded -->

where i ranges over all n 1 + n 2 observations and the least squares estimate b 0 is [see (10.15)]

<!-- formula-not-decoded -->

Substituting this into the regression equation, we obtain

<!-- formula-not-decoded -->

Let b ′ = ( b 1 , b 2 , . . . , bp ) be the vector of regression coefficients and R 2 be the squared multiple correlation. Then we have the following relationships:

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Thus with ordinary multiple regression, one can easily obtain T 2 and the discriminant function S -1 pl ( y 1 -y 2 ) . We simply define w i as above for each of the n 1 + n 2 observations, regress the w 's on the y 's, and use the resulting R 2 in (5.20). For b , delete the intercept from the regression coefficients for use in (5.21). Actually, since only the relative values of the elements of a = S -1 pl ( y 1 -y 2 ) are of interest, it is not necessary to convert from b to a in (5.21). We can use b directly or standardize the values b 1, b 2 , . . . , bp as in Section 8.5.

Example 5.6.2. We illustrate the regression approach to computation of T 2 using the psychological data in Table 5.1. We set w = n 2 /( n 1 + n 2 ) = 32 64 = 1 2 for each observation in group 1 (males) and equal to -n 1 /( n 1 + n 2 ) = -1 2 in the second group (females). When w is regressed on the 64 y 's, we obtain

By (5.20),

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

as was obtained before in Example 5.4.2. Note that b ′ = ( b 1 , b 2 , b 3 , b 4 ) = (. 051 , -. 020 , . 047 , -. 031 ) , with the intercept deleted, is proportional to the discriminant function coefficient vector a from Example 5.5, as we would expect from (5.21).

## 5.7 PAIRED OBSERVATIONS TEST

As usual, we begin with the univariate case to set the stage for the multivariate presentation.

## 5.7.1 Univariate Case

Suppose two samples are not independent because there exists a natural pairing between the i th observation yi in the first sample and the i th observation xi in the second sample for all i , as, for example, when a treatment is applied twice to the same individual or when subjects are matched according to some criterion, such as IQ or family background. With such pairing, the samples are often referred to as paired observations or matched pairs . The two samples thus obtained are correlated, and the two-sample test statistic in (5.9) is not appropriate because the samples must be independent in order for (5.9) to have a t -distribution. [The two-sample test in (5.9) is somewhat robust to heterogeneity of variances and to lack of normality but not to dependence.] We reduce the two samples to one by working with the differences between the paired observations, as in the following layout for two treatments applied to the same subject:

| Pair Number   | Treatment 1   | Treatment 2   | Difference d i = y i - x i   |
|---------------|---------------|---------------|------------------------------|
| 1             | y 1           | x 1           | d 1                          |
| 2             | y 2           | x 2           | d 2                          |
| .             | .             | .             | .                            |
| .             | .             | .             | .                            |
| .             | .             | .             | .                            |
| n             | y n           | x n           | d n                          |

To obtain a t -test, it is not sufficient to assume individual normality for each of y and x . To allow for the covariance between y and x , we need the additional assump-

tion that y and x have a bivariate normal distribution with

<!-- formula-not-decoded -->

It then follows by property 1a in Section 4.2 that di = yi -xi is N (µ y -µ x , σ 2 d ) , where σ 2 d = σ 2 y -2 σ yx + σ 2 x . From d 1, d 2 , . . . , dn we calculate

<!-- formula-not-decoded -->

To test H 0 : µ y = µ x , that is, H 0 : µ d = 0, we use the one-sample statistic

<!-- formula-not-decoded -->

which is distributed as tn -1 if H 0 is true. We reject H 0 in favor of H 1 : µ d /negationslash= 0 if | t | &gt; t α/ 2 , n -1. It is not necessary to assume σ 2 y = σ 2 x because there are no restrictions on 𝚺 .

This test has only n -1 degrees of freedom compared with 2 ( n -1 ) for the twoindependent-sample t -test (5.8). In general, the pairing reduces the within-sample variation sd and thereby increases the power.

If we mistakenly treated the two samples as independent and used (5.8) with n 1 = n 2 = n , we would have

<!-- formula-not-decoded -->

However,

<!-- formula-not-decoded -->

whereas var ( y -x ) = (σ 2 y + σ 2 x -2 σ yx )/ n . Thus if the test statistic for independent samples (5.8) is used for paired data, it does not have a t -distribution and, in fact, underestimates the true average t -value (assuming H 0 is false), since σ 2 y + σ 2 x &gt; σ 2 y + σ 2 x -2 σ yx if σ yx &gt; 0, which would be typical in this situation. One could therefore use

<!-- formula-not-decoded -->

but t = √ n d / sd in (5.22) is equal to it and somewhat simpler to use.

## 5.7.2 Multivariate Case

Here we assume the same natural pairing of sampling units as in the univariate case, but we measure p variables on each sampling unit. Thus y i from the first sample is paired with x i from the second sample, i = 1, 2 , . . . , n . In terms of two treatments applied to each sampling unit, this situation is as follows:

| Pair Number   | Treatment 1   | Treatment 2   | Difference d i = y i - x i   |
|---------------|---------------|---------------|------------------------------|
| 1             | y 1           | x 1           | d 1                          |
| 2             | y 2           | x 2           | d 2                          |
| . .           | . .           | . .           | . .                          |
| .             | .             | .             | .                            |
| n             | y n           | x n           | d n                          |

In Section 5.7.1, we made the assumption that y and x have a bivariate normal distribution, in which y and x are correlated. Here we assume y and x are correlated and have a multivariate normal distribution:

<!-- formula-not-decoded -->

To test H 0 : 𝛍 d = 0 , which is equivalent to H 0 : 𝛍 y = 𝛍 x since 𝛍 d = E ( y -x ) = 𝛍 y -𝛍 x , we calculate

<!-- formula-not-decoded -->

We then have

<!-- formula-not-decoded -->

Under H 0, this paired comparison T 2 -statistic is distributed as T 2 p , n -1 . We reject H 0 if T 2 &gt; T 2 α, p , n -1 . Note that S d estimates cov ( y -x ) = 𝚺 yy -𝚺 yx -𝚺 xy + 𝚺 xx , for which an equivalent estimator would be S yy -S yx -S xy + S xx [see (3.42)].

The cautions expressed in Section 5.7.1 for univariate paired observation data also apply here. If the two samples of multivariate observations are correlated because of a natural pairing of sampling units, the test in (5.24) should be used rather than the two-sample T 2 -test in (5.9), which assumes two independent samples. Misuse of (5.9) in place of (5.24) will lead to loss of power.

Since the assumption 𝚺 yy = 𝚺 xx is not needed for (5.24) to have a T 2 -distribution, this test can be used for independent samples when 𝚺 1 /negationslash= 𝚺 2 (as long as n 1 = n 2). The observations in the two samples would be paired in the order they were obtained or in an arbitrary order. However, in the case of independent

samples, the pairing achieves no gain in power to offset the loss of n -1 degrees of freedom.

By analogy with (5.14), the discriminant function coefficient vector for paired observation data becomes

<!-- formula-not-decoded -->

For tests on individual variables, we have

<!-- formula-not-decoded -->

The critical value for t j is t α/ 2 p , n -1 or t α/ 2 , n -1 depending on whether a T 2 -test is carried out first (see Section 5.5).

Example 5.7.2. To compare two types of coating for resistance to corrosion, 15 pieces of pipe were coated with each type of coating (Kramer and Jensen 1969b). Two pipes, one with each type of coating, were buried together and left for the same length of time at 15 different locations, providing a natural pairing of the observations. Corrosion for the first type of coating was measured by two variables, y 1 = maximum depth of pit in thousandths of an inch,

y 2 = number of pits,

Table 5.3. Depth of Maximum Pits and Number of Pits of Coated Pipes

|          | Coating 1   | Coating 1   | Coating 2   | Coating 2   | Difference   | Difference   |
|----------|-------------|-------------|-------------|-------------|--------------|--------------|
| Location | Depth y 1   | Number y 2  | Depth x 1   | Number x 2  | Depth d 1    | Number d 2   |
| 1        | 73          | 31          | 51          | 35          | 22           | - 4          |
| 2        | 43          | 19          | 41          | 14          | 2            | 5            |
| 3        | 47          | 22          | 43          | 19          | 4            | 3            |
| 4        | 53          | 26          | 41          | 29          | 12           | - 3          |
| 5        | 58          | 36          | 47          | 34          | 11           | 2            |
| 6        | 47          | 30          | 32          | 26          | 15           | 4            |
| 7        | 52          | 29          | 24          | 19          | 28           | 10           |
| 8        | 38          | 36          | 43          | 37          | - 5          | - 1          |
| 9        | 61          | 34          | 53          | 24          | 8            | 10           |
| 10       | 56          | 33          | 52          | 27          | 4            | 6            |
| 11       | 56          | 19          | 57          | 14          | - 1          | 5            |
| 12       | 34          | 19          | 44          | 19          | - 10         | 0            |
| 13       | 55          | 26          | 57          | 30          | - 2          | - 4          |
| 14       | 65          | 15          | 40          | 7           | 25           | 8            |
| 15       | 75          | 18          | 68          | 13          | 7            | 5            |

with x 1 and x 2 defined analogously for the second coating. The data and differences are given in Table 5.3. Thus we have, for example, y ′ 1 = ( 73 , 31 ) , x ′ 1 = ( 51 , 35 ) , and d ′ 1 = y ′ 1 -x ′ 1 = ( 22 , -4 ) . For the 15 difference vectors, we obtain

<!-- formula-not-decoded -->

By (5.24),

<!-- formula-not-decoded -->

Since T 2 = 10 . 819 &gt; T 2 . 05 , 2 , 14 = 8 . 197, we reject H 0 : 𝛍 d = 0 and conclude that the two coatings differ in their effect on corrosion.

## 5.8 TEST FOR ADDITIONAL INFORMATION

In this section, we are again considering two independent samples, as in Section 5.4.2. We start with a basic p × 1 vector y of measurements on each sampling unit and ask whether a q × 1 subvector x measured in addition to y (on the same unit) will significantly increase the separation of the two samples as shown by T 2 . It is not necessary that we add new variables. We may be interested in determining whether some of the variables we already have are redundant in the presence of other variables in terms of separating the groups. We have designated the subset of interest by x for notational convenience.

It is assumed that the two samples are from multivariate normal populations with a common covariance matrix; that is,

<!-- formula-not-decoded -->

where

<!-- formula-not-decoded -->

We partition the sample mean vectors and covariance matrix accordingly:

<!-- formula-not-decoded -->

where S pl is the pooled sample covariance matrix from the two samples.

We wish to test the hypothesis that x 1 and x 2 are redundant for separating the two groups, that is, that the extra q variables do not contribute anything significant beyond the information already available in y 1 and y 2 for separating the groups. This is in the spirit of a full and reduced model test in regression [see (5.31) and Section 10.2.5b]. However, here we are working with a subset of dependent variables as contrasted to the subset of independent variables in the regression setting. Thus both y and x are subvectors of dependent variables. In this setting, the independent variables would be grouping variables 1 and 2 corresponding to 𝛍 1 and 𝛍 2.

We are not asking if the x 's can significantly separate the two groups by themselves, but whether they provide additional separation beyond the separation already achieved by the y 's. If the x 's were independent of the y 's, we would have T 2 p + q = T 2 p + T 2 q , but this does not hold, because they are correlated. We must compare T 2 p + q for the full set of variables ( y 1 , . . . , yp , x 1 , . . . , xq ) with T 2 p based on the reduced set ( y 1 , . . . , yp ) . We are inquiring if the increase from T 2 p to T 2 p + q is significant.

By definition, the T 2 -statistic based on the full set of p + q variables is given by

<!-- formula-not-decoded -->

whereas T 2 for the reduced set of p variables is

<!-- formula-not-decoded -->

Then the test statistic for the significance of the increase from T 2 p to T 2 p + q is given by

<!-- formula-not-decoded -->

which is distributed as T 2 q ,ν -p . We reject the hypothesis of redundancy of x if T 2 ( x | y ) ≥ T 2 α, q ,ν -p .

By (5.7), T 2 ( x | y ) can be converted to an F -statistic:

<!-- formula-not-decoded -->

which is distributed as F q ,ν -p -q + 1, and we reject the hypothesis if F ≥ F α, q ,ν -p -q + 1. In both cases ν = n 1 + n 2 -2. Note that the first degrees-of-freedom parameter in both (5.29) and (5.30) is q , the number of x 's. The second parameter in (5.29) is ν -p because the statistic is adjusted for the p variables in y .

To prove directly that the statistic defined in (5.30) has an F -distribution, we can use a basic relationship from multiple regression [see (10.33)]:

<!-- formula-not-decoded -->

where R 2 p + q is the squared multiple correlation from the full model with p + q independent variables and R 2 p is from the reduced model with p independent variables. If we solve for R 2 in terms of T 2 from (5.20) and substitute this into (5.31), we readily obtain the test statistic in (5.30).

If we are interested in the effect of adding a single x , then q = 1, and both (5.29) and (5.30) reduce to

<!-- formula-not-decoded -->

and we reject the hypothesis of redundancy of x if t 2 ( x | y ) ≥ t 2 α/ 2 ,ν -p = F α, 1 ,ν -p .

Example 5.8. We use the psychological data of Table 5.1 to illustrate tests on subvectors. We begin by testing the significance of y 3 and y 4 above and beyond y 1 and y 2. (In the notation of the present section, y 3 and y 4 become x 1 and x 2.) For these subvectors, p = 2 and q = 2. The value of T 2 p + q for all four variables as given by (5.27) was obtained in Example 5.4.2 as 97.6015. For y 1 and y 2, we obtain, by (5.28),

<!-- formula-not-decoded -->

By (5.29), the test statistic is

<!-- formula-not-decoded -->

We reject the hypothesis that x = ( y 3 , y 4 ) ′ is redundant, since 42 . 955 &gt; T 2 . 01 , 2 , 60 = 10 . 137. We conclude that x = ( y 3 , y 4 ) ′ adds a significant amount of separation to y = ( y 1 , y 2 ) ′ .

To test the effect of each variable adjusted for the other three, we use (5.32). In this case, p = 3 , ν = 62, and ν -p = 59. The results are given below, where T 2 p + 1 = 97 . 6015 and T 2 p in each case is based on the three variables, excluding the variable in question. For example, T 2 p = 90 . 8348 for y 2 is based on y 1 , y 3, and y 4,

and t 2 ( y 2 | y 1 , y 2 , y 3 ) = 2 . 612:

| Variable   |   T 2 p |   (ν - p ) T 2 p + 1 - T ν + T 2 p |
|------------|---------|------------------------------------|
| y 1        | 78.8733 |                              7.844 |
| y 2        | 90.8348 |                              2.612 |
| y 3        | 32.6253 |                             40.513 |
| y          | 74.5926 |                              9.938 |

4

When we compare these four test statistic values with the critical value t 2 . 025 , 59 = 4 . 002, we see that each variable except y 2 makes a significant contribution to T 2 . Note that y 3 contributes most, followed by y 4 and then y 1. This order differs from that given by the raw discriminant function in Example 5.5 but agrees with the order for the standardized discriminant function given in the answer to Problem 8.7 in Appendix B.

## 5.9 PROFILE ANALYSIS

If y is Np ( 𝛍 , 𝚺 ) and the variables in y are commensurate (measured in the same units and with approximately equal variances as, for example, in the probe word data in Table 3.5), we may wish to compare the means µ 1, µ 2 , . . . , µ p in 𝛍 . This might be of interest when a measurement is taken on the same research unit at p successive times. Such situations are often referred to as repeated measures designs or growth curves , which are discussed in some generality in Sections 6.9 and 6.10. In the present section, we discuss one- and two-sample profile analysis . Profile analysis for several samples is covered in Section 6.8.

The pattern obtained by plotting µ 1, µ 2 , . . . , µ p as ordinates and connecting the points is called a profile ; we usually draw straight lines connecting the points ( 1 , µ 1 ) , ( 2 , µ 2 ), . . . , ( p , µ p ) . Profile analysis is an analysis of the profile or a comparison of two or more profiles. Profile analysis is often discussed in the context of administering a battery of p psychological or other tests.

In growth curve analysis, where the variables are measured at time intervals, the responses have a natural order. In profile analysis where the variables arise from test scores, there is ordinarily no natural order. A distinction is not always made between repeated measures of the same variable through time and profile analysis of several different commensurate variables on the same individual.

## 5.9.1 One-Sample Profile Analysis

We begin with a discussion of the profile of the mean vector 𝛍 from a single sample. A plot of 𝛍 might appear as in Figure 5.3, where we plot ( 1 , µ 1 ) , ( 2 , µ 2 ), . . . , ( p , µ p ) and connect the points.

or as

Figure 5.3. Profile of a mean vector.

<!-- image -->

In order to compare the means µ 1, µ 2 , . . . , µ p in 𝛍 , the basic hypothesis is that the profile is level or flat :

<!-- formula-not-decoded -->

The data matrix Y is given in (3.17). We cannot use univariate analysis of variance to test H 0 because the columns in Y are not independent. For a multivariate approach that allows for correlated variables, we first express H 0 as p -1 comparisons,

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

These two expressions can be written in the form H 0 : C 1 𝛍 = 0 and H 0 : C 2 𝛍 = 0 , where C 1 and C 2 are the ( p -1 ) × p matrices:

<!-- formula-not-decoded -->

In fact, any ( p -1 ) × p matrix C of rank p -1 such that Cj = 0 can be used in H 0 : C 𝛍 = 0 to produce H 0 : µ 1 = µ 2 = · · · = µ p . If Cj = 0 , each row c ′ i of C sums to zero by (2.38). A linear combination c ′ i 𝛍 = ci 1 µ 1 + ci 2 µ 2 +··· + cip µ p is called a contrast in the µ 's if the coefficients sum to zero, that is, if ∑ j ci j = 0. The p -1 contrasts in C 𝛍 must be linearly independent in order to express H 0 : µ 1 = µ 2 = · · · = µ p as H 0 : C 𝛍 = 0 . Thus rank ( C ) = p -1.

From a sample y 1, y 2 , . . . , y n , we obtain estimates y and S of population parameters 𝛍 and 𝚺 . To test H 0 : C 𝛍 = 0 , we transform each y i , i = 1, 2 , . . . , n , to z i = Cy i , which is ( p -1 ) × 1. By (3.62) and (3.64), the sample mean vector and covariance matrix of z i = Cy i , i = 1, 2 , . . . , n , are z = Cy and S z = CSC ′ , respectively. If y is Np ( 𝛍 , 𝚺 ) , then by property 1b in Section 4.2, z = Cy is Np -1 ( C 𝛍 , C 𝚺 C ′ ) . Thus when H 0 : C 𝛍 = 0 is true, Cy is Np -1 ( 0 , C 𝚺 C ′ / n ) , and

<!-- formula-not-decoded -->

is distributed as T 2 p -1 , n -1 . We reject H 0 : C 𝛍 = 0 if T 2 ≥ T 2 α, p -1 , n -1 . The dimension p -1 corresponds to the number of rows of C . Thus z = Cy is ( p -1 ) × 1 and S z = CSC ′ is ( p -1 ) × ( p -1 ) . Note that the C 's in (5.33) don't 'cancel' because C is ( p -1 ) × p and does not have an inverse. In fact, T 2 in (5.33) is less than T 2 = n y ′ S -1 y [see Rencher (1998, p. 84)].

If the variables have a natural ordering, as, for example, in the ramus bone data in Table 3.6, we could test for a linear trend or polynomial curve in the means by suitably choosing the rows of C . This is discussed in connection with growth curves in Section 6.10. Other comparisons of interest can be made as long as they are linearly independent.

## 5.9.2 Two-Sample Profile Analysis

Suppose two independent groups or samples receive the same set of p tests or measurements. If these tests are comparable, for example, all on a scale of 0 to 100, the variables will often be commensurate.

Rather than testing the hypothesis that 𝛍 1 = 𝛍 2, we wish to be more specific in comparing the profiles obtained by connecting the points ( j , µ 1 j ), j = 1 , 2 , . . . , p , and ( j , µ 2 j ) , j = 1 , 2 , . . . , p . There are three hypotheses of interest in comparing the profiles of two samples. The first of these hypotheses addresses the question, Are the two profiles similar in appearance, or more precisely, are they parallel? We illustrate this hypothesis in Figure 5.4. If the two profiles are parallel, then one group scored uniformly better than the other group on all p tests.

The parallelism hypothesis can be defined in terms of the slopes. The two profiles are parallel if the two slopes for each segment are the same. If the two profiles are parallel, the two increments for each segment are the same, and it is not necessary to use the actual slopes to express the hypothesis. We can simply compare the increase from one point to the next. The hypothesis can thus be expressed as H 01 : µ 1 j -

Figure 5.4. Comparison of two profiles under the hypothesis of parallelism.

<!-- image -->

<!-- formula-not-decoded -->

which can be written as H 01 : C 𝛍 1 = C 𝛍 2, using the contrast matrix

<!-- formula-not-decoded -->

From two samples, y 11, y 12 , . . . , y 1 n 1 and y 21, y 22 , . . . , y 2 n 2 , we obtain y 1 , y 2 , and S pl as estimates of 𝛍 1, 𝛍 2, and 𝚺 . As in the two-sample T 2 -test, we assume that each y 1 i in the first sample is Np ( 𝛍 1 , 𝚺 ) , and each y 2 i in the second sample is Np ( 𝛍 2 , 𝚺 ) . If C is a ( p -1 ) × p contrast matrix, as before, then Cy 1 i and Cy 2 i are distributed as Np -1 ( C 𝛍 1 , C 𝚺 C ′ ) and Np -1 ( C 𝛍 2 , C 𝚺 C ′ ) , respectively. Under H 01 : C 𝛍 1 -C 𝛍 2 = 0 , the random vector Cy 1 -Cy 2 is Np -1 [ 0 , C 𝚺 C ′ ( 1 / n 1 + 1 / n 2 ) ] , and

<!-- formula-not-decoded -->

is distributed as T 2 p -1 , n 1 + n 2 -2 . Note that the dimension p -1 is the number of rows of C .

By analogy with the discussion in Section 5.5, if H 01 is rejected, we can follow up with univariate tests on the individual components of C ( y 1 -y 2 ) . Alternatively, we can calculate the discriminant function

<!-- formula-not-decoded -->

as an indication of which slope differences contributed most to rejection of H 01 in the presence of the other components of C ( y 1 -y 2 ) . There should be less need in this case to standardize the components of a , as suggested in Section 5.5, because the variables are assumed to be commensurate. The vector a is ( p -1 ) × 1, corresponding to the p -1 segments of the profile. Thus if the second component of a , for example, is largest in absolute value, the divergence in slopes between the two profiles on the second segment contributes most to rejection of H 01.

If the data are arranged as in Table 5.4, we see an analogy to a two-way ANOV A model. A plot of the means is often made in a two-way ANOVA; a lack of parallelism corresponds to interaction between the two factors. Thus the hypothesis H 01 is analogous to the group by test (variable) interaction hypothesis.

However, the usual ANOVA assumption of independence of observations does not hold here because the variables (tests) are correlated. The ANOV A assumption of independence and homogeneity of variances would require cov ( y ) = 𝚺 = σ 2 I . Hence the test of H 01 cannot be carried out using a univariate ANOVA approach, since 𝚺 /negationslash= σ 2 I . We therefore proceed with the multivariate approach using T 2 .

The second hypothesis of interest in comparing two profiles is, Are the two populations or groups at the same level ? This hypothesis corresponds to a group (population) main effect in the ANOVA analogy. We can express this hypothesis in terms of the average level of group 1 compared to the average level of group 2:

<!-- formula-not-decoded -->

Table 5.4. Data Layout for Two-Sample Profile Analysis

|              |         | Tests (variables)   | Tests (variables)   | Tests (variables)   | Tests (variables)   |
|--------------|---------|---------------------|---------------------|---------------------|---------------------|
|              |         | 1                   | 2                   | · · ·               | p                   |
| Group 1      | Group 1 | Group 1             | Group 1             | Group 1             | Group 1             |
| y ′ 11       | =       | ( y 111             | y 112               | · · ·               | y 11 p )            |
| y ′ 12 . . . | =       | ( y 121 . . .       | y 122 . . .         | · · ·               | y 12 p ) . . .      |
| y ′ 1 n 1    | =       | ( y 1 n 1 1         | y 1 n 1 2           | · · ·               | y 1 n 1 p )         |
| Group 2      | Group 2 | Group 2             | Group 2             | Group 2             | Group 2             |
| y ′ 21       | =       | ( y 211             | y 212               | · · ·               | y 21 p )            |
| y ′ 22 . . . | =       | ( y 221 . . .       | y 222 . . .         | · · ·               | y 22 p ) . . .      |
| y ′ 2 n 2    | =       | ( y 2 n 2 1         | y 2 n 2 2           | · · ·               | y 2 n 2 p )         |

Figure 5.5. Hypothesis H 02 of equal group effect, assuming parallelism.

<!-- image -->

## By (2.37), this can be expressed as

H 02 : j ′ 𝛍 1 = j ′ 𝛍 2 .

If H 01 is true, H 02 can be pictured as in Figure 5.5 a . If H 02 is false, then the two profiles differ by a constant (given that H 01 is true), as in Figure 5.5 b .

The hypothesis H 02 can be true when H 01 does not hold. Thus the average level of population 1 can equal the average level of population 2 without the two profiles being parallel, as illustrated in Figure 5.6. In this case, the 'group main effect' is somewhat harder to interpret, as is the case in the analogous two-way ANOV A, where main effects are more difficult to describe in the presence of significant interaction. However, the test may still furnish useful information if a careful description of the results is provided.

Figure 5.6. Hypothesis H 02 of equal group effect without parallelism.

<!-- image -->

To test H 02 : j ′ ( 𝛍 1 -𝛍 2 ) = 0, we estimate j ′ ( 𝛍 1 -𝛍 2 ) by j ′ ( y 1 -y 2 ) , which is N [ 0 , j ′ 𝚺 j ( 1 / n 1 + 1 / n 2 ) ] when H 02 is true. We can therefore use

<!-- formula-not-decoded -->

The third hypothesis of interest, corresponding to the test (or variable) main effect, is, Are the profiles flat? Assuming parallelism (assuming H 01 is true), the 'flatness' hypothesis can be pictured as in Figure 5.7. If H 01 is not true, the test could be carried out separately for each group using the test in Section 5.9.1. If H 02 is true, the two profiles in Figure 5.7 a and Figure 5.7 b will be coincident.

and reject H 02 if | t | ≥ t α/ 2 , n 1 + n 2 -2.

To express the third hypothesis in a form suitable for testing, we note from Figure 5.7 a that the average of the two group means is the same for each test:

<!-- formula-not-decoded -->

or

<!-- formula-not-decoded -->

where C is a ( p -1 ) × p matrix such that Cj = 0 . From Figure 5.7 a , we see that H 03 could also be expressed as µ 11 = µ 12 = · · · = µ 1 p and µ 21 = µ 22 = · · · = µ 2 p , or

<!-- formula-not-decoded -->

To estimate 1 2 ( 𝛍 1 + 𝛍 2 ) , we use the sample grand mean vector based on a weighted average:

Figure 5.7. Hypothesis H 03 of equal tests (variables) assuming parallelism.

<!-- image -->

It can easily be shown that under H 03 (and H 01), E ( Cy ) = 0 and cov ( y ) = 𝚺 /( n 1 + n 2 ) . Therefore, Cy is Np -1 [ 0 , C 𝚺 C ′ /( n 1 + n 2 ) ] , and

<!-- formula-not-decoded -->

is distributed as T 2 p -1 , n 1 + n 2 -2 when both H 01 and H 03 are true. It can be readily shown that H 03 is unaffected by a difference in the profile levels (unaffected by the status of H 02).

Example 5.9.2. We use the psychological data in Table 5.1 to illustrate two-sample profile analysis. The values of y 1 , y 2 , and S pl are given in Example 5.4.2. The profiles of the two mean vectors y 1 and y 2 are plotted in Figure 5.8. There appears to be a lack of parallelism.

To test for parallelism, H 01 : C 𝛍 1 = C 𝛍 2, we use the matrix

<!-- formula-not-decoded -->

Figure 5.8. Profiles for the psychological data in Table 5.1.

<!-- image -->

and obtain

<!-- formula-not-decoded -->

Then, by (5.34),

<!-- formula-not-decoded -->

Upon comparison of this value with T 2 . 01 , 3 , 62 = 12 . 796 (obtained by interpolation in Table A.7), we reject the hypothesis of parallelism.

In Figure 5.8 the lack of parallelism is most notable in the second and third segments. This can also be seen in the relatively large values of the second and third components of

<!-- formula-not-decoded -->

To see which of these made the greatest statistical contribution, we can examine the discriminant function coefficient vector given in (5.35) as

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Thus the third segment contributed most to rejection in the presence of the other two segments.

To test for equal levels, H 02 : j ′ 𝛍 1 = j ′ 𝛍 2, we use (5.36),

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Comparing this with t . 005 , 62 = 2 . 658, we reject the hypothesis of equal levels. To test the flatness hypothesis, H 03 : 1 2 C ( 𝛍 1 + 𝛍 2 ) = 0 , we first calculate

<!-- formula-not-decoded -->

Using we obtain, by (5.39),

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

which exceeds T 2 . 01 , 3 , 62 = 12 . 796, so we reject the hypothesis of flatness. However, since the parallelism hypothesis was rejected, a more appropriate approach would be to test each of the two groups separately for flatness using the test of Section 5.9.1. By (5.33), we obtain

<!-- formula-not-decoded -->

Both of these exceed T 2 . 01 , 3 . 31 = 14 . 626, and we have significant lack of flatness.

## PROBLEMS

- 5.1 Show that the characteristic form of T 2 in (5.6) is the same as the original form in (5.5).
- 5.2 Show that the T 2 -statistic in (5.9) can be expressed in the characteristic form given in (5.10).
- 5.3 Show that t 2 ( a ) = T 2 , where t ( a ) is given by (5.13), T 2 is given by (5.9), and a S -1 pl ( y 1 y 2 ) as in (5.14).
- = -
- 5.4 Show that the paired observation t -test in (5.22), t = d /( sd / √ n ) , has the tn -1 distribution.
- 5.5 Show that s 2 d = ∑ n i = 1 ( di -d ) 2 /( n -1 ) = s 2 y + s 2 x -2 syx , as in a comparison of (5.22) and (5.23).
- 5.7 Use (5.7) to show that T 2 ( x | y ) in (5.29) can be converted to F as in (5.30).
- 5.6 Showthat T 2 = n d ′ S -1 d d in (5.24) has the characteristic form T 2 = d ′ ( S d / n ) -1 d .
- 5.8 Show that the test statistic in (5.30) for additional information in x above and beyond y has an F -distribution by solving for R 2 in terms of T 2 from (5.20) and substituting this into (5.31).
- 5.9 In Section 5.9.2, show that under H 03 and H 01, E ( Cy ) = 0 and cov ( y ) = 𝚺 /( n 1 + n 2 ) , where y = ( n 1 y 1 + n 2 y 2 )/( n 1 + n 2 ) and 𝚺 is the common covariance matrix of the two populations from which y 1 and y 2 are sampled.

- 5.10 Verify that T 2 = ( n 1 + n 2 )( Cy ) ′ ( CS pl C ′ ) -1 Cy in (5.39) has the T 2 p -1 , n 1 + n 2 -2 distribution.
- 5.11 Test H 0 : 𝛍 ′ = ( 6 , 11 ) using the data

<!-- formula-not-decoded -->

- 5.12 Use the probe word data in Table 3.5:
- (a) Test H 0 : 𝛍 = ( 30 , 25 , 40 , 25 , 30 ) ′ .
- (b) If H 0 is rejected, test each variable separately, using (5.3).
- 5.13 For the probe word data in Table 3.5, test H 0 : µ 1 = µ 2 = · · · = µ 5, using T 2 in (5.33).
- 5.14 Use the ramus bone data in Table 3.6:
- (a) Test H 0 : 𝛍 = ( 48 , 49 , 50 , 51 ) ′ .
- (b) If H 0 is rejected, test each variable separately, using (5.3).
- 5.15 For the ramus bone data in Table 3.6, test H 0 : µ 1 = µ 2 = µ 3 = µ 4, using T 2 in (5.33).
- 5.16 Four measurements were made on two species of flea beetles (Lubischew 1962). The variables were
- y 1 = distance of transverse groove from posterior border of prothorax ( µ m),

y 3 = length of second antennal joint ( µ m), y 2 = length of elytra (in .01 mm),

y 4 = length of third antennal joint ( µ m).

The data are given in Table 5.5.

- (a) Test H 0 : 𝛍 1 = 𝛍 2 using T 2 .
- (b) If the T 2 -test in part (a) rejects H 0, carry out a t -test on each variable, as in (5.15).
- (c) Calculate the discriminant function coefficient vector a = S -1 pl ( y 1 -y 2 ) .
- (d) Show that if the vector a found in part (c) is substituted into t 2 ( a ) from (5.13), the result is the same as the value of T 2 found in part (a).
- (e) Obtain T 2 using the regression approach in Section 5.6.2.
- (f) Test the significance of each variable adjusted for the other three.
- (g) Test the significance of y 3 and y 4 adjusted for y 1 and y 2.
- 5.17 Carry out a profile analysis on the beetles data in Table 5.5.

Table 5.5. Four Measurements on Two Species of Flea Beetles

| Haltica oleracea   | Haltica oleracea   | Haltica oleracea   | Haltica oleracea   | Haltica oleracea   | Haltica carduorum   | Haltica carduorum   | Haltica carduorum   | Haltica carduorum   | Haltica carduorum   |
|--------------------|--------------------|--------------------|--------------------|--------------------|---------------------|---------------------|---------------------|---------------------|---------------------|
| Experiment Number  | y 1                | y 2                | y 3                | y 4                | Experiment Number   | y 1                 | y 2                 | y 3                 | y 4                 |
| 1                  | 189                | 245                | 137                | 163                | 1                   | 181                 | 305                 | 184                 | 209                 |
| 2                  | 192                | 260                | 132                | 217                | 2                   | 158                 | 237                 | 133                 | 188                 |
| 3                  | 217                | 276                | 141                | 192                | 3                   | 184                 | 300                 | 166                 | 231                 |
| 4                  | 221                | 299                | 142                | 213                | 4                   | 171                 | 273                 | 162                 | 213                 |
| 5                  | 171                | 239                | 128                | 158                | 5                   | 181                 | 297                 | 163                 | 224                 |
| 6                  | 192                | 262                | 147                | 173                | 6                   | 181                 | 308                 | 160                 | 223                 |
| 7                  | 213                | 278                | 136                | 201                | 7                   | 177                 | 301                 | 166                 | 221                 |
| 8                  | 192                | 255                | 128                | 185                | 8                   | 198                 | 308                 | 141                 | 197                 |
| 9                  | 170                | 244                | 128                | 192                | 9                   | 180                 | 286                 | 146                 | 214                 |
| 10                 | 201                | 276                | 146                | 186                | 10                  | 177                 | 299                 | 171                 | 192                 |
| 11                 | 195                | 242                | 128                | 192                | 11                  | 176                 | 317                 | 166                 | 213                 |
| 12                 | 205                | 263                | 147                | 192                | 12                  | 192                 | 312                 | 166                 | 209                 |
| 13                 | 180                | 252                | 121                | 167                | 13                  | 176                 | 285                 | 141                 | 200                 |
| 14                 | 192                | 283                | 138                | 183                | 14                  | 169                 | 287                 | 162                 | 214                 |
| 15                 | 200                | 294                | 138                | 188                | 15                  | 164                 | 265                 | 147                 | 192                 |
| 16                 | 192                | 277                | 150                | 177                | 16                  | 181                 | 308                 | 157                 | 204                 |
| 17                 | 200                | 287                | 136                | 173                | 17                  | 192                 | 276                 | 154                 | 209                 |
| 18                 | 181                | 255                | 146                | 183                | 18                  | 181                 | 278                 | 149                 | 235                 |
| 19                 | 192                | 287                | 141                | 198                | 19                  | 175                 | 271                 | 140                 | 192                 |
|                    |                    |                    |                    |                    | 20                  | 197                 | 303                 | 170                 | 205                 |

## 5.18 Twenty engineer apprentices and 20 pilots were given six tests (Travers 1939). The variables were

y 1 = intelligence,

y 2 = form relations,

y 3 = dynamometer,

y 4 = dotting,

y 5 = sensory motor coordination,

y 6 = perseveration.

The data are given in Table 5.6.

- (a) Test H 0 : 𝛍 1 = 𝛍 2.
- (b) If the T 2 -test in part (a) rejects H 0, carry out a t -test for each variable, as in (5.15).
- (c) Test each variable adjusted for the other five.
- (d) Test the significance of y 4, y 5, y 6 adjusted for y 1, y 2, y 3.

Table 5.6. Comparison of Six Tests on Engineer Apprentices and Pilots

| Engineer   | Engineer   | Engineer   | Engineer   | Engineer   | Engineer   | Pilots   | Pilots   | Pilots   | Pilots   | Pilots   | Pilots   |
|------------|------------|------------|------------|------------|------------|----------|----------|----------|----------|----------|----------|
| y 1        | y 2        | y 3        | y 4        | y 5        | y 6        | y 1      | y 2      | y 3      | y 4      | y 5      | y 6      |
| 121        | 22         | 74         | 223        | 54         | 254        | 132      | 17       | 77       | 232      | 50       | 249      |
| 108        | 30         | 80         | 175        | 40         | 300        | 123      | 32       | 79       | 192      | 64       | 315      |
| 122        | 49         | 87         | 266        | 41         | 223        | 129      | 31       | 96       | 250      | 55       | 319      |
| 77         | 37         | 66         | 178        | 80         | 209        | 131      | 23       | 67       | 291      | 48       | 310      |
| 140        | 35         | 71         | 175        | 38         | 261        | 110      | 24       | 96       | 239      | 42       | 268      |
| 108        | 37         | 57         | 241        | 59         | 245        | 47       | 22       | 87       | 231      | 40       | 217      |
| 124        | 39         | 52         | 194        | 72         | 242        | 125      | 32       | 87       | 227      | 30       | 324      |
| 130        | 34         | 89         | 200        | 85         | 242        | 129      | 29       | 102      | 234      | 58       | 300      |
| 149        | 55         | 91         | 198        | 50         | 277        | 130      | 26       | 104      | 256      | 58       | 270      |
| 129        | 38         | 72         | 162        | 47         | 268        | 147      | 47       | 82       | 240      | 30       | 322      |
| 154        | 37         | 87         | 170        | 60         | 244        | 159      | 37       | 80       | 227      | 58       | 317      |
| 145        | 33         | 88         | 208        | 51         | 228        | 135      | 41       | 83       | 216      | 39       | 306      |
| 112        | 40         | 60         | 232        | 29         | 279        | 100      | 35       | 83       | 183      | 57       | 242      |
| 120        | 39         | 73         | 159        | 39         | 233        | 149      | 37       | 94       | 227      | 30       | 240      |
| 118        | 21         | 83         | 152        | 88         | 233        | 149      | 38       | 78       | 258      | 42       | 271      |
| 141        | 42         | 80         | 195        | 36         | 241        | 153      | 27       | 89       | 283      | 66       | 291      |
| 135        | 49         | 73         | 152        | 42         | 249        | 136      | 31       | 83       | 257      | 31       | 311      |
| 151        | 37         | 76         | 223        | 74         | 268        | 97       | 36       | 100      | 252      | 30       | 225      |
| 97         | 46         | 83         | 164        | 31         | 243        | 141      | 37       | 105      | 250      | 27       | 243      |
| 109        | 42         | 82         | 188        | 57         | 267        | 164      | 32       | 76       | 187      | 30       | 264      |

- 5.19 Data were collected in an attempt to find a screening procedure to detect carriers of Duchenne muscular dystrophy, a disease transmitted from female carriers to some of their male offspring (Andrews and Herzberg 1985, pp. 223-228). The following variables were measured on a sample of noncarriers and a sample of carriers:

y 1 = age, y 3 = creatine kinase,

y 2 = month in which measurements are taken, y 4 = hemopexin,

y 6 = pyruvate kinase.

y 5 = lactate dehydrogenase,

The data are given in Table 5.7.

- (a) Test H 0 : 𝛍 1 = 𝛍 2 using y 3, y 4, y 5, and y 6.
- (b) The variables y 3 and y 4 are relatively inexpensive to measure compared to y 5 and y 6. Do y 5 and y 6 contribute an important amount to T 2 above and beyond y 3 and y 4?

Table 5.7.  Comparison of Carriers and Noncarriers of Muscular Dystrophy

## Publisher's Note:

Permission to reproduce this image online was not granted by the copyright holder. Readers are kindly asked to refer to the printed version of this chapter.

- (c) The levels of y 3, y 4, y 5, and y 6 may depend on age and season, y 1 and y 2. Do y 1 and y 2 contribute a significant amount to T 2 when adjusted for y 3, y 4, y 5, and y 6?
- 5.20 Various aspects of economic cycles were measured for consumers' goods and producers' goods by Tintner (1946). The variables are

y 1 = length of cycle, y 3 = cyclical amplitude,

y 2 = percentage of rising prices, y 4 = rate of change.

The data for several items are given in Table 5.8.

Table 5.8. Cyclical Measurements of Consumer Goods and Producer Goods

| Item   | y 1            | y 2            | y 3            | y 4   | Item           | y 1            | y 2            | y 3            | y 4            |
|--------|----------------|----------------|----------------|-------|----------------|----------------|----------------|----------------|----------------|
|        | Consumer Goods | Consumer Goods | Consumer Goods |       | Producer Goods | Producer Goods | Producer Goods | Producer Goods | Producer Goods |
| 1      | 72             | 50             | 8              | .5    | 1              | 57             | 57             | 12.5           | .9             |
| 2      | 66.5           | 48             | 15             | 1.0   | 2              | 100            | 54             | 17             | .5             |
| 3      | 54             | 57             | 14             | 1.0   | 3              | 100            | 32             | 16.5           | .7             |
| 4      | 67             | 60             | 15             | .9    | 4              | 96.5           | 65             | 20.5           | .9             |
| 5      | 44             | 57             | 14             | .3    | 5              | 79             | 51             | 18             | .9             |
| 6      | 41             | 52             | 18             | 1.9   | 6              | 78.5           | 53             | 18             | 1.2            |
| 7      | 34.5           | 50             | 4              | .5    | 7              | 48             | 50             | 21             | 1.6            |
| 8      | 34.5           | 46             | 8.5            | 1.0   | 8              | 155            | 44             | 20.5           | 1.4            |
| 9      | 24             | 54             | 3              | 1.2   | 9              | 84             | 64             | 13             | .8             |
|        |                |                |                |       | 10             | 105            | 35             | 17             | 1.8            |

- (a) Test H 0 : 𝛍 1 = 𝛍 2 using T 2 .
- (b) Calculate the discriminant function coefficient vector.
- (c) Test for significance of each variable adjusted for the other three.
- 5.21 Each of 15 students wrote an informal and a formal essay (Kramer 1972, p. 100). The variables recorded were the number of words and the number of verbs:
- y 1 = number of words in the informal essay,

x 1 = number of words in the formal essay, y 2 = number of verbs in the informal essay,

x 2 = number of verbs in the formal essay.

Table 5.9. Number of Words and Number of Verbs

|         | Informal   | Informal   | Formal    | Formal    |             |                 |
|---------|------------|------------|-----------|-----------|-------------|-----------------|
| Student | Words y 1  | Verbs y 2  | Words x 1 | Verbs x 2 | d 1 = y 1 - | d 2 = y 2 - x 2 |
| 1       | 148        | 20         | 137       | 15        | + 11        | + 5             |
| 2       | 159        | 24         | 164       | 25        | - 5         | - 1             |
| 3       | 144        | 19         | 224       | 27        | - 80        | - 8             |
| 4       | 103        | 18         | 208       | 33        | - 105       | - 15            |
| 5       | 121        | 17         | 178       | 24        | - 57        | - 7             |
| 6       | 89         | 11         | 128       | 20        | - 39        | - 9             |
| 7       | 119        | 17         | 154       | 18        | - 35        | - 1             |
| 8       | 123        | 13         | 158       | 16        | - 35        | - 3             |
| 9       | 76         | 16         | 102       | 21        | - 26        | - 5             |
| 10      | 217        | 29         | 214       | 25        | + 3         | + 4             |
| 11      | 148        | 22         | 209       | 24        | - 61        | - 2             |
| 12      | 151        | 21         | 151       | 16        | 0           | + 5             |
| 13      | 83         | 7          | 123       | 13        | - 40        | - 6             |
| 14      | 135        | 20         | 161       | 22        | - 26        | - 2             |
| 15      | 178        | 15         | 175       | 23        | 3           | 8               |

+

-

Table 5.10. Survival Times for Bronchus Cancer Patients and Matched Controls

| Ascorbate Patients   | Ascorbate Patients   | Matched Controls   | Matched Controls   |
|----------------------|----------------------|--------------------|--------------------|
| y 1                  | y 2                  | x 1                | x 2                |
| 81                   | 74                   | 72                 | 33                 |
| 461                  | 423                  | 134                | 18                 |
| 20                   | 16                   | 84                 | 20                 |
| 450                  | 450                  | 98                 | 58                 |
| 246                  | 87                   | 48                 | 13                 |
| 166                  | 115                  | 142                | 49                 |
| 63                   | 50                   | 113                | 38                 |
| 64                   | 50                   | 90                 | 24                 |
| 155                  | 113                  | 30                 | 18                 |
| 151                  | 38                   | 260                | 34                 |
| 166                  | 156                  | 116                | 20                 |
| 37                   | 27                   | 87                 | 27                 |
| 223                  | 218                  | 69                 | 32                 |
| 138                  | 138                  | 100                | 27                 |
| 72                   | 39                   | 315                | 39                 |
| 245                  | 231                  | 188                | 65                 |

The data are given in Table 5.9. Since each student wrote both types of essays, the observation vectors are paired, and we use the paired comparison test.

- (a) Test H 0 : 𝛍 d = 0 .
- (b) Find the discriminant function coefficient vector.
- (c) Do a univariate t -test on each dj .
- 5.22 A number of patients with bronchus cancer were treated with ascorbate and compared with matched patients who received no ascorbate (Cameron and Pauling 1978). The data are given in Table 5.10. The variables measured were
- y 1 , x 1 = survival time (days) from date of first hospital admission,
- y 2 , x 2 = survival time from date of untreatability.

Compare y 1 and y 2 with x 1 and x 2 using a paired comparison T 2 -test.

- 5.23 Use the glucose data in Table 3.8:
- (a) Test H 0 : 𝛍 y = 𝛍 x using a paired comparison test.
- (b) Test the significance of each variable adjusted for the other two.

## C H A P T E R 6

## Multivariate Analysis of Variance

In this chapter we extend univariate analysis of variance to multivariate analysis of variance, in which we measure more than one variable on each experimental unit. For multivariate analysis of covariance, see Rencher (1998, Section 4.10).

## 6.1 ONE-WAY MODELS

We begin with a review of univariate analysis of variance (ANOV A) before covering multivariate analysis of variance (MANOVA) with several dependent variables.

## 6.1.1 Univariate One-Way Analysis of Variance (ANOVA)

In the balanced one-way ANOVA, we have a random sample of n observations from each of k normal populations with equal variances, as in the following layout:

|          | Sample 1 from N (µ 1 ,σ 2 )   | Sample 2 from N (µ 2 ,σ 2 )   | . . .       | Sample k from N (µ k ,σ 2 )   |
|----------|-------------------------------|-------------------------------|-------------|-------------------------------|
|          | y 11 y 12 . . .               | y 21 y 22 . . .               |             |                               |
|          |                               | y 2 n                         | · · · · · · | y k 1 y k 2 . . .             |
|          | y 1                           |                               | · · ·       | y kn                          |
|          | n                             |                               |             |                               |
| Total    | y 1 .                         | y 2 .                         | · · ·       | y k .                         |
| Mean     | y 1 . 2                       | y 2 . 2                       | · · ·       | y k . 2                       |
| Variance | s 1                           | s 2                           |             | s k                           |

· · ·

The k samples or the populations from which they arise are sometimes referred to as groups . The groups may correspond to treatments applied by the researcher in an experiment. We have used the 'dot' notation for totals and means for each group:

<!-- formula-not-decoded -->

The k samples are assumed to be independent. The assumptions of independence and common variance are necessary to obtain an F -test.

The model for each observation is

<!-- formula-not-decoded -->

where µ i = µ + α i is the mean of the i th population. We wish to compare the sample means y i . , i = 1, 2 , . . . , k , to see if they are sufficiently different to lead us to believe the population means differ. The hypothesis can be expressed as H 0 : µ 1 = µ 2 = · · · = µ k . Note that the notation for subscripts differs from that of previous chapters, in which the subscript i represented the observation. In this chapter, we use the last subscript in a model such as (6.2) to represent the observation.

If the hypothesis is true, all yi j are from the same population, N (µ, σ 2 ) , and we can obtain two estimates of σ 2 , one based on the sample variances s 2 1 , s 2 2 , . . . , s 2 k [see (3.4) and (3.5)] and the other based on the sample means y 1 . , y 2 . , . . . , y k . . The pooled 'within-sample' estimator of σ 2 is

<!-- formula-not-decoded -->

Our second estimate of σ 2 (under H 0) is based on the variance of the sample means,

<!-- formula-not-decoded -->

where y .. = ∑ k i = 1 y i . / k is the overall mean. If H 0 is true, s 2 y estimates σ 2 y = σ 2 / n [see remarks following (3.1) in Section 3.1], and therefore E ( ns 2 y ) = n (σ 2 / n ) = σ 2 , from which the estimate of σ 2 is

<!-- formula-not-decoded -->

where y .. = ∑ i yi . = ∑ i j yi j is the overall total. If H 0 is false, E ( ns 2 y ) = σ 2 + n ∑ i α 2 i /( k -1 ) , and ns 2 y will tend to reflect a larger spread in y 1 . , y 2 . , . . . , y k . . Since s 2 e is based on variability within each sample, it estimates σ 2 whether or not H 0 is true; thus E ( s 2 e ) = σ 2 in either case.

When sampling from normal distributions, s 2 e , a pooled estimator based on the k values of s 2 i , is independent of s 2 y , which is based on the y i . 's. We can justify this assertion by noting that y i . and s 2 i are independent in each sample (when sampling from the normal distribution) and that the k samples are independent of each other.

Since ns 2 y and s 2 e are independent and both estimate σ 2 , their ratio forms an F -statistic (see Section 7.3.1):

<!-- formula-not-decoded -->

where SSH = ∑ i y 2 i . / n -y 2 .. / kn and SSE = ∑ i j y 2 i j -∑ i y 2 i . / n are the 'between'sample sum of squares (due to the means) and 'within'-sample sum of squares, respectively, and MSH and MSE are the corresponding sample mean squares. The F -statistic (6.6) is distributed as Fk -1 , k ( n -1 ) when H 0 is true. We reject H 0 if F &gt; F α . The F -statistic (6.6) can be shown to be a simple function of the likelihood ratio.

## 6.1.2 Multivariate One-Way Analysis of Variance Model (MANOVA)

We often measure several dependent variables on each experimental unit instead of just one variable. In the multivariate case, we assume that k independent random samples of size n are obtained from p -variate normal populations with equal covariance matrices, as in the following layout for balanced one-way multivariate analysis of variance. (In practice, the observation vectors y i j would ordinarily be listed in row form, and sample 2 would appear below sample 1, and so on. See, for example, Table 6.2.)

|       | Sample 1 from N p ( 𝛍 1 , 𝚺 )   | Sample 2 from N p ( 𝛍 2 , 𝚺 )   | . . .   | Sample k from N p ( 𝛍 k , 𝚺 )   |
|-------|---------------------------------|---------------------------------|---------|---------------------------------|
|       | y 11 y 12 . . . y 1 n           | y 21 y 22 . . . y 2 n           | · · ·   | y k 1 y k 2 . . . y kn          |
|       |                                 |                                 | · · ·   |                                 |
|       |                                 |                                 | · · ·   |                                 |
| Total | y 1 .                           | y 2 .                           | · · ·   | y k .                           |
| Mean  | y 1 .                           | y 2 .                           |         | y k .                           |

Totals and means are defined as follows:

Total of the i th sample:

y i . = ∑ = y .. = ∑ k i = 1 ∑ n j = 1 y i j .

n

j

1

y

i j

.

Overall total:

Mean of the i

th sample: y i . = y i ./ n .

Overall mean:

y .. = y ../ kn .

· · ·

The model for each observation vector is

<!-- formula-not-decoded -->

In terms of the p variables in y i j , (6.8) becomes

<!-- formula-not-decoded -->

so that the model for the r th variable ( r = 1 , 2 , . . . , p ) in each vector y i j is

<!-- formula-not-decoded -->

We wish to compare the mean vectors of the k samples for significant differences. The hypothesis is, therefore,

<!-- formula-not-decoded -->

Equality of the mean vectors implies that the k means are equal for each variable; that is, µ 1 r = µ 2 r = · · · = µ kr for r = 1 , 2 , . . . , p . If two means differ for just one variable, for example, µ 23 /negationslash= µ 43, then H 0 is false and we wish to reject it. We can see this by examining the elements of the population mean vectors:

<!-- formula-not-decoded -->

Thus H 0 implies p sets of equalities:

<!-- formula-not-decoded -->

All p ( k -1 ) equalities must hold for H 0 to be true; failure of only one equality will falsify the hypothesis.

In the univariate case, we have 'between' and 'within' sums of squares SSH and SSE. By (6.3), (6.5), and (6.6), these are given by

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

By analogy, in the multivariate case, we have 'between' and 'within' matrices H and E , defined as

<!-- formula-not-decoded -->

The p × p 'hypothesis' matrix H has a between sum of squares on the diagonal for each of the p variables. Off-diagonal elements are analogous sums of products for each pair of variables. Assuming there are no linear dependencies in the variables, the rank of H is the smaller of p and ν H , min ( p , ν H ) , where ν H represents the degrees of freedom for hypothesis; in the one-way case ν H = k -1. Thus H can be singular. The p × p 'error' matrix E has a within sum of squares for each variable on the diagonal, with analogous sums of products off-diagonal. The rank of E is p , unless ν E is less than p .

Thus H has the form

<!-- formula-not-decoded -->

where, for example,

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

In these expressions, the subscript 1 or 2 indicates the first or second variable. Thus, for example, y i . 2 is the second element in y i . :

<!-- formula-not-decoded -->

The matrix E can be expressed in a form similar to (6.11):

where, for example,

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Note that the elements of E are sums of squares and products, not variances and covariances. To estimate 𝚺 , we use S p1 = E /( nk -k ) , so that

<!-- formula-not-decoded -->

## 6.1.3 Wilks' Test Statistic

The likelihood ratio test of H 0 : 𝛍 1 = 𝛍 2 = · · · = 𝛍 k is given by

<!-- formula-not-decoded -->

which is known as Wilks' /Lambda1 . (It has also been called Wilks' U .) We reject H 0 if /Lambda1 ≤ /Lambda1α, p ,ν H ,ν E . Note that rejection is for small values of /Lambda1 . Exact critical values /Lambda1α, p ,ν H ,ν E for Wilks' /Lambda1 are found in Table A.9, taken from Wall (1967). The parameters in Wilks' /Lambda1 distribution are p = number of variables (dimension),

ν E = degrees of freedom for error.

ν H = degrees of freedom for hypothesis,

Wilks' /Lambda1 compares the within sum of squares and products matrix E to the total sum of squares and products matrix E + H . This is similar to the univariate F -statistic in (6.6) that compares the between sum of squares to the within sum of squares. By using determinants, the test statistic /Lambda1 is reduced to a scalar. Thus the multivariate information in E and H about separation of mean vectors y 1 . , y 2 . , . . . , y k . is channeled into a single scale, on which we can determine if the separation of mean vectors is significant. This is typical of multivariate tests in general.

The mean vectors occupy a space of dimension s = min ( p , ν H ) , and within this space various configurations of these mean vectors are possible. This suggests the possibility that another test statistic may be more powerful than Wilks' /Lambda1 . Competing test statistics are discussed in Sections 6.1.4 and 6.1.5.

Some of the properties and characteristics of Wilks' /Lambda1 are as follows:

1. In order for the determinants in (6.13) to be positive, it is necessary that ν E ≥ p .
3. The parameters p and ν H can be interchanged; the distribution of /Lambda1 p ,ν H ,ν E is the same as that of /Lambda1ν H , p ,ν E + ν H -p .
2. For any MANOVA model, the degrees of freedom ν H and ν E are always the same as in the analogous univariate case. In the balanced one-way model, for example, ν H = k -1 and ν E = k ( n -1 ) .
4. Wilks' /Lambda1 in (6.13) can be expressed in terms of the eigenvalues λ 1, λ 2 , . . . , λ s of E -1 H , as follows:

<!-- formula-not-decoded -->

The number of nonzero eigenvalues of E -1 H is s = min ( p , ν H ) , which is the rank of H . The matrix HE -1 has the same eigenvalues as E -1 H (see Section 2.11.5) and could be used in its place to obtain /Lambda1 . However, we prefer E -1 H because we will use its eigenvectors later.

5. The range of /Lambda1 is 0 ≤ /Lambda1 ≤ 1, and the test based on Wilks' /Lambda1 is an inverse test in the sense that we reject H 0 for small values of /Lambda1 . If the sample mean vectors were equal, we would have H = O and /Lambda1 = | E | / | E + O | = 1. On the other hand, as the sample mean vectors become more widely spread apart compared to the within-sample variation, H becomes much 'larger' than E , and /Lambda1 approaches zero.
6. In Table A.9, the critical values decrease for increasing p . Thus the addition of variables will reduce the power unless the variables contribute to rejection of the hypothesis by producing a significant reduction in /Lambda1 .
7. When ν H = 1 or 2 or when p = 1 or 2 , Wilks' /Lambda1 transforms to an exact F -statistic. The transformations from /Lambda1 to F for these special cases are given in Table 6.1. The hypothesis is rejected when the transformed value of /Lambda1 exceeds

Table 6.1. Transformations of Wilks' Λ to Exact Upper Tail F -Tests

| Parameters p , ν H   | Statistic Having F -Distribution        | Degrees of Freedom     |
|----------------------|-----------------------------------------|------------------------|
| Any p , ν H = 1      | 1 - /Lambda1 /Lambda1 ν E - p + 1 p     | p , ν E - p + 1        |
| Any p , ν H = 2      | 1 - √ /Lambda1 √ /Lambda1 ν E - p + 1 p | 2 p , 2 (ν E - p + 1 ) |
| p = 1, any ν H       | 1 - /Lambda1 /Lambda1 ν E ν H √         | ν H , ν E              |
| p = 2, any ν H       | 1 - /Lambda1 √ /Lambda1 ν E - 1 ν H     | 2 ν H , 2 (ν E - 1 )   |

the upper α -level percentage point of the F -distribution, with degrees of freedom as shown.

8. For values of p and ν H other than those in Table 6.1, an approximate F -statistic is given by

<!-- formula-not-decoded -->

with df1 and df2 degrees of freedom, where

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

When p ν H = 2, t is set equal to 1. The approximate F in (6.15) reduces to the exact F -values given in Table 6.1, when either ν H or p is 1 or 2.

A (less accurate) approximate test is given by

<!-- formula-not-decoded -->

which has an approximate χ 2 -distribution with p ν H degrees of freedom. We reject H 0 if χ 2 &gt; χ 2 α . This approximation is accurate to three decimal places when p 2 + ν 2 H ≤ 1 3 f , where f = ν E -1 2 ( p -ν H + 1 ) .

9. If the multivariate test based on /Lambda1 rejects H 0, it could be followed by an F -test as in (6.6) on each of the p individual y 's. We can formulate a hypothesis comparing the means across the k groups for each variable, namely, H 0 r : µ 1 r = µ 2 r = ·· · = µ kr , r = 1 , 2 , . . . , p . It does not necessarily follow that any

of the F -tests on the p individual variables will reject the corresponding H 0 r . Conversely, it is possible that one or more of the F 's will reject H 0 r when the /Lambda1 -test accepts H 0. In either case, where the multivariate test and the univariate tests disagree, we use the multivariate test result rather than the univariate results. This is similar to the relationship between Z 2 -tests and z -tests shown in Figure 5.2.

In the three bivariate samples plotted in Figure 6.1, we illustrate the case where /Lambda1 rejects H 0 : 𝛍 1 = 𝛍 2 = 𝛍 3, but the F 's accept both of H 0 r : µ 1 r = µ 2 r = µ 3 r , r = 1 , 2, that is, for y 1 and y 2. There is no significant separation of the three samples in either the y 1 or y 2 direction alone. Other follow-up procedures are given in Sections 6.1.4 and 6.4.

10. The Wilks' /Lambda1 -test is the likelihood ratio test. Other approaches to test construction lead to different tests. Three such tests are given in Sections 6.1.4 and 6.1.5.

## 6.1.4 Roy's Test

In the union-intersection approach, we seek the linear combination zi j = a ′ y i j that maximizes the spread of the transformed means zi . = a ′ y i . relative to the withinsample spread of points. Thus we seek the vector a that maximizes

<!-- formula-not-decoded -->

Figure 6.1. Three samples with significant Wilks' /Lambda1 but nonsignificant F 's.

<!-- image -->

which, by analogy to s 2 z = a ′ Sa in (3.55), can be written as

<!-- formula-not-decoded -->

This is maximized by a 1, the eigenvector corresponding to λ 1, the largest eigenvalue of E -1 H (see Section 8.4.1), and we have

<!-- formula-not-decoded -->

Since max a F in (6.19) is maximized over all possible linear functions, it no longer has an F -distribution. To test H 0 : 𝛍 1 = 𝛍 2 = · · · = 𝛍 k based on λ 1, we use Roy's union-intersection test , also called Roy's largest root test . The test statistic is given by

<!-- formula-not-decoded -->

Critical values for θ are given in Table A.10 (Pearson and Hartley 1972, Pillai 1964, 1965). We reject H 0 : 𝛍 1 = 𝛍 2 = · · · = 𝛍 k if θ ≥ θα, s , m , N . The parameters s , m , and N are defined as

<!-- formula-not-decoded -->

For s = 1, use (6.34) and (6.37) in Section 6.1.7 to obtain an F -test.

The eigenvector a 1 corresponding to λ 1 is used in the discriminant function , z = a ′ 1 y . Since this is the function that best separates the transformed means zi . = a ′ y i . , i = 1, 2 , . . . , k [relative to the within-sample spread, see (6.17)], the coefficients a 11, a 12 , . . . , a 1 p in the linear combination z = a ′ 1 y can be examined for an indication of which variables contribute most to separating the means. The discriminant function is discussed further in Sections 6.1.8 and 6.4 and in Chapter 8.

We do not have a satisfactory F -approximation for θ or λ 1, but an 'upper bound' on F that is provided in some software programs is given by

<!-- formula-not-decoded -->

with degrees of freedom d and ν E -d -1, where d = max ( p , ν H ) . The term upper bound indicates that the F in (6.21) is greater than the 'true F '; that is, F &gt; Fd ,ν E -d -1. Therefore, we feel safe if H 0 is accepted by (6.21); but if rejection of H 0 is indicated, the information is virtually worthless.

Some computer programs do not provide eigenvalues of nonsymmetric matrices, such as E -1 H . However, the eigenvalues of E -1 H are the same as the eigenvalues of the symmetric matrices ( E 1 / 2 ) -1 H ( E 1 / 2 ) -1 and ( U ′ ) -1 HU -1 , where E 1 / 2 is the square root matrix of E given in (2.112) and U ′ U = E is the Cholesky factorization

of E (Section 2.7). We demonstrate this for the Cholesky approach. We first multiply the defining relationship ( E -1 H -λ I ) a = 0 by E to obtain

<!-- formula-not-decoded -->

Then substituting E = U ′ U into (6.22), multiplying by ( U ′ ) -1 , and inserting U -1 U = I , we have

<!-- formula-not-decoded -->

Thus ( U ′ ) -1 HU -1 has the same eigenvalues as E -1 H and has eigenvectors of the form Ua , where a is an eigenvector of E -1 H . Note that ( U ′ ) -1 HU -1 is positive semidefinite, and thus λ i ≥ 0 for all eigenvalues of E -1 H .

## 6.1.5 Pillai and Lawley-Hotelling Tests

There are two additional test statistics for H 0 : 𝛍 1 = 𝛍 2 = · · · = 𝛍 k based on the eigenvalues λ 1, λ 2 , . . . , λ s of E -1 H . The Pillai statistic is given by

<!-- formula-not-decoded -->

We reject H 0 for V ( s ) ≥ V ( s ) α . The upper percentage points, V ( s ) α , are given in Table A.11 (Schuurmann, Krishnaiah, and Chattopadhyay 1975), indexed by s , m , and N , which are defined as in Section 6.1.4 for Roy's test. For s = 1, use (6.34) and (6.37) in Section 6.1.7 to obtain an F -test.

Pillai's test statistic in (6.24) is an extension of Roy's statistic θ = λ 1 /( 1 + λ 1 ) . If the mean vectors do not lie in one dimension, the information in the additional terms λ i /( 1 + λ i ) , i = 2, 3 , . . . , s , may be helpful in rejecting H 0.

For parameter values not included in Table A.11, we can use an approximate F -statistic:

<!-- formula-not-decoded -->

which is approximately distributed as Fs ( 2 m + s + 1 ), s ( 2 N + s + 1 ) . Two alternative F -approximations are given by

<!-- formula-not-decoded -->

with p ν H and s (ν E -ν H + s ) degrees of freedom, and

<!-- formula-not-decoded -->

with sd and s (ν E -p + s ) degrees of freedom, where d = max ( p , ν H ) . It can be shown that F 3 in (6.27) is the same as F 1 in (6.25).

The Lawley-Hotelling statistic (Lawley 1938, Hotelling 1951) is defined as

<!-- formula-not-decoded -->

and is also known as Hotelling's generalized T 2 -statistic (see a comment at the end of Section 6.1.7). Table A.12 (Davis 1970a, b, 1980) gives upper percentage points of the test statistic

<!-- formula-not-decoded -->

We reject H 0 for large values of the test statistic. Note that in Table A.12, p ≤ ν H and p ≤ ν E . If p &gt; ν H , use (ν H , p , ν E + ν H -p ) in place of ( p , ν H , ν E ) . (This same pattern in the parameters is found in Wilks' /Lambda1 ; see property 3 in Section 6.1.3.) If ν H = 1 and p &gt; 1, use the relationship U ( 1 ) = T 2 /ν E [see (6.39) in Section 6.1.7]. For other values of the parameters not included in Table A.12, we can use an approximate F -statistic:

<!-- formula-not-decoded -->

which is approximately distributed as Fa , b , where

<!-- formula-not-decoded -->

Alternative F -approximations are given by

<!-- formula-not-decoded -->

with s ( 2 m + s + 1 ) and 2 ( sN + 1 ) degrees of freedom, and

<!-- formula-not-decoded -->

with p ν H and s (ν E -ν H -1 ) degrees of freedom. If p ≤ ν H , then F 3 in (6.31) is the same as F 2 in (6.30).

## 6.1.6 Unbalanced One-Way MANOVA

The balanced one-way model can easily be extended to the unbalanced case, in which there are ni observation vectors in the i th group. The model in (6.8) becomes

<!-- formula-not-decoded -->

The mean vectors become y i . = ∑ ni j = 1 y i j / ni and y .. = ∑ k i = 1 ∑ ni j = 1 y i j / N , where N = ∑ k i = 1 ni . Similarly, the total vectors are defined as y i . = ∑ ni j = 1 y i j and y .. = ∑ i j y i j . The H and E matrices are calculated as

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Wilks' /Lambda1 and the other tests have the same form as in Sections 6.1.3-6.1.5, using H and E from (6.32) and (6.33). In each test we have

<!-- formula-not-decoded -->

Note that N = ∑ i ni differs from N used as a parameter in Roy's and Pillai's tests in Sections 6.1.4 and 6.1.5.

## 6.1.7 Summary of the Four Tests and Relationship to T 2

We compare the four test statistics in terms of the eigenvalues λ 1 &gt; λ 2 &gt; . . . &gt; λ s of E -1 H , where s = min (ν H , p ) :

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

∑ i = 1 + Lawley-Hotelling: U ( s ) = s ∑ i = 1 λ i , Wilks' lambda: /Lambda1 = s ∏ i = 1 1 1 + λ i , Roy's largest root: θ λ 1 .

Note that for all four tests we must have ν E ≥ p . As noted in Section 6.1.3 and elsewhere, p is the number of variables, ν H is the degrees of freedom for the hypothesis, and ν E is the degrees of freedom for error.

Why do we use four different tests? All four are exact tests; that is, when H 0 is true, each test has probability α of rejecting H 0. However, the tests are not equivalent, and in a given sample they may lead to different conclusions even when H 0 is true; some may reject H 0 while others accept H 0. This is due to the multidimensional nature of the space in which the mean vectors 𝛍 1, 𝛍 2 , . . . , 𝛍 k lie. A comparison of power and other properties of the tests is given in Section 6.2.

When ν H = 1, then s is also equal to 1, and there is only one nonzero eigenvalue λ 1. In this case, all four test statistics are functions of each other and give equivalent results. In terms of θ , for example, the other three become

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

In the case of ν H = 1, all four statistics can be transformed to an exact F using

<!-- formula-not-decoded -->

which is distributed as Fp ,ν E -p + 1.

The equivalence of all four test statistics to Hotelling's T 2 when ν H = 1 was noted in Section 5.6.1. We now demonstrate the relationship T 2 = ( n 1 + n 2 -2 ) U ( 1 ) in (5.17). For H and E , we use (6.32) and (6.33), which allow unequal ni , since we do not require n 1 = n 2 in T 2 . In this case, with only two groups, H = ∑ 2 i = 1 ni ( y i . -y .. )( y i . -y .. ) ′ can be expressed as

<!-- formula-not-decoded -->

where c = n 1 n 2 /( n 1 + n 2 ) . Then by (6.34) and (6.28), U ( 1 ) becomes

<!-- formula-not-decoded -->

since E /( n 1 + n 2 -2 ) = S pl (see Section 5.4.2). Equations (5.16), (5.18), and (5.19) follow immediately from this result using (6.34)-(6.36).

Because of the direct relationship in (6.39) between U ( 1 ) and T 2 for the case of two groups, the Lawley-Hotelling statistic U ( s ) is often called the generalized T 2 -statistic .

Example 6.1.7. In a classical experiment carried out from 1918 to 1934, apple trees of different rootstocks were compared (Andrews and Herzberg 1985, pp. 357-360). The data for eight trees from each of six rootstocks are given in Table 6.2. The variables are y 1 = trunk girth at 4 years (mm × 100),

y 3 = trunk girth at 15 years (mm × 100), y 2 = extension growth at 4 years (m),

y 4 = weight of tree above ground at 15 years (lb × 1000).

The matrices H , E , and E + H are given by

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

In this case, the mean vectors represent six points in four-dimensional space. We can compare the mean vectors for significant differences using Wilks' /Lambda1 as given by (6.13):

<!-- formula-not-decoded -->

In this case, the parameters of the Wilks' /Lambda1 distribution are p = 4, ν H = 6 -1 = 5, and ν E = 6 ( 8 -1 ) = 42. We reject H 0 : 𝛍 1 = 𝛍 2 = · · · = 𝛍 6 because

<!-- formula-not-decoded -->

## Table 6.2. Rootstock Data

## Publisher's Note:

Permission to reproduce this image online was not granted by the copyright holder. Readers are kindly asked to refer to the printed version of this chapter.

Note the use of ν E = 40 in place of ν E = 42. This is a conservative approach that allows a table value to be used without interpolation.

To obtain an approximate F , we first calculate

<!-- formula-not-decoded -->

Then the approximate F is given by (6.15),

<!-- formula-not-decoded -->

which exceeds F . 001 , 20 , 120 = 2 . 53, and we reject H 0.

The four eigenvalues of E -1 H are 1.876, .791, .229, and .026. With these we can calculate the other three test statistics. For Pillai's statistic we have, by (6.24),

<!-- formula-not-decoded -->

To find a critical value for V ( s ) in Table A.11, we need

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Then V ( s ) . 05 = . 645 (by interpolation). Since 1 . 305 &gt; . 645, we reject H 0. For the Lawley-Hotelling statistic we obtain, by (6.28),

<!-- formula-not-decoded -->

To make the test, we calculate the test statistic

<!-- formula-not-decoded -->

The .05 critical value for ν EU ( s ) /ν H is given in Table A.12 as 7.6188 (using ν E = 40 ) , and we therefore reject H 0.

Roy's test statistic is given by (6.20) as

<!-- formula-not-decoded -->

which exceeds the .05 critical value .377 obtained (by interpolation) from Table A.10, and we reject H 0.

## 6.1.8 Measures of Multivariate Association

In multiple regression, a measure of association between the dependent variable y and the independent variables x 1, x 2 , . . . , xq is given by the squared multiple correlation

<!-- formula-not-decoded -->

Similarly, in one-way univariate ANOVA, Fisher's correlation ratio η 2 is defined as

<!-- formula-not-decoded -->

This is a measure of model fit similar to R 2 and gives the proportion of variation in the dependent variable y attributable to differences among the means of the groups. It answers the question, How well can we predict y by knowing what group it is from? Thus η 2 can be considered to be a measure of association between the dependent variable y and the grouping variable i associated with µ i or α i in the model (6.2). In fact, if the grouping variable is represented by k -1 dummy variables (also called indicator , or categorical , variables), then we have a dependent variable related to several independent variables as in multiple regression.

Adummyvariable takes on the value 1 for sampling units in a group (sample) and 0 for all other sampling units. (Values other than 0 and 1 could be used.) Thus for k samples (groups), the k -1 dummy variables are

<!-- formula-not-decoded -->

Only k -1 dummy variables are needed because if x 1 = x 2 = · · · = xk -1 = 0, the sampling unit must be from the k th group (see Section 11.6.2 for an illustration). The dependent variable y can be regressed on the k -1 dummy variables x 1, x 2 , . . . , xk -1 to produce results equivalent to the usual ANOV A calculations.

In (one-way) MANOVA, we need to measure the strength of the association between several dependent variables and several independent (grouping) variables. Various measurements of multivariate association have been proposed. Wilks (1932) suggested a 'generalized η 2 ':

<!-- formula-not-decoded -->

based on the use of | E | and | E + H | as generalizations of sums of squares. We use 1 -/Lambda1 because /Lambda1 is small if the spread in the means is large.

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Hence θ serves directly as a measure of multivariate association:

<!-- formula-not-decoded -->

It can be shown that the square root of this quantity,

<!-- formula-not-decoded -->

is the maximum correlation between a linear combination of the p dependent variables and a linear combination of the k -1 dummy group variables (see Section 11.6.2). This type of correlation is often called a canonical correlation (see Chapter 11) and is defined for each eigenvalue λ 1 , λ 2 , . . . , λ s as ri = √ λ i /( 1 + λ i ) .

Wenowconsider some measures of multivariate association suggested by Cramer and Nicewander (1979) and Muller and Peterson (1984). It is easily shown (Section 11.6.2) that /Lambda1 can be expressed as

<!-- formula-not-decoded -->

where r 2 i = λ i /( 1 + λ i ) is the i th squared canonical correlation described earlier. The geometric mean of a set of positive numbers a 1, a 2 , . . . , an is defined as ( a 1 a 2 · · · an ) 1 / n . Thus /Lambda1 1 / s is the geometric mean of the ( 1 -r 2 i ) 's, and another measure of multivariate association based on /Lambda1 , in addition to that in (6.41), is

<!-- formula-not-decoded -->

We now consider an η 2 based on Roy's statistic, θ . We noted in Section 6.1.4 that the discriminant function is the linear function z = a ′ 1 y that maximizes the spread of the means zi . = a ′ 1 y i . , i = 1 , 2 , . . . , k , where a 1 is the eigenvector of E -1 H corresponding to the largest eigenvalue λ 1. We measure the spread among the means by SSH = n ∑ k i = 1 ( zi . -z ..) 2 , divided by the within-sample spread SSE = ∑ i j ( zi j -zi .) 2 . The maximum value of this ratio is given by λ 1 [see (6.19)]. Thus and by (6.20),

In fact, as noted by Muller and Peterson, the F -approximation given in (6.15),

<!-- formula-not-decoded -->

is very similar to the univariate F -statistic (10.31) for testing significance in multiple regression,

<!-- formula-not-decoded -->

based on R 2 in (6.40).

Pillai's statistic is easily expressible as the sum of the squared canonical correlations:

<!-- formula-not-decoded -->

and the average of the r 2 i can be used as a measure of multivariate association:

<!-- formula-not-decoded -->

In terms of AP the F -approximation given in (6.26) becomes

<!-- formula-not-decoded -->

which has an obvious parallel to (6.47).

For the Lawley-Hotelling statistic U ( s ) , a multivariate measure of association can be defined as

<!-- formula-not-decoded -->

If s = 1, (6.51) reduces to (6.43). In fact, (6.43) is a special case of (6.51) because U ( s ) / s = ∑ s i = 1 λ i / s is the arithmetic average of the λ i 's. It is easily seen that the F -approximation F 3 for the Lawley-Hotelling statistic given in (6.31) can be expressed in terms of A LH as

<!-- formula-not-decoded -->

which resembles (6.47).

Example 6.1.8. We illustrate some measures of association for the root-stock data in Table 6.2:

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

There is a wide range of values among these measures of association.

## 6.2 COMPARISON OF THE FOUR MANOVA TEST STATISTICS

When H 0 : 𝛍 1 = 𝛍 2 = ··· = 𝛍 k is true, all the mean vectors are at the same point. Therefore, all four MANOVA test statistics have the same Type I error rate, α , as noted in Section 6.1.7; that is, all have the same probability of rejection when H 0 is true. However, when H 0 is false, the four tests have different probabilities of rejection. We noted in Section 6.1.7 that in a given sample the four tests need not agree, even if H 0 is true. One test could reject H 0 and the others accept H 0, for example.

Historically, Wilks' /Lambda1 has played the dominant role in significance tests in MANOVA because it was the first to be derived and has well-known χ 2 - and F -approximations. It can also be partitioned in certain ways we will find useful later. However, it is not always the most powerful among the four tests. The probability of rejecting H 0 when it is false is known as the power of the test.

In univariate ANOVA with p = 1, the means µ 1, µ 2 , . . . , µ k can be uniquely ordered along a line in one dimension, and the usual F -test is uniformly most powerful. In the multivariate case, on the other hand, with p &gt; 1, the mean vectors are points in s = min ( p , ν H ) dimensions. We have four tests, not one of which is uniformly most powerful. The relative powers of the four test statistics depend on the configuration of the mean vectors 𝛍 1, 𝛍 2 , . . . , 𝛍 k in the s -dimensional space. A given test will be more powerful for one configuration of mean vectors than another.

If ν H &lt; p , then s = ν H and the mean vectors lie in an s -dimensional subspace of the p -dimensional space of the observations. The points may, in fact, occupy a subspace of the s dimensions. For example, they may be confined to a line (one dimension) or a plane (two dimensions). This is illustrated in Figure 6.2.

An indication of the pattern of the mean vectors is given by the eigenvalues of E -1 H . If there is one large eigenvalue and the others are small, the mean vectors lie close to a line. If there are two large eigenvalues, the mean vectors lie mostly in two dimensions, and so on.

Figure 6.2. Two possible configurations for three mean vectors in 3-space.

<!-- image -->

Because Roy's test uses only the largest eigenvalue of E -1 H , it is more powerful than the others if the mean vectors are collinear. The other three tests have greater power than Roy's when the mean vectors are diffuse (spread out in several dimensions).

In terms of power, the tests are ordered θ ≥ U ( s ) ≥ /Lambda1 ≥ V ( s ) for the collinear case. In the diffuse case and for intermediate structure between collinear and diffuse, the ordering of power is reversed, V ( s ) ≥ /Lambda1 ≥ U ( s ) ≥ θ . The latter ordering also holds for accuracy of the Type I error rate when the population covariance matrices 𝚺 1, 𝚺 2 , . . . , 𝚺 k are not equal. These orderings are comparisons of power. For actual computation of power in a given experimental setting or to find the sample size needed to yield a desired level of power, see Rencher (1998, Section 4.4).

Generally, if group sizes are equal, the tests are sufficiently robust with respect to heterogeneity of covariance matrices so that we need not worry. If the ni 's are unequal and we have heterogeneity, then the α -level of the MANOVA test may be affected as follows. If the larger variances and covariances are associated with the larger samples, the true α -level is reduced and the tests become conservative. On the other hand, if the larger variances and covariances come from the smaller samples, α is inflated, and the tests become liberal. Box's M -test in Section 7.3.2 can be used to test for homogeneity of covariance matrices.

In conclusion, the use of Roy's θ is not recommended in any situation except the collinear case under standard assumptions. In the diffuse case its performance is inferior to that of the other three, both when the assumptions hold and when they do not. If the data come from nonnormal populations exhibiting skewness or positive kurtosis, any of the other three tests perform acceptably well. Among these three, V ( s ) is superior to the other two when there is heterogeneity of covariance matrices. Indeed V ( s ) is first in all rankings except those for the collinear case. However, /Lambda1 is not far behind, except when there is severe heterogeneity of covariance matrices. It seems likely that Wilks' /Lambda1 will continue its dominant role because of its flexi-

bility and historical precedence. [For references for this section, see Rencher (1998, Section 4.2).]

In practice, most MANOVA software programs routinely calculate all four test statistics, and they usually reach the same conclusion. In those cases when they differ as to acceptance or rejection of the hypothesis, one can examine the eigenvalues and covariance matrices and evaluate the conflicting conclusions in light of the test properties discussed previously.

Example 6.2. We inspect the eigenvalues of E -1 H for the rootstock data of Table 6.2 for an indication of the configuration of the six mean vectors in a fourdimensional space. The eigenvalues are 1.876, .791, .229, .026. The first eigenvalue, 1.876, constitutes a proportion

<!-- formula-not-decoded -->

of the sum of the eigenvalues. Therefore, the first eigenvalue does not dominate the others, and the mean vectors are not collinear. The first two eigenvalues account for a proportion

<!-- formula-not-decoded -->

of the sum of the eigenvalues, and thus the six mean vectors lie largely in two dimensions. Since the mean vectors are not collinear, the test statistics /Lambda1 , V ( s ) , and U ( s ) will be more appropriate than θ in this case.

## 6.3 CONTRASTS

As in Sections 6.1.1-6.1.5, we consider only the balanced model where n 1 = n 2 = · · · = nk = n . We begin with a review of contrasts in the univariate setting before moving to the multivariate case.

## 6.3.1 Univariate Contrasts

A contrast in the population means is defined as a linear combination

<!-- formula-not-decoded -->

where the coefficients satisfy

<!-- formula-not-decoded -->

An unbiased estimator of δ is given by

<!-- formula-not-decoded -->

The sample means y i . were defined in (6.1). Since the y i . 's are independent with variance σ 2 / n , the variance of ˆ δ is

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

where MSE was defined in (6.6) and (6.7) as SSE/ k n 1 .

The usual hypothesis to be tested by a contrast is

( -)

<!-- formula-not-decoded -->

For example, suppose k = 4 and that a contrast of interest to the researcher is 3 µ 1 -µ 2 -µ 3 -µ 4. If this contrast is set equal to zero, we have

<!-- formula-not-decoded -->

and the experimenter is comparing the first mean with the average of the other three. A contrast is often called a comparison among the treatment means.

Assuming normality, H 0 : δ = 0 can be tested by

<!-- formula-not-decoded -->

which is distributed as t ν E . Alternatively, since t 2 = F , we can use

<!-- formula-not-decoded -->

which is distributed as F 1 ,ν E . The numerator of (6.58) is often referred to as the sum of squares for the contrast.

If two contrasts δ = ∑ i ai µ i and γ = ∑ i bi µ i are such that ∑ i ai bi = 0, the contrasts are said to be orthogonal . The two estimated contrasts can be written in the form ∑ i ai y i . = a ′ y and ∑ i bi y i . = b ′ y , where y = ( y 1 . , y 2 . , . . . , y k . ) ′ . Then ∑ i ai bi = a ′ b = 0, and by the discussion following (3.14), the coefficient vectors a and b are perpendicular.

which can be estimated by

When two contrasts are orthogonal, the two corresponding sums of squares are independent. In fact, for k treatments, we can find k -1 orthogonal contrasts that partition the treatment sum of squares SSH into k -1 independent sums of squares, each with one degree of freedom. In the unbalanced case (Section 6.1.6), orthogonal contrasts such that ∑ i ai bi = 0 do not partition SSH into k -1 independent sums of squares. For a discussion of contrasts in the unbalanced case, see Rencher (1998, Sections 4.8.2 and 4.8.3; 2000, Section 14.2.2).

## 6.3.2 Multivariate Contrasts

There are two usages of contrasts in a multivariate setting. We have previously encountered one use in Section 5.9.1, where we considered the hypothesis H 0 : C 𝛍 = 0 with Cj = 0 . Each row of C sums to zero, and C 𝛍 is therefore a set of contrasts comparing the elements µ 1, µ 2 , . . . , µ p of 𝛍 with each other. In this section, on the other hand, we consider contrasts comparing several mean vectors, not the elements within a vector.

A contrast among the population mean vectors is defined as

<!-- formula-not-decoded -->

where ∑ k i = 1 ci = 0. An unbiased estimator of 𝛅 is given by the corresponding contrast in the sample mean vectors:

<!-- formula-not-decoded -->

The sample mean vectors y 1 . , y 2 . , . . . , y k . as defined in Section 6.1.2 were assumed to be independent and to have common covariance matrix, cov ( y i . ) = 𝚺 / n . Thus the covariance matrix for ˆ 𝛅 is given by

<!-- formula-not-decoded -->

which can be estimated by

<!-- formula-not-decoded -->

where S pl = E /ν E is an unbiased estimator of 𝚺 .

The hypothesis H 0 : 𝛅 = 0 or H 0 : c 1 𝛍 1 + c 2 𝛍 2 +··· + ck 𝛍 k = 0 makes comparisons among the population mean vectors. For example, 𝛍 1 -2 𝛍 2 + 𝛍 3 = 0 is equivalent to

<!-- formula-not-decoded -->

and we are comparing 𝛍 2 to the average of 𝛍 1 and 𝛍 3. Of course this implies that every element of 𝛍 2 must equal the corresponding element of 1 2 ( 𝛍 1 + 𝛍 3 ) :

<!-- formula-not-decoded -->

Under appropriate multivariate normality assumptions, H 0 : c 1 𝛍 1 + c 2 𝛍 2 +···+ ck 𝛍 k = 0 or H 0 : 𝛅 = 0 can be tested with the one-sample T 2 -statistic

<!-- formula-not-decoded -->

An equivalent test of H 0 can be made with Wilks' /Lambda1 . By analogy with the numerator of (6.58), the hypothesis matrix due to the contrast is given by which is distributed as T 2 p ,ν E . In the one-way model under discussion here, ν E = k ( n -1 ) .

<!-- formula-not-decoded -->

The rank of H 1 is 1, and the test statistic is

<!-- formula-not-decoded -->

which is distributed as /Lambda1 p , 1 ,ν E . The other three MANOVA test statistics can also be applied here using the single nonzero eigenvalue of E -1 H 1. Because ν H = 1 in this case, all four MANOVA statistics and T 2 give the same results; that is, all five transform to the same F -value using the formulations in Section 6.1.7. If k -1 orthogonal contrasts are used, they partition the H matrix into k -1 independent matrices H 1, H 2 , . . . , H k -1. Each H i matrix has one degree of freedom because rank ( H i ) = 1.

Example 6.3.2. We consider the following two orthogonal contrasts for the rootstock data in Table 6.2:

<!-- formula-not-decoded -->

The first compares 𝛍 1 and 𝛍 6 with the other four mean vectors. The second compares 𝛍 1 vs. 𝛍 6. Thus H 01 : 2 𝛍 1 -𝛍 2 -𝛍 3 -𝛍 4 -𝛍 5 + 2 𝛍 6 = 0 can be written as

<!-- formula-not-decoded -->

Dividing both sides by 4 to express this in terms of averages, we obtain

<!-- formula-not-decoded -->

Similarly, the hypothesis for the second contrast can be expressed as

<!-- formula-not-decoded -->

The mean vectors are given by

|   y 1 . |   y 2 . |   y 3 . |   y 4 . |   y 5 . |   y 6 . |
|---------|---------|---------|---------|---------|---------|
|    1.14 |    1.16 |    1.11 |    1.1  |    1.08 |    1.04 |
|    2.98 |    3.11 |    2.82 |    2.88 |    2.56 |    2.21 |
|    3.74 |    4.52 |    4.46 |    3.91 |    4.31 |    3.6  |
|    0.87 |    1.28 |    1.39 |    1.04 |    1.18 |    0.74 |

For the first contrast, we obtain H 1 from (6.63) as

Then

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

which is less than /Lambda1 . 05 , 4 , 1 , 40 = . 779 from Table A.9. We therefore reject H 01.

To test the significance of the second contrast, we have

Then

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

which is less than /Lambda1 . 05 , 4 , 1 , 40 = . 779, and we reject H 02.

## 6.4 TESTS ON INDIVIDUAL VARIABLES FOLLOWING REJECTION OF H 0 BY THE OVERALL MANOVA TEST

In Section 6.1, we considered tests of equality of mean vectors, H 0 : 𝛍 1 = 𝛍 2 = · · · = 𝛍 k , which implies equality of means for each of the p variables:

<!-- formula-not-decoded -->

This hypothesis could be tested for each variable by itself with an ordinary univariate ANOVA F -test, as noted in property 9 in Section 6.1.3. For example, if there are three mean vectors,

<!-- formula-not-decoded -->

we have H 01 : µ 11 = µ 21 = µ 31, H 02 : µ 12 = µ 22 = µ 32 , . . . , H 0 p : µ 1 p = µ 2 p = µ 3 p . Each of these p hypotheses can be tested with a simple ANOVA F -test.

If an F -test is made on each of the p variables regardless of whether the overall MANOVA test of H 0 : 𝛍 1 = 𝛍 2 = 𝛍 3 rejects H 0, then the overall α -level will increase beyond the nominal value because we are making p tests. As in Section 5.5, we define the overall α or experimentwise error rate as the probability of rejecting one or more of the p univariate tests when H 0 : 𝛍 1 = 𝛍 2 = 𝛍 3 is true. We could

'protect' against inflation of the experimentwise error rate by performing tests on individual variables only if the overall MANOVA test of H 0 : 𝛍 1 = 𝛍 2 = 𝛍 3 rejects H 0. In this procedure, the probability of rejection for the tests on individual variables is reduced, and these tests become more conservative.

Rencher and Scott (1990) compared these two procedures for testing the individual variables in a one-way MANOVA model. Since the focus was on α -levels, only the case where H 0 is true was considered. Specifically, the two procedures were as follows:

1. Aunivariate F -test is made on each variable, testing H 0 r : µ 1 r = µ 2 r = · · · = µ kr , r = 1 , 2 , . . . , p . In this context, the p univariate tests constitute an experiment and one or more rejections are counted as one experimentwise error. No multivariate test is made.
2. The overall MANOVA hypothesis H 0 : 𝛍 1 = 𝛍 2 = · · · = 𝛍 k is tested with Wilks' /Lambda1 , and if H 0 is rejected, p univariate F -tests on H 01, H 02 , . . . , H 0 p are carried out. Again, one or more rejections among the F -tests are counted as one experimentwise error.

The amount of intercorrelation among the multivariate normal variables was indicated by ∑ p i = 1 ( 1 /λ i )/ p , where λ 1, λ 2 , . . . , λ p are the eigenvalues of the population correlation matrix P ρ . Note that ∑ i ( 1 /λ i )/ p = 1 for the uncorrelated case ( P ρ = I ) and ∑ i ( 1 /λ i )/ p &gt; 1 for the correlated case ( P ρ /negationslash= I ) . When the variables are highly intercorrelated, one or more of the eigenvalues will be near zero (see Section 4.1.3), and i ( 1 /λ i )/ p will be large.

∑ The error rates of these two procedures were investigated for several values of p , n , k , and ∑ i ( 1 /λ i )/ p , where p is the number of variables, n is the number of observation vectors in each group, k is the number of groups, and ∑ i ( 1 /λ i )/ p is the measure of intercorrelation defined above. In procedure 1, the probability of rejecting one or more univariate tests when H 0 is true varied from .09 to .31 ( α was .05 in each test). Such experimentwise error rates are clearly unacceptable when the nominal value of α is .05. However, this approach is commonly used when the researcher is not familiar with the MANOVA approach or does not have access to appropriate software.

Table 6.3 contains the error rates for procedure 2, univariate F -tests following a rejection by Wilks' /Lambda1 . The values range from .022 to .057, comfortably close to the target value of .05. No apparent trends or patterns are seen; the values do not seem to depend on p , k , n , or the amount of intercorrelation as measured by ∑ i ( 1 /λ i )/ p . Thus when univariate tests are made only following a rejection of the overall test, the experimentwise error rate is about right.

Based on these results, we recommend making an overall MANOVA test followed by F -tests on the individual variables (at the same α -level as the MANOVA test) only if the MANOVA test leads to rejection of H 0.

Another procedure that can be used following rejection of the MANOVA test is an examination of the discriminant function coefficients. The discriminant function was defined in Section 6.1.4 as z = a ′ 1 y , where a 1 is the eigenvector asso-

Table 6.3. Experimentwise Error Rates for Procedure 2: Univariate F -Tests Following Rejection by Wilks' Λ

|    |    | ∑ i ( 1 /λ i )/ p   | ∑ i ( 1 /λ i )/ p   | ∑ i ( 1 /λ i )/ p   | ∑ i ( 1 /λ i )/ p   | ∑ i ( 1 /λ i )/ p   | ∑ i ( 1 /λ i )/ p   | ∑ i ( 1 /λ i )/ p   | ∑ i ( 1 /λ i )/ p   |
|----|----|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------|---------------------|
|    |    | 1                   | 1                   | 10                  | 10                  | 100                 | 100                 | 1000                | 1000                |
| n  | p  | k = 3               | k = 5               | k = 3               | k = 5               | k = 3               | k = 5               | k = 3               | k = 5               |
| 5  | 3  | .043                | .037                | .022                | .035                | .046                | .039                | .022                | .029                |
| 5  | 5  | .041                | .037                | .039                | .057                | .038                | .035                | .027                | .039                |
| 5  | 7  | .030                | .042                | .035                | .045                | .039                | .037                | .026                | .048                |
| 10 | 3  | .047                | .041                | .030                | .033                | .043                | .045                | .026                | .032                |
| 10 | 5  | .047                | .037                | .026                | .049                | .041                | .026                | .027                | .029                |
| 10 | 7  | .034                | .054                | .037                | .047                | .047                | .040                | .040                | .044                |
| 20 | 3  | .050                | .043                | .032                | .054                | .048                | .039                | .040                | .032                |
| 20 | 5  | .045                | .055                | .042                | .051                | .037                | .044                | .050                | .043                |
| 20 | 7  | .055                | .051                | .029                | .040                | .033                | .051                | .039                | .033                |

ciated with the largest eigenvalue λ 1 of E -1 H . Additionally, there are other discriminant functions using eigenvectors corresponding to the other eigenvalues. Since the first discriminant function maximally separates the groups, we can examine its coefficients for the contribution of each variable to group separation. Thus in z = a 11 y 1 + a 12 y 2 +··· + a 1 p y p , if a 12 is larger than the other a 1 r 's, we believe y 2 contributes more than any of the other variables to separation of groups. A method of standardization of the a 1 r 's to adjust for differences in the scale among the variables is given in Section 8.5.

The information in a 1 r (from z = a ′ 1 y ) about the contribution of yr to separation of the groups is fundamentally different from the information provided in a univariate F -test that considers yr alone (see property 9 in Section 6.1.3). The relative size of a 1 r shows the contribution of yr in the presence of the other variables and takes into account (1) the correlation of yr with the others y 's and (2) the contribution of yr to Wilks' /Lambda1 above and beyond the contribution of the other y 's. In contrast, the individual F -test on yr ignores the presence of the other variables. Because we are primarily interested in the collective behavior of the variables, the discriminant function coefficients provide more pertinent information than the tests on individual variables. For a detailed analysis of the effect of each variable in the presence of other variables, see Rencher (1993; 1998, Section 4.1.6).

Huberty (1975) compared the standardized coefficients to some correlations that can be shown to be related to individual variable tests (see Section 8.7.3). In a limited simulation, the discriminant coefficients were found to be more valid than the univariate tests in identifying those variables that contribute least to separation of groups. Considerable variation was found from sample to sample in ranking the relative potency of the variables.

Example 6.4. In Example 6.1.7, the hypothesis H 0 : 𝛍 1 = 𝛍 2 = ·· · = 𝛍 6 was rejected for the rootstock data of Table 6.2. We can therefore test the four individual

variables using the .05 level of significance. For the first variable, y 1 = 4-year trunk girth, we obtain the following ANOVA table:

| Source     |   Sum of Squares |   df | Mean Square   | F    |
|------------|------------------|------|---------------|------|
| Rootstocks |         0.07356  |    5 | .014712       | 1.93 |
| Error      |         0.319988 |   42 | .007619       |      |
| Total      |         0.393548 |   47 |               |      |

For F = 1 . 93 the p -value is .1094, and we do not reject H 0. For the other three variables we have

| Variable                      |     F | p -Value   |
|-------------------------------|-------|------------|
| y 2 = 4-year extension growth |  2.91 | .024       |
| y 3 = 15-year trunk girth     | 11.97 | < . 0001   |
| y 4 15-year weight            | 12.16 | < . 0001   |

=

Thus for three of the four variables, the six means differ significantly. We examine the standardized discriminant function coefficients for this set of data in Chapter 8 (Problem 8.12).

## 6.5 TWO-WAY CLASSIFICATION

We consider only balanced models, where each cell in the model has the same number of observations, n . For the unbalanced case with unequal cell sizes, see Rencher (1998, Section 4.8).

## 6.5.1 Review of Univariate Two-Way ANOVA

In the univariate two-way model, we measure one dependent variable y on each experimental unit. The balanced two-way fixed-effects model with factors A and B is

<!-- formula-not-decoded -->

where α i is the effect (on yi jk ) of the i th level of A , β j is the effect of the j th level of B , γ i j is the corresponding interaction effect, and µ i j is the population mean for the i th level of A and the j th level of B . In order to obtain F -tests, we further assume that the ε i j k 's are independently distributed as N ( 0 , σ 2 ) .

Let µ i . = ∑ j µ i j / b be the mean at the i th level of A and define µ. j and µ.. similarly. Then if we use side conditions ∑ i α i = ∑ j β j = ∑ i γ i j = ∑ j γ i j = 0,

the effect of the i th level of A can be defined as α i = µ i . -µ.. , with similar definitions of β j and γ i j . We can show that ∑ i α i = 0 if α i = µ i . -µ.. as follows:

<!-- formula-not-decoded -->

Many texts recommend that the interaction AB be tested first, and that if it is found to be significant, then the main effects should not be tested. However, with the side conditions imposed earlier (side conditions are not necessary in order to obtain tests), the effect of A is defined as the average effect over the levels of B , and the effect of B is defined similarly. With this definition of main effects, the tests for A and B make sense even if AB is significant. Admittedly, interpretation requires more care, and the effect of a factor may vary if the number of levels of the other factor is altered. But in many cases useful information can be gained about the main effects in the presence of interaction.

We illustrate the preceding statement that α i = µ i . -µ.. represents the effect of the i th level of A averaged over the levels of B . Suppose A has two levels and B has three. We represent the means of the six cells as follows:

<!-- image -->

The means of the rows (corresponding to levels of A ) and columns (levels of B ) are also given. Then α i = µ i . -µ.. can be expressed as the average of the effect of the i th level of A at the three levels of B . For example,

<!-- formula-not-decoded -->

To estimate α i , we can use ˆ α i = y i .. -y ... , with similar estimates for β j and γ i j . The notation y i .. indicates that yi jk is averaged over the levels of j and k to obtain the mean of all nb observations at the i th level of A , namely, y i .. = ∑ j k yi jk / nb . The means y . j . , y i j . , and y ... have analogous definitions.

To construct tests for the significance of factors A and B and the interaction AB , we use the usual sums of squares and degrees of freedom as shown in Table 6.4. Computational forms of the sums of squares can be found in many standard (univariate) methods texts.

Table 6.4. Univariate Two-Way Analysis of Variance

| Source   | Sum of Squares                                          | df                 |
|----------|---------------------------------------------------------|--------------------|
| A        | SSA = nb i ( y i .. - y ... ) 2                         | a - 1              |
| B        | ∑ SSB = na j ( y . j . - y ... ) 2                      | b - 1              |
| AB       | ∑ SSAB = n i j ( y i j . - y i .. - y . j . + y ... ) 2 | ( a - 1 )( b - 1 ) |
| Error    | ∑ SSE = i jk ( y i jk - y i j . ) 2                     | ab ( n - 1 )       |
| Total    | ∑ SST = i jk ( y i jk - y ... ) 2                       | abn - 1            |

∑

The sums of squares in Table 6.4 (for the balanced model) have the relationship

<!-- formula-not-decoded -->

and the four sums of squares on the right are independent. The sums of squares are divided by their corresponding degrees of freedom to obtain mean squares MSA, MSB,MSAB,andMSE.Forthe fixed effects model, each of MSA, MSB, and MSAB is divided by MSE to obtain an F -test. In the case of factor A , for example, the hypothesis can be expressed as

<!-- formula-not-decoded -->

and the test statistic is F = MSA/MSE, which is distributed as Fa -1 , ab ( n -1 ) .

In order to define contrasts among the levels of each main effect, we can conveniently use the model in the form given in (6.66),

<!-- formula-not-decoded -->

Acontrast among the levels of A is defined as ∑ a i = 1 ci µ i . , where ∑ i ci = 0. An estimate of the contrast is given by ∑ i ci y i .. , with variance σ 2 ∑ i c 2 i / nb , since each y i .. is based on nb observations and the y i .. 's are independent. To test H 0 : ∑ i ci µ i . = 0, we can use an F -statistic corresponding to (6.58),

<!-- formula-not-decoded -->

with 1 and ν E degrees of freedom. To preserve the experimentwise error rate, significance tests for more than one contrast could be carried out in the spirit of Section 6.4; that is, contrasts should be chosen prior to seeing the data, and tests should be made only if the overall F -test for factor A rejects H 0 A .

Contrasts ∑ j c j µ. j among the levels of B are tested in an entirely analogous manner.

## 6.5.2 Multivariate Two-Way MANOVA

A balanced two-way fixed-effects MANOVA model for p dependent variables can be expressed in vector form analogous to (6.65) and (6.66):

<!-- formula-not-decoded -->

where 𝛂 i is the effect of the i th level of A on each of the p variables in y i j k , 𝛃 j is the effect of the j th level of B , and 𝛄 i j is the AB interaction effect. We use side conditions ∑ i 𝛂 i = ∑ j 𝛃 j = ∑ i 𝛄 i j = ∑ j 𝛄 i j = 0 and assume the 𝛆 i j k 's are independently distributed as Np ( 0 , 𝚺 ) . Under the side condition ∑ i 𝛂 i = 0 , the effect of A is averaged over the levels of B ; that is, 𝛂 i = 𝛍 i . -𝛍 .. , where 𝛍 i . = j 𝛍 i j / b and 𝛍 .. = i j 𝛍 i j / ab . There are similar definitions for 𝛃 j and 𝛄 i j .

∑ ∑ As in the univariate usage, the mean vector y i .. indicates an average over the subscripts replaced by dots, that is, y i .. = ∑ j k y i j k / nb . The means y . j . , y i j . , and y ... have analogous definitions: y . j . = ∑ i k y i j k / na , y i j . = ∑ k y i j k / n , y ... = ∑ i j k y i j k / nab . The sum of squares and products matrices are given in Table 6.5. Note that the degrees of freedom in Table 6.5 are the same as in the univariate case in Table 6.4. For the two-way model with balanced data, the total sum of squares and products matrix is partitioned as

<!-- formula-not-decoded -->

The structure of any of the hypothesis matrices is similar to that of H in (6.11). For example, H A has on the diagonal the sum of squares for factor A for each of the p variables. The off-diagonal elements of H A are corresponding sums of products for all pairs of variables. Thus the r th diagonal element of H A corresponding to the r th variable, r = 1 , 2 , . . . , p , is given by

<!-- formula-not-decoded -->

where y i .. r and y ... r represent the r th components of y i .. and y ... , respectively, and yi .. r and y ... r are totals corresponding to y i .. r and y ... r . The ( rs )th off-diagonal element of H A is

<!-- formula-not-decoded -->

Table 6.5. Multivariate Two-Way Analysis of Variance

| Source   | Sum of Squares and Products Matrix                                                                 | df                 |
|----------|----------------------------------------------------------------------------------------------------|--------------------|
| A        | H A = nb i ( y i .. - y ... )( y i .. - y ... ) ′                                                  | a - 1              |
| B        | ∑ H B = na j ( y . j . - y ... )( y . j . - y ... ) ′                                              | b - 1              |
| AB       | ∑ H AB = n ∑ i j ( y i j . - y i .. - y . j . + y ... ) × ( y i j . - y i .. - y . j . + y ... ) ′ | ( a - 1 )( b - 1 ) |
| Error    | E = i jk ( y i jk - y i j . )( y i jk - y i j . ) ′                                                | ab ( n - 1 )       |
| Total    | ∑ T = i jk ( y i jk - y ... )( y i jk - y ... ) ′                                                  | abn - 1            |

∑

From (6.69) and Table 6.5, we obtain

<!-- formula-not-decoded -->

For the E matrix, computational formulas are based on (6.69):

<!-- formula-not-decoded -->

Thus the elements of E have the form

<!-- formula-not-decoded -->

The hypotheses matrices for interaction and main effects in this fixed-effects model can be compared to E to make a test. Thus for Wilks' /Lambda1 , we use E to test each of A , B , and AB :

<!-- formula-not-decoded -->

In each case, the indicated distribution holds when H 0 is true. To calculate the other three MANOVA test statistics for A , B , and AB , we use the eigenvalues of E -1 H A , E -1 H B , and E -1 H AB .

If the interaction is not significant, interpretation of the main effects is simpler. However, the comments in Section 6.5.1 about testing main effects in the presence of interaction apply to the multivariate model as well. If we define each main effect as the average effect over the levels of the other factor, then main effects can be tested even if the interaction is significant. One must be more careful with the interpretation in case of a significant interaction, but there is information to be gained.

By analogy with the univariate two-way ANOVA in Section 6.5.1, a contrast among the levels of factor A can be defined in terms of the mean vectors as follows: ∑ a i = 1 ci 𝛍 i . , where ∑ i ci = 0 and 𝛍 i . = ∑ j 𝛍 i j / b . Similarly, ∑ b j = 1 c j 𝛍 . j represents a contrast among the levels of B . The hypothesis that these contrasts are

0 can be tested by T 2 or any of the four MANOVA test statistics, as in (6.62), (6.63), and (6.64). To test H 0 : ∑ i ci 𝛍 i . = 0 , for example, we can use which is distributed as T 2 p ,ν E when H 0 is true. Alternatively, the hypothesis matrix

can be used in

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

which, under H 0, is /Lambda1 p , 1 ,ν E , with ν E = ab ( n -1 ) in the two-way model. The other three MANOVA test statistics can also be constructed from E -1 H 1. All five test statistics will give equivalent results because ν H = 1.

If follow-up tests on individual variables are desired, we can infer from Rencher and Scott (1990), as reported in Section 6.4, that if the MANOV A test on factor A or B leads to rejection of H 0, then we can proceed with the univariate F -tests on the individual variables with assurance that the experimentwise error rate will be close to α .

To determine the contribution of each variable in the presence of the others, we can examine the first discriminant function obtained from eigenvectors of E -1 H A or E -1 H B , as in Section 6.4 for one-way MANOVA. The first discriminant function for E -1 H A , for example, is z = a ′ y , where a is the eigenvector associated with the largest eigenvalue of E -1 H A . In z = a 1 y 1 + a 2 y 2 +··· + ap y p , if ar is larger than the other a 's, then yr contributes more than the other variables to the significance of /Lambda1 A . (In many cases, the ar 's should be standardized as in Section 8.5.) Note that the first discriminant function obtained from E -1 H A will not have the same pattern as the first discriminant function from E -1 H B . This is not surprising since we expect that the relative contribution of the variables to separating the levels of factor A will be different from the relative contribution to separating the levels of B .

A randomized block design or a two-way MANOVA without replication can easily be analyzed in a manner similar to that for the two-way model with replication given here; therefore, no specific details will be given.

Example 6.5.2. Table 6.6 contains data reported by Posten (1962) and analyzed by Kramer and Jensen (1970). The experiment involved a 2 × 4 design with 4 replications, for a total of 32 observation vectors. The factors were rotational velocity [ A 1 (fast) and A 2 (slow)] and lubricants (four types). The experimental units were

Table 6.6. Two-Way Classification of Measurements on Bar Steel

|           | A 1   | A 1   | A 2   | A 2   |
|-----------|-------|-------|-------|-------|
| Lubricant | y 1   | y 2   | y 1   | y 2   |
| B 1       | 7.80  | 90.4  | 7.12  | 85.1  |
|           | 7.10  | 88.9  | 7.06  | 89.0  |
|           | 7.89  | 85.9  | 7.45  | 75.9  |
|           | 7.82  | 88.8  | 7.45  | 77.9  |
| B 2       | 9.00  | 82.5  | 8.19  | 66.0  |
|           | 8.43  | 92.4  | 8.25  | 74.5  |
|           | 7.65  | 82.4  | 7.45  | 83.1  |
|           | 7.70  | 87.4  | 7.45  | 86.4  |
| B 3       | 7.28  | 79.6  | 7.15  | 81.2  |
|           | 8.96  | 95.1  | 7.15  | 72.0  |
|           | 7.75  | 90.2  | 7.70  | 79.9  |
|           | 7.80  | 88.0  | 7.45  | 71.9  |
| B 4       | 7.60  | 94.1  | 7.06  | 81.2  |
|           | 7.00  | 86.6  | 7.04  | 79.9  |
|           | 7.82  | 85.9  | 7.52  | 86.4  |
|           | 7.80  | 88.8  | 7.70  | 76.4  |

32 homogeneous pieces of bar steel. Two variables were measured on each piece of bar steel:

y 1 = ultimate torque, y 2 = ultimate strain.

We display the totals for each variable for use in computations. The numbers inside the box are cell totals (over the four replications), and the marginal totals are for each level of A and B :

| Totals for y 1   | Totals for y 1   |        |     | Totals for y 2   | Totals for y 2   |        |
|------------------|------------------|--------|-----|------------------|------------------|--------|
| A 1              | A 2              |        |     | A 1              | A 2              |        |
| 30.61            | 29.08            | 59.69  | B 1 | 354.0            | 327.9            | 681.9  |
| 32.61            | 31.34            | 64.12  | B 2 | 344.7            | 310.0            | 654.7  |
| 31.79            | 29.45            | 61.24  | B 3 | 352.9            | 305.0            | 657.9  |
| 30.22            | 29.32            | 59.54  | B 4 | 355.4            | 323.9            | 679.3  |
| 125.40           | 119.19           | 244.59 |     | 1407.0           | 1266.8           | 2673.8 |

Using computational forms for hArr in (6.70), the ( 1 , 1 ) element of H A (corresponding to y 1) is given by

<!-- formula-not-decoded -->

For the ( 2 , 2 ) element of H A (corresponding to y 2), we have

<!-- formula-not-decoded -->

For the ( 1 , 2 ) element of H A (corresponding to y 1 y 2), we use (6.71) for hArs to obtain

<!-- formula-not-decoded -->

Thus

<!-- formula-not-decoded -->

We obtain H B similarly:

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

For H AB we have, by (6.72),

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

The error matrix E is obtained using the computational forms given for err and ers in (6.73). For example, e 11 and e 12 are computed as

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Proceeding in this fashion, we obtain

<!-- formula-not-decoded -->

with ν E = ab ( n -1 ) = ( 2 )( 4 )( 4 -1 ) = 24.

To test the main effect of A with Wilks' /Lambda1 , we compute

<!-- formula-not-decoded -->

and we conclude that velocity has a significant effect on y 1 or y 2 or both.

For the B main effect, we have

<!-- formula-not-decoded -->

We conclude that the effect of lubricants is not significant.

For the AB interaction, we obtain

<!-- formula-not-decoded -->

Hence we conclude that the interaction effect is not significant.

We now obtain the other three MANOVA test statistics for each test. For A , the only nonzero eigenvalue of E -1 H A is 1.110. Thus

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

In this case, all three tests give results equivalent to that of /Lambda1 A because ν H = s = 1.

For B , ν H = 3 and p = s = 2. The eigenvalues of E -1 H B are .418 and .020, and we obtain

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

With s = 2, m = 0, and N = 10 . 5, we have V ( s ) = . 439 and θ. 05 = . 364. The .05 critical value of ν EU ( s ) /ν H is 5.1799. Thus V ( s ) , U ( s ) , and θ lead to acceptance of H 0, as does /Lambda1 . Of the four tests, θ appears to be closer to rejection. This is because λ 1 /(λ 1 + λ 2 ) = . 418 /(. 418 + . 020 ) = . 954, indicating that the mean vectors for factor B are essentially collinear, in which case Roy's test is more powerful. If the mean vectors y . 1 . , y . 2 . , y . 3 . , and y . 4 . for the four levels of B were a little further apart, we would have a situation in which the four MANOVA tests do not lead to the same conclusion.

For AB , the eigenvalues of E -1 H AB are .0651 and .0075, from which

<!-- formula-not-decoded -->

The critical values remain the same as for factor B , and all three tests accept H 0, as does Wilks' /Lambda1 . With a nonsignificant interaction, interpretation of the main effects is simplified.

## 6.6 OTHER MODELS

## 6.6.1 Higher Order Fixed Effects

A higher order (balanced) fixed-effects model or factorial experiment presents no new difficulties. As an illustration, consider a three-way classification with three factors A , B , and C and all interactions AB , AC , BC , and ABC . The observation vector y has p variables as usual. The MANOVA model allowing for main effects and interactions can be written as

<!-- formula-not-decoded -->

where, for example, 𝛂 i is the effect of the i th level of factor A on each of the p variables in y i j kl and 𝛅 i j is the AB interaction effect on each of the p variables.

Similarly, 𝛈 i k , 𝛕 j k , and 𝛟 i j k represent the AC , BC , and ABC interactions on each of the p variables.

The matrices of sums of squares and products for main effects, interactions, and error are defined in a manner similar to that for the matrices detailed for the two-way model in Section 6.5.2. The sum of squares (on the diagonal) for each variable is calculated exactly the same as in a univariate ANOV A for a three-way model. The sums of products (off-diagonal) are obtained analogously. Test construction parallels that for the two-way model, using the matrix for error to test all factors and interactions.

Degrees of freedom for each factor are the same as in the corresponding threeway univariate model. All four MANOVA test statistics can be computed for each test. Contrasts can be defined and tested in a manner similar to that in Section 6.5.2. Follow-up procedures on the individual variables ( F -tests and discriminant functions) can be used as discussed for the one-way or two-way models in Sections 6.4 and 6.5.2.

## 6.6.2 Mixed Models

There is a MANOVA counterpart for every univariate ANOVA design. This applies to fixed, random, and mixed models and to experimental structures that are crossed, nested, or a combination. Roebruck (1982) has provided a formal proof that univariate mixed models can be generalized to multivariate mixed models. Schott and Saw (1984) have shown that for the one-way multivariate random effects model, the likelihood ratio approach leads to the same test statistics involving the eigenvalues of E -1 H as in the fixed-effects model.

In the (balanced) MANOVA mixed model, the expected mean square matrices have exactly the same pattern as expected mean squares for the corresponding univariate ANOVA model. Thus a table of expected mean squares for the terms in the corresponding univariate model provides direction for choosing the appropriate error matrix to test each term in the MANOVA model. However, if the matrix indicated for 'error' has fewer degrees of freedom than p , it will not have an inverse and the test cannot be made.

To illustrate, suppose we have a (balanced) two-way MANOVA model with A fixed and B random. Then the (univariate) expected mean squares (EMS) and Wilks' /Lambda1 -tests are as follows:

| Source   | EMS                         | /Lambda1                  |
|----------|-----------------------------|---------------------------|
| A        | σ 2 + n σ 2 AB + nb σ ∗ 2 A | | H AB | / | H AB + H A | |
| B        | σ 2 + na σ 2 B              | | E | / | E + H B |       |
| AB Error | σ 2 + n σ 2 AB σ 2          | | E | / | E + H AB |      |

In the expected mean square for factor A , we have used the notation σ ∗ 2 A in place of ∑ a i = 1 α 2 i /( a -1 ) . The test for A using H AB for error matrix will be indeterminate (of the form 0 / 0) if ν AB ≤ p , where ν AB = ( a -1 )( b -1 ) . In this case, ν AB will often fail to exceed p . For example, suppose A has two levels and B has three.

Then ν AB = 2, which will ordinarily be less than p . In such a case, we would have little recourse except to compute univariate tests on the p individual variables. However, we would not have the multivariate test to protect against carrying out too many univariate tests and thereby inflating the experimentwise α (see Section 6.4). To protect against inflation of α when making p tests, we could use a Bonferroni correction, as in procedure 2 in Section 5.5. In the case of F -tests, we do not have a table of Bonferroni critical values, as we do for t -tests (Table A.8), but we can achieve an equivalent result by comparing the p -values for the F -tests against α/ p instead of against α .

As another illustration, consider the analysis for a (balanced) multivariate splitplot design. For simplicity, we show the associated univariate model in place of the multivariate model. We use the factor names A , B , AC , . . . to indicate parameters in the model:

<!-- formula-not-decoded -->

where A and C are fixed and B is random. Nesting is indicated by bracketed subscripts; for example, B and BC are nested in A . Table 6.7 shows the expected mean squares and corresponding Wilks tests.

Table 6.7. Wilks' Λ -Tests for a Typical Split-Plot Design

| Source            | df                                                                            | Expected Mean Squares                                                                                                    | Wilks' /Lambda1                                                                                                       |
|-------------------|-------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------|
| A B C AC BC Error | a - 1 a ( b - 1 ) c - 1 ( a - 1 )( c - 1 ) a ( b - 1 )( c - 1 ) abc ( e - 1 ) | σ 2 + ce σ 2 B + bce σ ∗ 2 A σ 2 + ce σ 2 B σ 2 + e σ 2 BC + abe σ ∗ 2 C σ 2 + e σ 2 BC + be σ ∗ 2 AC σ 2 + e σ 2 BC σ 2 | | H B | / | H A + H B | | E | / | H B + E | | H BC | / | H C + H BC | | H BC | / | H AC + H BC | | E | / | H BC + E | |

Since we use H B and H BC , as well as E , to make tests, the following must hold:

<!-- formula-not-decoded -->

To construct the other three MANOVA tests, we use eigenvalues of the following matrices:

| Source   | Matrix        |
|----------|---------------|
| A        | H - 1 B H A   |
| B        | E - 1 H B     |
| C        | H - 1 BC H C  |
| AC       | H - 1 BC H AC |
| BC       | E - 1 H BC    |

With a table of expected mean squares, such as those in Table 6.7, it is a simple matter to determine the error matrix in each case. For a given factor or interac-

tion, such as A , B , or AC , the appropriate error matrix is ordinarily the one whose expected mean square matches that of the given factor except for the last term. For example, factor C , with expected mean square σ 2 + e σ 2 BC + abe σ ∗ 2 C , is tested by BC , whose expected mean square is σ 2 + e σ 2 BC . If H 0 : σ ∗ 2 C = 0 is true, then C and BC have the same expected mean square.

In some mixed and random models, certain terms have no available error term. When this happens in the univariate case, we can construct an approximate test using Satterthwaites' (1941) or other synthetic mean square approach. For a similar approach in the multivariate case, see Khuri, Mathew, and Nel (1994).

## 6.7 CHECKING ON THE ASSUMPTIONS

In Section 6.2 we discussed the robustness of the four MANOVA test statistics to nonnormality and heterogeneity of covariance matrices. The MANOVA tests (except for Roy's) are rather robust to these departures from the assumptions, although, in general, as dimensionality increases, robustness decreases.

Even though MANOVA procedures are fairly robust to departures from multivariate normality, we may want to check for gross violations of this assumption. Any of the tests or plots from Section 4.4 could be used. For a two-way design, for example, the tests could be applied separately to the n observations in each individual cell (if n is sufficiently large) or to all the residuals. The residual vectors after fitting the model y i j k = 𝛍 i j + 𝛆 i j k would be

<!-- formula-not-decoded -->

It is also advisable to check for outliers, which can lead to either a Type I or Type II error. The tests of Section 4.5 can be run separately for each cell (for sufficiently large n ) or for all of the abn residuals, ˆ 𝛆 i j k = y i j k -y i j . .

A test of the equality of covariance matrices can be made using Box's M -test given in Section 7.3.2. Note the cautions expressed there about the sensitivity of this test to nonnormality and unequal sample sizes.

The assumption of independence of the observation vectors y i j k is even more important than the assumptions of normality and equality of covariance matrices. We are referring, of course, to independence from one observation vector to another. The variables within a vector are assumed to be correlated, as usual. In the univariate case, Barcikowski (1981) showed that a moderate amount of dependence among the observations produces an actual α much greater than the nominal α . This effect is to be expected, since the dependence leads to an underestimate of the variance, so that MSE is reduced and the F -statistic is inflated. We can assume that this effect on error rates carries over to MANOVA.

In univariate ANOVA, a simple measure of dependence among the kn observations in a one-way model is the intraclass correlation :

<!-- formula-not-decoded -->

where MSB and MSE are the between and within mean squares for the variable and n is the number of observations per group. This could be calculated for each variable in a MANOVA to check for independence.

In many experimental settings, we do not anticipate a lack of independence. But in certain cases the observations are dependent. For example, if the sampling units are people, they may influence each other as they interact together. In some educational studies, researchers must use entire classrooms as sampling units rather than use individual students. Another example of dependence is furnished by observations that are serially correlated , as in a time series , for example. Each observation depends to a certain extent on the preceding one, and its random movement is somewhat dampened as a result.

## 6.8 PROFILE ANALYSIS

The two-sample profile analysis of Section 5.9.2 can be extended to k groups. Again we assume that the variables are commensurate, as, for example, when each subject is given a battery of tests. Other assumptions, cautions, and comments expressed in Section 5.9.2 apply here as well.

The basic model is the balanced one-way MANOVA:

<!-- formula-not-decoded -->

To test H 0 : 𝛍 1 = 𝛍 2 = ·· · = 𝛍 k , we use the usual H and E matrices given in (6.9) and (6.10). If the variables are commensurate, we can be more specific and extend H 0 to an examination of the k profiles obtained by plotting the p values µ i 1, µ i 2 , . . . , µ i p in each 𝛍 i , as was done with two 𝛍 i 's in Section 5.9.2 (see, for example, Figure 5.8). We are interested in the same three hypotheses as before:

H 01 :

The k profiles are parallel.

H 02 :

The k profiles are all at the same level.

The k profiles are flat.

H 03 :

The hypothesis of parallelism for two groups was expressed in Section 5.9.2 as H 01 : C 𝛍 1 = C 𝛍 2, where C is any ( p -1 ) × p matrix of rank p -1 such that Cj = 0 , for example,

<!-- formula-not-decoded -->

For k groups, the analogous hypothesis of parallelism is

<!-- formula-not-decoded -->

The hypothesis (6.78) is equivalent to the hypothesis H 0 : 𝛍 z 1 = 𝛍 z 2 = · · · = 𝛍 zk in a one-way MANOVA on the transformed variables z i j = Cy i j . Since C has p -1 rows, Cy i j is ( p -1 ) × 1, C 𝛍 i is ( p -1 ) × 1, and C 𝚺 C ′ is ( p -1 ) × ( p -1 ) . By property 1b in Section 4.2, z i j is distributed as Np -1 ( C 𝛍 i , C 𝚺 C ′ ) .

By analogy with (3.64), the hypothesis and error matrices for testing H 01 in (6.78) are

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

which is distributed as /Lambda1 p -1 ,ν H ,ν E , where ν H = k -1 and ν E = k ( n -1 ) . The other three MANOVA test statistics can be obtained from the eigenvalues of ( CEC ′ ) -1 ( CHC ′ ) . The test for H 01 can easily be adjusted for unbalanced data, as in Section 6.1.6. We would calculate H and E by (6.32) and (6.33) and use ν E = i ni -k .

∑ The hypothesis that two profiles are at the same level is H 02 : j ′ 𝛍 1 = j ′ 𝛍 2 (see Section 5.9.2), which generalizes immediately to k profiles at the same level,

<!-- formula-not-decoded -->

For two groups we used a univariate t , as defined in (5.36), to test H 02. Similarly, for k groups we can employ an F -test for one-way ANOVA comparing k groups with observations j ′ y i j . Alternatively, we can utilize (6.79) with C = j ′ ,

<!-- formula-not-decoded -->

which is distributed as /Lambda1 1 ,ν H ,ν E ( p = 1 because j ′ y i j is a scalar). This is, of course, equivalent to the F -test on j ′ y i j , since by Table 6.1 in Section 6.1.3,

<!-- formula-not-decoded -->

We thus have is distributed as F ν H ,ν E .

The third hypothesis, that of 'flatness,' essentially states that the average of the k group means is the same for each variable [see (5.37)]:

<!-- formula-not-decoded -->

or by analogy with (5.38),

<!-- formula-not-decoded -->

where C is a ( p -1 ) × p matrix of rank p -1 such that Cj = 0 [see (6.78)]. The flatness hypothesis can also be stated as, the means of all p variables in each group are the same, or µ i 1 = µ i 2 = · · · = µ i p , i = 1 , 2 , . . . , k . This can be expressed as H 03 : C 𝛍 1 = C 𝛍 2 = · · · = C 𝛍 k = 0 .

To test H 03 as given by (6.83), we can extend the T 2 -test in (5.39). The grand mean vector ( 𝛍 1 + 𝛍 2 +···+ 𝛍 k )/ k in (6.83) can be estimated as in Section 6.1.2 by

<!-- formula-not-decoded -->

Under H 03 (and H 01), Cy .. is Np -1 ( 0 , C 𝚺 C ′ / kn ) , and H 03 can be tested by

<!-- formula-not-decoded -->

where E /ν E is an estimate of 𝚺 . As in the two-sample case, H 03 is unaffected by the status of H 02. When both H 01 and H 03 are true, T 2 in (6.84) is distributed as T 2 p -1 ,ν E .

Example 6.8. Three vitamin E diet supplements with levels zero, low, and high were compared for their effect on growth of guinea pigs (Crowder and Hand 1990, pp. 2129). Five guinea pigs received each supplement level and their weights were recorded at the end of weeks 1, 3, 4, 5, 6, and 7. These weights are given in Table 6.8.

Table 6.8. Weights of Guinea Pigs under Three Levels of Vitamin E Supplements

|   Group |   Animal |   Week 1 |   Week 3 |   Week 4 |   Week 5 |   Week 6 |   Week 7 |
|---------|----------|----------|----------|----------|----------|----------|----------|
|       1 |        1 |      455 |      460 |      510 |      504 |      436 |      466 |
|       1 |        2 |      467 |      565 |      610 |      596 |      542 |      587 |
|       1 |        3 |      445 |      530 |      580 |      597 |      582 |      619 |
|       1 |        4 |      485 |      542 |      594 |      583 |      611 |      612 |
|       1 |        5 |      480 |      500 |      550 |      528 |      562 |      576 |
|       2 |        6 |      514 |      560 |      565 |      524 |      552 |      597 |
|       2 |        7 |      440 |      480 |      536 |      484 |      567 |      569 |
|       2 |        8 |      495 |      570 |      569 |      585 |      576 |      677 |
|       2 |        9 |      520 |      590 |      610 |      637 |      671 |      702 |
|       2 |       10 |      503 |      555 |      591 |      605 |      649 |      675 |
|       3 |       11 |      496 |      560 |      622 |      622 |      632 |      670 |
|       3 |       12 |      498 |      540 |      589 |      557 |      568 |      609 |
|       3 |       13 |      478 |      510 |      568 |      555 |      576 |      605 |
|       3 |       14 |      545 |      565 |      580 |      601 |      633 |      649 |
|       3 |       15 |      472 |      498 |      540 |      524 |      532 |      583 |

The three mean vectors are

<!-- formula-not-decoded -->

and the overall mean vector is

<!-- formula-not-decoded -->

A profile plot of the means y 1 . , y 2 . , and y 3 . is given in Figure 6.3. There is a high degree of parallelism in the three profiles, with the possible exception of week 6 for group 1.

The E and H matrices are as follows:

<!-- formula-not-decoded -->

Figure 6.3. Profile of the three groups for the guinea pig data of Table 6.8.

<!-- image -->

<!-- formula-not-decoded -->

Using

<!-- formula-not-decoded -->

in the test statistic (6.79), we have, as a test for parallelism,

<!-- formula-not-decoded -->

Thus we do not reject the parallelism hypothesis.

To test the hypothesis that the three profiles are at the same level, we use (6.81),

<!-- formula-not-decoded -->

Hence we do not reject the levels hypothesis. This can also be seen by using (6.82) to transform /Lambda1 to F ,

<!-- formula-not-decoded -->

which is clearly nonsignificant ( p = . 378 ) .

To test the 'flatness' hypothesis, we use (6.84):

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Thus only the flatness hypothesis is rejected in this case.

## 6.9 REPEATED MEASURES DESIGNS

## 6.9.1 Multivariate vs. Univariate Approach

In repeated measures designs, the research unit is typically a human or animal subject. Each subject is measured under several treatments or at different points of time. The treatments might be tests, drug levels, various kinds of stimuli, and so on. If the treatments are such that the order of presentation to the various subjects can be varied, then the order should be randomized to avoid an ordering bias. If subjects are measured at successive time points, it may be of interest to determine the degree of polynomial required to fit the curve. This is treated in Section 6.10 as part of an analysis of growth curves.

When comparing means of the treatments applied to each subject, we are examining the within-subjects factor. There will also be a between-subjects factor if there are several groups of subjects that we wish to compare. In Sections 6.9.2-6.9.6, we consider designs up to a complexity level of two within-subjects factors and two between-subjects factors.

We now discuss univariate and multivariate approaches to hypothesis testing in repeated measures designs. As a framework for this discussion, consider the layout in Table 6.9 for a repeated measures design with one repeated measures (withinsubjects) factor, A , and one grouping (between-subjects) factor, B .

This design has often been analyzed as a univariate mixed-model nested design, also called a split-plot design, with subjects nested in factor B (whole-plot), which is crossed with factor A (repeated measures, or split-plot). The univariate model for each yi jr would be

<!-- formula-not-decoded -->

where the factor level designations ( B , S , A , and BA ) from (6.85) and Table 6.9 are used as parameter values and the subscript ( i ) j on S indicates that subjects are nested in factor B . In Table 6.9, the observations yi jr for r = 1 , 2 , . . . , p are enclosed in parentheses and denoted by y ′ i j to emphasize that these p variables are measured on one subject and thus constitute a vector of correlated variables. The ranges of the subscripts can be seen in Table 6.9: i = 1 , 2 , . . . , k ; j = 1 , 2 , . . . , n ; and r =

Table 6.9. Data Layout for k -Groups Repeated Measures Experiment

| Factor B   |                   | Factor A (Repeated Measures)   | Factor A (Repeated Measures)   | Factor A (Repeated Measures)   | Factor A (Repeated Measures)   | Factor A (Repeated Measures)   |
|------------|-------------------|--------------------------------|--------------------------------|--------------------------------|--------------------------------|--------------------------------|
| (Group)    | Subjects          | A 1                            | A 2                            | · · ·                          | A p                            |                                |
| B 1        | S 11 S 12 . . .   | ( y 111 ( y 121 . . .          | y 112 y 122 . . .              | · · · · · ·                    | y 11 p ) y 12 p ) . . .        | = y ′ 11 = y ′ 12              |
|            | S 1 n             | ( y 1 n 1                      | y 1 n 2                        | · · ·                          | y 1 np )                       | = y ′ 1 n                      |
| B 2        | S 21 S 22 . . .   | ( y 211 ( y 221 . . .          | y 212 y 222 . . .              | · · · . . .                    | y 21 p ) y 22 p ) . . .        | = y ′ 21 = y ′ 22              |
| . . .      | . . .             | . . .                          | . . .                          | . . .                          | . . .                          | = y ′                          |
| B k        | S k 1 S k 2 . . . | ( y k 11 ( y k 21 . . .        | y k 12 y k 22 . . .            | . . .                          | y k 1 p ) y k 2 p ) . . .      | k 1 = y ′ k 2                  |

## 1 , 2 , . . . , p . With factors A and B fixed and subjects random, the univariate ANOV A is given in Table 6.10.

However, our initial reaction would be to rule out the univariate ANOV A because the y 's in each row are correlated and the assumption of independence is critical, as noted in Section 6.7. We will discuss below some assumptions under which the univariate analysis would be appropriate.

In the multivariate approach, the p responses yi j 1, yi j 2 , . . . , yi j p (repeated measures) for subject Si j constitute a vector y i j , as shown in Table 6.9. The multivariate model for y i j is a simple one-way MANOVA model,

<!-- formula-not-decoded -->

Table 6.10. Univariate ANOVA for Data Layout in Table 6.9

| Source                  | df                   | MS   | F        |
|-------------------------|----------------------|------|----------|
| B (between)             | k - 1                | MSB  | MSB/MSS  |
| S (subjects)            | k ( n - 1 )          | MSS  |          |
| A (within or repeated)  | p - 1                | MSA  | MSA/MSE  |
| BA                      | ( k - 1 )( p - 1 )   | MSBA | MSBA/MSE |
| Error ( SA interaction) | k ( n - 1 )( p - 1 ) | MSE  |          |

where 𝛃 i is a vector of p main effects (corresponding to the p variables in y i j ) for factor B , and 𝛆 i j is an error vector for subject Si j . This model seems to include only factor B , but we show in Section 6.9.3 how to use an approach similar to profile analysis in Section 6.8 to obtain tests on factor A and the BA interaction. The MANOVA assumption that cov ( y i j ) = 𝚺 for all i and j allows the p repeated measures to be correlated in any pattern, since 𝚺 is completely general. On the other hand, the ANOVA assumptions of independence and homogeneity of variances can be expressed as cov ( y i j ) = σ 2 I . We would be very surprised if repeated measurements on the same subject were independent.

The univariate ANOVA approach has been found to be appropriate under less stringent conditions than 𝚺 = σ 2 I . Wilks (1946) showed that the ordinary F -tests of ANOVA remain valid for a covariance structure of the form

<!-- formula-not-decoded -->

where J is a square matrix of 1's, as defined in (2.12) [see Rencher (2000, pp. 150151)]. The covariance pattern (6.87) is variously known as uniformity, compound symmetry , or the intraclass correlation model. It allows for the variables to be correlated but restricts every variable to have the same variance and every pair of variables to have the same covariance. In a carefully designed experiment with appropriate randomization, this assumption may hold under the hypothesis of no A effect. Alternatively, we could use a test of the hypothesis that 𝚺 has the pattern (6.87) (see Section 7.2.3). If this hypothesis is accepted, one could proceed with the usual ANOV A F -tests.

Bock (1963) and Huynh and Feldt (1970) showed that the most general condition under which univariate F -tests remain valid is that

<!-- formula-not-decoded -->

where C is a ( p -1 ) × p matrix whose rows are orthonormal contrasts (orthogonal contrasts that have been normalized to unit length). We can construct C by choosing any p -1 orthogonal contrasts among the means µ 1, µ 2 , . . . , µ p of the repeated measures factor and dividing each contrast ∑ p r = 1 cr µ r by √ ∑ p r = 1 c 2 r . (This matrix C is different from C used in Section 6.8 and in the remainder of Section 6.9, whose rows are contrasts that are not normalized to unit length.) It can be shown that (6.87) is a special case of (6.88). The condition (6.88) is sometimes referred to as sphericity , although this term can also refer to the covariance pattern 𝚺 = σ 2 I on the untransformed y i j (see Section 7.2.2).

A simple way to test the hypothesis that (6.88) holds is to transform the data by z i j = Cy i j and test H 0 : 𝚺 z = σ 2 I , as in Section 7.2.2, using CS pl C ′ in place of S p1 = E /ν E .

Thus one procedure for repeated measures designs is to make a preliminary test for (6.87) or (6.88) and, if the hypothesis is accepted, use univariate F -tests, as in Table 6.10. Fehlberg (1980) investigated the use of larger α -values with a preliminary test of structure of the covariance matrix, as in (6.88). He concludes that using α = . 40 sufficiently controls the problem of falsely accepting sphericity so as to justify the use of a preliminary test.

If the univariate test for the repeated measures factor A is appropriate, it is more powerful because it has more degrees of freedom for error than the corresponding multivariate test. However, even mild departures from (6.88) seriously inflate the Type I error rate of the univariate test for factor A (Box 1954; Davidson 1972; Boik 1981). Because such departures can be easily missed in a preliminary test, Boik (1981) concludes that 'on the whole, the ordinary F tests have nothing to recommend them' (p. 248) and emphasized that 'there is no justification for employing ordinary univariate F tests for repeated measures treatment contrasts' (p. 254).

Another approach to analysis of repeated measures designs is to adjust the univariate F -test for the amount of departure from sphericity. Box (1954) and Greenhouse and Geisser (1959) showed that when 𝚺 /negationslash= σ 2 I , an approximate F -test for effects involving the repeated measures is obtained by reducing the degrees of freedom for both numerator and denominator by a factor of

<!-- formula-not-decoded -->

where J is a p × p matrix of 1's defined in (2.12). For example, in Table 6.10 the F -value for the BA interaction would be compared to F α with ε( k -1 )( p -1 ) and ε k ( n -1 )( p -1 ) degrees of freedom. An estimate ˆ ε can be obtained by substituting ˆ 𝚺 = E /ν E in (6.89). Greenhouse and Geisser (1959) showed that ε and ˆ ε vary between 1 /( p -1 ) and 1, with ε = 1 when sphericity holds and ε ≥ 1 /( p -1 ) for other values of 𝚺 . Thus ε is a measure of nonsphericity. For a conservative test, Greenhouse and Geisser recommend dividing numerator and denominator degrees of freedom by p -1. Huynh and Feldt (1976) provided an improved estimator of ε .

The behavior of the approximate univariate F -test with degrees of freedom adjusted by ˆ ε has been investigated by Collier et al. (1967), Huynh (1978), Davidson (1972), Rogan, Keselman, and Mendoza (1979), and Maxwell and Avery (1982). In these studies, the true α -level turned out to be close to the nominal α , and the power was close to that of the multivariate test. However, since the ε -adjusted F -test is only approximate and has no power advantage over the exact multivariate test, there appears to be no compelling reason to use it. The only case in which we need to fall back on a univariate test is when there are insufficient degrees of freedom to perform a multivariate test, that is, when p &gt; ν E .

In Sections 6.9.2-6.9.6, we discuss the multivariate approach to repeated measures. We will cover several models, beginning with the simple one-sample design.

## 6.9.2 One-Sample Repeated Measures Model

We illustrate some of the procedures in this section with p = 4. A one-sample design with four repeated measures on n subjects would appear as in Table 6.11. There is a superficial resemblance to a univariate randomized block design. However, in the repeated measures design, the observations yi 1, yi 2, yi 3, and yi 4 are correlated because they are measured on the same subject (experimental unit), whereas in a randomized block design yi 1, yi 2, yi 3, and yi 4 would be measured on four different experimental units. Thus we have a single sample of n observation vectors y 1 , y 2 , . . . , y n .

To test for significance of factor A , we compare the means of the four variables in y i ,

<!-- formula-not-decoded -->

The hypothesis is H 0 : µ 1 = µ 2 = µ 3 = µ 4, which can be reexpressed as H 0 : µ 1 -µ 2 = µ 2 -µ 3 = µ 3 -µ 4 = 0 or C 1 𝛍 = 0 , where

<!-- formula-not-decoded -->

To test H 0 : C 1 𝛍 = 0 for a general value of p ( p repeated measures on n subjects), we calculate y and S from y 1, y 2 , . . . , y n and extend C 1 to p -1 rows. Then when H 0 is true, C 1 y is Np -1 ( 0 , C 1 𝚺 C ′ 1 / n ) , and

<!-- formula-not-decoded -->

is distributed as T 2 p -1 , n -1 . We reject H 0 : C 1 𝛍 = 0 if T 2 ≥ T 2 α, p -1 , n -1 . Note that the dimension is p -1 because C 1 y is ( p -1 ) × 1 [see (5.33)].

The multivariate approach involving transformed observations z i = C 1 y i was first suggested by Hsu (1938) and has been discussed further by Williams (1970)

Table 6.11. Data Layout for a Single-Sample Repeated Measures Design

|          | Factor A (Repeated Measures)   | Factor A (Repeated Measures)   | Factor A (Repeated Measures)   | Factor A (Repeated Measures)   | Factor A (Repeated Measures)   |
|----------|--------------------------------|--------------------------------|--------------------------------|--------------------------------|--------------------------------|
| Subjects | A 1                            | A 2                            | A 3                            | A 4                            |                                |
| S 1      | ( y 11                         | y 12                           | y 13                           | y 14 )                         | = y ′ 1                        |
| S 2      | ( y 21                         | y 22                           | y 23                           | y 24 )                         | = y ′ 2                        |
| .        | .                              | .                              | .                              | .                              | .                              |
| . .      | .                              | . .                            | . .                            | . .                            | . .                            |
|          | .                              | y n                            | y                              |                                | y ′                            |
| S n      | ( y n 1                        | 2                              | n 3                            | y n 4 )                        | = n                            |

and Morrison (1972). Note that in C 1 y (for p = 4), we work with contrasts on the elements y 1 , y 2 , y 3 , and y 4 within the vector

<!-- formula-not-decoded -->

as opposed to the contrasts involving comparisons of several mean vectors themselves, as, for example, in Section 6.3.2.

The hypothesis H 0 : µ 1 = µ 2 = µ 3 = µ 4 can also be expressed as H 0 : µ 1 -µ 4 = µ 2 -µ 4 = µ 3 -µ 4 = 0, or C 2 𝛍 = 0 , where

<!-- formula-not-decoded -->

The matrix C 1 can be obtained from C 2 by simple row operations, for example, subtracting the second row from the first and the third row from the second. Hence, C 1 = AC 2, where

<!-- formula-not-decoded -->

In fact, H 0 : µ 1 = µ 2 = · · · = µ p can be expressed as C 𝛍 = 0 for any full-rank ( p -1 ) × p matrix C such that Cj = 0 , and the same value of T 2 in (6.90) will result. The contrasts in C can be either linearly independent or orthogonal.

The hypothesis H 0 : µ 1 = µ 2 = · · · = µ p = µ , say, can also be expressed as

<!-- formula-not-decoded -->

where j = ( 1 , 1 , . . . , 1 ) ′ . The maximum likelihood estimate of µ is

<!-- formula-not-decoded -->

The likelihood ratio test of H 0 is a function of

<!-- formula-not-decoded -->

Williams (1970) showed that for any ( p -1 ) × p matrix C of rank p -1 such that Cj = 0 ,

<!-- formula-not-decoded -->

and thus the T 2 -test in (6.90) is equivalent to the likelihood ratio test.

Example 6.9.2. The data in Table 6.12 were given by Cochran and Cox (1957, p. 130). As rearranged by Timm (1980), the observations constitute a one-sample repeated measures design with two within-subjects factors. Factor A is a comparison of two tasks; factor B is a comparison of two types of calculators. The measurements are speed of calculation.

To test the hypothesis H 0 : µ 1 = µ 2 = µ 3 = µ 4, we use the contrast matrix

<!-- formula-not-decoded -->

where the first row compares the two levels of A , the second row compares the two levels of B , and the third row corresponds to the AB interaction. From the five observation vectors in Table 6.12, we obtain

<!-- formula-not-decoded -->

For the overall test of equality of means, we have, by (6.90),

<!-- formula-not-decoded -->

Since the T 2 -test is not significant, we would ordinarily not proceed with tests based on the individual rows of C . We will do so, however, for illustrative purposes. (Note that the T 2 -test has very low power in this case, because n -1 = 4 is very small.)

To test A , B , and AB , we test each row of C , where T 2 = n ( c ′ i y ) ′ ( c ′ i Sc i ) -1 c ′ i y is the square of the t -statistic

Table 6.12. Calculator Speed Data

|          | A 1   | A 1   | A 2   | A 2   |
|----------|-------|-------|-------|-------|
| Subjects | B 1   | B 2   | B 1   | B 2   |
| S 1      | 30    | 21    | 21    | 14    |
| S 2      | 22    | 13    | 22    | 5     |
| S 3      | 29    | 13    | 18    | 17    |
| S 4      | 12    | 7     | 16    | 14    |
| S 5      | 23    | 24    | 23    | 8     |

<!-- formula-not-decoded -->

where c ′ i is the i th row of C .

The three results are as follows:

Factor A :

t 1 = 1 . 459 &lt; t . 025 , 4 = 2 . 776 ,

Factor B : 2 = . 005 , 4 =

t 5 . 247 &gt; t 4 . 604 ,

Interaction AB :

t 3 = -. 152 .

Thus only the main effect for B is significant. Note that in all but one case in Table 6.12, the value for B 1 is greater than that for B 2.

## 6.9.3 k -Sample Repeated Measures Model

We turn now to the k -sample repeated measures design depicted in Table 6.9. As noted in Section 6.9.1, the multivariate approach to this repeated measures design uses the one-way MANOVA model y i j = 𝛍 + 𝛃 i + 𝛆 i j = 𝛍 i + 𝛆 i j . From the k groups of n observation vectors each, we calculate y 1 . , y 2 . , . . . , y k . and the error matrix E .

The layout in Table 6.9 is similar to that of a k -sample profile analysis in Section 6.8. To test (the within-subjects) factor A , we need to compare the means of the variables y 1, y 2 , . . . , yp within y averaged across the levels of factor B . The p variables correspond to the levels of factor A . In the model y i j = 𝛍 i + 𝛆 i j , the mean vectors 𝛍 1, 𝛍 2 , . . . , 𝛍 k correspond to the levels of factor B and are estimated by y 1 . , y 2 . , . . . , y k . . To compare the means of y 1, y 2 , . . . , yp averaged across the levels of B , we use 𝛍 . = ∑ k i = 1 𝛍 i / k , which is estimated by y .. = ∑ k i = 1 y i . / k . The hypothesis H 0 : µ. 1 = µ. 2 = · · · = µ. p comparing the means of y 1, y 2 , . . . , yp (for factor A ) can be expressed using contrasts:

<!-- formula-not-decoded -->

where C is any ( p -1 ) × p full-rank contrast matrix with Cj = 0 . This is equivalent to the 'flatness' test of profile analysis, the third test in Section 6.8. Under H 0, the vector Cy .. is distributed as Np -1 ( 0 , C 𝚺 C ′ / N ) , where N = ∑ i ni for an unbalanced design and N = kn in the balanced case. We can, therefore, test H 0 with

<!-- formula-not-decoded -->

where S pl = E /ν E . The T 2 -statistic in (6.93) is distributed as T 2 p -1 ,ν E when H 0 is true, where ν E = N -k [see (6.84) and the comments following]. Note that the dimension of T 2 is p -1 because Cy .. is ( p -1 ) × 1.

For the grouping or between-subjects factor B , we wish to compare the means for the k levels of B . The mean response for the i th level of B (averaged over the levels of A ) is ∑ p r = 1 µ ir / p = j ′ 𝛍 i / p . The hypothesis can be expressed as

<!-- formula-not-decoded -->

which is analogous to (6.80), the 'levels' hypothesis in profile analysis. This is easily tested by calculating a univariate F -statistic for a one-way ANOVA on zi j = j ′ y i j , i = 1 , 2 , . . . , k ; j = 1 , 2 , . . . , ni . There is a zi j corresponding to each subject, Si j . The observation vector for each subject is thus reduced to a single scalar observation, and we have a one-way ANOVA comparing the means j ′ y 1 . , j ′ y 2 . , . . . , j ′ y k . . (Note that j ′ y i . / p is an average over the p levels of A .)

The AB interaction hypothesis is equivalent to the parallelism hypothesis in profile analysis [see (6.78)],

<!-- formula-not-decoded -->

In other words, differences or contrasts among the levels of factor A are the same across all levels of factor B . This is easily tested by performing a one-way MANOVA on z i j = Cy i j or directly by

<!-- formula-not-decoded -->

[see (6.78)], which is distributed as /Lambda1 p -1 ,ν H ,ν E , with ν H = k -1 and ν E = N -k ; that is, ν E = ∑ i ( ni -1 ) for the unbalanced model or ν E = k ( n -1 ) in the balanced model.

## 6.9.4 Computation of Repeated Measures Tests

Some statistical software packages have automated repeated measures procedures that are easily implemented. However, if one is unsure as to how the resulting tests correspond to the tests in Section 6.9.3, there are two ways to obtain the tests directly. One approach is to calculate (6.93) and (6.96) outright using a matrix manipulation routine. We would need to have available the E and H matrices of a one-way MANOVA using a data layout as in Table 6.9.

The second approach uses simple data transformations available in virtually all programs. To test (6.92) for factor A , we would transform each y i j to z i j = Cy i j by using the rows of C . For example, if

<!-- formula-not-decoded -->

then each y ′ = ( y 1 , y 2 , y 3 , y 4 ) becomes z ′ = ( y 1 -y 2 , y 2 -y 3 , y 3 -y 4 ) . We then test H 0 : 𝛍 z = 0 using a one-sample T 2 on all N of the z i j 's,

<!-- formula-not-decoded -->

where N = ∑ i ni , z = ∑ i j z i j / N , and S z = E z /ν E is the pooled covariance matrix. Reject H 0 if T 2 ≥ T 2 α, p -1 ,ν E .

To test (6.94) for factor B , we sum the components of each observation vector to obtain zi j = j ′ y i j = yi j 1 + yi j 2 +···+ yi j p and compare the means z 1 . , z 2 . , . . . , zk . by an F -test, as in one-way ANOVA.

To test the interaction hypothesis (6.95), we transform each y i j to z i j = Cy i j using the rows of C , as before. Note that z i j is ( p -1 ) × 1. We then do a one-way MANOVA on z i j to obtain

<!-- formula-not-decoded -->

## 6.9.5 Repeated Measures with Two Within-Subjects Factors and One Between-Subjects Factor

The repeated measures model with two within-subjects factors A and B and one between-subjects factor C corresponds to a one-way MANOVA design in which each observation vector includes measurements on a two-way factorial arrangement of treatments. Thus each subject receives all treatment combinations of the two factors A and B . As usual, the sequence of administration of treatment combinations should be randomized for each subject. A design of this type is illustrated in Table 6.13.

Each y i j in Table 6.13 has nine elements, consisting of responses to the nine treatment combinations A 1 B 1, A 1 B 2 , . . . , A 3 B 3. We are interested in the same hypotheses as in a univariate split-plot design, but we use a multivariate approach to allow for correlated y 's. The model for the observation vectors is the one-way MANOVA model

<!-- formula-not-decoded -->

where 𝛄 i is the C effect.

To test factors A , B , and AB in Table 6.13, we use contrasts in the y 's. As an example of contrast matrices, consider

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Table 6.13. Data Layout for Repeated Measures with Two Within-Subjects Factors and One Between-Subjects Factor

| Between   |                         | Within-Subjects Factors   | Within-Subjects Factors   | Within-Subjects Factors   | Within-Subjects Factors   | Within-Subjects Factors   | Within-Subjects Factors       | Within-Subjects Factors   | Within-Subjects Factors   | Within-Subjects Factors   |
|-----------|-------------------------|---------------------------|---------------------------|---------------------------|---------------------------|---------------------------|-------------------------------|---------------------------|---------------------------|---------------------------|
| Subjects  |                         | A 1                       | A 1                       | A 1                       | A 2                       | A 2                       | A 2                           | A 3                       | A 3                       | A 3                       |
| Factor    | Subjects                | B 1                       | B 2                       | B 3                       | B 1                       | B 2                       | B 3                           | B 1                       | B 2                       | B 3                       |
| C 1       | S 11 S 12 . . . S 1 n 1 | ( y 111                   | y 112                     | y 113                     | y 114                     | y 115                     | y 116 y ′ 12 . . . y ′ 1 n 1  | y 117                     | y 118                     | y 119 ) = y ′ 11          |
| C 2       | S 21 S 22 . . . S 2 n 2 |                           |                           |                           |                           |                           | y ′ 21 y ′ 22 . . . y ′ 2 n 2 |                           |                           |                           |
| C 3       | S 31 S 32 . . . S 3 n 3 |                           |                           |                           |                           |                           | y ′ 31 y ′ 32 . . . y ′ 3 n 3 |                           |                           |                           |

<!-- formula-not-decoded -->

The rows of A are orthogonal contrasts with two comparisons:

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Other orthogonal (or linearly independent) contrasts could be used for A and B . The matrix G is for the AB interaction and is obtained from products of the corresponding elements of the rows of A and the rows of B .

As before, we define y .. = ∑ i j y i j / N , S pl = E /ν E , and N = ∑ i ni . If there were k levels of C in Table 6.13 with mean vectors 𝛍 1, 𝛍 2 , . . . , 𝛍 k , then 𝛍 . = ∑ k i = 1 𝛍 i / k , and the A main effect corresponding to H 0 : A 𝛍 . = 0 could be tested

Similarly, B compares

with

<!-- formula-not-decoded -->

which is distributed as T 2 2 ,ν E under H 0, where ν E = ∑ k i = 1 ( ni -1 ) . The dimension is 2, corresponding to the two rows of A .

Similarly, to test H 0 : B 𝛍 . = 0 and H 0 : G 𝛍 . = 0 for the B main effect and the AB interaction, respectively, we have

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

which are distributed as T 2 2 ,ν E and T 2 4 ,ν E , respectively. In general, if factor A has a levels and factor B has b levels, then A has a -1 rows, B has b -1 rows, and G has ( a -1 )( b -1 ) rows. The T 2 -statistics are then distributed as T 2 a -1 ,ν E , T 2 b -1 ,ν E , and T 2 ( a -1 )( b -1 ),ν E , respectively.

Factors A , B , and AB can be tested with Wilks' /Lambda1 as well as T 2 . Define H ∗ = N y .. y ′ .. from the partitioning ∑ i j y i j y ′ i j = E + H + N y .. y ′ .. . This can be used to test H 0 : 𝛍 . = 0 (not usually a hypothesis of interest) by means of

<!-- formula-not-decoded -->

which is /Lambda1 p , 1 ,ν E if H 0 is true. Then the hypothesis of interest, H 0 : A 𝛍 . = 0 for factor A , can be tested with

<!-- formula-not-decoded -->

which is distributed as /Lambda1 a -1 , 1 ,ν E when H 0 is true, where a is the number of levels of factor A . There are similar expressions for testing factors B and AB . Note that the dimension of /Lambda1 in (6.105) is a -1, because AEA ′ is ( a -1 ) × ( a -1 ) .

The T 2 and Wilks /Lambda1 expressions in (6.101) and (6.105) are related by

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

We can establish (6.106) as follows. By (6.28),

<!-- formula-not-decoded -->

By (6.14),

<!-- formula-not-decoded -->

Since rank ( H ∗ ) = 1, only λ 1 is nonzero, and

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

which is the same as (6.106).

Factor C is tested exactly as factor B in Section 6.9.3. The hypothesis is

<!-- formula-not-decoded -->

as in (6.94), and we perform a univariate F -test on zi j = j ′ y i j in a one-way ANOVA layout.

The AC , BC , and ABC interactions are tested as follows.

AC Interaction

The AC interaction hypothesis is

<!-- formula-not-decoded -->

which states that contrasts in factor A are the same across all levels of factor C . This can be tested by

<!-- formula-not-decoded -->

which is distributed as /Lambda1 2 ,ν H ,ν E , where a -1 = 2 is the number of rows of A and ν H and ν E are from the multivariate one-way model. Alternatively, the test can be carried out by transforming y i j to z i j = Ay i j and doing a one-way MANOVA on z i j .

BC Interaction

The BC interaction hypothesis,

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

which is /Lambda1 2 ,ν H ,ν E , where b -1 = 2; H 0 can also be tested by doing MANOVA on z i j = By i j .

ABC Interaction

The ABC interaction hypothesis,

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

which is /Lambda1 4 ,ν H ,ν E , or by doing MANOVA on z i j = Gy i j . In this case the dimension is ( a -1 )( b -1 ) = 4.

The preceding tests for AC , BC , or ABC can be also carried out with the other three MANOVA test statistics using eigenvalues of the appropriate matrices. For example, for AC we would use ( AEA ′ ) -1 ( AHA ′ ) .

Example 6.9.5. The data in Table 6.14 represent a repeated measures design with two within-subjects factors and one between-subjects factor (Timm 1980). Since A and B have three levels each, as in the illustration in this section, we will use the A , B , and G matrices in (6.98), (6.99), and (6.100). The E and H matrices are 9 × 9 and will not be shown. The overall mean vector is given by

<!-- formula-not-decoded -->

By (6.101), the test for factor A is

<!-- formula-not-decoded -->

is tested by is tested by

Table 6.14. Data from a Repeated Measures Experiment with Two Within-Subjects Factors and One Between-Subjects Factor

| Between   |          | Within-Subjects Factors   | Within-Subjects Factors   | Within-Subjects Factors   | Within-Subjects Factors   | Within-Subjects Factors   | Within-Subjects Factors   | Within-Subjects Factors   | Within-Subjects Factors   | Within-Subjects Factors   |
|-----------|----------|---------------------------|---------------------------|---------------------------|---------------------------|---------------------------|---------------------------|---------------------------|---------------------------|---------------------------|
| Subjects  |          | A 1                       | A 1                       | A 1                       |                           | A 2                       |                           |                           | A 3                       |                           |
| Factor    | Subjects | B 1                       | B 2                       | B 3                       | B 1                       | B 2                       | B 3                       | B 1                       | B 2                       | B 3                       |
| C 1       | S 11     | 20                        | 21                        | 21                        | 32                        | 42                        | 37                        | 32                        | 32                        | 32                        |
|           | S 12     | 67                        | 48                        | 29                        | 43                        | 56                        | 48                        | 39                        | 40                        | 41                        |
|           | S 13     | 37                        | 31                        | 25                        | 27                        | 28                        | 30                        | 31                        | 33                        | 34                        |
|           | S 14     | 42                        | 40                        | 38                        | 37                        | 36                        | 28                        | 19                        | 27                        | 35                        |
|           | S 15     | 57                        | 45                        | 32                        | 27                        | 21                        | 25                        | 30                        | 29                        | 29                        |
|           | S 16     | 39                        | 39                        | 38                        | 46                        | 54                        | 43                        | 31                        | 29                        | 28                        |
|           | S 17     | 43                        | 32                        | 20                        | 33                        | 46                        | 44                        | 42                        | 37                        | 31                        |
|           | S 18     | 35                        | 34                        | 34                        | 39                        | 43                        | 39                        | 35                        | 39                        | 42                        |
|           | S 19     | 41                        | 32                        | 23                        | 37                        | 51                        | 39                        | 27                        | 28                        | 30                        |
|           | S 1 , 10 | 39                        | 32                        | 24                        | 30                        | 35                        | 31                        | 26                        | 29                        | 32                        |
| C 2       | S 21     | 47                        | 36                        | 25                        | 31                        | 36                        | 29                        | 21                        | 24                        | 27                        |
|           | S 22     | 53                        | 43                        | 32                        | 40                        | 48                        | 47                        | 46                        | 50                        | 54                        |
|           | S 23     | 38                        | 35                        | 33                        | 38                        | 42                        | 45                        | 48                        | 48                        | 49                        |
|           | S 24     | 60                        | 51                        | 41                        | 54                        | 67                        | 60                        | 53                        | 52                        | 50                        |
|           | S 25     | 37                        | 36                        | 35                        | 40                        | 45                        | 40                        | 34                        | 40                        | 46                        |
|           | S 26     | 59                        | 48                        | 37                        | 45                        | 52                        | 44                        | 36                        | 44                        | 52                        |
|           | S 27     | 67                        | 50                        | 33                        | 47                        | 61                        | 46                        | 31                        | 41                        | 50                        |
|           | S 28     | 43                        | 35                        | 27                        | 32                        | 36                        | 35                        | 33                        | 33                        | 32                        |
|           | S 29     | 64                        | 59                        | 53                        | 58                        | 62                        | 51                        | 40                        | 42                        | 43                        |
|           | S 2 , 10 | 41                        | 38                        | 34                        | 41                        | 47                        | 42                        | 37                        | 41                        | 46                        |

For factor B , we use (6.102) to obtain

<!-- formula-not-decoded -->

By (6.103), the test for the AB interaction is given by

<!-- formula-not-decoded -->

To test factor C , we carry out a one-way ANOVA on zi j = j ′ y i j / 9:

| Source   |   Sum of Squares |   df |   Mean Square | F    |
|----------|------------------|------|---------------|------|
| Between  |          3042.22 |    1 |       3042.22 | 8.54 |
| Error    |          6408.98 |   18 |        356.05 |      |

The observed F , 8.54, has a p -value of .0091 and is therefore significant.

The AC interaction is tested by (6.108) as

<!-- formula-not-decoded -->

For the BC interaction, we have

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

In summary, factors A , B , and C and the AB interaction are significant.

## 6.9.6 Repeated Measures with Two Within-Subjects Factors and Two Between-Subjects Factors

In this section we consider a balanced two-way MANOVA design in which each observation vector arises from a two-way factorial arrangement of treatments. This is illustrated in Table 6.15 for a balanced design with three levels of all factors. Each y i j k has nine elements, consisting of responses to the nine treatment combinations A 1 B 1, A 1 B 2 , . . . , A 3 B 3 (see Table 6.13).

To test A , B , and AB , we can use the same contrast matrices A , B , and G as in (6.98)-(6.100). We define a grand mean vector y ... = ∑ i j k y i j k / N , where N is the total number of observation vectors; in this illustration, N = 27. In general, N = cdn , where c and d are the number of levels of factors C and D and n is the number of replications in each cell (in the illustration, n = 3). The test statistics for A , B , and AB are as follows, where S pl = E /ν E and the E matrix is obtained from the two-way MANOVA with ν E = cd ( n -1 ) degrees of freedom.

For ABC , we obtain

Table 6.15. Data Layout for Repeated Measures with Two Within-Subjects Factors and Two Between-Subjects Factors

| Between-Subjects   | Between-Subjects   |                                                               | Within-Subjects Factors   | Within-Subjects Factors   | Within-Subjects Factors   | Within-Subjects Factors   | Within-Subjects Factors                                         | Within-Subjects Factors   | Within-Subjects Factors   | Within-Subjects Factors   | Within-Subjects Factors   |
|--------------------|--------------------|---------------------------------------------------------------|---------------------------|---------------------------|---------------------------|---------------------------|-----------------------------------------------------------------|---------------------------|---------------------------|---------------------------|---------------------------|
| Factors            | Factors            |                                                               | A 1                       | A 1                       | A 1                       | A 2                       | A 2                                                             | A 2                       | A 3                       | A 3                       | A 3                       |
| C                  | D                  | Subject                                                       | B 1                       | B 2                       | B 3                       | B 1                       | B 2                                                             | B 3                       | B 1                       | B 2                       | B 3                       |
| C 1                | D 1 D 2            | S 111 S 112 S 113 S 121                                       |                           |                           |                           |                           | y ′ 111 y ′ 112 y ′ 113 y ′ 121 ′                               |                           |                           |                           |                           |
| C                  | D 3 D 2 D 3        | 123 S 131 S 132 S 133 S 211 S 212 S 213 S 221 . . . . . . . . |                           |                           |                           |                           | ′ 123 y ′ 131 y ′ 132 y ′ 133 y ′ y ′ y ′ y ′ . . . . . . . . . |                           |                           |                           |                           |
| 2                  | D                  |                                                               |                           |                           |                           |                           | 211 212 213 221                                                 |                           |                           |                           |                           |
|                    | 1                  |                                                               |                           |                           |                           |                           |                                                                 |                           |                           |                           |                           |
| C                  | D 1                |                                                               |                           |                           |                           |                           |                                                                 |                           |                           |                           |                           |
| 3                  | D 2                | .                                                             |                           |                           |                           |                           |                                                                 |                           |                           |                           |                           |
|                    | D 3                | S 333                                                         |                           |                           |                           |                           | y ′ 333                                                         |                           |                           |                           |                           |

Factor A

is distributed as T 2 a -1 ,ν E .

Factor B

is distributed as T 2 b -1 ,ν E .

AB Interaction

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

is distributed as T 2 ( a -1 )( b -1 ),ν E .

To test factors C , D , and CD , we transform to zi jk = j ′ y i j k and carry out univariate F -tests on a two-way ANOVA design.

To test factors AC , AD , and ACD , we perform a two-way MANOVA on Ay i j k . Then, the C main effect on Ay i j k compares the levels of C on Ay i j k , which is an effective description of the AC interaction. Similarly, the D main effect on Ay i j k yields the AD interaction, and the CD interaction on Ay i j k gives the ACD interaction.

To test factors BC , BD , and BCD , we carry out a two-way MANOVA on By i j k . The C main effect on By i j k gives the BC interaction, the D main effect on By i j k yields the BD interaction, and the CD interaction on By i j k corresponds to the BCD interaction.

Finally, to test factors ABC , ABD , and ABCD , we perform a two-way MANOVA on Gy i j k . Then the C main effect on Gy i j k gives the ABC interaction, the D main effect on Gy i j k yields the ABD interaction, and the CD interaction on Gy i j k corresponds to the ABCD interaction.

## 6.9.7 Additional Topics

Wang (1983) and Timm (1980) give a method for obtaining univariate mixed-model sums of squares from the multivariate E and H matrices. Crepeau et al. (1985) consider repeated measures experiments with missing data. Federer (1986) discusses the planning of repeated measures designs, emphasizing such aspects as determining the length of treatment period, eliminating carry-over effects, the nature of pre- and posttreatment, the nature of a response to a treatment, treatment sequences, and the choice of a model. Vonesh (1986) discusses sample size requirements to achieve a given power level in repeated measures designs. Patel (1986) presents a model that accommodates both within- and between-subjects covariates in repeated measures designs. Jensen (1982) compares the efficiency and robustness of various procedures.

A multivariate or multiresponse repeated measurement design will result if more than one variable is measured on each subject at each treatment combination. Such designs are discussed by Timm (1980), Reinsel (1982), Wang (1983), and Thomas (1983). Bock (1975) refers to observations of this type as doubly multivariate data.

## 6.10 GROWTHCURVES

When the subject responds to a treatment or stimulus at successive time periods, the pattern of responses is often referred to as a growth curve . As in repeated measures experiments, subjects are usually human or animal. We consider estimation and testing hypotheses about the form of the response curve for a single sample in Section 6.10.1 and extend to growth curves for several samples in Section 6.10.2.

## 6.10.1 Growth Curve for One Sample

The data layout for a single sample growth curve experiment is analogous to Table 6.11, with the levels of factor A representing time periods. Thus we have

a sample of n observation vectors y 1, y 2 , . . . , y n , for which we compute y and S . The usual approach is to approximate the shape of the growth curve by a polynomial function of time. If the time points are equally spaced, we can use orthogonal polynomials. This approach will be described first, followed by a method suitable for unequal time intervals.

Orthogonal polynomials are special contrasts that are often used in testing for linear, quadratic, cubic, and higher order trends in quantitative factors. For a more complete description and derivation see Guttman (1982, pp. 194-207), Morrison (1983, pp. 182-188), or Rencher (2000, pp. 323-331). Here we give only a heuristic introduction to the use of these contrasts.

Suppose we administer a drug to some subjects and measure a certain reaction at 3-min intervals. Let µ 1, µ 2, µ 3, µ 4, and µ 5 designate the average responses at 0, 3, 6, 9, and 12 min, respectively. To test the hypothesis that there are no trends in the µ j 's, we could test H 0 : µ 1 = µ 2 = · · · = µ 5 or H 0 : C 𝛍 = 0 using the contrast matrix

<!-- formula-not-decoded -->

in T 2 = n ( Cy ) ′ ( CSC ′ ) -1 ( Cy ) , as in (6.90). The four rows of C are orthogonal polynomials that test for linear, quadratic, cubic, and quartic trends in the means. As noted in Section 6.9.2 , any set of orthogonal contrasts in C will give the same value of T 2 to test H 0 : µ 1 = µ 2 = · · · = µ 5. However, in this case we will be interested in using a subset of the rows of C to determine the shape of the response curve.

<!-- formula-not-decoded -->

Table A.13 (Kleinbaum, Kupper, and Muller 1988) gives orthogonal polynomials for p = 3 , 4 , . . . , 10. The p -1 entries for each value of p constitute the matrix C . Some software programs will generate these automatically.

As with all orthogonal contrasts, the rows of C in (6.109) sum to zero and are mutually orthogonal. It is also apparent that the coefficients in each row increase and decrease in conformity with the desired pattern. Thus the entries in the first row, ( -2, -1, 0, 1, 2), increase steadily in a straight-line trend. The values in the second row dip down and back up in a quadratic-type bend. The third-row entries increase, decrease, then increase in a cubic pattern with two bends. The fourth row bends three times in a quartic curve.

To further illustrate how the orthogonal polynomials pinpoint trends in the means when testing H 0 : C 𝛍 = 0 , consider the three different patterns for 𝛍 depicted in Figure 6.4, where 𝛍 ′ a = ( 8 , 8 , 8 , 8 , 8 ) , 𝛍 ′ b = ( 20 , 16 , 12 , 8 , 4 ) , and 𝛍 ′ c = ( 5 , 12 , 15 , 12 , 5 ) . Let us denote the rows of C in (6.109) as c ′ 1 , c ′ 2 , c ′ 3 , and c ′ 4 . It is clear that c ′ i 𝛍 a = 0 for i = 1 , 2 , 3 , 4; that is, when H 0 : µ 1 = · · · = µ 5 is true, all four comparisons confirm it. If 𝛍 has the pattern 𝛍 b , only c ′ 1 𝛍 b is nonzero. The other rows are not sensitive to a linear pattern. We illustrate this for c ′ 1 and c ′ 2 :

<!-- formula-not-decoded -->

Figure 6.4. Three different patterns for 𝛍 .

<!-- image -->

For 𝛍 c , only c ′ 2 𝛍 c is nonzero. For example,

<!-- formula-not-decoded -->

Thus each orthogonal polynomial independently detects the type of curvature it is designed for and ignores other types. Of course real curves generally exhibit a mixture of more than one type of curvature, and in practice more than one orthogonal polynomial contrast may be significant.

To test hypotheses about the shape of the curve, we therefore use the appropriate rows of C in (6.109). Suppose we suspected a priori that there would be a combined linear and quadratic trend. Then we would partition C as follows:

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

We would test H 0 : C 1 𝛍 = 0 by

<!-- formula-not-decoded -->

which is distributed as T 2 2 , n -1 , where 2 is the number of rows of C 1, n is the number of subjects in the sample, and y and S are the mean vector and covariance matrix for

the sample. Similarly, H 0 : C 2 𝛍 = 0 is tested by

<!-- formula-not-decoded -->

which is T 2 2 , n -1 . In this case we might expect the first to reject H 0 and the second to accept H 0.

If we have no a priori expectations as to the shape of the curve, we could proceed as follows. Test the overall hypothesis H 0 : C 𝛍 = 0 , and if H 0 is rejected, use each of the rows of C separately to test H 0 : c ′ i 𝛍 = 0, i = 1 , 2 , 3 , 4. The respective test statistics are

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

In a case where p is large so that 𝛍 has a large number of levels, say 10 or more, we would likely want to stop testing after the first four or five rows of C and test the remaining rows in one group. However, for larger values of p , most tables of orthogonal polynomials give only the first few rows and omit those corresponding to higher degrees of curvature. We can find a matrix whose rows are orthogonal to the rows of a given matrix as follows. Suppose p = 11 so that C is 10 × 11 and C 1 contains the first five orthogonal polynomials. Then a matrix C 2, with rows orthogonal to those of C 1, can be obtained by selecting five linearly independent rows of which is T 2 4 , n -1 , and

each of which is distributed as tn -1 (see Example 6.9.2).

<!-- formula-not-decoded -->

whose rows can easily be shown to be orthogonal to those of C 1. The matrix B is not full rank, and some care must be exercised in choosing linearly independent rows. However, if an incorrect choice of C 2 is made, the computer algorithm should indicate this as it attempts to invert C 2 SC ′ 2 in T 2 = n ( C 2 y ) ′ ( C 2 SC ′ 2 ) -1 ( C 2 y ) .

Alternatively, to check for significant curvature beyond the rows of C 1 without finding C 2, we can use the test for additional information in a subset of variables in Section 5.8. We need not find C 2 in order to find the overall T 2 , since, as noted in Section 6.9.2 , any full rank ( p -1 ) × p matrix C such that Cj = 0 will give the same value in the overall T 2 -test of H 0 : C 𝛍 = 0 . We can conveniently use a simple contrast matrix such as

<!-- formula-not-decoded -->

in where

<!-- formula-not-decoded -->

which is T 2 p -1 , n -1 . Let p 1 be the number of orthogonal polynomials in C 1 and p 2 be the number of rows of C 2 if it were available; that is p 1 + p 2 = p -1. Then the test statistic for the p 1 orthogonal polynomials in C 1 is

<!-- formula-not-decoded -->

which is T 2 p 1 , n -1 . We wish to compare T 2 1 in (6.112) to T 2 in (6.111) to check for significant curvature beyond the rows of C 1. However, the test for additional information in a subset of variables in Section 5.8 was for the two-sample case. We can adapt (5.29) for use with the one-sample case, as follows. The test for significance of any curvature remaining after that accounted for in C 1 is made by comparing

<!-- formula-not-decoded -->

with the critical value T 2 α, p 2 , n -p 1 -1 .

We now describe an approach that can be used when the time points are not equally spaced. It may also be of interest in the equal-time-increment case because it provides an estimate of the response function.

Suppose we observe the response of the subject at p time points t 1, t 2 , . . . , t p and that the average response µ at any time point t is a polynomial in t of degree k &lt; p :

<!-- formula-not-decoded -->

This holds for each point t r and the corresponding average response µ r . Thus our hypothesis becomes

<!-- formula-not-decoded -->

which can be expressed in matrix notation as

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

In practice, it may be useful to transform the t r 's by subtracting the mean or the smallest value in order to reduce their size for computational purposes.

The following method of testing H 0 is due to Rao (1959, 1973). The model 𝛍 = A 𝛃 is similar to a regression model E ( y ) = X 𝛃 (see Section 10.2.1). However, in this case, we have cov ( y ) = 𝚺 rather than σ 2 I , as in the standard regression assumption. In place of the usual regression approach of seeking ˆ 𝛃 to minimize SSE = ( y -X ˆ 𝛃 ) ′ ( y -X ˆ 𝛃 ) [see (10.4) and (10.6)], we use a standardized distance as in (3.80), ( y -A ˆ 𝛃 ) ′ S -1 ( y -A ˆ 𝛃 ) . The value of ˆ 𝛃 that minimizes ( y -A ˆ 𝛃 ) ′ S -1 ( y -A ˆ 𝛃 ) is

<!-- formula-not-decoded -->

[see Rencher (2000, Section 7.8.1)], and H 0 : 𝛍 = A 𝛃 can be tested by

<!-- formula-not-decoded -->

which is distributed as T 2 p -k -1 , n -1 . The dimension of T 2 is reduced from p to p -k -1 because k + 1 parameters have been estimated in ˆ 𝛃 . The T 2 -statistic in (6.116) is usually given in the equivalent form

<!-- formula-not-decoded -->

The mean response at the r th time point,

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Simultaneous confidence intervals for all possible a ′ 𝛃 are given by

<!-- formula-not-decoded -->

where T α = √ T 2 α, k + 1 , n -1 is from Table A.7 and T 2 is given by (6.116) or (6.117).

can be estimated by

The intervals in (6.119) for a ′ 𝛃 include, of course, a ′ r 𝛃 for the p rows of A , that is, confidence intervals for the p time points. If a ′ r 𝛃 , r = 1 , 2 , . . . , p , are the only values of interest, we can shorten the intervals in (6.119) by using a Bonferroni coefficient t α/ 2 p in place of T α :

<!-- formula-not-decoded -->

where t α/ 2 p = t α/ 2 p , n -1. Bonferroni critical values t α/ 2 p ,ν are given in Table A.8. See procedures 2 and 3 in Section 5.5 for additional comments on the use of t α/ 2 p and T α .

Example 6.10.1. Potthoff and Roy (1964) reported measurements in a dental study on boys and girls from ages 8 to 14. The data are given in Table 6.16.

To illustrate the methods of this section, we use the data for the boys alone. In Example 6.10.2 we will compare the growth curves of the boys with those of the girls. We first test the overall hypothesis H 0 : C 𝛍 = 0 , where C contains orthogonal polynomials for linear, quadratic, and cubic effects:

<!-- formula-not-decoded -->

Table 6.16. Dental Measurements

|         | Girls' Ages in Years   | Girls' Ages in Years   | Girls' Ages in Years   | Girls' Ages in Years   |         |   Boys' Ages in Years |   Boys' Ages in Years |   Boys' Ages in Years |   Boys' Ages in Years |
|---------|------------------------|------------------------|------------------------|------------------------|---------|-----------------------|-----------------------|-----------------------|-----------------------|
| Subject | 8                      | 10                     | 12                     | 14                     | Subject |                   8   |                  10   |                  12   |                  14   |
| 1       | 21.0                   | 20.0                   | 21.5                   | 23.0                   | 1       |                  26   |                  25   |                  29   |                  31   |
| 2       | 21.0                   | 21.5                   | 24.0                   | 25.5                   | 2       |                  21.5 |                  22.5 |                  23   |                  26.5 |
| 3       | 20.5                   | 24.0                   | 24.5                   | 26.0                   | 3       |                  23   |                  22.5 |                  24   |                  27.5 |
| 4       | 23.5                   | 24.5                   | 25.0                   | 26.5                   | 4       |                  25.5 |                  27.5 |                  26.5 |                  27   |
| 5       | 21.5                   | 23.0                   | 22.5                   | 23.5                   | 5       |                  20   |                  23.5 |                  22.5 |                  26   |
| 6       | 20.0                   | 21.0                   | 21.0                   | 22.5                   | 6       |                  24.5 |                  25.5 |                  27   |                  28.5 |
| 7       | 21.5                   | 22.5                   | 23.0                   | 25.0                   | 7       |                  22   |                  22   |                  24.5 |                  26.5 |
| 8       | 23.0                   | 23.0                   | 23.5                   | 24.0                   | 8       |                  24   |                  21.5 |                  24.5 |                  25.5 |
| 9       | 20.0                   | 21.0                   | 22.0                   | 21.5                   | 9       |                  23   |                  20.5 |                  31   |                  26   |
| 10      | 16.5                   | 19.0                   | 19.0                   | 19.5                   | 10      |                  27.5 |                  28   |                  31   |                  31.5 |
| 11      | 24.5                   | 25.0                   | 28.0                   | 28.0                   | 11      |                  23   |                  23   |                  23.5 |                  25   |
|         |                        |                        |                        |                        | 12      |                  21.5 |                  23.5 |                  24   |                  28   |
|         |                        |                        |                        |                        | 13      |                  17   |                  24.5 |                  26   |                  29.5 |
|         |                        |                        |                        |                        | 14      |                  22.5 |                  25.5 |                  25.5 |                  26   |
|         |                        |                        |                        |                        | 15      |                  23   |                  24.5 |                  26   |                  30   |
|         |                        |                        |                        |                        | 16      |                  22   |                  21.5 |                  23.5 |                  25   |

From the 16 observation vectors we obtain

<!-- formula-not-decoded -->

To test H 0 : C 𝛍 = 0 , we calculate

<!-- formula-not-decoded -->

which exceeds T 2 . 01 , 3 , 15 = 19 . 867. We now test H 0 : c ′ i 𝛍 = 0 for each row of C to determine the shape of the growth curve. For the linear effect, using the first row, c ′ 1 , we obtain

<!-- formula-not-decoded -->

The test of significance of the quadratic component using the second row yields

<!-- formula-not-decoded -->

To test for a cubic trend, we use the third row of C :

<!-- formula-not-decoded -->

Thus only the linear trend is needed to describe the growth curve.

To model the curve for each variable, we use (6.113),

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

where

The values in the second column of A are obtained as t = age -11. By (6.115), we obtain

<!-- formula-not-decoded -->

and our prediction equation is

<!-- formula-not-decoded -->

## 6.10.2 Growth Curves for Several Samples

For the case of several samples or groups, the data layout would be similar to that in Table 6.9, where the p levels of factor A represent time points. Assuming the time points are equally spaced, we can use orthogonal polynomials in the ( p -1 ) × p contrast matrix C and express the basic hypothesis in the form H 0 : C 𝛍 . = 0 , where 𝛍 . = ∑ k i = 1 𝛍 i / k . This is equivalent to H 0 : µ. 1 = µ. 2 = · · · = µ. p , which compares the means of the p time points averaged across groups. As in Section 6.9.3, let us denote the sample mean vectors for the k groups as y 1 . , y 2 . , . . . , y k . , with grand mean y .. and pooled covariance matrix S pl = E /ν E . For the overall test of H 0 : C 𝛍 . = 0 we use the test statistic

<!-- formula-not-decoded -->

which is T 2 p -1 ,ν E as in (6.93), where N = ∑ k i = 1 ni for unbalanced data or N = kn for balanced data. The corresponding degrees of freedom for error is given by ν E = N -k or ν E = k ( n -1 ) . A test that the average growth curve (averaged over groups) has a particular form can be tested with C 1, containing a subset of the rows of C :

<!-- formula-not-decoded -->

which is distributed as T 2 p 1 ,ν E , where p 1 is the number of rows in C 1.

The growth curves for the k groups can be compared by the interaction or parallelism test of Section 6.9.3 using either C or C 1. We do a one-way MANOVA on Cy i j or C 1 y i j , or equivalently calculate by (6.96),

<!-- formula-not-decoded -->

which are distributed as /Lambda1 p -1 , k -1 ,ν E and /Lambda1 p 1 , k -1 ,ν E , respectively.

Example 6.10.2. In Example 6.10.1, we found a linear trend for the growth curve for dental measurements of boys in Table 6.16. We now consider the growth curve for the combined group and also compare the girls' group with the boys' group.

The two sample sizes are unequal and we use (6.33) to calculate the E matrix for the two groups,

<!-- formula-not-decoded -->

from which we obtain S pl = E /ν E . Using the C matrix in (6.121), we can test the basic hypothesis of equal means for the combined samples, H 0 : C 𝛍 . = 0 , using (6.122):

<!-- formula-not-decoded -->

To test for a linear trend, we use the first row of C in (6.123):

<!-- formula-not-decoded -->

This is, of course, the square of a t -statistic, but in the T 2 form it can readily be compared with the preceding T 2 using all three rows of C . The linear trend is seen to dominate the relationship among the means.

We now compare the growth curves of the two groups using (6.124). For C , we obtain

<!-- formula-not-decoded -->

For the linear trend, we have

<!-- formula-not-decoded -->

Thus the overall comparison does not reach significance, but the more specific comparison of linear trends does give a significant result.

## 6.10.3 Additional Topics

Jackson and Bryce (1981) presented methods of analyzing growth curves based on univariate linear models. Snee (1972) and Snee Acuff, and Gibson (1979) proposed the use of eigenvalues and eigenvectors of a matrix derived from residuals after fitting the model. If one of the eigenvalues is dominant, certain simplifications result. Bryce (1980) discussed a similar simplification for the two-group case. Geisser (1980) and Fearn (1975, 1977) gave the Bayesian approach to growth curves, including estimation and prediction. Zerbe (1979a, b) provided a randomization test requiring fewer assumptions than normal-based tests.

## 6.11 TESTS ON A SUBVECTOR

## 6.11.1 Test for Additional Information

In Section 5.8, we considered tests of significance of the additional information in a subvector when comparing two groups. We now extend these concepts to several groups and use similar notation.

Let y be a p × 1 vector of measurements and x be a q × 1 vector measured in addition to y . We are interested in determining whether x makes a significant contribution to the test of H 0 : 𝛍 1 = 𝛍 2 = · · · = 𝛍 k above and beyond y . Another way to phrase the question is, Can the separation of groups achieved by x be predicted from the separation achieved by y ? It is not necessary, of course, that x represent new variables. It may be that ( y x ) is a partitioning of the present variables, and we wish to know if the variables in x can be deleted because they do not contribute to rejecting H 0.

We consider here only the one-way MANOVA, but the results could be extended to higher order designs, where various possibilities arise. In a two-way context, for example, it may happen that x contributes nothing to the A main effect but does contribute significantly to the B main effect.

It is assumed that we have k samples,

<!-- formula-not-decoded -->

from which we calculate

<!-- formula-not-decoded -->

where E and H are ( p q ) ( p q ) and E yy and H yy are p p .

Then

+ × + ×

<!-- formula-not-decoded -->

is distributed as /Lambda1 p + q ,ν H ,ν E and tests the significance of group separation using the full vector ( y x ) . In the balanced one-way model, the degrees of freedom are ν H = k -1 and ν E = k ( n -1 ) . To test group separation using the reduced vector y , we can compute

<!-- formula-not-decoded -->

which is distributed as /Lambda1 p ,ν H ,ν E .

To test the hypothesis that the extra variables in x do not contribute anything significant to separating the groups beyond the information already available in y ,

we calculate

<!-- formula-not-decoded -->

which is distributed as /Lambda1 q ,ν H ,ν E -p . Note that the dimension of /Lambda1( x | y ) is q , the number of x 's. The error degrees of freedom, ν E -p , has been adjusted for the p y 's. Thus to test for the contribution of additional variables to separation of groups, we take the ratio of Wilks' /Lambda1 for the full set of variables in (6.125) to Wilks' /Lambda1 for the reduced set in (6.126). If the addition of x makes /Lambda1( y , x ) sufficiently smaller than /Lambda1( y ) , then /Lambda1( x | y ) in (6.127) will be small enough to reject the hypothesis.

If we are interested in the effect of adding a single x , then q = 1, and (6.127) becomes

<!-- formula-not-decoded -->

which is distributed as /Lambda1 1 ,ν H ,ν E -p . In this test we are inquiring whether x reduces the overall /Lambda1 by a significant amount. With a dimension of 1, the /Lambda1 -statistic in (6.128) has an exact F -transformation from Table 6.1,

<!-- formula-not-decoded -->

which is distributed as F ν H ,ν E -p . The statistic (6.128) is often referred to as a partial /Lambda1 -statistic ; correspondingly, (6.129) is called a partial F-statistic .

In (6.128) and (6.129), we have a test of the significance of a variable in the presence of the other variables. For a breakdown of precisely how the contribution of a variable depends on the other variables, see Rencher (1993; 1998, Section 4.1.6).

We can rewrite (6.128) as

<!-- formula-not-decoded -->

which shows that Wilks' /Lambda1 can only decrease with an additional variable.

Example 6.11.1. Weuse the rootstock data of Table 6.2 to illustrate tests on subvectors. From Example 6.1.7, we have, for all four variables, /Lambda1( y 1 , y 2 , y 3 , y 4 ) = . 1540 . For the first two variables, we obtain /Lambda1( y 1 , y 2 ) = . 6990. Then to test the significance of y 3 and y 4 adjusted for y 1 and y 2, we have by (6.127),

<!-- formula-not-decoded -->

which is less than the critical value /Lambda1 . 05 , 2 , 5 , 40 = . 639.

Similarly, the test for y 4 adjusted for y 1, y 2, and y 3 is given by (6.128) as

<!-- formula-not-decoded -->

For each of the other variables, we have a similar test:

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Thus the two variables y 3 and y 4, either individually or together, contribute a significant amount to separation of the six groups.

## 6.11.2 Stepwise Selection of Variables

If there are no variables for which we have a priori interest in testing for significance, we can do a data-directed search for the variables that best separate the groups. Such a strategy is often called stepwise discriminant analysis , although it could more aptly be called stepwise MANOVA. The procedure appears in many software packages.

We first describe an approach that is usually called forward selection . At the first step calculate /Lambda1( yi ) for each individual variable and choose the one with minimum /Lambda1( yi ) (or maximum associated F ). At the second step calculate /Lambda1( yi | y 1 ) for each of the p -1 variables not entered at the first step, where y 1 indicates the first variable entered. For the second variable we choose the one with minimum /Lambda1( yi | y 1 ) (or maximum associated partial F ), that is, the variable that adds the maximum separation to the one entered at step 1. Denote the variable entered at step 2 by y 2. At the third step calculate /Lambda1( yi | y 1 , y 2 ) for each of the p -2 remaining variables and choose the one that minimizes /Lambda1( yi | y 1 , y 2 ) (or maximizes the associated partial F ). Continue this process until the F falls below some predetermined threshold value, say, F in .

A stepwise procedure follows a similar sequence, except that after a variable has entered, the variables previously selected are reexamined to see if each still contributes a significant amount. The variable with smallest partial F will be removed if the partial F is less than a second threshold value, F out . If F out is the same as F in , there is a very small possibility that the procedure will cycle continuously without stopping. This possibility can be eliminated by using a value of F out slightly less than F in. For an illustration of the stepwise procedure, see Example 8.9.

## PROBLEMS

- 6.1 Verify the computational forms given in (6.3) and (6.5); that is, show that
- (a) ∑ i j ( yi j -y i . ) 2 = ∑ i j y 2 i j -∑ i y 2 i . / n , (b) n ∑ i ( y i . -y .. ) 2 = ∑ i y 2 i . / n -y 2 .. / kn . 6.2 Show that Wilks' /Lambda1 can be expressed in terms of the eigenvalues of E -1 H as in (6.14).
- 6.3 Showthat the eigenvalues of E -1 H are the same as those of ( E 1 / 2 ) -1 H ( E 1 / 2 ) -1 , as noted in Section 6.1.4, where E 1 / 2 is the square root matrix defined in (2.112).
- 6.4 Show that F 3 in (6.27) is the same as F 1 in (6.25).
- 6.5 Show that if p ≤ ν H , then F 3 in (6.31) is the same as F 2 in (6.30).
- 6.6 Show that if there is only one nonzero eigenvalue λ 1, then U ( 1 ) , V ( 1 ) , and /Lambda1 can be expressed in terms of θ , as in (6.34)-(6.36).
- 6.7 Show that (5.16), (5.18), and (5.19), which relate T 2 to /Lambda1 , V ( s ) , and θ , follow from (6.34)-(6.36) and (6.39), U ( 1 ) = T 2 /( n 1 + n 2 -2 ) .
- 6.8 Verify the computational forms of H and E in (6.32) and (6.33); that is, show that
- (a) ∑ k i = 1 ni ( y i . -y .. )( y i . -y .. ) ′ = ∑ k i = 1 y i . y ′ i . / ni -y .. y ′ .. / N , (b) ∑ k i = 1 ∑ ni j = 1 ( y i j -y i . )( y i j -y i . ) ′ = ∑ k i = 1 ∑ ni j = 1 y i j y ′ i j -∑ k i = 1 y i . y ′ i . / ni .
- 6.9 Show that for two groups, H = ∑ 2 i = 1 ni ( y i . -y .. )( y i . -y .. ) ′ can be expressed as H = [ n 1 n 2 /( n 1 + n 2 ) ] ( y 1 . -y 2 . )( y 1 . -y 2 . ) ′ , thus verifying (6.38). Note that

<!-- formula-not-decoded -->

- 6.10 Show that θ can be expressed as θ = SSH ( z )/ [ SSE ( z ) + SSH ( z ) ] as in (6.42).
- 6.11 Show that

<!-- formula-not-decoded -->

as in (6.45), where r 2 i = λ i /( 1 + λ i ) .

- 6.12 Show that the F -approximation based on AP in (6.50) reduces to (6.26) if AP = V ( s ) / s , as in (6.49).
- 6.13 Show that if s = 1, A LH in (6.51) reduces to (6.43).
- 6.14 Show that the F -approximation denoted by F 3 in (6.31) is equivalent to (6.52).
- 6.15 Show that cov ( ˆ 𝛅 ) = 𝚺 n ∑ k i = 1 c 2 i as in (6.61).

- 6.16 If z i j = Cy i j , where C is ( p -1 ) × p , show that H z = CHC ′ and E z = CEC ′ , as used in (6.79).
- 6.17 Why do C and C ′ not 'cancel out' of Wilks' /Lambda1 in (6.79)?
- 6.18 Show that under H 03 and H 01, Cy .. is Np -1 ( 0 , C 𝚺 C ′ / kn ) , as noted preceding (6.84).
- 6.19 Show that T 2 = kn ( Cy .. ) ′ ( CEC ′ /ν E ) -1 Cy .. in (6.84) is distributed as T 2 p -1 ,ν E .
- 6.20 For ε defined by (6.89), show that ε = 1 when 𝚺 = σ 2 I .
- 6.22 Provide an alternative derivation of (6.106), /Lambda1 = ν E /(ν E + T 2 ) , starting with (6.105).
- 6.21 Give a justification of the Wilks' /Lambda1 test of H 0 : 𝛍 . = 0 in (6.104).
- 6.23 Obtain T 2 in terms of /Lambda1 in (6.107), starting with (6.106).
- 6.24 Show that the rows of C 1 are orthogonal to those of B = I -C ′ 1 ( C 1 C ′ 1 ) -1 C 1 in (6.110).
- 6.25 Show that ˆ 𝛃 in (6.115) minimizes ( y -A ˆ 𝛃 ) ′ S -1 ( y -A ˆ 𝛃 ) .
- 6.26 Show that T 2 in (6.117) is equivalent to T 2 in (6.116).
- 6.27 Baten, Tack, and Baeder (1958) compared judges' scores on fish prepared by three methods. Twelve fish were cooked by each method, and several judges tasted fish samples and rated each on four variables: y 1 = aroma, y 2 = flavor, y 3 = texture, and y 4 = moisture. The data are in Table 6.17. Each entry is an average score for the judges on that fish.
- (a) Compare the three methods using all four MANOVA tests.

Table 6.17. Judges' Scores on Fish Prepared by Three Methods

| Method 1   | Method 1   | Method 1   | Method 1   | Method 2   | Method 2   | Method 2   | Method 2   | Method 3   | Method 3   | Method 3   | Method 3   |
|------------|------------|------------|------------|------------|------------|------------|------------|------------|------------|------------|------------|
| y 1        | y 2        | y 3        | y 4        | y 1        | y 2        | y 3        | y 4        | y 1        | y 2        | y 3        | y 4        |
| 5.4        | 6.0        | 6.3        | 6.7        | 5.0        | 5.3        | 5.3        | 6.5        | 4.8        | 5.0        | 6.5        | 7.0        |
| 5.2        | 6.2        | 6.0        | 5.8        | 4.8        | 4.9        | 4.2        | 5.6        | 5.4        | 5.0        | 6.0        | 6.4        |
| 6.1        | 5.9        | 6.0        | 7.0        | 3.9        | 4.0        | 4.4        | 5.0        | 4.9        | 5.1        | 5.9        | 6.5        |
| 4.8        | 5.0        | 4.9        | 5.0        | 4.0        | 5.1        | 4.8        | 5.8        | 5.7        | 5.2        | 6.4        | 6.4        |
| 5.0        | 5.7        | 5.0        | 6.5        | 5.6        | 5.4        | 5.1        | 6.2        | 4.2        | 4.6        | 5.3        | 6.3        |
| 5.7        | 6.1        | 6.0        | 6.6        | 6.0        | 5.5        | 5.7        | 6.0        | 6.0        | 5.3        | 5.8        | 6.4        |
| 6.0        | 6.0        | 5.8        | 6.0        | 5.2        | 4.8        | 5.4        | 6.0        | 5.1        | 5.2        | 6.2        | 6.5        |
| 4.0        | 5.0        | 4.0        | 5.0        | 5.3        | 5.1        | 5.8        | 6.4        | 4.8        | 4.6        | 5.7        | 5.7        |
| 5.7        | 5.4        | 4.9        | 5.0        | 5.9        | 6.1        | 5.7        | 6.0        | 5.3        | 5.4        | 6.8        | 6.6        |
| 5.6        | 5.2        | 5.4        | 5.8        | 6.1        | 6.0        | 6.1        | 6.2        | 4.6        | 4.4        | 5.7        | 5.6        |
| 5.8        | 6.1        | 5.2        | 6.4        | 6.2        | 5.7        | 5.9        | 6.0        | 4.5        | 4.0        | 5.0        | 5.9        |
| 5.3        | 5.9        | 5.8        | 6.0        | 5.1        | 4.9        | 5.3        | 4.8        | 4.4        | 4.2        | 5.6        | 5.5        |

Source : Baten, Tack, and Baeder (1958, p. 8).

- (b) Compute the following measures of multivariate association from Section 6.1.8: η 2 /Lambda1 , η 2 θ , A /Lambda1 , A LH, AP .
- (c) Based on the eigenvalues, is the essential dimensionality of the space containing the mean vectors equal to 1 or 2?
- (d) Using contrasts, test the following two comparisons of methods: 1 and 2 vs. 3, and 1 vs. 2.
- (e) If any of the four tests in (a) is significant, run an ANOV A F -test on each yi and examine the discriminant function z = a ′ y (Section 6.4).
- (f) Test the significance of y 3 and y 4 adjusted for y 1 and y 2.
- (g) Test the significance of each variable adjusted for the other three.
- 6.28 Table 6.18, from Keuls, Martakis, and Magid (1984), gives data from a twoway (fixed-effects) MANOVA on snap beans showing the results of four vari-

Table 6.18. Snapbean Data

|   S | V   |    y |   1 |   y 2 |   y 3 | y 4   | S V   |    |   y 1 |   y 2 |   y 3 |   y 4 |
|-----|-----|------|-----|-------|-------|-------|-------|----|-------|-------|-------|-------|
|   1 | 1 1 | 59.3 | 4.5 |  38.4 |   295 | 3     | 1     |  1 |  68.1 |   3.4 |  42.2 |   280 |
|   1 | 2   | 60.3 | 4.5 |  38.6 |   302 |       |       |  2 |  68   |   2.9 |  42.4 |   284 |
|   1 | 3   | 60.9 | 5.3 |  37.2 |   318 |       |       |  3 |  68.5 |   3.3 |  41.5 |   286 |
|   1 | 4   | 60.6 | 5.8 |  38.1 |   345 |       |       |  4 |  68.6 |   3.1 |  41.9 |   284 |
|   1 | 5   | 60.4 | 6   |  38.8 |   325 |       |       |  5 |  68.6 |   3.3 |  42.1 |   268 |
|   1 | 2 1 | 59.3 | 6.7 |  37.9 |   275 | 3     | 2     |  1 |  64   |   3.6 |  40.9 |   233 |
|   1 | 2   | 59.4 | 4.8 |  36.6 |   290 |       |       |  2 |  63.4 |   3.9 |  41.4 |   248 |
|   1 | 3   | 60   | 5.1 |  38.7 |   295 |       |       |  3 |  63.5 |   3.7 |  41.6 |   244 |
|   1 | 4   | 58.9 | 5.8 |  37.5 |   296 |       |       |  4 |  63.4 |   3.7 |  41.4 |   266 |
|   1 | 5   | 59.5 | 4.8 |  37   |   330 |       |       |  5 |  63.5 |   4.1 |  41.1 |   244 |
|   1 | 3 1 | 59.4 | 5.1 |  38.7 |   299 | 3     | 3     |  1 |  68   |   3.7 |  42.3 |   293 |
|   1 | 2   | 60.2 | 5.3 |  37   |   315 |       |       |  2 |  68.7 |   3.5 |  41.6 |   284 |
|   1 | 3   | 60.7 | 6.4 |  37.4 |   304 |       |       |  3 |  68.7 |   3.8 |  40.7 |   277 |
|   1 | 4   | 60.5 | 7.1 |  37   |   302 |       |       |  4 |  68.4 |   3.5 |  42   |   299 |
|   1 | 5   | 60.1 | 7.8 |  36.9 |   308 |       |       |  5 |  68.6 |   3.4 |  42.4 |   285 |
|   2 | 1 1 | 63.7 | 5.4 |  39.5 |   271 | 4     | 1     |  1 |  69.8 |   1.4 |  48.4 |   265 |
|   2 | 2   | 64.1 | 5.4 |  39.2 |   284 |       |       |  2 |  69.5 |   1.3 |  47.8 |   247 |
|   2 | 3   | 63.4 | 5.4 |  39   |   281 |       |       |  3 |  69.5 |   1.3 |  46.9 |   231 |
|   2 | 4   | 63.2 | 5.3 |  39   |   291 |       |       |  4 |  69.9 |   1.3 |  47.5 |   268 |
|   2 | 5   | 63.2 | 5   |  39   |   270 |       |       |  5 |  70.3 |   1.1 |  47.1 |   247 |
|   2 | 2 1 | 60.6 | 6.8 |  38.1 |   248 | 4     | 2     |  1 |  66.6 |   1.8 |  45.7 |   205 |
|   2 | 2   | 61   | 6.5 |  38.6 |   264 |       |       |  2 |  66.5 |   1.7 |  46.8 |   239 |
|   2 | 3   | 60.7 | 6.8 |  38.8 |   257 |       |       |  3 |  67.1 |   1.7 |  46.3 |   230 |
|   2 | 4   | 60.6 | 7.1 |  38.6 |   260 |       |       |  4 |  65.8 |   1.8 |  46.3 |   235 |
|   2 | 5   | 60.3 | 6   |  38.5 |   261 |       |       |  5 |  65.6 |   1.9 |  46.1 |   220 |
|   2 | 3 1 | 63.8 | 5.7 |  40.5 |   282 | 4     | 3     |  1 |  70.1 |   1.7 |  48.1 |   253 |
|   2 | 2   | 63.2 | 6.1 |  40.2 |   284 |       |       |  2 |  72.3 |   0.7 |  47.8 |   249 |
|   2 | 3   | 63.3 | 6   |  40   |   291 |       |       |  3 |  69.7 |   1.5 |  46.7 |   226 |
|   2 | 4   | 63.2 | 5.9 |  40   |   299 |       |       |  4 |  69.9 |   1.3 |  47.1 |   248 |
|   2 | 5   | 63.1 | 5.4 |  39.7 |   295 |       |       |  5 |  69.8 |   1.4 |  46.7 |   236 |

ables: y 1 = yield earliness, y 2 = specific leaf area (SLA) earliness, y 3 = total yield, and y 4 = average SLA. The factors are sowing date ( S ) and variety ( V ).

- (a) Test for main effects and interaction using all four MANOV A statistics.
- (b) In previous experiments, the second variety gave higher yields. Compare variety 2 with varieties 1 and 3 by means of a test on a contrast.
- (c) Test linear, quadratic, and cubic contrasts for sowing date. (Interpretation of these for mean vectors is not as straightforward as for univariate means.)
- (d) If any of the tests in part (a) rejects H 0, carry out ANOVA F -tests on the four variables.
- (e) Test the significance of y 3 and y 4 adjusted for y 1 and y 2 in main effects and interaction.
- (f) Test the significance of each variable adjusted for the other three in main effects and interaction.
- 6.29 The bar steel data in Table 6.6 were analyzed in Example 6.5.2 as a two-way fixed-effects design. Consider lubricants to be random so that we have a mixed model. Test for main effects and interaction.
- 6.30 In Table 6.19, we have a comparison of four reagents (Burdick 1979). The first reagent is the one presently in use and the other three are less expen-

Table 6.19. Blood Data

|         | Reagent 1   | Reagent 1   | Reagent 1   | Reagent 2   | Reagent 2   | Reagent 2   | Reagent 3   | Reagent 3   | Reagent 3   | Reagent 4   | Reagent 4   | Reagent 4   |
|---------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|-------------|
| Subject | y 1         | y 2         | y 3         | y 1         | y 2         | y 3         | y 1         | y 2         | y 3         | y 1         | y 2         | y 3         |
| 1       | 8.0         | 3.96        | 12.5        | 8.0         | 3.93        | 12.7        | 7.9         | 3.86        | 13.0        | 7.9         | 3.87        | 13.2        |
| 2       | 4.0         | 5.37        | 16.9        | 4.2         | 5.35        | 17.2        | 4.1         | 5.39        | 17.2        | 4.0         | 5.35        | 17.3        |
| 3       | 6.3         | 5.47        | 17.1        | 6.3         | 5.39        | 17.5        | 6.0         | 5.39        | 17.2        | 6.1         | 5.41        | 17.4        |
| 4       | 9.4         | 5.16        | 16.2        | 9.4         | 5.16        | 16.7        | 9.4         | 5.17        | 16.7        | 9.1         | 5.16        | 16.7        |
| 5       | 8.2         | 5.16        | 17.0        | 8.0         | 5.13        | 17.5        | 8.1         | 5.10        | 17.4        | 7.8         | 5.12        | 17.5        |
| 6       | 11.0        | 4.67        | 14.3        | 10.7        | 4.60        | 14.7        | 10.6        | 4.52        | 14.6        | 10.5        | 4.58        | 14.7        |
| 7       | 6.8         | 5.20        | 16.2        | 6.8         | 5.16        | 16.7        | 6.9         | 5.13        | 16.8        | 6.7         | 5.19        | 16.8        |
| 8       | 9.0         | 4.65        | 14.7        | 9.0         | 4.57        | 15.0        | 8.9         | 4.58        | 15.0        | 8.6         | 4.55        | 15.1        |
| 9       | 6.1         | 5.22        | 16.3        | 6.0         | 5.16        | 16.9        | 6.1         | 5.14        | 16.9        | 6.0         | 5.21        | 16.9        |
| 10      | 6.4         | 5.13        | 15.9        | 6.4         | 5.11        | 16.4        | 6.4         | 5.11        | 16.4        | 6.3         | 5.07        | 16.3        |
| 11      | 5.6         | 4.47        | 13.3        | 5.5         | 4.45        | 13.6        | 5.3         | 4.46        | 13.6        | 5.3         | 4.44        | 13.7        |
| 12      | 8.2         | 5.22        | 16.0        | 8.2         | 5.14        | 16.5        | 8.0         | 5.14        | 16.5        | 7.8         | 5.16        | 16.5        |
| 13      | 5.7         | 5.10        | 14.9        | 5.6         | 5.05        | 15.3        | 5.5         | 5.02        | 15.4        | 5.4         | 5.05        | 15.5        |
| 14      | 9.8         | 5.25        | 16.1        | 9.8         | 5.15        | 16.6        | 8.1         | 5.10        | 13.8        | 9.4         | 5.16        | 16.6        |
| 15      | 5.9         | 5.28        | 15.8        | 5.8         | 5.25        | 16.4        | 5.7         | 5.26        | 16.4        | 5.6         | 5.29        | 16.2        |
| 16      | 6.6         | 4.65        | 12.8        | 6.4         | 4.59        | 13.2        | 6.3         | 4.58        | 13.1        | 6.4         | 4.57        | 13.2        |
| 17      | 5.7         | 4.42        | 14.5        | 5.5         | 4.31        | 14.9        | 5.5         | 4.30        | 14.9        | 5.4         | 4.32        | 14.8        |
| 18      | 6.7         | 4.38        | 13.1        | 6.5         | 4.32        | 13.4        | 6.5         | 4.32        | 13.6        | 6.5         | 4.31        | 13.5        |
| 19      | 6.8         | 4.67        | 15.6        | 6.6         | 4.57        | 15.8        | 6.5         | 4.55        | 16.0        | 6.5         | 4.56        | 15.9        |
| 20      | 9.6         | 5.64        | 17.0        | 9.5         | 5.58        | 17.5        | 9.3         | 5.50        | 17.4        | 9.2         | 5.46        | 17.5        |

Table 6.20. Wear of Coated Fabrics in Three Periods (mg)

|           |        | Proportion of Filler   | Proportion of Filler   | Proportion of Filler   | Proportion of Filler   | Proportion of Filler   | Proportion of Filler   | Proportion of Filler   | Proportion of Filler   | Proportion of Filler   |
|-----------|--------|------------------------|------------------------|------------------------|------------------------|------------------------|------------------------|------------------------|------------------------|------------------------|
| Surface   |        | P 1 (25%)              | P 1 (25%)              | P 1 (25%)              | P 2 (50%)              | P 2 (50%)              | P 2 (50%)              | P 3 (75%)              | P 3 (75%)              | P 3 (75%)              |
| Treatment | Filler | y 1                    | y 2                    | y 3                    | y 1                    | y 2                    | y 3                    | y 1                    | y 2                    | y 3                    |
| T 0       | F 1    | 194                    | 192                    | 141                    | 233                    | 217                    | 171                    | 265                    | 252                    | 207                    |
|           |        | 208                    | 188                    | 165                    | 241                    | 222                    | 201                    | 269                    | 283                    | 191                    |
|           | F 2    | 239                    | 127                    | 90                     | 224                    | 123                    | 79                     | 243                    | 117                    | 100                    |
|           |        | 187                    | 105                    | 85                     | 243                    | 123                    | 110                    | 226                    | 125                    | 75                     |
| T 1       | F 1    | 155                    | 169                    | 151                    | 198                    | 187                    | 176                    | 235                    | 225                    | 166                    |
|           |        | 173                    | 152                    | 141                    | 177                    | 196                    | 167                    | 229                    | 270                    | 183                    |
|           | F 2    | 137                    | 82                     | 77                     | 129                    | 94                     | 78                     | 155                    | 76                     | 92                     |
|           |        | 160                    | 82                     | 83                     | 98                     | 89                     | 48                     | 132                    | 105                    | 67                     |

sive reagents that we wish to compare with the first. All four reagents are used with a blood sample from each patient. The three variables measured for each reagent are y 1 = white blood count, y 2 = red blood count, and y 3 = hemoglobin count.

- (a) Analyze as a randomized block design with subjects as blocks.
- (b) Compare the first reagent with the other three using a contrast.
- 6.31 The data in Table 6.20, from Box (1950), show the amount of fabric wear y 1, y 2, and y 3 in three successive periods: (1) the first 1000 revolutions, (2) the second 1000 revolutions, and (3) the third 1000 revolutions of the abrasive wheel. There were three factors: type of abrasive surface, type of filler, and proportion of filler. There were two replications. Carry out a three-way MANOV A, testing for main effects and interactions. (Ignore the repeated measures aspects of the data.)
- 6.32 The fabric wear data in Table 6.20 can be considered to be a growth curve model, with the three periods ( y 1 , y 2 , y 3 ) representing repeated measurements on the same specimen. We thus have one within-subjects factor, to which we should assign polynomial contrasts ( -1 , 0 , 1 ) and ( -1 , 2 , -1 ) , and a threeway between-subjects classification. Test for period and the interaction of period with the between-subjects factors and interactions.
- 6.33 Carry out a profile analysis on the fish data in Table 6.17, testing for parallelism, equal levels, and flatness.
- 6.34 Rao (1948) measured the weight of cork borings taken from the north (N), east (E), south (S), and west (W) directions of 28 trees. The data are given in Table 6.21. It is of interest to compare the bark thickness (and hence weight) in the four directions. This can be done by analyzing the data as a one-sample repeated measures design. Since the primary comparison of interest is north and south vs. east and west, use the contrast matrix

Table 6.21. Weights of Cork Borings (cg) in Four Directions for 28 Trees

|   Tree |   N |   E |   S |   W |   Tree |   N |   E |   S |   W |
|--------|-----|-----|-----|-----|--------|-----|-----|-----|-----|
|      1 |  72 |  66 |  76 |  77 |     15 |  91 |  79 | 100 |  75 |
|      2 |  60 |  53 |  66 |  63 |     16 |  56 |  68 |  47 |  50 |
|      3 |  56 |  57 |  64 |  58 |     17 |  79 |  65 |  70 |  61 |
|      4 |  41 |  29 |  36 |  38 |     18 |  81 |  80 |  68 |  58 |
|      5 |  32 |  32 |  35 |  36 |     19 |  78 |  55 |  67 |  60 |
|      6 |  30 |  35 |  34 |  26 |     20 |  46 |  38 |  37 |  38 |
|      7 |  39 |  39 |  31 |  27 |     21 |  39 |  35 |  34 |  37 |
|      8 |  42 |  43 |  31 |  25 |     22 |  32 |  30 |  30 |  32 |
|      9 |  37 |  40 |  31 |  25 |     23 |  60 |  50 |  67 |  54 |
|     10 |  33 |  29 |  27 |  36 |     24 |  35 |  37 |  48 |  39 |
|     11 |  32 |  30 |  34 |  28 |     25 |  39 |  36 |  39 |  31 |
|     12 |  63 |  45 |  74 |  63 |     26 |  50 |  34 |  37 |  40 |
|     13 |  54 |  46 |  60 |  52 |     27 |  43 |  37 |  39 |  50 |
|     14 |  47 |  51 |  52 |  43 |     28 |  48 |  54 |  57 |  43 |

<!-- formula-not-decoded -->

- (b) If the test in (a) rejects H 0, test each row of C .
- (a) Test H 0 : µ N = µ E = µ S = µ W using the entire matrix C .
- 6.35 Analyze the glucose data in Table 3.8 as a one-sample repeated measures design with two within-subjects factors. Factor A is a comparison of fasting test vs. 1 hour posttest. The three levels of factor B are y 1 (and x 1), y 2 (and x 2), and y 3 (and x 3).
- 6.36 Table 6.22 gives survival times for cancer patients (Cameron and Pauling 1978; see also Andrews and Herzberg 1985, pp. 203-206). The factors in this twoway design are gender (1 = male, 2 = female) and type of cancer (1 = stomach, 2 = bronchus, 3 = colon, 4 = rectum, 5 = bladder, 6 = kidney). The variables (repeated measures) are y 1 = survival time (days) of patient

Table 6.22. Survival Times for Cancer Patients

|   Type of Cancer |   Gender |   Age |   y 1 |   y 2 |   y 3 |   y 4 |
|------------------|----------|-------|-------|-------|-------|-------|
|                1 |        2 |    61 |   124 |   264 |   124 |    38 |
|                1 |        1 |    69 |    42 |    62 |    12 |    18 |
|                1 |        2 |    62 |    25 |   149 |    19 |    36 |
|                1 |        2 |    66 |    45 |    18 |    45 |    12 |
|                1 |        1 |    63 |   412 |   180 |   257 |    64 |
|                1 |        1 |    79 |    51 |   142 |    23 |    20 |
|                1 |        1 |    76 |  1112 |    35 |   128 |    13 |

Table 6.22. ( Continued )

|   Type of Cancer |   Gender |   Age |   y 1 |   y 2 |   y 3 |   y 4 |
|------------------|----------|-------|-------|-------|-------|-------|
|                1 |        1 |    54 |    46 |   299 |    46 |    51 |
|                1 |        1 |    62 |   103 |    85 |    90 |    10 |
|                1 |        1 |    46 |   146 |   361 |   123 |    52 |
|                1 |        1 |    57 |   340 |   269 |   310 |    28 |
|                1 |        2 |    59 |   396 |   130 |   359 |    55 |
|                2 |        1 |    74 |    81 |    72 |    74 |    33 |
|                2 |        1 |    74 |   461 |   134 |   423 |    18 |
|                2 |        1 |    66 |    20 |    84 |    16 |    20 |
|                2 |        1 |    52 |   450 |    98 |   450 |    58 |
|                2 |        2 |    48 |   246 |    48 |    87 |    13 |
|                2 |        2 |    64 |   166 |   142 |   115 |    49 |
|                2 |        1 |    70 |    63 |   113 |    50 |    38 |
|                2 |        1 |    77 |    64 |    90 |    50 |    24 |
|                2 |        1 |    71 |   155 |    30 |   113 |    18 |
|                2 |        1 |    39 |   151 |   260 |    38 |    34 |
|                2 |        1 |    70 |   166 |   116 |   156 |    20 |
|                2 |        1 |    70 |    37 |    87 |    27 |    27 |
|                2 |        1 |    55 |   223 |    69 |   218 |    32 |
|                2 |        1 |    74 |   138 |   100 |   138 |    27 |
|                2 |        1 |    69 |    72 |   315 |    39 |    39 |
|                2 |        1 |    73 |   245 |   188 |   231 |    65 |
|                3 |        2 |    76 |   248 |   292 |   135 |    18 |
|                3 |        2 |    58 |   377 |   492 |    50 |    30 |
|                3 |        1 |    49 |   189 |   462 |   189 |    65 |
|                3 |        1 |    69 |  1843 |   235 |  1267 |    17 |
|                3 |        2 |    70 |   180 |   294 |   155 |    57 |
|                3 |        2 |    68 |   537 |   144 |   534 |    16 |
|                3 |        1 |    50 |   519 |   643 |   502 |    25 |
|                3 |        2 |    74 |   455 |   301 |   126 |    21 |
|                3 |        1 |    66 |   406 |   148 |    90 |    17 |
|                3 |        2 |    76 |   365 |   641 |   365 |    42 |
|                3 |        2 |    56 |   942 |   272 |   911 |    40 |
|                3 |        2 |    74 |   372 |    37 |   366 |    28 |
|                3 |        1 |    58 |   163 |   199 |   156 |    31 |
|                3 |        2 |    60 |   101 |   154 |    99 |    28 |
|                3 |        1 |    77 |    20 |   649 |    20 |    33 |
|                3 |        1 |    38 |   283 |   162 |   274 |    80 |
|                4 |        2 |    56 |   185 |   422 |    62 |    38 |
|                4 |        2 |    75 |   479 |    82 |   226 |    10 |
|                4 |        2 |    57 |   875 |   551 |   437 |    62 |
|                4 |        1 |    56 |   115 |   140 |    85 |    13 |
|                4 |        1 |    68 |   362 |   106 |   122 |    36 |
|                4 |        1 |    54 |   241 |   645 |   198 |    80 |
|                4 |        1 |    59 |  2175 |   407 |   759 |    64 |
|                5 |        1 |    93 |  4288 |   464 |   260 |    29 |

Table 6.22. ( Continued )

|   Type of Cancer |   Gender |   Age |   y 1 |   y 2 |   y 3 |   y 4 |
|------------------|----------|-------|-------|-------|-------|-------|
|                5 |        2 |    70 |  3658 |   694 |   305 |    22 |
|                5 |        2 |    77 |    51 |   221 |    37 |    21 |
|                5 |        2 |    72 |   278 |   490 |   109 |    16 |
|                5 |        1 |    44 |   548 |   433 |    37 |    11 |
|                6 |        2 |    71 |   205 |   332 |     8 |    91 |
|                6 |        2 |    63 |   538 |   377 |    96 |    47 |
|                6 |        2 |    51 |   203 |   147 |   190 |    35 |
|                6 |        1 |    53 |   296 |   500 |    64 |    34 |
|                6 |        1 |    57 |   870 |   299 |   260 |    19 |
|                6 |        1 |    73 |   331 |   585 |   326 |    37 |
|                6 |        1 |    69 |  1685 |  1056 |    46 |    15 |

treated with ascorbate measured from date of first hospital attendance, y 2 = mean survival time for the patient's 10 matched controls (untreated with ascorbate), y 3 = survival time after ascorbate treatment ceased, and y 4 = mean survival time after all treatment ceased for the patient's 10 matched controls. Analyze as a repeated measures design with one within-subjects factor ( y 1, y 2, y 3, y 4) and a two-way (unbalanced) design between subjects. Since the twoway classification of subjects is unbalanced, you will need to use a program that allows for this or delete some observations to achieve a balanced design.

## 6.37 Analyze the ramus bone data of Table 3.8 as a one-sample growth curve design.

- (a) Using a matrix C of orthogonal polynomial contrasts, test the hypothesis of overall equality of means, H 0 : C 𝛍 = 0 .

Table 6.23. Weights of 13 Male Mice Measured at Successive Intervals of 3 Days over 21 Days from Birth to Weaning

|   Mouse |   Day 3 |   Day 6 |   Day 9 |   Day 12 |   Day 15 |   Day 18 |   Day 21 |
|---------|---------|---------|---------|----------|----------|----------|----------|
|       1 |   0.19  |   0.388 |   0.621 |    0.823 |    1.078 |    1.132 |    1.191 |
|       2 |   0.218 |   0.393 |   0.568 |    0.729 |    0.839 |    0.852 |    1.004 |
|       3 |   0.211 |   0.394 |   0.549 |    0.7   |    0.783 |    0.87  |    0.925 |
|       4 |   0.209 |   0.419 |   0.645 |    0.85  |    1.001 |    1.026 |    1.069 |
|       5 |   0.193 |   0.362 |   0.52  |    0.53  |    0.641 |    0.64  |    0.751 |
|       6 |   0.201 |   0.361 |   0.502 |    0.53  |    0.657 |    0.762 |    0.888 |
|       7 |   0.202 |   0.37  |   0.498 |    0.65  |    0.795 |    0.858 |    0.91  |
|       8 |   0.19  |   0.35  |   0.51  |    0.666 |    0.819 |    0.879 |    0.929 |
|       9 |   0.219 |   0.399 |   0.578 |    0.699 |    0.709 |    0.822 |    0.953 |
|      10 |   0.225 |   0.4   |   0.545 |    0.69  |    0.796 |    0.825 |    0.836 |
|      11 |   0.224 |   0.381 |   0.577 |    0.756 |    0.869 |    0.929 |    0.999 |
|      12 |   0.187 |   0.329 |   0.441 |    0.525 |    0.589 |    0.621 |    0.796 |
|      13 |   0.278 |   0.471 |   0.606 |    0.77  |    0.888 |    1.001 |    1.105 |

- (b) If the overall hypothesis in (a) is rejected, find the degree of growth curve by testing each row of C .
- 6.38 Table 6.23 contains the weights of 13 male mice measured every 3 days from birth to weaning. The data set was reported and analyzed by Williams and Izenman (1981) and by Izenman and Williams (1989) and has been further analyzed by Rao (1984, 1987) and by Lee (1988). Analyze as a one-sample growth curve design.
- (a) Using a matrix C of orthogonal polynomial contrasts, test the hypothesis of overall equality of means, H 0 : C 𝛍 = 0 .
- (b) If the overall hypothesis in (a) is rejected, find the degree of growth curve by testing each row of C .
- 6.39 In Table 6.24, we have measurements of proportions of albumin at four time points on three groups of trout (Beauchamp and Hoel 1974).
- (a) Using a matrix C of orthogonal contrasts, test the hypothesis of overall equality of means, H 0 : C 𝛍 . = 0 , for the combined samples, as in Section 6.10.2.
- (b) If the overall hypothesis is rejected, find the degree of growth curve for the combined samples by testing each row of C .
- (c) Compare the three groups using the entire matrix C .
- (d) Compare the three groups using each row of C .
- 6.40 Table 6.25 contains weight gains for three groups of rats (Box 1950).

Table 6.24. Measurements of Trout

|       |   Time Point |   Time Point |   Time Point |   Time Point |
|-------|--------------|--------------|--------------|--------------|
| Group |        1     |        2     |        3     |        4     |
| 1     |        0.257 |        0.288 |        0.328 |        0.358 |
| 1     |        0.266 |        0.282 |        0.315 |        0.464 |
| 1     |        0.256 |        0.303 |        0.293 |        0.261 |
| 1     |        0.272 |        0.456 |        0.288 |        0.261 |
| 2     |        0.312 |        0.3   |        0.273 |        0.253 |
| 2     |        0.253 |        0.22  |        0.314 |        0.261 |
| 2     |        0.239 |        0.261 |        0.279 |        0.224 |
| 2     |        0.254 |        0.243 |        0.304 |        0.254 |
| 3     |        0.272 |        0.279 |        0.259 |        0.295 |
| 3     |        0.246 |        0.292 |        0.279 |        0.302 |
| 3     |        0.262 |        0.311 |        0.263 |        0.264 |
| 3     |        0.292 |        0.261 |        0.314 |        0.244 |

The variables are yi = gain in i th week, i = 1 , 2 , 3 , 4.

The groups are 1 = controls, 2 = thyroxin added to drinking water, and 3 = thiouracil added to drinking water.

Table 6.25. Weekly Gains in Weight for 27 Rats

| Group 1   | Group 1   | Group 1   | Group 1   | Group 1   | Group 2   | Group 2   | Group 2   | Group 2   | Group 2   | Group 3   | Group 3   | Group 3   | Group 3   | Group 3   |
|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
| Rat       | y 1       | y 2       | y 3       | y 4       | Rat       | y 1       | y 2       | y 3       | y 4       | Rat       | y 1       | y 2       | y 3       | y 4       |
| 1         | 29        | 28        | 25        | 33        | 11        | 26        | 36        | 35        | 35        | 18        | 25        | 23        | 11        | 9         |
| 2         | 33        | 30        | 23        | 31        | 12        | 17        | 19        | 20        | 28        | 19        | 21        | 21        | 10        | 11        |
| 3         | 25        | 34        | 33        | 41        | 13        | 19        | 33        | 43        | 38        | 20        | 26        | 21        | 6         | 27        |
| 4         | 18        | 33        | 29        | 35        | 14        | 26        | 31        | 32        | 29        | 21        | 29        | 12        | 11        | 11        |
| 5         | 25        | 23        | 17        | 30        | 15        | 15        | 25        | 23        | 24        | 22        | 24        | 26        | 22        | 17        |
| 6         | 24        | 32        | 29        | 22        | 16        | 21        | 24        | 19        | 24        | 23        | 24        | 17        | 8         | 19        |
| 7         | 20        | 23        | 16        | 31        | 17        | 18        | 35        | 33        | 33        | 24        | 22        | 17        | 8         | 5         |
| 8         | 28        | 21        | 18        | 24        |           |           |           |           |           | 25        | 11        | 24        | 21        | 24        |
| 9         | 18        | 23        | 22        | 28        |           |           |           |           |           | 26        | 15        | 17        | 12        | 17        |
| 10        | 25        | 28        | 29        | 30        |           |           |           |           |           | 27        | 19        | 17        | 15        | 18        |

- (a) Using a matrix C of orthogonal contrasts, test the hypothesis of overall equality of means, H 0 : C 𝛍 . = 0 , for the combined samples, as in Section 6.10.2.
- (b) If the overall hypothesis is rejected, find the degree of growth curve for the combined samples by testing each row of C .
- (c) Compare the three groups using the entire matrix C .
- (d) Compare the three groups using each row of C .
- 6.41 Table 6.26 contains measurements of coronary sinus potassium at 2-min intervals after coronary occlusion on four groups of dogs (Grizzle and Allen 1969). The groups are 1 = control dogs, 2 = dogs with extrinsic cardiac denervation 3 wk prior to coronary occlusion, 3 = dogs with extrinsic cardiac denervation immediately prior to coronary occlusion, and 4 = dogs with bilateral thoracic sympathectomy and stellectomy 3 wk prior to coronary occlusion.

Table 6.26. Coronary Sinus Potassium Measured at 2-min Intervals on Dogs

|       |   Time |   Time |   Time |   Time |   Time |   Time |   Time |
|-------|--------|--------|--------|--------|--------|--------|--------|
| Group |    1   |    3   |    5   |    7   |    9   |   11   |   13   |
| 1     |    4   |    4   |    4.1 |    3.6 |    3.6 |    3.8 |    3.1 |
| 1     |    4.2 |    4.3 |    3.7 |    3.7 |    4.8 |    5   |    5.2 |
| 1     |    4.3 |    4.2 |    4.3 |    4.3 |    4.5 |    5.8 |    5.4 |
| 1     |    4.2 |    4.4 |    4.6 |    4.9 |    5.3 |    5.6 |    4.9 |
| 1     |    4.6 |    4.4 |    5.3 |    5.6 |    5.9 |    5.9 |    5.3 |
| 1     |    3.1 |    3.6 |    4.9 |    5.2 |    5.3 |    4.2 |    4.1 |
| 1     |    3.7 |    3.9 |    3.9 |    4.8 |    5.2 |    5.4 |    4.2 |
| 1     |    4.3 |    4.2 |    4.4 |    5.2 |    5.6 |    5.4 |    4.7 |
| 1     |    4.6 |    4.6 |    4.4 |    4.6 |    5.4 |    5.9 |    5.6 |
| 2     |    3.4 |    3.4 |    3.5 |    3.1 |    3.1 |    3.7 |    3.3 |

Table 6.26. ( Continued )

|       |   Time |   Time |   Time |   Time |   Time |   Time |   Time |
|-------|--------|--------|--------|--------|--------|--------|--------|
| Group |    1   |    3   |    5   |    7   |    9   |   11   |   13   |
| 2     |    3   |    3.1 |    3.2 |    3   |    3.3 |    3   |    3   |
| 2     |    3   |    3.2 |    3   |    3   |    3.1 |    3.2 |    3.1 |
| 2     |    3.1 |    3.2 |    3.2 |    3.2 |    3.3 |    3.1 |    3.1 |
| 2     |    3.8 |    3.9 |    4   |    2.9 |    3.5 |    3.5 |    3.4 |
| 2     |    3   |    3.6 |    3.2 |    3.1 |    3   |    3   |    3   |
| 2     |    3.3 |    3.3 |    3.3 |    3.4 |    3.6 |    3.1 |    3.1 |
| 2     |    4.2 |    4   |    4.2 |    4.1 |    4.2 |    4   |    4   |
| 2     |    4.1 |    4.2 |    4.3 |    4.3 |    4.2 |    4   |    4.2 |
| 2     |    4.5 |    4.4 |    4.3 |    4.5 |    5.3 |    4.4 |    4.4 |
| 3     |    3.2 |    3.3 |    3.8 |    3.8 |    4.4 |    4.2 |    3.7 |
| 3     |    3.3 |    3.4 |    3.4 |    3.7 |    3.7 |    3.6 |    3.7 |
| 3     |    3.1 |    3.3 |    3.2 |    3.1 |    3.2 |    3.1 |    3.1 |
| 3     |    3.6 |    3.4 |    3.5 |    4.6 |    4.9 |    5.2 |    4.4 |
| 3     |    4.5 |    4.5 |    5.4 |    5.7 |    4.9 |    4   |    4   |
| 3     |    3.7 |    4   |    4.4 |    4.2 |    4.6 |    4.8 |    5.4 |
| 3     |    3.5 |    3.9 |    5.8 |    5.4 |    4.9 |    5.3 |    5.6 |
| 3     |    3.9 |    4   |    4.1 |    5   |    5.4 |    4.4 |    3.9 |
| 4     |    3.1 |    3.5 |    3.5 |    3.2 |    3   |    3   |    3.2 |
| 4     |    3.3 |    3.2 |    3.6 |    3.7 |    3.7 |    4.2 |    4.4 |
| 4     |    3.5 |    3.9 |    4.7 |    4.3 |    3.9 |    3.4 |    3.5 |
| 4     |    3.4 |    3.4 |    3.5 |    3.3 |    3.4 |    3.2 |    3.4 |
| 4     |    3.7 |    3.8 |    4.2 |    4.3 |    3.6 |    3.8 |    3.7 |
| 4     |    4   |    4.6 |    4.8 |    4.9 |    5.4 |    5.6 |    4.8 |
| 4     |    4.2 |    3.9 |    4.5 |    4.7 |    3.9 |    3.8 |    3.7 |
| 4     |    4.1 |    4.1 |    3.7 |    4   |    4.1 |    4.6 |    4.7 |
| 4     |    3.5 |    3.6 |    3.6 |    4.2 |    4.8 |    4.9 |    5   |

- (a) Using a matrix C of orthogonal contrasts, test the hypothesis of overall equality of means, H 0 : C 𝛍 . = 0 , for the combined samples, as in Section 6.10.2.
- (b) If the overall hypothesis is rejected, find the degree of growth curve for the combined samples by testing each row of C .
- (c) Compare the four groups using the entire matrix C .
- (d) Compare the four groups using each row of C .
- 6.42 Table 6.27 contains blood pressure measurements at intervals after inducing a heart attack for four groups of rats: group 1 is the controls and groups 2-4 have been exposed to halothane concentrations of .25%, .50%, 1.0%, respectively (Crepeau et al. 1985).
- (a) Find the degree of growth curve for the combined sample using the methods in (6.113)-(6.118).

Table 6.27. Blood Pressure Data

|       |   Number of |   Number of |   Number of |   Number of |   Number of |   Number of |
|-------|-------------|-------------|-------------|-------------|-------------|-------------|
| Group |         1   |         5   |        10   |        15   |        30   |        60   |
| 1     |       112.5 |       100.5 |       102.5 |       102.5 |       107.5 |       107.5 |
| 1     |        92.5 |       102.5 |       105   |       100   |       110   |       117.5 |
| 1     |       132.5 |       125   |       115   |       112.5 |       110   |       110   |
| 1     |       102.5 |       107.5 |       107.5 |       102.5 |        90   |       112.5 |
| 1     |       110   |       130   |       115   |       105   |       112.5 |       110   |
| 1     |        97.5 |        97.5 |        80   |        82.5 |        82.5 |       102.5 |
| 1     |        90   |        70   |        85   |        85   |        92.5 |        97.5 |
| 2     |       115   |       115   |       107.5 |       107.5 |       112.5 |       107.5 |
| 2     |       125   |       125   |       120   |       120   |       117.5 |       125   |
| 2     |        95   |        90   |        95   |        90   |       100   |       107.5 |
| 2     |        87.5 |        65.5 |        85   |        90   |       105   |        90   |
| 2     |        90   |        87.5 |        97.5 |        95   |       100   |        95   |
| 2     |        97.5 |        92.5 |        57.5 |        55   |        90   |        97.5 |
| 2     |       107.5 |       107.5 |       145   |       110   |       105   |       112.5 |
| 2     |       102.5 |       130   |        85   |        80   |       127.5 |        97.5 |
| 3     |       107.5 |       107.5 |       102.5 |       102.5 |       102.5 |        97.5 |
| 3     |        97.5 |       108.5 |        94.5 |       102.5 |       102.5 |       107.5 |
| 3     |       100   |       105   |       105   |       105   |       110   |       110   |
| 3     |        95   |        95   |        90   |       100   |       100   |       100   |
| 3     |        85   |        92.5 |        92.5 |        92.5 |        90   |       110   |
| 3     |        82.5 |        77.5 |        75   |        65.5 |        65   |        72.5 |
| 3     |        62.5 |        75   |       115   |       110   |       100   |       100   |
| 4     |        70   |        67.5 |        67.5 |        77.5 |        77.5 |        77.5 |
| 4     |        45   |        37.5 |        45   |        45   |        47.5 |        45   |
| 4     |        52.5 |        22.5 |        90   |        65   |        60   |        65.5 |
| 4     |       100   |       100   |       100   |       100   |        97.5 |        92.5 |
| 4     |       115   |       110   |       100   |       110   |       105   |       105   |
| 4     |        97.5 |        97.5 |        97.5 |       105   |        95   |        92.5 |
| 4     |        95   |       125   |       130   |       125   |       115   |       117.5 |
| 4     |        72.5 |        87.5 |        65   |        57.5 |        92.5 |        82.5 |
| 4     |       105   |       105   |       105   |       105   |       102.5 |       100   |

- (b) Repeat (a) for group 1.
- (c) Repeat (a) for groups 2-4 combined.
- 6.43 Table 6.28, from Zerbe (1979a), compares 13 control and 20 obese patients on a glucose tolerance test using plasma inorganic phosphate. Delete the observations corresponding to 1 2 and 1 1 2 hours so that the time points are equally spaced.
- (a) For the control group, use orthogonal polynomials to find the degree of growth curve.
- (b) Repeat (a) for the obese group.

246

Table 6.28. Plasma Inorganic Phosphate (mg/dl)

|         | Hours after Glucose Challenge   | Hours after Glucose Challenge   | Hours after Glucose Challenge   | Hours after Glucose Challenge   | Hours after Glucose Challenge   | Hours after Glucose Challenge   | Hours after Glucose Challenge   | Hours after Glucose Challenge   |
|---------|---------------------------------|---------------------------------|---------------------------------|---------------------------------|---------------------------------|---------------------------------|---------------------------------|---------------------------------|
| Patient | 0                               | 1 2                             | 1                               | 1 1 2                           | 2                               | 3                               | 4                               | 5                               |
|         | Control                         | Control                         | Control                         | Control                         | Control                         | Control                         | Control                         | Control                         |
| 1       | 4.3                             | 3.3                             | 3.0                             | 2.6                             | 2.2                             | 2.5                             | 3.4                             | 4.4 a                           |
| 2       | 3.7                             | 2.6                             | 2.6                             | 1.9                             | 2.9                             | 3.2                             | 3.1                             | 3.9                             |
| 3       | 4.0                             | 4.1                             | 3.1                             | 2.3                             | 2.9                             | 3.1                             | 3.9                             | 4.0                             |
| 4       | 3.6                             | 3.0                             | 2.2                             | 2.8                             | 2.9                             | 3.9                             | 3.8                             | 4.0                             |
| 5       | 4.1                             | 3.8                             | 2.1                             | 3.0                             | 3.6                             | 3.4                             | 3.6                             | 3.7                             |
| 6       | 3.8                             | 2.2                             | 2.0                             | 2.6                             | 3.8                             | 3.6                             | 3.0                             | 3.5                             |
| 7       | 3.8                             | 3.0                             | 2.4                             | 2.5                             | 3.1                             | 3.4                             | 3.5                             | 3.7                             |
| 8       | 4.4                             | 3.9                             | 2.8                             | 2.1                             | 3.6                             | 3.8                             | 4.0                             | 3.9                             |
| 9       | 5.0                             | 4.0                             | 3.4                             | 3.4                             | 3.3                             | 3.6                             | 4.0                             | 4.3                             |
| 10      | 3.7                             | 3.1                             | 2.9                             | 2.2                             | 1.5                             | 2.3                             | 2.7                             | 2.8                             |
| 11      | 3.7                             | 2.6                             | 2.6                             | 2.3                             | 2.9                             | 2.2                             | 3.1                             | 3.9                             |
| 12      | 4.4                             | 3.7                             | 3.1                             | 3.2                             | 3.7                             | 4.3                             | 3.9                             | 4.8                             |
| 13      | 4.7                             | 3.1                             | 3.2                             | 3.3                             | 3.2                             | 4.2                             | 3.7                             | 4.3                             |
| 1       | 4.3                             | 3.3                             | 3.0                             | 2.6                             | 2.2                             | 2.5                             | 2.4                             | 3.4 a                           |
| 2       | 5.0                             | 4.9                             | 4.1                             | 3.7                             | 3.7                             | 4.1                             | 4.7                             | 4.9                             |
| 3       | 4.6                             | 4.4                             | 3.9                             | 3.9                             | 3.7                             | 4.2                             | 4.8                             | 5.0                             |
| 4       | 4.3                             | 3.9                             | 3.1                             | 3.1                             | 3.1                             | 3.1                             | 3.6                             | 4.0                             |
| 5       | 3.1                             | 3.1                             | 3.3                             | 2.6                             | 2.6                             | 1.9                             | 2.3                             | 2.7                             |
| 6       | 4.8                             | 5.0                             | 2.9                             | 2.8                             | 2.2                             | 3.1                             | 3.5                             | 3.6                             |
| 7       | 3.7                             | 3.1                             | 3.3                             | 2.8                             | 2.9                             | 3.6                             | 4.3                             | 4.4                             |
| 8       | 5.4                             | 4.7                             | 3.9                             | 4.1                             | 2.8                             | 3.7                             | 3.5                             | 3.7                             |
| 9       | 3.0                             | 2.5                             | 2.3                             | 2.2                             | 2.1                             | 2.6                             | 3.2                             | 3.5                             |
| 10      | 4.9                             | 5.0                             | 4.1                             | 3.7                             | 3.7                             | 4.1                             | 4.7                             | 4.9                             |
| 11      | 4.8                             | 4.3                             | 4.7                             | 4.6                             | 4.7                             | 3.7                             | 3.6                             | 3.9                             |
| 12      | 4.4                             | 4.2                             | 4.2                             | 3.4                             | 3.5                             | 3.4                             | 3.9                             | 4.0                             |
| 13      | 4.9                             | 4.3                             | 4.0                             | 4.0                             | 3.3                             | 4.1                             | 4.2                             | 4.3                             |
| 14      | 5.1                             | 4.1                             | 4.6                             | 4.1                             | 3.4                             | 4.2                             | 4.4                             | 4.9                             |
| 15      | 4.8                             | 4.6                             | 4.6                             | 4.4                             | 4.1                             | 4.0                             | 3.8                             | 3.8                             |
| 16      | 4.2                             | 3.5                             | 3.8                             | 3.6                             | 3.3                             | 3.1                             | 3.5                             | 3.9                             |
| 17      |                                 |                                 |                                 |                                 |                                 | 3.8                             |                                 |                                 |
|         | 6.6                             | 6.1                             | 5.2                             | 4.1                             | 4.3                             |                                 | 4.2                             | 4.8                             |
| 18      | 3.6                             | 3.4                             | 3.1                             | 2.8                             | 2.1                             | 2.4                             | 2.5                             | 3.5                             |
| 19      | 4.5                             | 4.0                             | 3.7                             | 3.3                             | 2.4                             | 2.3                             | 3.1                             | 3.3                             |

a The similarity in the data for patient 1 in the control group and patient 1 in the obese group is coincidental.

Table 6.29. Mandible Measurements

|       |         | Activator Treatment   | Activator Treatment   | Activator Treatment   | Activator Treatment   | Activator Treatment   | Activator Treatment   | Activator Treatment   | Activator Treatment   | Activator Treatment   |
|-------|---------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|
|       |         | 1                     | 1                     | 1                     |                       |                       |                       | 3                     | 3                     | 3                     |
| Group | Subject | y 1                   | y 2                   | y 3                   | y 1                   | y 2                   | y 3                   | y 1                   | y 2                   | y 3                   |
| 1     | 1       | 117.0                 | 117.5                 | 118.5                 | 59.0                  | 59.0                  | 60.0                  | 10.5                  | 16.5                  | 16.5                  |
|       | 2       | 109.0                 | 110.5                 | 111.0                 | 60.0                  | 61.5                  | 61.5                  | 30.5                  | 30.5                  | 30.5                  |
|       | 3       | 117.0                 | 120.0                 | 120.5                 | 60.0                  | 61.5                  | 62.0                  | 23.5                  | 23.5                  | 23.5                  |
|       | 4       | 122.0                 | 126.0                 | 127.0                 | 67.5                  | 70.5                  | 71.5                  | 33.0                  | 32.0                  | 32.5                  |
|       | 5       | 116.0                 | 118.5                 | 119.5                 | 61.5                  | 62.5                  | 63.5                  | 24.5                  | 24.5                  | 24.5                  |
|       | 6       | 123.0                 | 126.0                 | 127.0                 | 65.5                  | 61.5                  | 67.5                  | 22.0                  | 22.0                  | 22.0                  |
|       | 7       | 130.5                 | 132.0                 | 134.5                 | 68.5                  | 69.5                  | 71.0                  | 33.0                  | 32.5                  | 32.0                  |
|       | 8       | 126.5                 | 128.5                 | 130.5                 | 69.0                  | 71.0                  | 73.0                  | 20.0                  | 20.0                  | 20.0                  |
|       | 9       | 113.0                 | 116.5                 | 118.0                 | 58.0                  | 59.0                  | 60.5                  | 25.0                  | 25.0                  | 24.5                  |
| 2     | 1       | 128.0                 | 129.0                 | 131.5                 | 67.0                  | 67.5                  | 69.0                  | 24.0                  | 24.0                  | 24.0                  |
|       | 2       | 116.5                 | 120.0                 | 121.5                 | 63.5                  | 65.0                  | 66.0                  | 28.5                  | 29.5                  | 29.5                  |
|       | 3       | 121.5                 | 125.5                 | 127.0                 | 64.5                  | 67.5                  | 69.0                  | 26.5                  | 27.0                  | 27.0                  |
|       | 4       | 109.5                 | 112.0                 | 114.0                 | 54.0                  | 55.5                  | 57.0                  | 18.0                  | 18.5                  | 19.0                  |
|       | 5       | 133.0                 | 136.0                 | 137.5                 | 72.0                  | 73.5                  | 75.5                  | 34.5                  | 34.5                  | 34.5                  |
|       | 6       | 120.0                 | 124.5                 | 126.0                 | 62.5                  | 65.0                  | 66.0                  | 26.0                  | 26.0                  | 26.0                  |
|       | 7       | 129.5                 | 133.5                 | 134.5                 | 65.0                  | 68.0                  | 69.0                  | 18.5                  | 18.5                  | 18.5                  |
|       | 8       | 122.0                 | 124.0                 | 125.5                 | 64.5                  | 65.5                  | 66.0                  | 18.5                  | 18.5                  | 18.5                  |
|       | 9       | 125.0                 | 127.0                 | 128.0                 | 65.5                  | 66.5                  | 67.0                  | 21.5                  | 21.5                  | 21.6                  |

- (c) Find the degree of growth curve for the combined groups, and compare the growth curves of the two groups.
- 6.44 Consider the complete data from Table 6.28 including the observations corresponding to 1 2 and 1 1 2 hours. Use the methods in (6.113)-(6.118) for unequally spaced time points to analyze each group separately and the combined groups.
- 6.45 Table 6.29 contains mandible measurements (Timm 1980). There were two groups of subjects. Each subject was measured at three time points y 1, y 2, and y 3 for each of three types of activator treatment. Analyze as a repeated measures design with two within-subjects factors and one between-subjects factor. Use linear and quadratic contrasts for time (growth curve).

## C H A P T E R 7

## Tests on Covariance Matrices

## 7.1 INTRODUCTION

We now consider tests of hypotheses involving the variance-covariance structure. These tests are often carried out to check assumptions pertaining to other tests. In Sections 7.2-7.4, we cover three basic types of hypotheses: (1) the covariance matrix has a particular structure, (2) two or more covariance matrices are equal, and (3) certain elements of the covariance matrix are zero, thus implying independence of the corresponding (multivariate normal) random variables. In most cases we use the likelihood ratio approach (Section 5.4.3). The resulting test statistics often involve the ratio of the determinants of the sample covariance matrix under the null hypothesis and under the alternative hypothesis.

## 7.2 TESTING A SPECIFIED PATTERN FOR Σ

In this section, the discussion is in terms of a sample covariance matrix S from a single sample. However, the tests can be applied to a sample covariance matrix S pl = E /ν E obtained by pooling across several samples. To allow for either possibility, the degrees-of-freedom parameter has been indicated by ν . For a single sample, ν = n -1; for a pooled covariance matrix, ν = ∑ k i = 1 ( ni -1 ) = ∑ k i = 1 ni -k = N -k .

## 7.2.1 Testing H 0 : Σ = Σ 0

We begin with the basic hypothesis H 0 : 𝚺 = 𝚺 0 vs. H 1 : 𝚺 /negationslash= 𝚺 0. The hypothesized covariance matrix 𝚺 0 is a target value for 𝚺 or a nominal value from previous experience. Note that 𝚺 0 is completely specified in H 0, whereas 𝛍 is not specified.

To test H 0, we obtain a random sample of n observation vectors y 1, y 2 , . . . , y n from Np ( 𝛍 , 𝚺 ) and calculate S . To see if S is significantly different from 𝚺 0, we use the following test statistic, which is a modification of the likelihood ratio (Sec-

tion 5.4.3):

where ν represents the degrees of freedom of S (seecomments at the beginning of Section 7.2), ln is the natural logarithm (base e ), and tr is the trace of a matrix (Section 2.9). Note that if S = 𝚺 0, then u = 0; otherwise u increases with the'distance' between S and 𝚺 0 [see(7.4) and the comment following].

<!-- formula-not-decoded -->

When ν is large, the statistic u in (7.1) is approximately distributed as χ 2 [ 1 2 p ( p + 1 ) ] if H 0 is true. For moderate size ν ,

<!-- formula-not-decoded -->

is a better approximation to the χ 2 [ 1 2 p ( p + 1 ) ] distribution. We reject H 0 if u or u ′ is greater than χ 2 [ α, 1 2 p ( p + 1 ) ] . Note that the degrees of freedom for the χ 2 -statistic, 1 2 p ( p + 1 ) , is the number of distinct parameters in 𝚺 .

We can express u in terms of the eigenvalues λ 1, λ 2 , . . . , λ p of S 𝚺 -1 0 by noting that tr ( S 𝚺 -1 0 ) and ln | 𝚺 0 | -ln | S | become

<!-- formula-not-decoded -->

from which (7.1) can be written as

<!-- formula-not-decoded -->

A plot of y = x -ln x will show that x -ln x ≥ 1 for all x &gt; 0, with equality holding only for x = 1. Thus p i 1 (λ i -ln λ i ) &gt; p and u &gt; 0.

∑ = The hypothesis that the variables are independent and have unit variance,

<!-- formula-not-decoded -->

can be tested by simply setting 𝚺 0 = I in (7.1).

## 7.2.2 Testing Sphericity

The hypothesis that the variables y 1, y 2 , . . . , yp in y are independent and have the same variance can be expressed as H 0 : 𝚺 = σ 2 I versus H 1 : 𝚺 /negationslash= σ 2 I , where σ 2 is the unknown common variance. This hypothesis is of interest in repeated measures (see Section 6.9.1). Under H 0, the ellipsoid ( y -𝛍 ) ′ 𝚺 -1 ( y -𝛍 ) = c 2 reduces to ( y -𝛍 ) ′ ( y -𝛍 ) = σ 2 c 2 , the equation of a sphere; hence the term sphericity is applied to the covariance structure 𝚺 = σ 2 I . Another sphericity hypothesis of interest in repeated measures is H 0 : C 𝚺 C ′ = σ 2 I , where C is any full-rank ( p -1 ) × p matrix of orthonormal contrasts (see Section 6.9.1).

For a random sample y 1, y 2 , . . . , y n from Np ( 𝛍 , 𝚺 ) , the likelihood ratio for testing H 0 : 𝚺 = σ 2 I is

<!-- formula-not-decoded -->

In some cases that we have considered previously, the likelihood ratio is a simple function of a test statistic such as F , T 2 , Wilks' /Lambda1 , and so on. However, LR in (7.5) does not reduce to a standard statistic, and we resort to an approximation for its distribution. It has been shown that for a general likelihood ratio statistic LR,

<!-- formula-not-decoded -->

for large n , where ν is the total number of parameters minus the number estimated under the restrictions imposed by H 0.

For the likelihood ratio statistic in (7.5), we obtain

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

By (2.107) and (2.108), u becomes

<!-- formula-not-decoded -->

where λ 1, λ 2 , . . . , λ p are the eigenvalues of S . An improvement over -n ln u is given by

<!-- formula-not-decoded -->

where

where ν is the degrees of freedom for S (see comments at the beginning of Section 7.2). The statistic u ′ has an approximate χ 2 -distribution with 1 2 p ( p + 1 ) -1 degrees of freedom. We reject H 0 if u ′ ≥ χ 2 [ α, 1 2 p ( p + 1 ) -1 ] . As noted before, the degrees of freedom in the χ 2 -approximation is equal to the total number of parameters minus the number of parameters estimated under H 0. The number of parameters in 𝚺 is p + p 2 = 1 2 p ( p + 1 ) , and the loss of 1 degree of freedom is due to estimation of σ 2 .

To test H 0 : C 𝚺 C ′ = σ 2 I , use CSC ′ in place of S in (7.7) and use p -1 in place of p in (7.7)-(7.9) and in the degrees of freedom for χ 2 .

( ) We see from (7.8) and (7.9) that if the sample λ i 's are all equal, u = 1 and u ′ = 0. Hence, this statistic also tests the hypothesis of equality of the population eigenvalues.

The likelihood ratio (7.5) was first given by Mauchly (1940), and his name is often associated with this test. Nagarsenker and Pillai (1973) gave the exact distribution of u and provided a table for p = 4 , 5 , . . . , 10. Venables (1976) showed that u can be obtained by a union-intersection approach (Section 6.1.4).

Example 7.2.2. Weuse the probe word data in Table 3.5 to illustrate tests of sphericity. The five variables appear to be commensurate, and the hypothesis H 0 : µ 1 = µ 2 = ··· = µ 5 may be of interest. We would expect the variables to be correlated, and H 0 would ordinarily be tested using a multivariate approach, as in Sections 5.9.1 and 6.9.2. However, if 𝚺 = σ 2 I or C 𝚺 C ′ = σ 2 I , then the hypothesis H 0 : µ 1 = µ 2 = ··· = µ 5 can be tested with a univariate ANOVA F -test (see Section 6.9.1).

We first test H 0 : 𝚺 = σ 2 I . The sample covariance matrix S was obtained in Example 3.9.1. By (7.7),

<!-- formula-not-decoded -->

Then by (7.9), with n = 11 and p = 5, we have

<!-- formula-not-decoded -->

The approximate χ 2 -test has 1 2 p ( p + 1 ) -1 = 14 degrees of freedom. We therefore compare u ′ = 26 . 177 with χ 2 . 05 , 14 = 23 . 68 and reject H 0 : 𝚺 = σ 2 I .

To test H 0 : C 𝚺 C ′ = σ 2 I , we use the following matrix of orthonormalized contrasts:

<!-- formula-not-decoded -->

Then using CSC ′ in place of S and with p -1 = 4 for the four rows of C , we obtain

<!-- formula-not-decoded -->

For degrees of freedom, we now have 1 2 ( 4 )( 5 ) -1 = 9, and the critical value is χ 2 . 05 , 9 = 16 . 92. Hence, we do not reject H 0 : C 𝚺 C ′ = σ 2 I , and a univariate F -test of H 0 : µ 1 = µ 2 = · · · = µ 5 may be justified.

## 7.2.3 Testing H 0 : Σ = σ 2 [ (1 -ρ )I + ρ J ]

In Section 6.9.1, it was noted that univariate ANOV A remains valid if

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

where J is a square matrix of 1's, as defined in (2.12), and ρ is the population correlation between any two variables. This pattern of equal variances and equal covariances in 𝚺 is variously referred to as uniformity, compound symmetry , or the intraclass correlation model .

We now consider the hypothesis that (7.10) holds:

<!-- formula-not-decoded -->

From a sample, we obtain the sample covariance matrix S . Estimates of σ 2 and σ 2 ρ under H 0 are given by

<!-- formula-not-decoded -->

respectively, where s j j and s jk are from S . Thus s 2 is an average of the variances on the diagonal of S , and s 2 r is an average of the off-diagonal covariances in S . An estimate of ρ can be obtained as r = s 2 r / s 2 . Using s 2 and s 2 r in (7.12), the estimate of 𝚺 under H 0 is then

where

<!-- formula-not-decoded -->

We reject H 0 : 𝚺 = σ 2 [ ( 1 -ρ) I + ρ J ] if F &gt; F α,γ 1 ,γ 2 .

Example 7.2.3. To illustrate this test, we use the cork data of Table 6.21. In Problem 6.34, a comparison is made of average thickness, and hence weight, in the four directions. A standard ANOVA approach to this repeated measures design would be valid if (7.10) holds. To check this assumption, we test H 0 : 𝚺 = σ 2 [ ( 1 -ρ) I + ρ J ] .

<!-- formula-not-decoded -->

To compare S and S 0, we use the following function of the likelihood ratio:

<!-- formula-not-decoded -->

which can be expressed in the alternative form

<!-- formula-not-decoded -->

By analogy with (7.9), the test statistic is given by

<!-- formula-not-decoded -->

where ν is the degrees of freedom of S (see comments at the beginning of Section 7.2). The statistic u ′ is approximately χ 2 [ 1 2 p ( p + 1 ) -2 ] , and we reject H 0 if u ′ &gt; χ 2 [ α, 1 2 p ( p + 1 ) -2 ] . Note that 2 degrees of freedom are lost due to estimation of σ 2 and ρ .

An alternative approximate test that is more precise when p is large and ν is relatively small is given by

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

The sample covariance matrix is given by

<!-- formula-not-decoded -->

from which we obtain

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

From (7.15) and (7.16), we now have

<!-- formula-not-decoded -->

Since 19 . 511 &gt; χ 2 . 05 , 8 = 15 . 5, we reject H 0 and conclude that 𝚺 does not have the pattern in (7.10).

## 7.3 TESTS COMPARING COVARIANCE MATRICES

An assumption for T 2 or MANOVA tests comparing two or more mean vectors is that the corresponding population covariance matrices are equal: 𝚺 1 = 𝚺 2 = · · · = 𝚺 k . Under this assumption, the sample covariance matrices S 1, S 2 , . . . , S k reflect a common population 𝚺 and are therefore pooled to obtain an estimate of 𝚺 . If 𝚺 1 = 𝚺 2 = · · · = 𝚺 k is not true, large differences in S 1, S 2 , . . . , S k may possibly lead to rejection of H 0 : 𝛍 1 = 𝛍 2 = · · · = 𝛍 k . However, the T 2 and MANOVA tests are fairly robust to heterogeneity of covariance matrices as long as the sample sizes are large and equal. For other cases it is useful to have available a test of equality of covariance matrices. We begin with a review of the univariate case.

## 7.3.1 Univariate Tests of Equality of Variances

The two-sample univariate hypothesis H 0 : σ 2 1 = σ 2 2 vs. H 1 : σ 2 1 /negationslash= σ 2 2 is tested with

<!-- formula-not-decoded -->

where s 2 1 and s 2 2 are the variances of the two samples. If H 0 is true (and assuming normality), f is distributed as F ν 1 ,ν 2 , where ν 1 and ν 2 are the degrees of freedom of s 2 1 and s 2 2 (typically, n 1 -1 and n 2 -1). Note that s 2 1 and s 2 2 must be independent, which will hold if the two samples are independent.

For the several-sample case, various procedures have been proposed. We present Bartlett's (1937) test of homogeneity of variances because it has been extended to the multivariate case. To test

<!-- formula-not-decoded -->

we calculate

<!-- formula-not-decoded -->

where s 2 1 , s 2 2 , . . . , s 2 k are independent sample variances with ν 1, ν 2 , . . . , ν k degrees of freedom, respectively. Then

<!-- formula-not-decoded -->

We reject H 0 if m / c &gt; χ 2 α, k -1 .

For an F -approximation, we use c and m and calculate in addition

<!-- formula-not-decoded -->

Then

<!-- formula-not-decoded -->

We reject H 0 if F &gt; F α .

Note that an assumption for either form of the preceding test is independence of s 2 1 , s 2 2 , . . . , s 2 k , which will hold for random samples from k distinct populations. This test would therefore be inappropriate for comparing s 11, s 22 , . . . , s pp from the diagonal of S , since the s j j 's are correlated.

## 7.3.2 Multivariate Tests of Equality of Covariance Matrices

For k multivariate populations, the hypothesis of equality of covariance matrices is

<!-- formula-not-decoded -->

The test of H 0 : 𝚺 1 = 𝚺 2 for two groups is treated as a special case by setting k = 2. There is no exact test of H 0 : 𝚺 1 = 𝚺 2 as there is in the analogous univariate case [see (7.17)]. We assume independent samples of size n 1, n 2 , . . . , nk from multivariate normal distributions, as in an unbalanced one-way MANOVA, for example. To make the test, we calculate

<!-- formula-not-decoded -->

in which ν i = ni -1, S i is the covariance matrix of the i th sample, and S pl is the pooled sample covariance matrix

<!-- formula-not-decoded -->

The statistic M is a modification of the likelihood ratio and varies between 0 and 1, with values near 1 favoring H 0 in (7.18) and values near 0 leading to rejection of H 0. It is not immediately obvious that M in (7.19) behaves in this way, and we offer the following heuristic argument. First we note that (7.19) can be expressed as where E is given by (6.33) and ν E = ∑ k i = 1 ν i = ∑ i ni -k . It is clear that we must have every ν i &gt; p ; otherwise | S i | = 0 for some i , and M would be zero. Exact upper percentage points of -2ln M = ν( k ln | S pl | -∑ i ln | S i | ) for the special case of ν 1 = ν 2 = · · · = ν k = ν are given in Table A.14 for p = 2, 3, 4, 5 and k = 2 , 3 , . . . , 10 (Lee, Chiang, and Krishnaiah 1977). We can easily modify (7.19) and (7.20) to compare covariance matrices for the cells of a two-way model by using ν i j = nij -1.

<!-- formula-not-decoded -->

If S 1 = S 2 = · · · = S k = S pl, then M = 1. As the disparity among S 1, S 2 , . . . , S k increases, M approaches zero. To see this, note that the determinant of the pooled covariance matrix, | S pl | , lies somewhere near the'middle' of the | S i | 's and that as a set of variables z 1, z 2 , . . . , zn increases in spread, z ( 1 )/ z reduces the product more than z ( n )/ z increases it, where z ( 1 ) and z ( n ) are the minimum and maximum values, respectively. We illustrate this with the two sets of numbers, 4, 5, 6 and 1, 5, 9, which have the same mean but different spread. If we assume ν 1 = ν 2 = ν 3 = ν , then for the first set,

<!-- formula-not-decoded -->

and for the second set,

<!-- formula-not-decoded -->

In M 2, the smallest value, .2, reduces the product proportionally more than the largest value, 1.8, increases it. Another illustration is found in Problem 7.9.

Box (1949, 1950) gave χ 2 - and F -approximations for the distribution of M . Either of these approximate tests is referred to as Box's M-test . For the χ 2 -approximation, calculate

Then

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

where M is defined in (7.19), and

<!-- formula-not-decoded -->

We reject H 0 if u &gt; χ 2 α . If ν 1 = ν 2 = · · · = ν k = ν , then c 1 simplifies to

<!-- formula-not-decoded -->

To justify the degrees of freedom of the χ 2 -approximation, note that the total number of parameters estimated under H 1 is k [ 1 2 p ( p + 1 ) ] , whereas under H 0 we estimate only 𝚺 , which has p + ( p 2 ) = 1 2 p ( p + 1 ) parameters. The difference is ( k -1 ) [ 1 2 p ( p + 1 ) ] . The quantity k [ 1 2 p ( p + 1 ) ] arises from the assumption that all 𝚺 i , i = 1, 2 , . . . , k , are different. Technically, H 1 can be stated as 𝚺 i /negationslash= 𝚺 j for some i /negationslash= j . However, the most general case is all 𝚺 i different, and the distribution of M is derived accordingly.

For the F -approximation, we use c 1 from (7.22) and calculate, additionally,

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

If c 2 &gt; c 2 1 ,

If c 2 &lt; c 2 1 ,

<!-- formula-not-decoded -->

In either case, we reject H 0 if F &gt; F α . If ν 1 = ν 2 = · · · = ν k = ν , then c 1 simplifies as in (7.25) and c 2 simplifies to

<!-- formula-not-decoded -->

Box's M -test is calculated routinely in many computer programs for MANOVA. However, Olson (1974) showed that the M -test with equal ν i may detect some forms of heterogeneity that have only minor effects on the MANOVA tests. The test is also sensitive to some forms of nonnormality. For example, it is sensitive to kurtosis for which the MANOVA tests are rather robust. Thus the M -test may signal covariance heterogeneity in some cases where it is not damaging to the MANOVA tests. Hence we may not wish to automatically rule out standard MANOVA tests if the M -test leads to rejection of H 0. Olson showed that the skewness and kurtosis statistics b 1 , p and b 2 , p (see Section 4.4.2) have similar shortcomings.

Example 7.3.2. We test the hypothesis H 0 : 𝚺 1 = 𝚺 2 for the psychological data of Table 5.1. The covariance matrices S 1, S 2, and S pl were given in Example 5.4.2. Using these, we obtain, by (7.24),

<!-- formula-not-decoded -->

For an exact test, we compare

<!-- formula-not-decoded -->

with 19.74, its critical value from Table A.14.

For the χ 2 -approximation, we compute, by (7.25) and (7.23),

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

For an approximate F -test, we first calculate the following:

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Since c 2 = . 005463 &gt; c 2 1 = . 00481, we use (7.27) to obtain

<!-- formula-not-decoded -->

Thus all three tests accept H 0.

## 7.4 TESTS OF INDEPENDENCE

## 7.4.1 Independence of Two Subvectors

Suppose the observation vector is partitioned into two subvectors of interest, which we label y and x , as in Section 3.8.1, where y is p × 1 and x is q × 1. By (3.46), the corresponding partitioning of the population covariance matrix is

<!-- formula-not-decoded -->

with analogous partitioning of S and R as in (3.42):

<!-- formula-not-decoded -->

The hypothesis of independence of y and x can be expressed as

<!-- formula-not-decoded -->

Thus independence of y and x means that every variable in y is independent of every variable in x . Note that there is no restriction on 𝚺 yy or 𝚺 xx .

The likelihood ratio test statistic for H 0 : 𝚺 yx = O is given by

<!-- formula-not-decoded -->

which is distributed as /Lambda1 p , q , n -1 -q . We reject H 0 if /Lambda1 ≤ /Lambda1α . We thus have an exact test for H 0 : 𝚺 yx = O . Critical values for Wilks' /Lambda1 are given in Table A.9 using ν H = q and ν E = n -1 -q . The test statistic in (7.30) is equivalent (when H 0 is true) to the /Lambda1 -statistic (10.55) in Section 10.5.1 for testing the significance of the regression of y on x .

By the symmetry of

<!-- formula-not-decoded -->

/Lambda1 in (7.30) is also distributed as /Lambda1 q , p , n -1 -p . This is equivalent to property 3 in Section 6.1.3.

Note that | S yy || S xx | in (7.30) is an estimate of | 𝚺 yy || 𝚺 xx | , which by (2.92) is the determinant of 𝚺 when 𝚺 yx = O . Thus Wilks' /Lambda1 compares an estimate of 𝚺 without restrictions to an estimate of 𝚺 under H 0 : 𝚺 yx = O . We can see intuitively that | S | &lt; | S yy || S xx | by noting from (2.94) that | S | = | S xx || S yy -S yx S -1 xx S xy | , and since S yx S -1 xx S xy is positive definite, | S yy -S yx S -1 xx S xy | &lt; | S yy | . This can be illustrated for the case p = q = 1:

<!-- formula-not-decoded -->

Wilks' /Lambda1 in (7.30) can be expressed in terms of eigenvalues:

<!-- formula-not-decoded -->

where s = min ( p , q ) and the r 2 i 's are the nonzero eigenvalues of S -1 xx S xy S -1 yy S yx . We could also use S -1 yy S yx S -1 xx S xy , since the (nonzero) eigenvalues of S -1 yy S yx S -1 xx S xy are the same as those of S -1 xx S xy S -1 yy S yx (these two matrices are of the form AB and BA ; see Section 2.11.5). The number of nonzero eigenvalues is s = min ( p , q ) , since s is the rank of both S -1 yy S yx S -1 xx S xy and S -1 xx S xy S -1 yy S yx . The eigenvalues are designated r 2 i because they are the squared canonical correlations between y and x (see Chapter 11). In the special case p = 1, (7.31) becomes

<!-- formula-not-decoded -->

where R 2 is the square of the multiple correlation between y and ( x 1 , x 2 , . . . , xq ) .

∣ As s 2 yx increases, | S | decreases.

The other test statistics, U ( s ) , V ( s ) , and Roy's θ , can also be defined in terms of the r 2 i 's (see Section 11.4.1).

Example 7.4.1. Consider the diabetes data in Table 3.4. There is a natural partitioning in the variables, with y 1 and y 2 of minor interest and x 1, x 2, and x 3 of major interest. We test independence of the y 's and the x 's, that is, H 0 : 𝚺 yx = O . From Example 3.8.1, the partitioned covariance matrix is

<!-- formula-not-decoded -->

To make the test, we compute

<!-- formula-not-decoded -->

Thus we reject the hypothesis of independence. Note the use of 40 in /Lambda1 . 05 , 2 , 3 , 40 in place of n -1 -q = 46 -1 -3 = 42. This is a conservative approach that allows the use of a table value without interpolation.

## 7.4.2 Independence of Several Subvectors

Let there be k sets of variates so that y and 𝚺 are partitioned as

<!-- formula-not-decoded -->

with pi variables in y i , where p 1 + p 2 +···+ pk = p . Note that y 1, y 2 , . . . , y k represents a partitioning of y , not a random sample of independent vectors. The hypothesis that the subvectors y 1, y 2 , . . . , y k are mutually independent can be expressed as H 0 : 𝚺 i j = O for all i /negationslash= j , or

<!-- formula-not-decoded -->

The likelihood ratio statistic is

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

where S and R are obtained from a random sample of n observations and are partitioned as 𝚺 above, conforming to y 1, y 2 , . . . , y k . Note that the denominator of (7.33) is the determinant of S restricted by H 0, that is, with S i j = O for all i /negationslash= j [see (2.92)]. The statistic u does not have Wilks' /Lambda1 -distribution as it does in (7.30) when k = 2, but a good χ 2 -approximation to its distribution is given by

<!-- formula-not-decoded -->

where

<!-- formula-not-decoded -->

and ν is the degrees of freedom of S or R (see comments at the beginning of Section 7.2). We reject the independence hypothesis if u ′ &gt; χ 2 α, f .

The degrees of freedom, f = 1 2 a 2, arises from thefollowing consideration. The number of parameters in 𝚺 unrestricted by the hypothesis is 1 2 p ( p + 1 ) . Under the hypothesis (7.32), the number of parameters in each 𝚺 ii is 1 2 pi ( pi + 1 ) , for a total of 1 2 ∑ k i = 1 pi ( pi + 1 ) . The difference is

<!-- formula-not-decoded -->

Example 7.4.2. For 30 brands of Japanese Seishu wine, Siotani et al. (1963) studied the relationship between y 1 = taste,

y 2 = odor,

Table 7.1. Seishu Measurements

| y 1           | y 2 x 1   |   x 2 |   x 3 | x 4     |   x 5 |   x 6 |   x 7 |   x 8 |
|---------------|-----------|-------|-------|---------|-------|-------|-------|-------|
| 1.0           | .8 4.05   |  1.68 |  0.85 | 3.0     |  3.97 |  5    | 16.9  | 122   |
| .1 .2         | 3.81      |  1.39 |  0.3  | .6      |  3.62 |  4.52 | 15.8  |  62   |
| .5 .0         | 4.20      |  1.63 |  0.92 | - 2 . 3 |  3.48 |  4.46 | 15.8  | 139   |
| .7 .7         | 4.35      |  1.43 |  0.97 | - 1 . 6 |  3.45 |  3.98 | 15.4  | 150   |
| - . 1 - 1 . 1 | 4.35      |  1.53 |  0.87 | - 2 . 0 |  3.67 |  4.22 | 15.4  | 138   |
| .4 .5         | 4.05      |  1.84 |  0.95 | - 2 . 5 |  3.61 |  5    | 16.78 | 123   |
| .2 - . 3      | 4.20      |  1.61 |  1.09 | - 1 . 7 |  3.25 |  4.15 | 15.81 | 172   |
| .3 - . 1      | 4.32      |  1.43 |  0.93 | - 5 . 0 |  4.16 |  5.45 | 16.78 | 144   |
| .7 .4         | 4.21      |  1.74 |  0.95 | - 1 . 5 |  3.4  |  4.25 | 16.62 | 153   |
| .5 - . 1      | 4.17      |  1.72 |  0.92 | - 1 . 2 |  3.62 |  4.31 | 16.7  | 121   |
| - . 1 .1      | 4.45      |  1.78 |  1.19 | - 2 . 0 |  3.09 |  3.92 | 16.5  | 176   |
| .5 - . 5      | 4.45      |  1.48 |  0.86 | - 2 . 0 |  3.32 |  4.09 | 15.4  | 128   |
| .5 .8         | 4.25      |  1.53 |  0.83 | - 3 . 0 |  3.48 |  4.54 | 15.55 | 126   |
| .6 .2         | 4.25      |  1.49 |  0.86 | 2.0     |  3.13 |  3.45 | 15.6  | 128   |
| .0 - . 5      | 4.05      |  1.48 |  0.3  | .0      |  3.67 |  4.52 | 15.38 |  99   |
| - . 2 - . 2   | 4.22      |  1.64 |  0.9  | - 2 . 2 |  3.59 |  4.49 | 16.37 | 122.8 |
| .0 - . 2      | 4.10      |  1.55 |  0.85 | 1.8     |  3.02 |  3.62 | 15.31 | 114   |
| .2 .2         | 4.28      |  1.52 |  0.75 | - 4 . 8 |  3.64 |  4.93 | 15.77 | 125   |
| - . 1 - . 2   | 4.32      |  1.54 |  0.83 | - 2 . 0 |  3.17 |  4.62 | 16.6  | 119   |
| .6 .1         | 4.12      |  1.68 |  0.84 | - 2 . 1 |  3.72 |  4.83 | 16.93 | 111   |
| .8 .5         | 4.30      |  1.5  |  0.92 | - 1 . 5 |  2.98 |  3.92 | 15.1  |  68   |
| .5 .2         | 4.55      |  1.5  |  1.14 | .9      |  2.6  |  3.45 | 15.7  | 197   |
| .4 .7         | 4.15      |  1.62 |  0.78 | - 7 . 0 |  4.11 |  5.55 | 15.5  | 106   |
| .6 - . 3      | 4.15      |  1.32 |  0.31 | .8      |  3.56 |  4.42 | 15.4  |  49.5 |
| - . 7 - . 3   | 4.25      |  1.77 |  1.12 | .5      |  2.84 |  4.15 | 16.65 | 164   |
| - . 2 .0      | 3.95      |  1.36 |  0.25 | 1.0     |  3.67 |  4.52 | 15.98 |  29.5 |
| .3 - . 1      | 4.35      |  1.42 |  0.96 | - 2 . 5 |  3.4  |  4.12 | 15.3  | 131   |
| .1 .4         | 4.15      |  1.17 |  1.06 | - 4 . 5 |  3.89 |  5    | 16.79 | 168.2 |
| .4 .5         | 4.16      |  1.61 |  0.91 | - 2 . 1 |  3.93 |  4.35 | 15.7  | 118   |
| - . 6 - . 3   | 3.85      |  1.32 |  0.3  | .7      |  3.61 |  4.29 | 15.71 |  48   |

and

<!-- formula-not-decoded -->

## The data are in Table 7.1.

We test independence of the following four subsets of variables:

<!-- formula-not-decoded -->

The sample covariance matrix is

| . 16         | . 10   | . 01   | . 006   | . 02   | - . 04    | . 02    | . 01   | - . 02   | 1 . 44    |
|--------------|--------|--------|---------|--------|-----------|---------|--------|----------|-----------|
|  . 10       | . 19   | - . 01 | . 009   | . 02   | - . 16    | . 03    | . 05   | . 04     | 1 . 03    |
|   . 01     | - . 01 | . 03   | . 004   | . 03   | - . 11    | - . 03  | - . 03 | - . 01   | 4 . 45    |
|  . 006      | . 009  | . 004  | . 024   | . 020  | - . 012   | - . 009 | . 0004 | . 038    | 2 . 23    |
|   . 02     | . 02   | . 03   | . 020   | . 07   | - . 18    | - . 03  | - . 03 | . 05     | 9 . 03    |
|    - . 04 | - . 16 | - . 11 | - . 012 | - . 18 | 5 . 02    | - . 35  | - . 67 | - . 12   | - 23 . 11 |
|  . 02       | . 03   | - . 03 | - . 009 | - . 03 | - . 35    | . 13    | . 15   | . 05     | - 4 . 26  |
|   . 01     | . 05   | - . 03 | . 0004  | - . 03 | - . 67    | . 15    | . 26   | . 13     | - 3 . 47  |
| - . 02       | . 04   | - . 01 | . 038   | . 05   | - . 12    | . 05    | . 13   | . 35     | 6 . 73    |
|    1 . 44 | 1 . 03 | 4 . 45 | 2 . 23  | 9 . 03 | . 23 . 11 | 4 . 26  | 3 . 47 | 6 . 73   | 1541      |

<!-- formula-not-decoded -->

where S 11 is 2 × 2, S 22 is 3 × 3, S 33 is 3 × 3, and S 44 is 2 × 2. We first obtain

<!-- formula-not-decoded -->

For the χ 2 -approximation, we calculate

<!-- formula-not-decoded -->

Then,

<!-- formula-not-decoded -->

which exceeds χ 2 . 001 , 37 = 69 . 35, and we reject the hypothesis of independence of the four subsets.

,

## 7.4.3 Test for Independence of All Variables

If all pi = 1 in the hypothesis (7.32) in Section 7.4.2, we have the special case in which all the variables are mutually independent, H 0 : σ j k = 0 for j /negationslash= k , or

<!-- formula-not-decoded -->

There is no restriction on the σ j j 's. With σ j k = 0 for all j /negationslash= k , the corresponding ρ j k 's are also 0, and an equivalent form of the hypothesis is H 0 : P ρ = I , where P ρ is thepopulation correlation matrix defined in (3.37).

Since all pi = 1, the statistics (7.33) and (7.34) reduce to

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

which has an approximate χ 2 f -distribution, where ν is the degrees of freedom of S or R (see a comment at the beginning of Section 7.2) and f = 1 2 p ( p -1 ) is the degrees of freedom of χ 2 . We reject H 0 if u ′ &gt; χ 2 α, f . Exact percentage points of u ′ for selected values of n and p are given in Table A.15 (Mathai and Katiyar 1979). Percentage points for the limiting χ 2 -distribution are also given for comparison.

Note that | R | in (7.37) varies between 0 and 1. If the variables were uncorrelated (in the sample), we would have R = I and | R | = 1. On the other hand, if two or more variables were linearly related, R would not be full rank and we would have | R | = 0. If the variables were highly correlated, | R | would be close to 0; if the correlations were all small, | R | would be close to 1. This can be illustrated for p = 2:

<!-- formula-not-decoded -->

∣ ∣ Example 7.4.3. To test the hypothesis H 0 : σ j k = 0, j /negationslash= k , for the probe word data from Table 3.5, we calculate

<!-- formula-not-decoded -->

and (7.35) becomes

Then by (7.37) and (7.38),

<!-- formula-not-decoded -->

′

- = u =

The exact .01 critical value for u from Table A.15 is 23.75, and we therefore reject H 0. The approximate χ 2 critical value for u ′ is χ 2 . 01 , 10 = 23 . 21, with which we also reject H 0.

## PROBLEMS

- 7.1 Show that if S 𝚺 0 in (7.1), then 0.
- 7.2 Verify (7.3); that is, show that ln | 𝚺 0 | -ln | S | = -ln | S 𝚺 -1 0 | .
- 7.5 Show that u = 1 and u ′ = 0 if all the λ i 's are equal, as noted in Section 7.2.2, where u is given by (7.8) and u ′ by (7.9).
- 7.3 Verify (7.4); that is, show that -ln ( ∏ p i = 1 λ i ) + ∑ p i = 1 λ i = ∑ p i = 1 (λ i -ln λ i ) . 7.4 Show that the likelihood ratio for H 0 : 𝚺 = σ 2 I is given by (7.5), LR = [| S | /( tr S / p ) p ] n / 2 .
- 7.6 Show that the covariance matrix in (7.10) can be written in the form σ 2 [ ( 1 -ρ) I ρ J , as given in (7.11).
- 7.7
+ ]
- Obtain (7.15) from (7.14) as follows:
- (a) Show that the p × p matrix J has a single nonzero eigenvalue equal to p and corresponding eigenvector proportional to j .
- (b) Show that S 0 = s 2 [ ( 1 -r ) I + r J ] in (7.13) can be written in the form S 0 = s 2 ( 1 -r )( I + r 1 -r J ) .
- (c) Use Section 2.11.2 and (2.108) to obtain (7.15).
- 7.8 Show that M in (7.19) can be expressed in the form given in (7.21).
- 7.9 (a) Calculate M as given in (7.21) for

<!-- formula-not-decoded -->

Assume ν 1 = ν 2 = 5.

- (b) Calculate M for

<!-- formula-not-decoded -->

Assume ν 1 = ν 2 = 5.

- In (b), S 1 and S 2 differ more than in (a) and M is accordingly much smaller. This illustrates the comments following (7.21).

- 7.10 Obtain (7.31), /Lambda1 = s i = 1 ( 1 -r 2 i ) , by using (2.94) to write | S | in the form S S xx S yy S yx S 1 S xy .
- 7.11 Show that the forms of u in (7.33) and (7.34) reduce to (7.37) when all pi = 1.
- ∏ | | = | | | --xx |
- 7.12 Show that when all pi = 1, c in (7.36) reduces to 1 -( 2 p + 5 )/ 6 ν , so that (7.35) becomes (7.38).
- 7.13 Give a justification for the degrees of freedom f = 1 2 p ( p -1 ) for the approximate χ 2 test statistic u ′ in (7.38).
- 7.14 In Example 5.2.2, we assumed that for the height and weight data of Table 3.1, the population covariance matrix is

<!-- formula-not-decoded -->

Test this as a hypothesis using (7.2).

- 7.15 Test H 0 : 𝚺 = σ 2 I and H 0 : C 𝚺 C ′ = σ 2 I for the calculator speed data of
- Table 6.12.
- 7.16 Test H 0 : 𝚺 = σ 2 I and H 0 : C 𝚺 C ′ = σ 2 I for the ramus bone data of Table 3.6.
- 7.18 Test H 0 : 𝚺 = σ 2 [ ( 1 -ρ) I + ρ J ] for the probe word data in Table 3.5. Use both χ 2 - and F -approximations.
- 7.17 Test H 0 : 𝚺 = σ 2 I and H 0 : C 𝚺 C ′ = σ 2 I for the cork data of Table 6.21.
- 7.19 Test H 0 : 𝚺 = σ 2 [ ( 1 -ρ) I + ρ J ] for the calculator speed data in Table 6.12. Use both χ 2 - and F -approximations.
- 7.20 Test H 0 : 𝚺 = σ 2 [ ( 1 -ρ) I + ρ J ] for the ramus bone data in Table 3.6. Use both χ 2 - and F -approximations.
- 7.21 Test H 0 : 𝚺 1 = 𝚺 2 for the beetles data of Table 5.5. Use an exact critical value from Table A.14 as well as χ 2 - and F -approximations.
- 7.22 Test H 0 : 𝚺 1 = 𝚺 2 for the engineer data of Table 5.6. Use an exact critical value from Table A.14 as well as χ 2 - and F -approximations.
- 7.23 Test H 0 : 𝚺 1 = 𝚺 2 for the dystrophy data of Table 5.7. Use an exact critical value from Table A.14 as well as χ 2 - and F -approximations.
- 7.24 Test H 0 : 𝚺 1 = 𝚺 2 for the cyclical data of Table 5.8. Use an exact critical value from Table A.14 as well as χ 2 - and F -approximations.
- 7.25 Test H 0 : 𝚺 1 = 𝚺 2 = 𝚺 3 for the fish data of Table 6.17. Use an exact critical value from Table A.14 as well as χ 2 - and F -approximations.
- 7.26 Test H 0 : 𝚺 1 = 𝚺 2 = ·· · = 𝚺 6 for the rootstock data in Table 6.2. Use an exact critical value from Table A.14 as well as χ 2 - and F -approximations.
- 7.27 Test H 0 : 𝚺 11 = 𝚺 12 = · · · = 𝚺 43 for the snap bean data in Table 6.18. Use both χ 2 - and F -approximations.

- 7.28 Test independence of ( y 1 , y 2 ) and ( x 1 , x 2 ) for the sons data in Table 3.7.
- 7.29 Test independence of ( y 1 , y 2 , y 3 ) and ( x 1 , x 2 , x 3 ) for the glucose data in Table 3.8.
- 7.30 Test independence of ( y 1 , y 2 ) and ( x 1 , x 2 , . . . , x 8 ) for the Seishu data of Table 7.1.
- 7.31 The data in Table 7.2 relate temperature, humidity, and evaporation (courtesy of R. J. Freund). The variables are

y 1 = maximum daily air temperature, y 3 = integrated area under daily air temperature curve, that is, a measure of average air temperature,

y 2 = minimum daily air temperature,

y

4

maximum daily soil temperature, y 5 = minimum daily soil temperature,

=

y 6 = integrated area under soil temperature curve, y 8 = minimum daily relative humidity,

y 7 = maximum daily relative humidity, y 9 = integrated area under daily humidity curve,

y 11 = evaporation.

y 10 = total wind, measured in miles per day,

Test independence of the following five groups of variables: ( y 1 , y 2 , y 3 ) , ( y 4 , y 5 , y 6 ) , ( y 7 , y 8 , y 9 ) , y 10, and y 11.

- 7.32 Test the independence of all the variables for the calcium data of Table 3.3.
- 7.33 Test the independence of all the variables for the calculator speed data of Table 6.12.
- 7.34 Test the independence of all the variables for the ramus bone data of Table 3.6.
- 7.35 Test the independence of all the variables for the cork data of Table 6.21.

Table 7.2. Temperature, Humidity, and Evaporation

| y 1   | y 2   | y 3     | y 4   | y 5   | y 6     | y 7   | y 8   | y 9     | y 10    | y 11   |
|-------|-------|---------|-------|-------|---------|-------|-------|---------|---------|--------|
| 84    | 65    | 147     | 85    | 59    | 151     | 95    | 40    | 398     | 273     | 30     |
| 84    | 65    | 149     | 86    | 61    | 159     | 94    | 28    | 345     | 140     | 34     |
| 79    | 66    | 142     | 83    | 64    | 152     | 94    | 41    | 368     | 318     | 33     |
| 81    | 67    | 147     | 83    | 65    | 158     | 94    | 50    | 406     | 282     | 26     |
| 84    | 68    | 167     | 88    | 69    | 180     | 93    | 46    | 379     | 311     | 41     |
| 74    | 66    | 131     | 77    | 67    | 147     | 96    | 73    | 478     | 446     | 4      |
| 73    | 66    | 131     | 78    | 69    | 159     | 96    | 72    | 462     | 294     | 5      |
| 75    | 67    | 134     | 84    | 68    | 159     | 95    | 70    | 464     | 313     | 20     |
| 84    | 68    | 161     | 89    | 71    | 195     | 95    | 63    | 430     | 455     | 31     |
| 86    | 72    | 169     | 91    | 76    | 206     | 93    | 56    | 406     | 604     | 36     |
| 88    | 73    | 176     | 91    | 76    | 206     | 94    | 55    | 393     | 610     | 43     |
| 90    | 74    | 187     | 94    | 76    | 211     | 94    | 51    | 385     | 520     | 47     |
| 88    | 72    | 171     | 94    | 75    | 211     | 96    | 54    | 405     | 663     | 45     |
| 58    | 72    | 171     | 92    | 70    | 201     | 95    | 51    | 392     | 467     | 45     |
| 81    | 69    | 154     | 87    | 68    | 167     | 95    | 61    | 448     | 184     | 11     |
| 79    | 68    | 149     | 83    | 68    | 162     | 95    | 59    | 436     | 177     | 10     |
| 84    | 69    | 160     | 87    | 66    | 173     | 95    | 42    | 392     | 173     | 30     |
| 84    | 70    | 160     | 87    | 68    | 177     | 94    | 44    | 392     | 76      | 29     |
| 84    | 70    | 168     | 88    | 70    | 169     | 95    | 48    | 396     | 72      | 23     |
| 77    | 67    | 147     | 83    | 66    | 170     | 97    | 60    | 431     | 183     | 16     |
| 87    | 67    | 166     | 92    | 67    | 196     | 96    | 44    | 379     | 76      | 37     |
| 89    | 69    | 171     | 92    | 72    | 199     | 94    | 48    | 393     | 230     | 50     |
| 89    | 72    | 180     | 94    | 72    | 204     | 95    | 48    | 394     | 193     | 36     |
| 93    | 72    | 186     | 92    | 73    | 201     | 94    | 47    | 386     | 400     | 54     |
| 93    | 74    | 188     | 93    | 72    | 206     | 95    | 47    | 389     | 339     | 44     |
| 94    | 75    | 199     | 94    | 72    | 208     | 96    | 45    | 370     | 172     | 41     |
| 93    | 74    | 193     | 95    | 73    | 214     | 95    | 50    | 396     | 238     | 45     |
| 93    | 74    | 196     | 95    | 70    | 210     | 96    | 45    | 380     | 118     | 42     |
| 96    | 75    | 198     | 95    | 71    | 207     | 93    | 40    | 365     | 93      | 50     |
| 95    | 76    | 202     | 95    | 69    | 202     | 93    | 39    | 357     | 269     | 48     |
| 84    | 73    | 173     | 96    | 69    | 173     | 94    | 58    | 418     | 128     | 17     |
| 91    | 71    | 170     | 91    | 69    | 168     | 94    | 44    | 420     | 423     | 20     |
| 88    | 72    | 179     | 89    | 70    | 189     | 93    | 50    | 399     | 415     | 15     |
| 89    | 72    | 179     | 95    | 71    | 210     | 98    | 46    | 389     | 300     | 42     |
| 91    | 72    | 182     | 96    | 73    | 208     | 95    | 43    | 384     | 193     | 44     |
| 92    | 74    | 196     | 97    | 75    | 215     | 96    | 46    | 389     | 195     | 41     |
| 94    | 75    | 192     | 96    | 69    | 198     | 95    | 36    | 380     | 215     | 49     |
| 96    | 75    | 195     | 95    | 67    | 196     | 97    | 24    | 354     | 185     | 53     |
| 93    | 76    | 198     | 94    | 75    | 211     | 93    | 43    | 364     | 466     | 53     |
| 88    | 74    | 188     | 92    | 73    | 198     | 95    | 52    | 405     | 399     | 21     |
| 88    | 74    | 178     | 90    | 74    | 197     | 95    | 61    | 447     | 232     | 1      |
| 91    | 72    | 175     | 94    | 70    | 205     | 94    | 42    | 380     | 275     | 44     |
| 92    | 72    | 190     | 95    | 71    | 209     | 96    | 44    | 379     | 166     | 44     |
| 92    | 73    | 189     | 96    | 72    | 208     | 93    | 42    | 372     | 189 164 | 46 47  |
| 94 96 | 75 76 | 194 202 | 95 96 | 71 71 | 208 208 | 93 94 | 43 40 | 373 368 | 139     | 50     |

## C H A P T E R 8

## Discriminant Analysis: Description of Group Separation

## 8.1 INTRODUCTION

We use the term group to represent either a population or a sample from the population. There are two major objectives in separation of groups:

1. Description of group separation, in which linear functions of the variables (discriminant functions) are used to describe or elucidate the differences between two or more groups. The goals of descriptive discriminant analysis include identifying the relative contribution of the p variables to separation of the groups and finding the optimal plane on which the points can be projected to best illustrate the configuration of the groups.
2. Prediction or allocation of observations to groups, in which linear or quadratic functions of the variables (classification functions) are employed to assign an individual sampling unit to one of the groups. The measured values in the observation vector for an individual or object are evaluated by the classification functions to find the group to which the individual most likely belongs.

For consistency we will use the term discriminant analysis only in connection with objective 1. We will refer to all aspects of objective 2 as classification analysis , which is the subject of Chapter 9. Unfortunately, there is no general agreement with regard to usage of the terms discriminant analysis and discriminant functions. Many writers, perhaps the majority, use the term discriminant analysis in connection with the second objective, prediction or allocation. The linear functions contributing to the first objective, description of group separation, are often referred to as canonical variates or discriminant coordinates. To avoid confusion, we prefer to reserve the term canonical for canonical correlation analysis in Chapter 11.

Discriminant functions are linear combinations of variables that best separate groups. They were introduced in Section 5.5 for two groups and in Sections 6.1.4 and 6.4 for several groups. In those sections, interest was centered on follow-up to Hotelling's T 2 -tests and MANOVA tests. In this chapter, we further develop these useful multivariate tools.

## 8.2 THE DISCRIMINANT FUNCTION FOR TWO GROUPS

Weassume that the two populations to be compared have the same covariance matrix 𝚺 but distinct mean vectors 𝛍 1 and 𝛍 2. We work with samples y 11, y 12 , . . . , y 1 n 1 and y 21, y 22 , . . . , y 2 n 2 from the two populations. As usual, each vector y i j consists of measurements on p variables. The discriminant function is the linear combination of these p variables that maximizes the distance between the two (transformed) group mean vectors. A linear combination z = a ′ y transforms each observation vector to a scalar:

<!-- formula-not-decoded -->

Hence the n 1 + n 2 observation vectors in the two samples, are transformed to scalars,

| y 11    | y 21      |
|---------|-----------|
| y 12    | y 22      |
| . .     | . .       |
| .       | .         |
| y 1 n 1 | y 2 n 2 , |
| z 11    | z 21      |
| z 12    | z 22      |
| . .     | . .       |
| .       | .         |
| z 1 n 1 | z 2 n 2 . |

We find the means z 1 = ∑ n 1 i = 1 z 1 i / n 1 = a ′ y 1 and z 2 = a ′ y 2 by (3.54), where y 1 = ∑ n 1 i = 1 y 1 i / n 1 and y 2 = ∑ n 2 i = 1 y 2 i / n 2. We then wish to find the vector a that maximizes the standardized difference ( z 1 -z 2 )/ sz . Since ( z 1 -z 2 )/ sz can be negative, we use the squared distance ( z 1 -z 2 ) 2 / s 2 z , which, by (3.54) and (3.55), can be expressed as

<!-- formula-not-decoded -->

The maximum of (8.1) occurs when

<!-- formula-not-decoded -->

or when a is any multiple of S -1 pl ( y 1 -y 2 ) . Thus the maximizing vector a is not unique. However, its 'direction' is unique; that is, the relative values or ratios of a 1, a 2 , . . . , ap are unique, and z = a ′ y projects points y onto the line on which

<!-- formula-not-decoded -->

( z 1 -z 2 ) 2 / s 2 z is maximized. Note that in order for S -1 pl to exist, we must have n 1 + n 2 -2 &gt; p .

The optimum direction given by a = S -1 pl ( y 1 -y 2 ) is effectively parallel to the line joining y 1 and y 2 , because the squared distance ( z 1 -z 2 ) 2 / s 2 z is equivalent to the standardized distance between y 1 and y 2 . This can be seen by substituting (8.2) into (8.1) to obtain

<!-- formula-not-decoded -->

for z = a ′ y with a = S -1 pl ( y 1 -y 2 ) . Since a ′ = ( y 1 -y 2 ) ′ S -1 pl , we can write (8.3) as ( z 1 -z 2 ) 2 / s 2 z = a ′ ( y 1 -y 2 ) , and any other direction than that represented by a = S -1 pl ( y 1 -y 2 ) would yield a smaller difference between a ′ y 1 and a ′ y 2 (see Section 5.5).

Figure 8.1 illustrates the separation of two bivariate normal ( p = 2 ) groups along the single dimension represented by the discriminant function z = a ′ y , where a is given by (8.2). In this illustration the population covariance matrices are equal. The linear combinations z 1 i = a ′ y 1 i = a 1 y 1 i 1 + a 2 y 1 i 2 and z 2 i = a ′ y 2 i = a 1 y 2 i 1 + a 2 y 2 i 2 project the points y 1 i and y 2 i onto the line of optimum separation of the two groups. Since the two variables y 1 and y 2 are bivariate normal, a linear combination z = a 1 y 1 + a 2 y 2 = a ′ y is univariate normal (see property 1a in Section 4.2). We have therefore indicated this by two univariate normal densities along the line representing z .

The point where the line joining the points of intersection of the two ellipses intersects the discriminant function line z is the point of maximum separation (minimum

1

Figure 8.1. Two-group discriminant analysis.

<!-- image -->

Figure 8.2. Separation achieved by the discriminant function.

<!-- image -->

overlap) of points projected onto the line. If the two populations are multivariate normal with common covariance matrix 𝚺 , as illustrated in Figure 8.1, it can be shown that all possible group separation is expressed in a single new dimension.

In Figure 8.2, we illustrate the optimum separation achieved by the discriminant function. Projection in another direction denoted by z ′ gives a smaller standardized distance between the transformed means z ′ 1 and z ′ 2 and also a larger overlap between the projected points.

Example 8.2. Samples of steel produced at two different rolling temperatures are compared in Table 8.1 (Kramer and Jensen 1969a). The variables are y 1 = yield

Table 8.1. Yield Point and Ultimate Strength of Steel Produced at Two Rolling Temperatures

| Temperature 1   | Temperature 1   | Temperature 2   | Temperature 2   |
|-----------------|-----------------|-----------------|-----------------|
| y 1             | y 2             | y 1             | y 2             |
| 33              | 60              | 35              | 57              |
| 36              | 61              | 36              | 59              |
| 35              | 64              | 38              | 59              |
| 38              | 63              | 39              | 61              |
| 40              | 65              | 41              | 63              |
|                 |                 | 43              | 65              |
|                 |                 | 41              | 59              |

Figure 8.3. Ultimate strength and yield point for steel rolled at two temperatures.

<!-- image -->

point and y 2 = ultimate strength. From the data, we calculate

<!-- formula-not-decoded -->

A plot of the data appears in Figure 8.3. We see that if the points were projected on either the y 1 or the y 2 axis, there would be considerable overlap. In fact, when the two groups are compared by means of a t -statistic for each variable separately, both t 's are nonsignificant:

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

However, it is clear in Figure 8.3 that the two groups can be separated. If they are projected in an appropriate direction, as in Figure 8.1, there will be no overlap. The single dimension onto which the points would be projected is the discriminant function

<!-- formula-not-decoded -->

Table 8.2. Discriminant Function z = -1 . 633 y 1 + 1 . 820 y 2 Evaluated for Data in Table 8.1

| Temperature 1   |   Temperature 2 |
|-----------------|-----------------|
| 55.29           |           46.56 |
| 52.20           |           48.57 |
| 59.30           |           45.3  |
| 52.58           |           47.3  |
| 52.95           |           47.68 |
|                 |           48.05 |
|                 |           40.4  |

where a is obtained as

<!-- formula-not-decoded -->

The values of the projected points are found by calculating z for each observation vector y in the two groups. The results are given in Table 8.2, where the separation provided by the discriminant function is clearly evident.

## 8.3 RELATIONSHIP BETWEEN TWO-GROUP DISCRIMINANT ANALYSIS AND MULTIPLE REGRESSION

The mutual connection between multiple regression and two-group discriminant analysis was introduced as a computational device in Section 5.6.2. The roles of independent and dependent variables are reversed in the two models. The dependent variables ( y 's) of discriminant analysis become the independent variables in regression.

Let w be a grouping variable (identifying groups 1 and 2) such that w = 0 and define b = ( b 1 , b 2 , . . . , bp ) ′ as the vector of regression coefficients when w is fit to the y 's. Then by (5.21), b is proportional to the discriminant function coefficient vector a = S -1 pl ( y 1 -y 2 ) :

<!-- formula-not-decoded -->

where T 2 = [ n 1 n 2 /( n 1 + n 2 ) ] ( y 1 -y 2 ) ′ S -1 pl ( y 1 -y 2 ) as in (5.9). From (5.20) the squared multiple correlation R 2 is related to T 2 by

<!-- formula-not-decoded -->

The test statistic (5.29) for the hypothesis that q of the p + q variables are redundant for separating the groups can also be obtained in terms of regression by (5.31) as

<!-- formula-not-decoded -->

where R 2 p + q and R 2 p are from regressions with p + q and p variables, respectively.

The link between two-group discriminant analysis and multiple regression was first noted by Fisher (1936). Flury and Riedwyl (1985) give further insights into the relationship.

Example 8.3. In Example 5.6.2, the psychological data of Table 5.1 were used in an illustration of the regression approach to computation of a and T 2 . We use the same data to obtain b and R 2 from a and T 2 .

From the results of Examples 5.4.2 and 5.5, we have

<!-- formula-not-decoded -->

To find b from a and T 2 , we use (8.4):

<!-- formula-not-decoded -->

To find R 2 , we use (8.5):

<!-- formula-not-decoded -->

We can also use the relationship with T 2 in (8.5):

<!-- formula-not-decoded -->

## 8.4 DISCRIMINANT ANALYSIS FOR SEVERAL GROUPS

## 8.4.1 Discriminant Functions

In discriminant analysis for several groups, we are concerned with finding linear combinations of variables that best separate the k groups of multivariate observations. Discriminant analysis for several groups may serve any one of various purposes:

1. Examine group separation in a two-dimensional plot. When there are more than two groups, it requires more than one discriminant function to describe group separation. If the points in the p -dimensional space are projected onto a two-dimensional space represented by the first two discriminant functions, we obtain the best possible view of how the groups are separated.
2. Find a subset of the original variables that separates the groups almost as well as the original set. This topic was introduced in Section 6.11.2.
3. Rank the variables in terms of their relative contribution to group separation. This use for discriminant functions has been mentioned in Sections 5.5, 6.1.4, 6.1.8, and 6.4. In Section 8.5, we discuss standardized discriminant function coefficients that provide a more valid comparison of the variables.
4. Interpret the new dimensions represented by the discriminant functions.
5. Follow up to fixed-effects MANOVA.

Purposes 3 and 4 are closely related. Any of the first four can be used to accomplish purpose 5. Methods of achieving these five goals of discriminant analysis are discussed in subsequent sections. In the present section we review discriminant functions for the several-group case and discuss attendant assumptions. For alternative estimators of discriminant functions that may be useful in the presence of multicollinearity or outliers, see Rencher (1998, Section 5.11).

For k groups (samples) with ni observations in the i th group, we transform each observation vector y i j to obtain zi j = a ′ y i j , i = 1, 2 , . . . , k ; j = 1, 2 , . . . , ni , and find the means zi = a ′ y i , where y i = ∑ ni j = 1 y i j / ni . As in the two-group case, we seek the vector a that maximally separates z 1, z 2 , . . . , zk . To express separation among z 1, z 2 , . . . , zk , we extend the separation criterion (8.1) to the k -group case. Since a ′ ( y 1 -y 2 ) = ( y 1 -y 2 ) ′ a , we can express (8.1) in the form

<!-- formula-not-decoded -->

To extend (8.7) to k groups, we use the H matrix from MANOVA in place of ( y 1 -y 2 )( y 1 -y 2 ) ′ [see (6.38)] and E in place of S pl to obtain

<!-- formula-not-decoded -->

which can also be expressed as which can be written in the form

<!-- formula-not-decoded -->

The solutions of (8.12) are the eigenvalues λ 1, λ 2 , . . . , λ s and associated eigenvectors a 1, a 2 , . . . , a s of E -1 H . As in previous discussions of eigenvalues, we consider them to be ranked λ 1 &gt; λ 2 &gt; · · · &gt; λ s . The number of (nonzero) eigenvalues s is the rank of H , which can be found as the smaller of k -1 or p . Thus the largest eigenvalue λ 1 is the maximum value of λ = a ′ Ha / a ′ Ea in (8.8), and the coefficient vector that produces the maximum is the corresponding eigenvector a 1. (This can be verified using calculus.) Hence the discriminant function that maximally separates the means is z 1 = a ′ 1 y ; that is, z 1 represents the dimension or direction that maximally separates the means.

From the s eigenvectors a 1 , a 2 , . . . , a s of E -1 H corresponding to λ 1, λ 2 , . . . , λ s , we obtain s discriminant functions z 1 = a ′ 1 y , z 2 = a ′ 2 y , . . . , zs = a ′ s y , which show the dimensions or directions of differences among y 1 , y 2 , . . . , y k . These discriminant functions are uncorrelated, but they are not orthogonal ( a ′ i a j = 0 for i /negationslash= j ) because E -1 H is not symmetric [see Rencher (1998, pp. 203-204)]. Note that the numbering z 1, z 2 , . . . , zs corresponds to the eigenvalues, not to the k groups as was done earlier in this section.

The relative importance of each discriminant function zi can be assessed by considering its eigenvalue as a proportion of the total:

<!-- formula-not-decoded -->

By this criterion, two or three discriminant functions will often suffice to describe the group differences. The discriminant functions associated with small eigenvalues can

<!-- formula-not-decoded -->

where SSH ( z ) and SSE ( z ) are the between and within sums of squares for z [see (6.42)].

We can write (8.8) in the form

<!-- formula-not-decoded -->

We examine values of λ and a that are solutions of (8.10) in a search for the value of a that results in maximum λ . The solution a ′ = 0 ′ is not permissible because it gives λ = 0 / 0 in (8.8). Other solutions are found from

<!-- formula-not-decoded -->

be neglected. A test of significance for each discriminant function is also available (see Section 8.6).

The matrix E -1 H is not symmetric. Many algorithms for computation of eigenvalues and eigenvectors accept only symmetric matrices. In Section 6.1.4, it was shown that the eigenvalues of the symmetric matrix ( U -1 ) ′ HU -1 are the same as those of E -1 H , where E = U ′ U is the Cholesky factorization of E . However, an adjustment is needed for the eigenvectors. If b is an eigenvector of ( U -1 ) ′ HU -1 , then a = U -1 b is an eigenvector of E -1 H .

The preceding discussion was presented in terms of unequal sample sizes n 1 , n 2 , . . . , nk . In applications, this situation is common and can be handled with no difficulty. Ideally, the smallest ni should exceed the number of variables, p . This is not required mathematically but will lead to more stable discriminant functions.

Example 8.4.1. The data in Table 8.3 were collected by G. R. Bryce and R. M. Barker (Brigham Young University) as part of a preliminary study of a possible link between football helmet design and neck injuries.

Six head measurements were made on each subject. There were 30 subjects in each of three groups: high school football players (group 1), college football players (group 2), and nonfootball players (group 3). The six variables are

WDIM = head width at widest dimension,

FBEYE = front-to-back measurement at eye level,

CIRCUM = head circumference,

EYEHD = eye-to-top-of-head measurement,

JAW = jaw width.

EARHD = ear-to-top-of-head measurement,

The eigenvalues of E -1 H are λ 1 = 1 . 9178 and λ 2 = . 1159. The corresponding eigenvectors are

<!-- formula-not-decoded -->

The first eigenvalue accounts for a substantial proportion of the total:

<!-- formula-not-decoded -->

Thus the mean vectors lie largely in one dimension, and one discriminant function suffices to describe most of the separation among the three groups.

Table 8.3. Head Measurements for Three Groups

|   Group |   WDIM |   CIRCUM |   FBEYE |   EYEHD |   EARHD |   JAW |
|---------|--------|----------|---------|---------|---------|-------|
|       1 |   13.5 |     57.2 |    19.5 |    12.5 |    14   |  11   |
|       1 |   15.5 |     58.4 |    21   |    12   |    16   |  12   |
|       1 |   14.5 |     55.9 |    19   |    10   |    13   |  12   |
|       1 |   15.5 |     58.4 |    20   |    13.5 |    15   |  12   |
|       1 |   14.5 |     58.4 |    20   |    13   |    15.5 |  12   |
|       1 |   14   |     61   |    21   |    12   |    14   |  13   |
|       1 |   15   |     58.4 |    19.5 |    13.5 |    15.5 |  13   |
|       1 |   15   |     58.4 |    21   |    13   |    14   |  13   |
|       1 |   15.5 |     59.7 |    20.5 |    13.5 |    14.5 |  12.5 |
|       1 |   15.5 |     59.7 |    20.5 |    13   |    15   |  13   |
|       1 |   15   |     57.2 |    19   |    14   |    14.5 |  11.5 |
|       1 |   15.5 |     59.7 |    21   |    13   |    16   |  12.5 |
|       1 |   16   |     57.2 |    19   |    14   |    14.5 |  12   |
|       1 |   15.5 |     62.2 |    21.5 |    14   |    16   |  12   |
|       1 |   15.5 |     57.2 |    19.5 |    13.5 |    15   |  12   |
|       1 |   14   |     61   |    20   |    15   |    15   |  12   |
|       1 |   14.5 |     58.4 |    20   |    12   |    14.5 |  12   |
|       1 |   15   |     56.9 |    19   |    13   |    14   |  12.5 |
|       1 |   15.5 |     59.7 |    20   |    12.5 |    14   |  12.5 |
|       1 |   15   |     57.2 |    19.5 |    12   |    14   |  11   |
|       1 |   15   |     56.9 |    19   |    12   |    13   |  12   |
|       1 |   15.5 |     56.9 |    19.5 |    14.5 |    14.5 |  13   |
|       1 |   17.5 |     63.5 |    21.5 |    14   |    15.5 |  13.5 |
|       1 |   15.5 |     57.2 |    19   |    13   |    15.5 |  12.5 |
|       1 |   15.5 |     61   |    20.5 |    12   |    13   |  12.5 |
|       1 |   15.5 |     61   |    21   |    14.5 |    15.5 |  12.5 |
|       1 |   15.5 |     63.5 |    21.8 |    14.5 |    16.5 |  13.5 |
|       1 |   14.5 |     58.4 |    20.5 |    13   |    16   |  10.5 |
|       1 |   15.5 |     56.9 |    20   |    13.5 |    14   |  12   |
|       1 |   16   |     61   |    20   |    12.5 |    14.5 |  12.5 |
|       2 |   15.5 |     60   |    21.1 |    10.3 |    13.4 |  12.4 |
|       2 |   15.4 |     59.7 |    20   |    12.8 |    14.5 |  11.3 |
|       2 |   15.1 |     59.7 |    20.2 |    11.4 |    14.1 |  12.1 |
|       2 |   14.3 |     56.9 |    18.9 |    11   |    13.4 |  11   |
|       2 |   14.8 |     58   |    20.1 |     9.6 |    11.1 |  11.7 |
|       2 |   15.2 |     57.5 |    18.5 |     9.9 |    12.8 |  11.4 |
|       2 |   15.4 |     58   |    20.8 |    10.2 |    12.8 |  11.9 |
|       2 |   16.3 |     58   |    20.1 |     8.8 |    13   |  12.9 |
|       2 |   15.5 |     57   |    19.6 |    10.5 |    13.9 |  11.8 |
|       2 |   15   |     56.5 |    19.6 |    10.4 |    14.5 |  12   |
|       2 |   15.5 |     57.2 |    20   |    11.2 |    13.4 |  12.4 |
|       2 |   15.5 |     56.5 |    19.8 |     9.2 |    12.8 |  12.2 |
|       2 |   15.7 |     57.5 |    19.8 |    11.8 |    12.6 |  12.5 |
|       2 |   14.4 |     57   |    20.4 |    10.2 |    12.7 |  12.3 |
|       2 |   14.9 |     54.8 |    18.5 |    11.2 |    13.8 |  11.3 |

Table 8.3. (Continued)

|   Group |   WDIM |   CIRCUM |   FBEYE | EYEHD     |   EARHD |   JAW |
|---------|--------|----------|---------|-----------|---------|-------|
|       2 |   16.5 |     59.8 |    20.2 | 9.4       |    14.3 |  12.2 |
|       2 |   15.5 |     56.1 |    18.8 | 9.8       |    13.8 |  12.6 |
|       2 |   15.3 |     55   |    19   | 10.1      |    14.2 |  11.6 |
|       2 |   14.5 |     55.6 |    19.3 | 12.0      |    12.6 |  11.6 |
|       2 |   15.5 |     56.5 |    20   | 9.9       |    13.4 |  11.5 |
|       2 |   15.2 |     55   |    19.3 | 9.9       |    14.4 |  11.9 |
|       2 |   15.3 |     56.5 |    19.3 | 9.1       |    12.8 |  11.7 |
|       2 |   15.3 |     56.8 |    20.2 | 8.6       |    14.2 |  11.5 |
|       2 |   15.8 |     55.5 |    19.2 | 8.2       |    13   |  12.6 |
|       2 |   14.8 |     57   |    20.2 | 9.8       |    13.8 |  10.5 |
|       2 |   15.2 |     56.9 |    19.1 | 9.6       |    13   |  11.2 |
|       2 |   15.9 |     58.8 |    21   | 8.6       |    13.5 |  11.8 |
|       2 |   15.5 |     57.3 |    20.1 | 9.6       |    14.1 |  12.3 |
|       2 |   16.5 |     58   |    19.5 | 9.0       |    13.9 |  13.3 |
|       2 |   17.3 |     62.6 |    21.5 | 10.3      |    13.8 |  12.8 |
|       3 |   14.9 |     56.5 |    20.4 | 7.4       |    13   |  12   |
|       3 |   15.4 |     57.5 |    19.5 | 10.5      |    13.8 |  11.5 |
|       3 |   15.3 |     55.4 |    19.2 | 9.7       |    13.3 |  11.5 |
|       3 |   14.6 |     56   |    19.8 | 8.5       |    12   |  11.5 |
|       3 |   16.2 |     56.5 |    19.5 | 11.5      |    14.5 |  11.8 |
|       3 |   14.6 |     58   |    19.9 | 13.0      |    13.4 |  11.5 |
|       3 |   15.9 |     56.7 |    18.7 | 10.8      |    12.8 |  12.6 |
|       3 |   15.5 |     58.5 |    19.4 | 11.5      |    13.4 |  11.9 |
|       3 |   16.1 |     60   |    20.3 | 10.6      |    13.7 |  12.2 |
|       3 |   15.2 |     57.8 |    19.9 | 10.4      |    13.5 |  11.4 |
|       3 |   15.1 |     56   |    19.4 | 10.0      |    13.1 |  10.9 |
|       3 |   15.9 |     59.8 |    20.5 | 12.0      |    13.6 |  11.5 |
|       3 |   16.1 |     57.7 |    19.7 | 10.2      |    13.6 |  11.5 |
|       3 |   15.7 |     58.7 |    20.7 | 11.3      |    13.6 |  11.3 |
|       3 |   15.3 |     56.9 |    19.6 | 10.5      |    13.5 |  12.1 |
|       3 |   15.3 |     56.9 |    19.5 | 9.9       |    14   |  12.1 |
|       3 |   15.2 |     58   |    20.6 | 11.0      |    15.1 |  11.7 |
|       3 |   16.6 |     59.3 |    19.9 | 12.1 11.7 |    14.6 |  12.1 |
|       3 |   15.5 |     58.2 |    19.7 |           |    13.8 |  12.1 |
|       3 |   15.8 |     57.5 |    18.9 | 11.8      |    14.7 |  11.8 |
|       3 |   16   |     57.2 |    19.8 | 10.8      |    13.9 |  12   |
|       3 |   15.4 |     57   |    19.8 | 11.3      |    14   |  11.4 |
|       3 |   16   |     59.2 |    20.8 | 10.4      |    13.8 |  12.2 |
|       3 |   15.4 |     57.6 |    19.6 | 10.2      |    13.9 |  11.7 |
|       3 |   15.8 |     60.3 |    20.8 | 12.4      |    13.4 |  12.1 |
|       3 |   15.4 |     55   |    18.8 | 10.7      |    14.2 |  10.8 |
|       3 |   15.5 |     58.4 |    19.8 | 13.1      |    14.5 |  11.7 |
|       3 |   15.7 |     59   |    20.4 | 12.1      |    13   |  12.7 |
|       3 |   17.3 |     61.7 |    20.7 | 11.9      |    13.3 |  13.3 |

## 8.4.2 A Measure of Association for Discriminant Functions

Measures of association between the dependent variables y 1, y 2 , . . . , yp and the independent grouping variable i associated with 𝛍 i , i = 1 , 2 , . . . , k , were presented in Section 6.1.8. These measures attempt to answer the question, How well do the variables separate the groups? It was noted that Roy's statistic θ serves as an R 2 -like measure of association, since it is the ratio of between to total sum of squares for the first discriminant function, z 1 = a ′ 1 y :

<!-- formula-not-decoded -->

[see (6.42) and (8.9)]. Another interpretation of η 2 θ is the maximum squared correlation between the first discriminant function and the best linear combination of the k -1 (dummy) group membership variables [see a comment following (6.40) in Section 6.1.8]. Dummy variables were defined in the first two paragraphs of Section 6.1.8. The maximum correlation is called the (first) canonical correlation (see Chapter 11). The squared canonical correlation can be calculated for each discriminant function:

<!-- formula-not-decoded -->

The average squared canonical correlation was used as a measure of association in (6.49).

Example 8.4.2. For the football data of Table 8.3, we obtain the squared canonical correlation between each of the two discriminant functions and the grouping variables,

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

## 8.5 STANDARDIZED DISCRIMINANT FUNCTIONS

In Section 5.5, it was noted that in the two-group case the relative contribution of the y 's to separation of the two groups can best be assessed by comparing the coefficients ar , r = 1 , 2 , . . . , p , in the discriminant function

<!-- formula-not-decoded -->

Similar comments appeared in Section 6.1.4, 6.1.8, and 6.4 concerning the use of discriminant functions to assess contribution of the y 's to separation of several groups.

However, such comparisons are informative only if the y 's are commensurate, that is, measured on the same scale and with comparable variances. If the y 's are not commensurate, we need coefficients a ∗ r that are applicable to standardized variables.

Consider the case of two groups. For the i th observation vector y 1 i or y 2 i in group 1 or 2, we can express the discriminant function in terms of standardized variables as

<!-- formula-not-decoded -->

where y ′ 1 = ( y 11 , y 12 , . . . , y 1 p ) and y ′ 2 = ( y 21 , y 22 , . . . , y 2 p ) are the mean vectors for the two groups, and sr is the within-sample standard deviation of the r th variable, obtained as the square root of the r th diagonal element of S pl. Clearly, these standardized coefficients must be of the form

<!-- formula-not-decoded -->

In vector form, this becomes

<!-- formula-not-decoded -->

For the several-group case, we can standardize the discriminant functions in an analogous fashion. If we denote the r th coefficient in the m th discriminant function by amr , m = 1 , 2 , . . . , s ; r = 1 , 2 , . . . , p , then the standardized form is

<!-- formula-not-decoded -->

where sr is the within-group standard deviation obtained from the diagonal of S pl = E /ν E . Note that a ∗ mr has two subscripts because there are several discriminant functions, whereas a ∗ r in (8.16) has only one subscript because there is one discriminant function for two groups.

Alternatively, since the m th eigenvector is unique only up to multiplication by a scalar, we can simplify the standardization by using

<!-- formula-not-decoded -->

where err is the r th diagonal element of E . For further discussion of the use of standardized discriminant function coefficients to gauge the relative contribution of the variables to group separation, see Section 8.7.1 [see also Rencher and Scott (1990) and Rencher (1998, Section 5.4)].

Example 8.5. In Example 8.4.1, we obtained the discriminant function coefficient vectors a 1 and a 2 for the football data of Table 8.3. Since λ 1 /(λ 1 + λ 2 ) = . 94, we concentrate on a 1. To standardize a 1, we need the within-sample standard deviations of the variables. The pooled covariance matrix is given by

<!-- formula-not-decoded -->

Using the square roots of the diagonal elements of S pl, we obtain

<!-- formula-not-decoded -->

Thus the fourth, first, sixth, and fifth variables contribute most to separating the groups, in that order. The second and third variables are not useful (in the presence of the others) in distinguishing groups.

## 8.6 TESTS OF SIGNIFICANCE

In order to test hypotheses, we need the assumption of multivariate normality. This was not explicitly required for the development of discriminant functions.

## 8.6.1 Tests for the Two-Group Case

By (8.3) we see that the separation of transformed means, ( z 1 -z 2 ) 2 / s 2 z , achieved by the discriminant function z = a ′ y is equivalent to the standardized distance between the mean vectors y 1 and y 2 . This standardized distance is proportional to the twogroup T 2 in (5.9) in Section 5.4.2. Hence the discriminant function coefficient vector a is significantly different from 0 if T 2 is significant. More formally, if the population discriminant function coefficient vector is expressed as 𝛂 = 𝚺 -1 ( 𝛍 1 -𝛍 2 ) , then H 0 : 𝛂 = 0 is equivalent to H 0 : 𝛍 1 = 𝛍 2.

To test the significance of a subset of the discriminant function coefficients, we can use the test of the corresponding subset of y 's given in Section 5.9. To test the hypothesis that the population discriminant function has a specified form a ′ 0 y , see Rencher (1998, Section 5.5.1).

## 8.6.2 Tests for the Several-Group Case

In Section 8.4.1 we noted that the discriminant criterion λ = a ′ Ha / a ′ Ea is maximized by λ 1, the largest eigenvalue of E -1 H , and that the remaining eigenvalues λ 2 , . . . , λ s correspond to other discriminant dimensions. These eigenvalues are the same as those in the Wilks /Lambda1 -test in (6.14) for significant differences among mean vectors,

<!-- formula-not-decoded -->

which is distributed as /Lambda1 p , k -1 , N -k , where N = ∑ i ni for an unbalanced design or N = kn in the balanced case. Since /Lambda1 1 is small if one or more λ i 's are large, Wilks' /Lambda1 tests for significance of the eigenvalues and thereby for the discriminant functions. The s eigenvalues represent s dimensions of separation of the mean vectors y 1 , y 2 , . . . , y k . We are interested in which, if any, of these dimensions are significant. In the context of discriminant functions, Wilks' /Lambda1 is more useful than the other three MANOVA test statistics, because it can be used on a subset of eigenvalues, as we see shortly.

In addition to the exact test provided by the critical values for /Lambda1 found in Table A.9, we can use the χ 2 -approximation for /Lambda1 1 given in (6.16), with ν E = N -k = ∑ i ni -k and ν H = k -1:

<!-- formula-not-decoded -->

which is approximately χ 2 with p ( k -1 ) degrees of freedom. The test statistic /Lambda1 1 and its approximation (8.19) test the significance of all of λ 1, λ 2 , . . . , λ s . If the test leads to rejection of H 0, we conclude that at least one of the λ 's is significantly different from zero, and therefore there is at least one dimension of separation of mean vectors. Since λ 1 is the largest, we are only sure of its significance, along with that of z 1 = a ′ 1 y .

To test the significance of λ 2, λ 3 , . . . , λ s , we delete λ 1 from Wilks' /Lambda1 and the associated χ 2 -approximation to obtain

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

which is approximately χ 2 with ( p -1 )( k -2 ) degrees of freedom. If this test leads to rejection of H 0, we conclude that at least λ 2 is significant along with the associated discriminant function z 2 = a ′ 2 y . We can continue in this fashion, testing each λ i in turn until a test fails to reject H 0. (To compensate for making several tests, an adjustment to the α -level of each test could be made as in procedure 2, Section 5.5.) The test statistic at the m th step is

<!-- formula-not-decoded -->

which is distributed as /Lambda1 p -m + 1 , k -m , N -k -m + 1. The statistic

<!-- formula-not-decoded -->

has an approximate χ 2 -distribution with ( p -m + 1 )( k -m ) degrees of freedom. In some cases, more λ 's will be statistically significant than the researcher considers to be of practical importance. If λ i / ∑ j λ j is small, the associated discriminant function may not be of interest, even if it is significant.

We can also use an F -approximation for each /Lambda1 i . For

<!-- formula-not-decoded -->

we use (6.15), with ν E = N -k and ν H = k -1:

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

where

For

we use

<!-- formula-not-decoded -->

with p -m + 1 and k -m in place of p and k -1:

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Example 8.6.2. We test the significance of the two discriminant functions obtained in Example 8.4.1 for the football data. For the overall test we have, by (8.18),

<!-- formula-not-decoded -->

With p = 6, k = 3, and N -k = 87, the critical value from Table A.9 is /Lambda1 . 05 , 6 , 2 , 80 = . 762. By (8.19), the χ 2 -approximation is

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

which exceeds the critical value χ 2 . 01 , 12 = 26 . 217. Thus at least the first discriminant function is significant.

To test the second discriminant function, we have, by (8.20),

<!-- formula-not-decoded -->

With m = 2, the (conservative) critical value is /Lambda1 . 05 , 5 , 1 , 80 = . 867. Since this is close to /Lambda1 = . 896, we interpolate in Table A.9 to obtain /Lambda1 . 05 , 5 , 1 , 86 = . 875. By (8.21), the χ 2 -approximation is

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

For the F -approximation for /Lambda1 1, we obtain by (8.24)

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

The p -value for F = 10 . 994 is less than .0001. For the F -approximation for /Lambda1 2, we reduce p and k by 1 and obtain by (8.25)

<!-- formula-not-decoded -->

The p -value for F = 1 . 924 is .099. Thus only the first discriminant function significantly separates groups. The exact test using /Lambda1 2 appears to be somewhat closer to rejection than are the approximate tests.

## 8.7 INTERPRETATION OF DISCRIMINANT FUNCTIONS

There is a close correspondence between interpreting discriminant functions and determining the contribution of each variable, and we shall not always make a distinction. In interpretation, the signs of the coefficients are taken into account; in ascertaining the contribution, the signs are ignored, and the coefficients are ranked in absolute value. (We discuss this distinction further in Section 8.7.1.) We are more commonly interested in assessing the contribution of the variables than in interpreting the function.

In the next three sections, we cover three common approaches to assessing the contribution of each variable (in the presence of the other variables) to separating the

groups. The three methods are (1) examine the standardized discriminant function coefficients, (2) calculate a partial F -test for each variable, and (3) calculate a correlation between each variable and the discriminant function. The third method is the most widely recommended, but we note in Section 8.7.3 that it is the least useful.

## 8.7.1 Standardized Coefficients

To offset differing scales among the variables, the discriminant function coefficients can be standardized using (8.16) or (8.17), in which the coefficients have been adjusted so that they apply to standardized variables. For the observations in the first of two groups, for example, we have by (8.15),

<!-- formula-not-decoded -->

The standardized variables ( y 1 ir -y 1 r )/ sr are scale free, and the standardized coefficients a ∗ r = sr ar , r = 1 , 2 , . . . , p , therefore correctly reflect the joint contribution of the variables to the discriminant function z as it maximally separates the groups. For the case of several groups, each discriminant function coefficient vector a = ( a 1 , a 2 , . . . , ap ) ′ is an eigenvector of E -1 H , and as such, it takes into account the sample correlations among the variables as well as the influence of each variable in the presence of the others.

As noted in Section 8.5, this standardization is carried out for each of the s discriminant functions. Typically, each will have a different interpretation; that is, the pattern of the coefficients a ∗ r will vary from one function to another.

The absolute values of the coefficients can be used to rank the variables in order of their contribution to separating the groups. If we wish to go further and interpret or 'name' a discriminant function, the signs can be taken into account. Thus, for example, z 1 = . 8 y 1 -. 9 y 2 + . 5 y 3 has a different meaning than does z 2 = . 8 y 1 + . 9 y 2 + . 5 y 3, since z 1 depends on the difference between y 1 and y 2, whereas z 2 is related to the sum of y 1 and y 2.

The discriminant function is subject to the same limitations as other linear combinations such as a regression equation: for example, (1) the coefficient for a variable may change notably if variables are added or deleted, and (2) the coefficients may not be stable from sample to sample unless the sample size is large relative to the number of variables. With regard to limitation 1, we note that the coefficients reflect the contribution of each variable in the presence of the particular variables at hand. This is, in fact, what we want the coefficients to do. As to limitation 2, the processing of a substantial number of variables is not 'free.' More stable estimates will be obtained from 50 observations on 2 variables than from 50 observations on 20 variables. In other words, if N / p is too small, the variables that rank high in one sample may emerge as less important in another sample.

## 8.7.2 Partial F -Values

For any variable yr , we can calculate a partial F -test showing the significance of yr after adjusting for the other variables, that is, the separation provided by yr in addition to that due to the other variables. After computing the partial F for each variable, the variables can then be ranked.

In the case of two groups, the partial F is given by (5.32) as

<!-- formula-not-decoded -->

where T 2 p is the two-sample Hotelling T 2 with all p variables, T 2 p -1 is the T 2 -statistic with all variables except yr , and ν = n 1 + n 2 -2. The F -statistic in (8.26) is distributed as F 1 ,ν -p + 1.

For the several-group case, the partial /Lambda1 for yr adjusted for the other p -1 variables is given by (6.128) as

<!-- formula-not-decoded -->

where /Lambda1 p is Wilks' /Lambda1 for all p variables and /Lambda1 p -1 involves all variables except yr . The corresponding partial F is given by (6.129) as

<!-- formula-not-decoded -->

where /Lambda1 is defined in (8.27), ν E = N -k , and ν H = k -1. The partial /Lambda1 -statistic in (8.27) is distributed as /Lambda1 1 ,ν H ,ν E -p + 1, and the partial F in (8.28) is distributed as F ν H ,ν E -p + 1.

Apartial index of association for yr similar to the overall measure for all y 's given in (6.41), η 2 /Lambda1 = 1 -/Lambda1 , can be defined by

The partial F -values in (8.26) and (8.28) are not associated with a single dimension of group separation, as are the standardized discriminant function coefficients. For example, y 2 will have a different contribution in each of the s discriminant functions, but the partial F for y 2 constitutes an overall index of the contribution of y 2 to group separation taking into account all dimensions. However, the partial F -values will often rank the variables in the same order as the standardized coefficients for the first discriminant function, especially if λ 1 / ∑ j λ j is very large so that the first function accounts for most of the available separation.

<!-- formula-not-decoded -->

where /Lambda1 r is the partial /Lambda1 in (8.27) for yr . This partial R 2 is a measure of association between the grouping variables and yi after adjusting for the other p -1 y 's.

## 8.7.3 Correlations between Variables and Discriminant Functions

Many textbooks and research papers assert that the best measure of variable importance is the correlation between each variable and a discriminant function, ryi z j . It is claimed that these correlations are more informative than standardized coefficients with respect to the joint contribution of the variables to the discriminant functions. The correlations are often referred to as loadings or structure coefficients and are routinely provided in many major programs. However, Rencher (1988; 1992b; 1998, Section 5.7) has shown that the correlations in question show the contribution of each variable in a univariate context rather than in a multivariate one. The correlations actually reproduce the t or F for each variable, and consequently they show only how each variable by itself separates the groups, ignoring the presence of the other variables. Hence these correlations provide no information about how the variables contribute jointly to separation of the groups. They become misleading if used for interpretation of discriminant functions.

Upon reflection, we could have anticipated this failure of the correlations to provide multivariate information. The objection to standardized coefficients is based on the argument that they are 'unstable' because they change if some variables are deleted and others added. However, we actually want them to behave this way, so as to reflect the mutual influence of the variables on each other. In a multivariate analysis, interest is centered on the joint performance of the set of variables at hand. To ask for the contribution of each variable independent of all other variables is to request a univariate index that ignores the other variables. The correlations ryi z j are stable and do not change when variables are added or deleted; this should be a clear signal that they are univariate in nature. There is no middle ground between the univariate and multivariate realms.

## 8.7.4 Rotation

Rotation of the discriminant function coefficients is sometimes recommended. This is a procedure (see Section 13.5) that attempts to produce a pattern with (absolute values of) coefficients closer to 0 or 1. Discriminant functions with such coefficients might be easier to interpret, but they have two deficiencies: they no longer maximize group separation and they are correlated.

Accordingly, for interpretation of discriminant functions we recommend standardized coefficients rather than correlations or rotated coefficients.

## 8.8 SCATTER PLOTS

One benefit of the dimension reduction effected by discriminant analysis is the potential for plotting. It was noted in Section 6.2 that the number of large eigenvalues of E -1 H reflects the dimensionality of the space occupied by the mean vectors. In many data sets, the first two discriminant functions account for most of λ 1 + λ 2 +···+ λ s , and consequently the pattern of the mean vectors can be effectively portrayed in a

two-dimensional plot. If the essential dimensionality is greater than 2, there may be some distortion in intergroup configuration in a two-dimensional plot; that is, some groups that overlap in two dimensions may be well separated in a third dimension.

To plot the first two discriminant functions for the individual observation vectors y i j , simply calculate z 1 i j = a ′ 1 y i j and z 2 i j = a ′ 2 y i j for i = 1, 2 , . . . , k ; j = 1, 2 , . . . , ni , and plot a scatter plot of the N = ∑ i ni values of

<!-- formula-not-decoded -->

The transformed mean vectors,

<!-- formula-not-decoded -->

should be plotted along with the individual values, z i j . In some cases, a plot would show only the transformed mean vectors z 1, z 2 , . . . , z k . For confidence regions for 𝛍 zi = A 𝛍 i , see Rencher (1998, Section 5.8).

We note that the eigenvalues of E -1 H reveal the dimensionality of the mean vectors, not of the individual points. The dimensionality of the individual observations is p , although the essential dimensionality may be less because the variables are correlated. (The dimensionality of the observation vectors is the concern of principal components; see Chapter 12.) If s = 2, for example, so that the mean vectors occupy only two dimensions, the individual observation vectors ordinarily lie in more than two dimensions, and their inclusion in a plot constitutes a projection onto the twodimensional plane of the mean vectors.

It was noted in Section 8.4.1 that the discriminant functions are uncorrelated but not orthogonal. Thus the angle between a 1 and a 2 as given by (3.14) is not 90 ◦ (that is, a ′ 1 a 2 /negationslash= 0 ) . In practice, however, the usual procedure is to plot discriminant functions on a rectangular coordinate system. The resulting distortion is generally not serious.

Example 8.8. Figure 8.4 contains a scatter plot of ( z 1 , z 2 ) for the observations in the football data of Table 8.3. Each observation in group 1 is denoted by a square, observations in group 2 are denoted by circles, and observations in group 3 are indicated by triangles. We see that the first discriminant function z 1 (the horizontal direction) effectively separates group 1 from groups 2 and 3, whereas the second discriminant function z 2 (the vertical direction) is less successful in separating group 2 from group 3.

The group mean vectors are indicated by solid circles. They are almost collinear, as we would expect since λ 1 = 1 . 92 dominates λ 2 = . 12.

Figure 8.4. Scatter plot of discriminant function values for the football data of Table 8.3.

<!-- image -->

## 8.9 STEPWISE SELECTION OF VARIABLES

In many applications, a large number of dependent variables is available and the experimenter would like to discard those that are redundant (in the presence of the other variables) for separating the groups. Our discussion is limited to procedures that delete or add variables one at a time. We emphasize that we are selecting dependent variables ( y 's), and therefore the basic model (one-way MANOVA) does not change. In subset selection in regression, on the other hand, we select independent variables with a consequent alteration of the model.

A forward selection method was discussed in Section 6.11.2. We begin with a single variable, the one that maximally separates the groups by itself. Then the variable entered at each step is the one that maximizes the partial F -statistic based on Wilks' /Lambda1 , thus obtaining the maximal additional separation of groups above and beyond the separation already attained by the other variables. Since we choose the variable with maximum partial F at each step, the proportion of these maximum F 's that exceed F α is greater than α . This bias is discussed in Rencher and Larson (1980) and Rencher (1998, Section 5.10).

Backward elimination is a similar operation in which we begin with all the variables and then at each step, the variable that contributes least is deleted, as indicated by the partial F .

Stepwise selection is a combination of the forward and backward approaches. Variables are added one at a time, and at each step, the variables are reexamined to see if any variable that entered earlier has become redundant in the presence of recently added variables. The procedure stops when the largest partial F among the variables available for entry fails to exceed a preset threshold value. The stepwise procedure has long been popular with practitioners. Some detail about the steps in this procedure was given in Section 6.11.2.

All the preceding procedures are commonly referred to as stepwise discriminant analysis . However, as noted in Section 6.11.2, we are actually doing stepwise MANOVA. No discriminant functions are calculated in the selection process. After the subset selection is completed, we can calculate discriminant functions for the selected variables. We could also use the variables in a classification analysis, as described in Chapter 9.

Example 8.9. We use the football data of Table 8.3 to illustrate the stepwise procedure outlined in this section and in Section 6.11.2. At the first step, we carry out a univariate F (using ordinary ANOVA) for each variable to determine which variable best separates the three groups by itself:

| Variable   |      F | p -Value         |
|------------|--------|------------------|
| WDIM       |  2.55  | .0839            |
| CIRCUM     |  6.231 | .0030            |
| FBEYE      |  1.668 | .1947            |
| EYEHD      | 58.162 | 1 . 11 × 10 - 16 |
| EARHD      | 22.427 | 1 . 40 × 10 - 8  |
| JAW        |  4.511 | .0137            |

Thus EYEHD is the first variable to 'enter.' The Wilks /Lambda1 value equivalent to F = 58 . 162 is /Lambda1( y 1 ) = . 4279 (see Table 6.1 with p = 1). At the second step we calculate a partial /Lambda1 and accompanying partial F using (8.27) and (8.28):

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

where y 1 indicates the variable selected at step 1 (EYEHD) and yr represents each of the five variables to be examined at step 2. The results are

| Variable   |   Partial /Lambda1 |   Partial F |   p -Value |
|------------|--------------------|-------------|------------|
| WDIM       |             0.9355 |       2.964 |     0.0569 |
| CIRCUM     |             0.9997 |       0.012 |     0.9881 |
| FBEYE      |             0.9946 |       0.235 |     0.7911 |
| EARHD      |             0.9525 |       2.143 |     0.1235 |
| JAW        |             0.954  |       2.072 |     0.1322 |

The variable WDIM would enter at this step, since it has the largest partial F . With a p -value of .0569, entering this variable may be questionable, but we will continue the procedure for illustrative purposes. We next check to see if EYEHD is still significant now that WDIM has entered. The partial /Lambda1 and F for EYEHD adjusted for WDIM

are /Lambda1 = . 424 and F = 58 . 47. Thus EYEHD stays in. The overall Wilks' /Lambda1 for EYEHD and WDIM is /Lambda1( y 1 , y 2 ) = . 4003.

At step 3 we check each of the four remaining variables for possible entry using

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

where y 1 = EYEHD, y 2 = WDIM, and yr represents each of the other four variables. The results are

| Variable   |   Partial /Lambda1 |   Partial F |   p -Value |
|------------|--------------------|-------------|------------|
| CIRCUM     |             0.9774 |       0.981 |     0.3793 |
| FBEYE      |             0.9748 |       1.098 |     0.3381 |
| EARHD      |             0.9292 |       3.239 |     0.0441 |
| JAW        |             0.8451 |       7.791 |     0.0008 |

The indicated variable for entry at this step is JAW. To determine whether one of the first two should be removed after JAW has entered, we calculate the partial /Lambda1 and F for each, adjusted for the other two:

| Variable   |   Partial /Lambda1 |   Partial F | p -Value       |
|------------|--------------------|-------------|----------------|
| WDIM       |             0.8287 |       8.787 | .0003          |
| EYEHD      |             0.4634 |      49.211 | 6 . 33 10 - 15 |

×

Thus both previously entered variables remain in the model. The overall Wilks /Lambda1 for EYEHD, WDIM, and JAW is /Lambda1( y 1 , y 2 , y 3 ) = . 3383.

At step 4 there are three candidate variables for entry. The partial /Lambda1 - and F -statistics are

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

where y 1, y 2, and y 3 are the three variables already entered and yr represents each of the other three remaining variables. The results are

| Variable   |   Partial /Lambda1 |   Partial F |   p -Value |
|------------|--------------------|-------------|------------|
| CIRCUM     |             0.9987 |       0.055 |     0.9462 |
| FBEYE      |             0.9955 |       0.189 |     0.8282 |
| EARHD      |             0.908  |       4.257 |     0.0173 |

Hence EARHD enters at this step, and we check to see if any of the three previously entered variables has now become redundant. The partial /Lambda1 and partial F for each of these three are

| Variable   |   Partial /Lambda1 |   Partial F | p -Value         |
|------------|--------------------|-------------|------------------|
| WDIM       |             0.7889 |      11.237 | 4 . 74 × 10 - 15 |
| EYEHD      |             0.6719 |      20.508 | 5 . 59 × 10 - 8  |
| JAW        |             0.8258 |       8.861 | .0003            |

Consequently, all three variables are retained. The overall Wilks' /Lambda1 for all four variables is now /Lambda1( y 1 , y 2 , y 3 , y 4 ) = . 3072.

At step 5, the partial /Lambda1 - and F -values are

| Variable   |   Partial /Lambda1 |   Partial F |   p -Value |
|------------|--------------------|-------------|------------|
| CIRCUM     |             0.9999 |       0.003 |     0.9971 |
| FBEYE      |             0.9999 |       0.004 |     0.9965 |

Thus no more variables will enter.

We summarize the selection process as follows:

|   Step | Variable Entered   |   Overall /Lambda1 |   Partial /Lambda1 |   Partial F | p -Value         |
|--------|--------------------|--------------------|--------------------|-------------|------------------|
|      1 | EYEHD              |             0.4279 |             0.4279 |      58.162 | 1 . 11 × 10 - 16 |
|      2 | WDIM               |             0.4003 |             0.9355 |       2.964 | .0569            |
|      3 | JAW                |             0.3383 |             0.8451 |       7.791 | .0008            |
|      4 | EARHD              |             0.3072 |             0.908  |       4.257 | .0173            |

## PROBLEMS

- 8.1 Show that if a = S -1 p 1 ( y 1 -y 2 ) is substituted into [ a ′ ( y 1 -y 2 ) ] 2 / a ′ S pl a , the result is (8.3).
- 8.2 Verify (8.4) for the relationship between b and a .
- 8.3 Verify the relationship between R 2 and T 2 shown in (8.5).
- 8.4 Show that [ a ′ ( y 1 -y 2 ) ] 2 = a ′ ( y 1 -y 2 )( y 1 -y 2 ) ′ a as in (8.7).
- 8.5 Show that Ha -λ Ea = 0 can be written in the form ( E -1 H -λ I ) a = 0 , as in (8.12).
- 8.6 Verify (8.16) by substituting a ∗ r = sr ar into (8.15) to obtain z 1 i = a 1 y 1 i 1 + a 2 y 1 i 2 +··· + ap y 1 i p -a ′ y 1 .
- 8.7 For the psychological data in Table 5.1, the discriminant function coefficient vector was given in Example 5.5.

- (a) Find the standardized coefficients.
- (b) Calculate t -tests for the individual variables.
- (c) Compare the results of (a) and (b) as to the contribution of the variables to separation of the two groups.
- (d) Find the partial F for each variable, as in (8.26), and compare with the standardized coefficients.

## 8.8 Using the beetle data of Table 5.5, do the following:

- (a) Find the discriminant function coefficient vector.
- (b) Find the standardized coefficients.
- (c) Calculate t -tests for individual variables.
- (d) Compare the results of (b) and (c) as to the contribution of each variable to separation of the groups.
- (e) Find the partial F for each variable, as in (8.26). Do the partial F 's rank the variables in the same order of importance as the standardized coefficients?

## 8.9 Using the dystrophy data of Table 5.7, do the following:

- (a) Find the discriminant function coefficient vector.
- (b) Find the standardized coefficients.
- (c) Calculate t -tests for individual variables.
- (d) Compare the results of (b) and (c) as to the contribution of each variable to separation of the groups.
- (e) Find the partial F for each variable, as in (8.26). Do the partial F 's rank the variables in the same order of importance as the standardized coefficients?

## 8.10 For the cyclical data of Table 5.8, do the following:

- (a) Find the discriminant function coefficient vector.
- (b) Find the standardized coefficients.
- (c) Calculate t -tests for individual variables.
- (d) Compare the results of (b) and (c) as to the contribution of each variable to separation of the groups.
- (e) Find the partial F for each variable, as in (8.26). Do the partial F 's rank the variables in the same order of importance as the standardized coefficients?

## 8.11 Using the fish data in Table 6.17, do the following:

- (a) Find the eigenvectors of E -1 H .
- (b) Carry out tests of significance for the discriminant functions and find the relative importance of each as in (8.13), λ i / ∑ j λ j . Do these two procedures agree as to the number of important discriminant functions?
- (c) Find the standardized coefficients and comment on the contribution of the variables to separation of groups.
- (d) Find the partial F for each variable, as in (8.28). Do they rank the variables in the same order as the standardized coefficients for the first discriminant function?

- (e) Plot the first two discriminant functions for each observation and for the mean vectors.
- 8.12 For the rootstock data of Table 6.2, do the following:
- (a) Find the eigenvalues and eigenvectors of E -1 H .
- (b) Carry out tests of significance for the discriminant functions and find the relative importance of each as in (8.13), λ i / ∑ j λ j . Do these two procedures agree as to the number of important discriminant functions?
- (c) Find the standardized coefficients and comment on the contribution of the variables to separation of groups.
- (d) Find the partial F for each variable, as in (8.28). Do they rank the variables in the same order as the standardized coefficients for the first discriminant function?
- (e) Plot the first two discriminant functions for each observation and for the mean vectors.
- 8.13 Carry out a stepwise selection of variables on the rootstock data of Table 6.2.
- 8.14 Carry out a stepwise selection of variables on the engineer data of Table 5.6.
- 8.15 Carry out a stepwise selection of variables on the fish data of Table 6.17.

## C H A P T E R 9

## Classification Analysis: Allocation of Observations to Groups

## 9.1 INTRODUCTION

The descriptive aspect of discriminant analysis, in which group separation is characterized by means of discriminant functions, was covered in Chapter 8. We turn now to allocation of observations to groups, which is the predictive aspect of discriminant analysis. We prefer to call this classification analysis to clearly distinguish it from the descriptive aspect. However, classification is often referred to simply as discriminant analysis. In engineering and computer science, classification is usually called pattern recognition . Some writers use the term classification analysis to describe cluster analysis , in which the observations are clustered according to variable values rather than into predefined groups (see Chapter 14).

In classification, a sampling unit (subject or object) whose group membership is unknown is assigned to a group on the basis of the vector of p measured values, y , associated with the unit. To classify the unit, we must have available a previously obtained sample of observation vectors from each group. Then one approach is to compare y with the mean vectors y 1 , y 2 , . . . , y k of the k samples and assign the unit to the group whose y i is closest to y .

In this chapter, the term groups may refer to either the k samples or the k populations from which they were taken. It should be clear from the context which of the two uses is intended in every case.

We give some examples to illustrate the classification technique:

1. A university admissions committee wants to classify applicants as likely to succeed or likely to fail. The variables available are high school grades in various subject areas, standardized test scores, rating of high school, number of advanced placement courses, etc.
2. A psychiatrist gives a battery of diagnostic tests in order to assign a patient to the appropriate mental illness category.
3. A college student takes aptitude and interest tests in order to determine which vocational area his or her profile best matches.

4. African, or 'killer,' bees cannot be distinguished visually from ordinary domestic honey bees. Ten variables based on chromatograph peaks can be used to readily identify them (Lavine and Carlson 1987).
5. The Air Force wishes to classify each applicant into the training program where he or she has the most potential.
6. Twelve of the Federalist Papers were claimed by both Madison and Hamilton. Can we identify authorship by measuring frequencies of word usage (Mosteller and Wallace 1984)?
7. Variables such as availability of fingerprints, availability of eye witnesses, and time until police arrive can be used to classify burglaries into solvable and unsolvable.
8. One approach to speech recognition by computer consists of an attempt to identify phonemes based on the energy levels in speech waves.
9. A number of variables are measured at five weather stations. Based on these variables, we wish to predict the ceiling at a particular airport in 2 hours. The ceiling categories are closed, low instrument, high instrument, low open, and high open (Lachenbruch 1975, p. 2).

## 9.2 CLASSIFICATION INTO TWO GROUPS

In the case of two populations, we have a sampling unit (subject or object) to be classified into one of two populations. The information we have available consists of the p variables in the observation vector y measured on the sampling unit. In the first illustration in Section 9.1, for example, we have an applicant with high school grades and various test scores recorded in y . We do not know if the applicant will succeed or fail at the university, but we have data on previous students at the university for whom it is now known whether they succeeded or failed. By comparing y with y 1 for those who succeeded and y 2 for those who failed, we attempt to predict the group to which the applicant will eventually belong.

When there are two populations, we can use a classification procedure due to Fisher (1936). The principal assumption for Fisher's procedure is that the two populations have the same covariance matrix ( 𝚺 1 = 𝚺 2 ) . Normality is not required. We obtain a sample from each of the two populations and compute y 1 , y 2 , and S pl. A simple procedure for classification can be based on the discriminant function,

<!-- formula-not-decoded -->

(see Sections 5.5, 5.6, 8.2, and 8.5), where y is the vector of measurements on a new sampling unit that we wish to classify into one of the two groups (populations). For convenience we speak of classifying y rather than classifying the subject or object associated with y .

To determine whether y is closer to y 1 or y 2 , we check to see if z in (9.1) is closer to the transformed mean z 1 or to z 2. We evaluate (9.1) for each observation

y 1 i from the first sample and obtain z 11, z 12 , . . . , z 1 n 1 , from which, by (3.54), z 1 = ∑ n 1 i = 1 z 1 i / n 1 = a ′ y 1 = ( y 1 -y 2 ) ′ S -1 pl y 1 . Similarly, z 2 = a ′ y 2 . Denote the two groups by G 1 and G 2. Fisher's (1936) linear classification procedure assigns y to G 1 if z = a ′ y is closer to z 1 than to z 2 and assigns y to G 2 if z is closer to z 2. This is illustrated in Figure 9.1.

For the configuration in Figure 9.1, we see that z is closer to z 1 if

<!-- formula-not-decoded -->

This is true in general because z 1 is always greater than z 2, which can easily be shown as follows:

<!-- formula-not-decoded -->

because S -1 pl is positive definite. Thus z 1 &gt; z 2. [If a were of the form a ′ = ( y 2 -y 1 ) ′ S -1 pl , then z 2 -z 1 would be positive.] Since 1 2 ( z 1 + z 2 ) is the midpoint, z &gt; 1 2 ( z 1 + z 2 ) implies that z is closer to z 1. By (9.3) the distance from z 1 to z 2 is the same as that from y 1 to y 2 .

To express the classification rule in terms of y , we first write 1 2 ( z 1 + z 2 ) in the form

<!-- formula-not-decoded -->

Then the classification rule becomes: Assign y to G 1 if

<!-- formula-not-decoded -->

Figure 9.1. Fisher's procedure for classification into two groups.

<!-- image -->

and assign y to G 2 if

<!-- formula-not-decoded -->

This linear classification rule employs the same discriminant function z = a ′ y used in Section 8.2 in connection with descriptive separation of groups. Thus in the two-group case, the discriminant function serves as a linear classification function as well. However, in the several-group case in Section 9.3, we use classification functions that are different from the descriptive discriminant functions in Section 8.4.

Fisher's (1936) approach using (9.5) and (9.6) is essentially nonparametric because no distributional assumptions were made. However, if the two populations are normal with equal covariance matrices, then this method is (asymptotically) optimal; that is, the probability of misclassification is minimized [see comments following (9.8)].

If prior probabilities p 1 and p 2 are known for the two populations, the classification rule can be modified to exploit this additional information. We define the prior probabilities as follows: p 1 is the proportion of observations in G 1 and p 2 is the proportion in G 2, where p 2 = 1 -p 1. For example, suppose that at a certain university 70% of entering freshmen ultimately graduate. Then p 1 = . 7 and p 2 = . 3.

In order to use the prior probabilities, the density functions for the two populations, f ( y | G 1 ) and f ( y | G 2 ) , must also be known. Then the optimal classification rule (Welch 1939) that minimizes the probability of misclassification is: Assign y to G 1 if

<!-- formula-not-decoded -->

and to G 2 otherwise. Note that f ( y | G 1 ) is a convenient notation for the density when sampling from the population represented by G 1. It does not represent a conditional distribution in the usual sense (Section 4.2).

Assuming that the two densities are multivariate normal with equal covariance matrices, namely, f ( y | G 1 ) = Np ( 𝛍 1 , 𝚺 ) and f ( y | G 2 ) = Np ( 𝛍 2 , 𝚺 ) , then from (9.7) we obtain the following rule (with estimates in place of 𝛍 1, 𝛍 2, and 𝚺 ): Assign y to G 1 if

<!-- formula-not-decoded -->

and to G 2 otherwise [see Rencher (1998, p. 231)]. Because we have substituted estimates for the parameters, the rule in (9.8) is no longer optimal, as is (9.7). However, it is asymptotically optimal (approaches optimality as the sample size increases).

If p 1 = p 2, the normal-based classification rule in (9.8) becomes the same as Fisher's procedure given in (9.5) and (9.6). Thus Fisher's rule, which is not based on a normality assumption, has optimal properties when the data come from multivariate normal populations with 𝚺 1 = 𝚺 2 and p 1 = p 2. [For the case when 𝚺 1 /negationslash= 𝚺 2, see Rencher (1998, Section 6.2.2).] Hence, even though Fisher's method is nonparametric, it works better for normally distributed populations or other populations with

Figure 9.2. Two populations with nonlinear separation.

<!-- image -->

linear trends. For example, suppose two populations have 95% contours, as in Figure 9.2. If the points are projected in any direction onto a straight line, there will be almost total overlap. A linear discriminant procedure will not successfully separate the two populations.

Example 9.2. For the psychological data of Table 5.1, y 1 , y 2 , and S pl were obtained in Example 5.4.2. The discriminant function coefficients were obtainedin Example 5.5 as a ′ = (. 5104 , -. 2032 , . 4660 , -. 3097 ) . For G 1 (the male group), we find

<!-- formula-not-decoded -->

Similarly, for G 2 (the female group), z 2 = a ′ y 2 = 4 . 4426. Thus we assign an observation vector y to G 1 if

<!-- formula-not-decoded -->

and assign y to G 2 if z &lt; 7 . 4927.

There are no new observations available, so we will illustrate the procedure by classifying two of the observations in G 1. For y ′ 11 = ( 15 , 17 , 24 , 14 ) , the first observation in G 1, we have z 11 = a ′ y 11 = . 5104 ( 15 ) -. 2032 ( 17 ) + . 4660 ( 24 ) -. 3097 ( 14 ) = 11 . 0498, which is greater than 7.4927, and y 11 would be correctly classified as belonging to G 1. For y ′ 14 = ( 13 , 12 , 10 , 16 ) , the fourth observation in G 1, we find z 14 = 3 . 9016, which would misclassify y 14 into G 2.

## 9.3 CLASSIFICATION INTO SEVERAL GROUPS

In this section we discuss classification rules for several groups. As in the two-group case, we use a sample from each of the k groups to find the sample mean vectors y 1 , y 2 , . . . , y k . For a vector y whose group membership is unknown, one approach is to use a distance function to find the mean vector that y is closest to and assign y to the corresponding group.

## 9.3.1 Equal Population Covariance Matrices: Linear Classification Functions

In this section we assume 𝚺 1 = 𝚺 2 = ·· · = 𝚺 k . We can estimate the common population covariance matrix by a pooled sample covariance matrix

<!-- formula-not-decoded -->

where ni and S i are the sample size and covariance matrix of the i th group, E is the error matrix from one-way MANOVA, and N = ∑ i ni . We compare y to each y i , i = 1, 2 , . . . , k , by the distance function

<!-- formula-not-decoded -->

and assign y to the group for which D 2 i ( y ) is smallest.

We can obtain a linear classification rule by expanding (9.9):

<!-- formula-not-decoded -->

The term y ′ S -1 pl y on the right can be neglected since it is not a function of i and, consequently, does not change from group to group. The second term is a linear function of y , and the third does not involve y . We thus delete y ′ S -1 pl y and obtain a linear classification function , which we denote by Li ( y ) . If we multiply by -1 2 to agree with the rule based on the normal distribution and prior probabilities given in (9.12), our linear classification rule becomes: Assign y to the group for which

<!-- formula-not-decoded -->

is a maximum (we reversed the sign when multiplying by -1 2 ). To highlight the linearity of (9.10) as a function of y , we can express it as

<!-- formula-not-decoded -->

where c ′ i = y ′ i S -1 pl and ci 0 = -1 2 y ′ i S -1 pl y i . To assign y to a group using this procedure, we calculate c i and ci 0 for each of the k groups, evaluate Li ( y ) , i = 1, 2 , . . . , k , and

allocate y to the group for which Li ( y ) is largest. This will be the same group for which D 2 i ( y ) in (9.9) is smallest, that is, the group whose mean vector y i is closest to y .

For the case of several groups, the optimal rule in (9.7) extends to:

<!-- formula-not-decoded -->

With this rule, the probability of misclassification is minimized. If we assume normality with equal covariance matrices and with prior probabilities of group membership, p 1, p 2 , . . . , pk , then f ( y | Gi ) = Np ( 𝛍 i , 𝚺 ) , and the rule in (9.11) becomes (with estimates in place of parameters): Calculate

<!-- formula-not-decoded -->

and assign y to the group with maximum value of L ′ i ( y ) . Note that if p 1 = p 2 = · · · = pk , then (9.12), which optimizes the classification rate for the normal distribution, reduces to (9.10), which was based on the heuristic approach of minimizing the distance of y to y i .

The linear functions Li ( y ) defined in (9.10) are called linear classification functions (many writers refer to them as linear discriminant functions ). They are different from the linear discriminant functions in Sections 6.1.4, 6.4, and 8.4.1, whose coefficients are eigenvectors of E -1 H . In fact, there will be k classification functions and s = min ( p , k -1 ) discriminant functions, where k is the number of groups and p is the number of variables. In many cases we do not need all s discriminant functions to effectively describe group differences, whereas all k classification functions must be used in assigning observations to groups.

Example 9.3.1. For the football data of Table 8.3, the mean vectors for the three groups are as follows:

<!-- formula-not-decoded -->

Using these values of y i and the pooled covariance matrix S pl, given in Example 8.5, the linear classification functions (9.10) become

<!-- formula-not-decoded -->

We note that y 2 and y 3 have essentially the same coefficients in all three functions and hence do not contribute to classification of y . These same two variables were eliminated in the stepwise discriminant analysis in Example 8.9.

We illustrate the use of these linear functions for the first and third observations in group 1. For the first observation, y 11, we obtain

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

We classify y 11 into group 1 since L 1 ( y 11 ) = 582 . 1 exceeds L 2 ( y 11 ) and L 3 ( y 11 ) . For the third observation in group 1, y 13, we obtain

<!-- formula-not-decoded -->

This observation is misclassified into group 2 since L 2 ( y 13 ) = 570 . 290 exceeds L 1 ( y 13 ) and L 3 ( y 13 ) .

## 9.3.2 Unequal Population Covariance Matrices: Quadratic Classification Functions

The linear classification functions in Section 9.3.1 are based on the assumption 𝚺 1 = 𝚺 2 = ·· · = 𝚺 k . The resulting classification rules are sensitive to heterogeneity of covariance matrices. Observations tend to be classified too frequently into groups whose covariance matrices have larger variances on the diagonal. Thus the population covariance matrices should not be assumed to be equal if there is reason to suspect otherwise.

If 𝚺 1 = 𝚺 2 = ·· · = 𝚺 k does not hold, the classification rules can easily be altered to preserve optimality of classification rates. In place of (9.9), we can use

<!-- formula-not-decoded -->

where S i is the sample covariance matrix for the i th group. As before, we would assign y to the group for which D 2 i ( y ) is smallest. With S i in place of S pl, (9.13) cannot be reduced to a linear function of y as in (9.10) but remains a quadratic function. Hence rules based on S i are called quadratic classification rules .

If we assume normality with unequal covariance matrices and with prior probabilities p 1 , p 2 , . . . , pk , then f ( y | Gi ) = Np ( 𝛍 i , 𝚺 i ) , and the optimal rule in (9.11) based on pi f ( y | Gi ) becomes: Assign y to the group for which

<!-- formula-not-decoded -->

is maximum. If p 1 = p 2 = · · · = pk or if the pi 's are unknown, the term ln pi is deleted.

In order to use a quadratic classification rule based on S i , each ni must be greater than p so that S -1 i will exist. This restriction does not apply to linear classification rules based on S pl. Since more parameters are estimated with quadratic classification functions, larger values of the ni 's are needed for stability of estimates. Note the distinction between p , the number of variables, and pi , the prior probability for the i th group.

## 9.4 ESTIMATING MISCLASSIFICATION RATES

In Chapter 8, we assessed the effectiveness of the discriminant functions in group separation by the use of significance tests or by examining λ i / ∑ j λ j . To judge the ability of classification procedures to predict group membership, we usually use the probability of misclassification, which is known as the error rate . We could also use its complement, the correct classification rate .

A simple estimate of the error rate can be obtained by trying out the classification procedure on the same data set that has been used to compute the classification functions. This method is commonly referred to as resubstitution . Each observation vector y i j is submitted to the classification functions and assigned to a group. We then count the number of correct classifications and the number of misclassifications. The proportion of misclassifications resulting from resubstitution is called the apparent error rate . The results can be conveniently displayed in a classification table or confusion matrix , such as Table 9.1 for two groups.

Among the n 1 observations in G 1 , n 11 are correctly classified into G 1, and n 12 are misclassified into G 2, where n 1 = n 11 + n 12. Similarly, of the n 2 observations in G 2 , n 21 are misclassified into G 1, and n 22 are correctly classified into G 2, where n 2 = n 21 + n 22. Thus

<!-- formula-not-decoded -->

Similarly, we can define

<!-- formula-not-decoded -->

Table 9.1. Classification Table for Two Groups

| Actual   | Number of    | Predicted Group   |
|----------|--------------|-------------------|
| Group    | Observations | 1 2               |
| 1        | n 1          | n 11 n 12         |
| 2        | n 2          | n 21 n 22         |

Clearly,

Apparent error rate = 1 -apparent correct classification rate .

The method of resubstitution can be readily extended to the case of several groups.

The apparent error rate is easily obtained and is routinely provided by most classification software programs. It is an estimate of the probability that our classification functions based on the present sample will misclassify a future observation. This probability is called the actual error rate . Unfortunately, the apparent error rate underestimates the actual error rate because the data set used to compute the classification functions is also used to evaluate them. The classification functions are optimized for the particular sample and may be capitalizing on chance to some degree, especially for small samples. For other estimates of error rates, see Rencher (1998, Section 6.4). In Section 9.5 we consider some approaches to reducing the bias in the apparent error rate.

Example 9.4.(a). We use the psychological data of Table 5.1 to illustrate the apparent error rate obtained by the resubstitution method for two groups. The hypothesis H 0 : 𝚺 1 = 𝚺 2 was not rejected in Example 7.3.2, and we therefore classify each of the 64 observations using the linear classification procedure obtained in Example 9.2: Classify as G 1 if a ′ y &gt; 7 . 4927 and as G 2 otherwise. The resulting classification table is given in Table 9.2. By (9.15),

<!-- formula-not-decoded -->

Example 9.4.(b). We use the football data of Table 8.3 to illustrate the use of the resubstitution method for estimating the error rate in the case of more than two groups. The sample covariance matrices for the three groups are almost significantly different, and we will use both linear and quadratic classification functions.

The linear classification functions Li ( y ) from(9.10) were given in Example 9.3.1 for the football data. Using these, we classify each of the 90 observations. The results are shown in Table 9.3.

An examination of this data set in Example 8.8 showed that groups 2 and 3 are harder to separate than 1 and 2 or 1 and 3. This pattern is reflected here in the misclas-

Table 9.2. Classification Table for the Psychological Data of Table 5.1

| Actual Group   | Number of Observations   | Predicted Group 1 2   |
|----------------|--------------------------|-----------------------|
|                | 32                       | 28 4                  |
| Male           |                          |                       |
| Female         | 32                       | 4 28                  |

Table 9.3. Classification Table for the Football Data of Table 8.3 Using Linear Classification Functions

| Actual   | Number of    |   Predicted Group |   Predicted Group |
|----------|--------------|-------------------|-------------------|
| Group    | Observations |                 1 |                 3 |
| 1        | 30           |                26 |                 3 |
| 2        | 30           |                 1 |                 9 |
| 3        | 30           |                 2 |                20 |

Apparent correct classification rate

=

26 + 20 + 20

90

=

.

733

Apparent error rate = 1 -. 733 = . 267

Table 9.4. Classification Table for the Football Data of Table 8.3 Using Quadratic Classification Functions

| Actual   | Number of    |   Predicted Group |   Predicted Group |
|----------|--------------|-------------------|-------------------|
| Group    | Observations |                 1 |                 3 |
| 1        | 30           |                27 |                 2 |
| 2        | 30           |                 2 |                 7 |
| 3        | 30           |                 1 |                25 |

Apparent correct classification rate

+

+

21

90

Apparent error rate = 1 -. 811 = . 189

=

=

sifications. Only 4 of the observation vectors in group 1 are misclassified, whereas 10 observations in each of groups 2 and 3 are misclassified.

Using the quadratic classification functions Qi ( y ) , i = 1 , 2 , 3, in (9.14) and assuming p 1 = p 2 = p 3, we obtain the classification results in Table 9.4. There is some improvement in the apparent error rate using quadratic classification functions.

## 9.5 IMPROVED ESTIMATES OF ERROR RATES

For large samples, the apparent error rate has only a small amount of bias for estimating the actual error rate and can be used with little concern. For small samples, however, it is overly optimistic (biased downward), as noted before. We discuss two techniques for reducing the bias in the apparent error rate, that is, increasing the apparent error rate to a more realistic level.

27

25

811

.

## 9.5.1 Partitioning the Sample

One way to avoid bias is to split the sample into two parts, a training sample used to construct the classification rule and a validation sample used to evaluate it. With the training sample, we calculate linear or quadratic classification functions. We then submit each observation vector in the validation sample to the classification functions obtained from the training sample. Since these observations are not used in calculating the classification functions, the resulting error rate is unbiased. To increase the information gained, we could also reverse the roles of the two samples so that the classification functions are obtained from the validation sample and evaluated on the training sample. The two estimates of error could then be averaged.

Partitioning the sample has at least two disadvantages:

1. It requires large samples that may not be available.
2. It does not evaluate the classification function we will use in practice. The estimate of error based on half the sample may vary considerably from that based on the entire sample. We prefer to use all or almost all the data to construct the classification functions so as to minimize the variance of our error rate estimate.

## 9.5.2 Holdout Method

The holdout method is an improved version of the sample-splitting procedure in Section 9.5.1. In the holdout procedure, all but one observation is used to compute the classification rule, and this rule is then used to classify the omitted observation. We repeat this procedure for each observation, so that, in a sample of size N = ∑ i ni , each observation is classified by a function based on the other N -1 observations. The computation load is increased because N distinct classification procedures have to be constructed. The holdout procedure is also referred to as the leaving-one-out method or as cross validation . Note that this procedure is used to estimate error rates. The actual classification rule for future observations would be based on all N observations.

Example 9.5.2. Weuse the football data of Table 8.3 to illustrate the holdout method for estimating the error rate. Each of the 90 observations is classified by linear classification functions based on the other 89 observations. To begin the procedure, the first observation in group 1 ( y 11 ) is held out and the linear classification functions Li ( y ) , i = 1, 2, 3, in (9.10) are calculated using the remaining 29 observations in group 1 and the 60 observations in groups 2 and 3. The observation y 11 is now classified using L 1 ( y ) , L 2 ( y ) , and L 3 ( y ). Then y 11 is reinserted in group 1, and y 12 is held out. The functions L 1 ( y ) , L 2 ( y ) , and L 3 ( y ) are recomputed and y 12 is then classified. This procedure is followed for each of the 90 observations, and the results are in Table 9.5.

Table 9.5. Classification Table for the Football Data of Table 8.3 Using the Holdout Method Based on Linear Classification Functions

| Actual   | Number of Observations   |   Predicted Group |   Predicted Group |   Predicted Group |
|----------|--------------------------|-------------------|-------------------|-------------------|
| Group    |                          |                 1 |                 2 |                 3 |
| 1        | 30                       |                26 |                 1 |                 3 |
| 2        | 30                       |                 1 |                18 |                11 |
| 3        | 30                       |                 2 |                 9 |                19 |

<!-- formula-not-decoded -->

As expected, the holdout error rate has increased somewhat from the apparent error rate based on resubstitution in Tables 9.3 and 9.4 in Example 9.4.(b). An error rate of .300 is a less optimistic (more realistic) estimate of what the classification functions can do with future samples.

## 9.6 SUBSET SELECTION

The experimenter often has available a large number of variables and wishes to keep any that might aid in predicting group membership but at the same time to delete any superfluous variables that do not contribute to allocation. A reduction in the number of redundant variables may in fact lead to improved error rates. As an additional consideration, there is an increase in robustness to nonnormality of linear and quadratic classification functions as p (the number of variables) decreases.

The majority of selection schemes for classification analysis are based on stepwise discriminant analysis or a similar approach (Section 8.9). One finds the subset of variables that best separates groups using Wilks' /Lambda1 , for example, and then uses these variables to construct classification functions. Most of the major statistical software packages offer this method. When the 'best' subset is selected in this way, an optimistic bias in error rates is introduced. For a discussion of this bias, see Rencher (1992a; 1998, Section 6.7).

Another link between separation and classification is the use of error rates in an informal stopping rule in a stepwise discriminant analysis. Thus, for example, if a subset of 5 variables out of 10 gives a misclassification rate of 33% compared to 30% for the full set of variables, we may decide that the 5 variables are adequate for separating the groups. We could try several subsets of decreasing sizes to see when the error rate begins to escalate noticeably.

Example 9.6.(a). In Example 8.9, a stepwise discriminant analysis based on a partial Wilks' /Lambda1 (or partial F ) was carried out for the football data of Table 8.3. Four variables were selected: EYEHD, WDIM, JAW, and EARHD. These same four vari-

Table 9.6. Classification Table for the Football Data of Table 8.3 Using Linear Classification Functions Based on Four Variables Chosen by Stepwise Selection

| Actual   | Number of    |   Predicted Group |   Predicted Group |   Predicted Group |
|----------|--------------|-------------------|-------------------|-------------------|
| Group    | Observations |                 1 |                 2 |                 3 |
| 1        | 30           |                26 |                 1 |                 3 |
| 2        | 30           |                 1 |                20 |                 9 |
| 3        | 30           |                 2 |                 8 |                20 |

ables are indicated by the coefficients in the linear classification functions in Example 9.3.1. We now use these four variables to classify the observations using the method of resubstitution to obtain the apparent error rate.

The linear classification functions (9.10) are

<!-- formula-not-decoded -->

When each observation vector is classified using these linear functions, we obtain the classification results in Table 9.6.

Table 9.6 is identical to Table 9.3 in Example 9.4.(b), where all six variables were used. Thus the four selected variables can classify the sample as well as all six variables classify it.

Example 9.6.(b). We illustrate the use of error rates as an informal stopping rule in a stepwise discriminant analysis. Fifteen teacher and pupil behaviors were observed during 5-min intervals of reading instruction in elementary school classrooms (Rencher, Wadham, and Young 1978). The observations were recorded in rate of occurrences per minute for each variable. The variables were the following:

## Teacher Behaviors

1. Explains -Explains task to learner.
2. Models -Models the task response for the learner.
3. Questions -Asks a question to elicit a task response.
4. Directs -Gives a direct signal to elicit a task response.
5. Controls -Controls management behavior with direction statements or gestures.
6. Positive -Gives a positive (affirmative) statement or gesture.
7. Negative -Gives a negative statement or gesture.

## Pupil Behaviors

8. Overt delayed -An overt learner response to task signals that cannot be judged correct or incorrect until later.
9. Correct -Acorrect learner response with relationship to task signals.
10. Incorrect -An incorrect learner response with relationship to task signals.
11. No response -Learner gives no response with relationship to task signals.
12. Asks -Learner asks a question about the task.
13. Statement -Learner gives a positive statement or gestures not related to the task.
14. Inappropriate -Learner gives in appropriate management behavior.
15. Other -Other learner than one being observed gives responses as teacher directs task signals.

The teachers were grouped into four categories:

Group 1:

Outstanding teachers,

Group 2:

Poor teachers,

Group 3:

First-year teachers,

Group 4:

Teacher aides.

The sample sizes in groups 1-4 were 62, 61, 57, and 41, respectively. Because of the large values of N and p ( N = 221, p = 15), the data are not given here.

The stepwise discriminant analysis was run several times with different threshold F -to-enter values in order to select subsets with different sizes. A classification analysis based on resubstitution was carried out with each of the resulting subsets of variables. In Table 9.7, we compare the overall Wilks' /Lambda1 and the apparent correct classification rate.

According to the correct classification rate, we would choose to stop at five variables because of the abrupt change from 5 to 4. On the other hand, the changes in Wilks' /Lambda1 are more gradual, and no clear stopping point is indicated.

Table 9.7. Stepwise Selection Statistics for the Teacher Data

|   Number of Variables |   Overall Wilks' /Lambda1 |   Percentage of Correct Classification |
|-----------------------|---------------------------|----------------------------------------|
|                    15 |                     0.132 |                                   77.4 |
|                    10 |                     0.159 |                                   72.4 |
|                     9 |                     0.17  |                                   73.3 |
|                     8 |                     0.182 |                                   70.6 |
|                     7 |                     0.195 |                                   72.9 |
|                     6 |                     0.211 |                                   70.1 |
|                     5 |                     0.231 |                                   70.6 |
|                     4 |                     0.256 |                                   65.6 |

## 9.7 NONPARAMETRIC PROCEDURES

We have previously discussed both parametric and nonparametric procedures. Welch's optional rule in (9.7) and (9.11) is parametric, whereas Fisher's linear classification rule for two groups as given in (9.5) and (9.6) is essentially nonparametric, since no distributional assumptions were involved in its derivation. However, Fisher's procedure also turns out to be equivalent to the optimal normal-based approach in (9.8). Nonparametric procedures for estimating error rate include the resubstitution and holdout methods. In the next three sections we discuss three additional nonparametric classification procedures.

## 9.7.1 Multinomial Data

We now consider data in which an observation vector consists of responses on each of several categorical variables. The various combinations of categories constitute the possible outcomes of a multinomial random variable. For example, consider the following four categorical variables: gender (male or female), political party (Republican, Democrat, other), size of city of residence (under 10,000, between 10,000 and 100,000, over 100,000), and education (less than high school graduation, high school graduate, college graduate, advanced degree). An observation vector might be (2, 1, 3, 4), that is, a female Republican who lives in a city of over 100,000 and is a college graduate. The total number of possible outcomes in this multinomial distribution is the product of the number of states of the individual variables: 2 × 3 × 3 × 4 = 72. We will use this example to illustrate classification procedures for multinomial data. Suppose we are attempting to predict whether or not a person will vote. Then there are two groups, G 1 and G 2, and we assign a person to one of the groups after observing which of the 72 possible outcomes he or she gives.

Welch's (1939) optimum rule given in (9.7) can be written as: Assign y to G 1 if

<!-- formula-not-decoded -->

and to G 2 otherwise. In our categorical example, f ( y | G 1 ) is represented by q 1 i , i = 1, 2 , . . . , 72, and f ( y | G 2 ) becomes q 2 i , i = 1, 2 , . . . , 72, where q 1 i is the probability that a person in group 1 will give the i th outcome, with an analogous definition for q 2 i . In terms of these multinomial probabilities, the classification rule in (9.17) becomes: If a person gives the i th outcome, assign him or her to G 1 if

<!-- formula-not-decoded -->

and to G 2 otherwise. If the probabilities q 1 i and q 2 i were known, it would be easy to check (9.18) for each i and partition the 72 possible outcomes into two subsets, those for which the person would be assigned to G 1 and those corresponding to G 2.

The values of q 1 i and q 2 i are usually unknown and must be estimated from a sample. Let n 1 i and n 2 i be the numbers of persons in groups 1 and 2 who give the

i th outcome, i = 1, 2 , . . . , 72. Then we estimate q 1 i and q 2 i by

<!-- formula-not-decoded -->

where N 1 = ∑ i n 1 i and N 2 = ∑ i n 2 i . However, a large sample size would be required for stable estimates; in any given example, some of the n 's may be zero.

Multinomial data can also be classified by ordinary linear classification functions. We must distinguish between ordered and unordered categories. If all the variables have ordered categories, the data can be submitted directly to an ordinary classification program. In the preceding example, city size and education are variables of this type. It is customary to assign ordered categories ranked values such as 1, 2, 3, 4. It has been shown that linear classification functions perform reasonably well on (ordered) discrete data of this type [see Lachenbruch (1975, p. 45), Titterington et al. (1981), and Gilbert (1968)].

Unordered categorical variables cannot be handled this same way. Thus the political party variable in the preceding example should not be coded 1, 2, 3 and entered into the computation of the classification functions. However, an unordered categorical variable with k categories can be replaced by k -1 dummy variables (see Sections 6.1.8 and 11.6.2) for use with linear classification functions. For example, the political preference variable with three categories can be converted to two dummy variables as follows:

<!-- formula-not-decoded -->

Thus the ( y 1 , y 2 ) pair takes the value ( 1 , 0 ) for a Republican, ( 0 , 1 ) for a Democrat, and ( 0 , 0 ) for other. Many software programs will create dummy variables automatically. Note that if a subset selection program is used, the dummy variables for a given categorical variable must be kept together; that is, they must all be included in the chosen subset or all excluded, because all are necessary to describe the categorical variable.

In some cases, such as in medical data collection, there is a mixture of continuous and categorical variables. Various approaches to classification with such data have been discussed by Krzanowski (1975, 1976, 1977, 1979, 1980), Lachenbruch and Goldstein (1979), Tu and Han (1982), and Bayne et al. (1983). See Rencher (1998, Section 6.8) for a discussion of logistic and probit classification, which are useful for certain types of continuous and discrete data that are not normally distributed.

## 9.7.2 Classification Based on Density Estimators

In (9.8), (9.12), and (9.14) we have linear and quadratic classification rules based on the multivariate normal density and prior probabilities. These normal-based rules arose from Welch's optimal rule that assigns y to the group for which pi f ( y | Gi ) is maximum. If the form of f ( y | Gi ) is nonnormal and unknown, the density can be

estimated directly from the data. The approach we describe is known as the kernel estimator.

We first describe the kernel method for a univariate continuous random variable y . Suppose y has density f ( y ) , which we wish to estimate using a sample y 1 , y 2 , . . . , yn . A simple estimate of f ( y 0 ) for an arbitrary point y 0 can be based on the proportion of points in the interval ( y 0 -h , y 0 + h ) . If the number of points in the interval is denoted by N ( y 0 ) , then the proportion N ( y 0 )/ n is an estimate of P ( y 0 -h &lt; y &lt; y 0 + h ) , which is approximately equal to 2 hf ( y 0 ) . Thus we estimate f ( y 0 ) by

<!-- formula-not-decoded -->

We can express ˆ f ( y 0 ) as a function of all yi in the sample by defining

<!-- formula-not-decoded -->

so that N ( y 0 ) = 2 ∑ n i = 1 K [ ( y 0 -yi )/ h ] , and (9.20) becomes

<!-- formula-not-decoded -->

The function K ( u ) is called the kernel . In (9.22), K [ ( y 0 -yi )/ h ] is 1 2 for any point yi in the interval ( y 0 -h , y 0 + h ) and is zero for points outside the interval. Points in the interval add 1 / 2 hn to the density and points outside the interval contribute nothing.

Kernel estimators were first proposed by Rosenblatt (1956) and Parzen (1962). A good review of nonparametric density estimation including kernel estimators has been given by Silverman (1986), who noted that classification analysis provided the initial motivation for the development of density estimation.

The kernel defined by (9.21) is rectangular, and the graph of ˆ f ( y 0 ) plotted as a function of y 0 will be a step function, since there will be a jump (or drop) whenever y 0 is a distance h from one of the yi 's. (A moving average has a similar property.) To obtain a smooth estimator of f ( y ) , we must choose a smooth kernel. Two possibilities are

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

which have the property that all n sample points y 1, y 2 , . . . , yn contribute to ˆ f ( y 0 ) , with the closest points weighted heavier than the more distant points. Even though K ( u ) in (9.24) has the form of the normal distribution, this does not imply any

assumption about the density f ( y ) . We have used the normal density function because it is symmetric and unimodal. Other density functions could be used as kernels.

Cacoullos (1966) provided kernel estimates for multivariate density functions; see also Scott (1992). If y ′ 0 = ( y 01 , y 02 , . . . , y 0 p ) is an arbitrary point whose density we wish to estimate, then the extension of (9.22) is

<!-- formula-not-decoded -->

An estimate ˆ f ( y 0 ) based on a multivariate normal kernel is given by

<!-- formula-not-decoded -->

where h 1 = h 2 = · · · = h p = h and S pl is the pooled covariance matrix from the k groups in the sample. The covariance matrix S pl could be replaced by other forms. Two examples are (1) S i for the i th group and (2) a diagonal matrix.

The choice of the smoothing parameter h is critical in a kernel density estimator. The size of h determines how much each y i contributes to ˆ f ( y 0 ) . If h is too small, ˆ f ( y 0 ) has a peak at each y i , and if h is too large, ˆ f ( y 0 ) is almost uniform (overly smoothed). Therefore, the value chosen for h must depend on the sample size n to avoid too much or too little smoothing; the larger the sample size, the smaller h should be. In practice, we could try several values of h and check the resulting error rates from the classification analysis.

To use the kernel method of density estimation in classification, we can apply it to each group to obtain ˆ f ( y 0 | G 1 ) , ˆ f ( y 0 | G 2 ), . . . , ˆ f ( y 0 | Gk ) , where y 0 is the vector of measurements for an individual of unknown group membership. The classification rule then becomes: Assign y 0 to the group Gi for which

<!-- formula-not-decoded -->

Habbema, Hermans, and Van den Broek (1974) proposed a forward selection method for classification based on density estimation. Wegman (1972) and Habbema, Hermans, and Remme (1978) found that the size of the hi 's is more important than the shape of the kernel. The choice of h was investigated by Pfeiffer (1985) in a stepwise mode. Remme, Habbema, and Hermans (1980) compared linear, quadratic, and kernel classification methods for two groups and reported that for multivariate normal data with equal covariance matrices, the linear classifications were clearly superior. For some cases with departures from these assumptions, the kernel methods gave better results.

Example 9.7.2. We illustrate the density estimation method of classification for the football data of Table 8.3. We use the multivariate normal kernel estimator in (9.26) with h = 2 to obtain ˆ f ( y 0 | Gi ) , i = 1, 2, 3, for the three groups. Using p 1 =

p 2 = p 3, the rule in (9.27) becomes: Assign y 0 to the group for which ˆ f ( y 0 | Gi ) is greatest. To obtain an apparent error rate, we follow this procedure for each of the 90 observations and obtain the classification results in Table 9.8.

Applying a holdout method in which the observation y i j being classified is excluded from computation of ˆ f ( y i j | G 1 ) , ˆ f ( y i j | G 2 ) , and ˆ f ( y i j | G 3 ) , we obtain the classification results in Table 9.9. As expected, the holdout error rate has increased somewhat from the apparent error rate in Table 9.8.

## 9.7.3 Nearest Neighbor Classification Rule

The earliest nonparametric classification method was the nearest neighbor rule of Fix and Hodges (1951), also known as the k nearest neighbor rule. The procedure is conceptually simple. We compute the distance from an observation y i to all other points y j using the distance function

<!-- formula-not-decoded -->

Table 9.8. Classification Table for the Football Data of Table 8.3 Using the Density Estimation Method of Classification with Multivariate Normal Kernel

| Actual   | Number of    |   Predicted Group |   Predicted Group |   Predicted Group |
|----------|--------------|-------------------|-------------------|-------------------|
| Group    | Observations |                 1 |                 2 |                 3 |
| 1        | 30           |                25 |                 1 |                 4 |
| 2        | 30           |                 0 |                12 |                18 |
| 3        | 30           |                 0 |                 3 |                27 |

Apparent correct classification rate = 25 + 12 + 90

=

27

.

711

Apparent error rate = 1 -. 711 = . 289

Table 9.9. Classification Table for the Football Data of Table 8.3 Using the Holdout Method Based on Density Estimation

| Actual   | Number of    |   Predicted Group |   Predicted Group |   Predicted Group |
|----------|--------------|-------------------|-------------------|-------------------|
| Group    | Observations |                 1 |                 2 |                 3 |
| 1        | 30           |                24 |                 1 |                 5 |
| 2        | 30           |                 0 |                10 |                20 |
| 3        | 30           |                 1 |                 3 |                26 |

Correct classification rate = 90 = . 667

24 + 10 + 26

Error rate = 71 -. 667 = . 333

To classify y i into one of two groups, the k points nearest to y i are examined, and if the majority of the k points belong to G 1, assign y i to G 1; otherwise assign y i to G 2. If we denote the number of points from G 1 as k 1, with the remaining k 2 points from G 2, where k = k 1 + k 2, then the rule can be expressed as: Assign y i to G 1 if

<!-- formula-not-decoded -->

and to G 2 otherwise. If the sample sizes n 1 and n 2 differ, we may wish to use proportions in place of counts: Assign y i to G 1 if

<!-- formula-not-decoded -->

A further refinement can be made by taking into account prior probabilities: Assign y i to G 1 if

<!-- formula-not-decoded -->

These rules are easily extended to more than two groups. For example, (9.29) becomes: Assign the observation to the group that has the highest proportion ki / ni , where ki is the number of observations from Gi among the k nearest neighbors of the observation in question.

A decision must be made as to the value of k . Loftsgaarden and Quesenberry (1965) suggest choosing k near √ n i for a typical ni . In practice, one could try several values of k and use the one with the best error rate.

Reviews and extensions of the nearest neighbor method have been given by Hart (1968), Gates (1972), Hand and Batchelor (1978), Chidananda Gowda and Krishna (1979), Rogers and Wagner (1978), and Brown and Koplowitz (1979).

Example 9.7.3. Weuse the football data of Table 8.3 to illustrate the k nearest neighbor method of estimating error rate, with k = 5. Since n 1 = n 2 = n 3 = 30 and the pi 's are also assumed to be equal, we simply examine the five points closest to a

Table 9.10. Classification Table for the Football Data of Table 8.3 Using the k Nearest Neighbor Method with k = 5

| Actual   | Number of    | Predicted Group   |   Predicted Group |
|----------|--------------|-------------------|-------------------|
| Group    | Observations | 1 2               |                 3 |
| 1        | 30           | 26 0              |                 1 |
| 2        | 30           | 1 19              |                 9 |
| 3        | 30           | 1 4               |                22 |

Correct classification rate = 26 + 19 + 22 83 = . 807

Error rate = 1 -. 807 = . 193

point y and classify y into the group that has the most points among the five points. If there is a tie, we do not classify the point. For example, if the numbers from G 1, G 2, and G 3 were 1, 2, and 2, respectively, then we do not assign y to either G 2 or G 3.

For each point y i j , i = 1, 2, 3; j = 1, 2 , . . . , 30, we find the five nearest neighbors and classify the point accordingly. Table 9.10 gives the classification results. As can be seen, there were 3 observations in group 1 that were not classified because of ties, 1 in group 2, and 3 in group 3. This left a total of 83 observations classified.

## PROBLEMS

- 9.1 Show that if z 1 i = a ′ y 1 i , i = 1, 2 , . . . , n 1, and z 2 i = a ′ y 2 i , i = 1, 2 , . . . , n 2, where z is the discriminant function defined in (9.1), then z 1 -z 2 = ( y 1 -y 2 ) ′ S -1 pl ( y 1 -y 2 ) as in (9.3).
- 9.2 With z = a ′ y as in (9.1) and z 1 = a ′ y 1 , z 2 = a ′ y 2 , show that 1 2 ( z 1 + z 2 ) = 1 2 ( y 1 -y 2 ) ′ S -1 pl ( y 1 + y 2 ) as in (9.4).
- 9.3 Obtain the normal-based classification rule in (9.8).
- 9.4 Derive the linear classification rule in (9.12).
- 9.5 Derive the quadratic classification function in (9.14).
- 9.6 Do a classification analysis on the beetle data in Table 5.5 as follows:
- (a) Find the classification function z = ( y 1 -y 2 ) ′ S -1 pl y and the cutoff point 1 2 ( z 1 + z 2 ) .
- (b) Find the classification table using the linear classification function in part (a).
- (c) Find the classification table using the nearest neighbor method.
- 9.7 Do a classification analysis on the dystrophy data of Table 5.7 as follows:
- (a) Find the classification function z = ( y 1 -y 2 ) ′ S -1 pl y and the cutoff point 1 2 ( z 1 + z 2 ) .
- (b) Find the classification table using the linear classification function in part (a).
- (c) Repeat part (b) using p 1 and p 2 proportional to sample sizes.
- 9.8 Do a classification analysis on the cyclical data of Table 5.8 as follows:
- (a) Find the classification function z = ( y 1 -y 2 ) ′ S -1 pl y and the cutoff point 1 2 ( z 1 + z 2 ) .
- (b) Find the classification table using the linear classification function in part (a).
- (c) Find the classification table using the holdout method.
- (d) Find the classification table using a kernel density estimator method.

- 9.9 Using the engineer data of Table 5.6, carry out a classification analysis as follows:
- (a) Find the classification table using the linear classification function.
- (b) Carry out a stepwise discriminant selection of variables (see Problem 8.14).
- (c) Find the classification table for the variables selected in part (b).
- 9.10 Do a classification analysis on the fish data in Table 6.17 as follows. Assume p 1 = p 2 = p 3.
- (a) Find the linear classification functions.
- (b) Find the classification table using the linear classification functions in part (a) (assuming 𝚺 1 = 𝚺 2 = 𝚺 3).
- (c) Find the classification table using quadratic classification functions (assuming population covariance matrices are not equal).
- (d) Find the classification table using linear classification functions and the holdout method.
- (e) Find the classification table using a nearest neighbor method.
- 9.11 Do a classification analysis on the rootstock data of Table 6.2 as follows:
- (a) Find the linear classification functions.
- (b) Find the classification table using the linear classification functions in part (a) (assuming 𝚺 1 = 𝚺 2 = 𝚺 3).
- (c) Find the classification table using quadratic classification functions (assuming population covariance matrices are not equal).
- (d) Find the classification table using the nearest neighbor method.
- (e) Find the classification table using a kernel density estimator method.

## C H A P T E R 10

## Multivariate Regression

## 10.1 INTRODUCTION

In this chapter, we consider the linear relationship between one or more y 's (the dependent or response variables) and one or more x 's (the independent or predictor variables). We will use a linear model to relate the y 's to the x 's and will be concerned with estimation and testing of the parameters in the model. One aspect of interest will be choosing which variables to include in the model if this is not already known.

We can distinguish three cases according to the number of variables:

1. Simple linear regression: one y and one x . For example, suppose we wish to predict college grade point average (GPA) based on an applicant's high school GPA.
2. Multiple linear regression: one y and several x 's. We could attempt to improve our prediction of college GPA by using more than one independent variable, for example, high school GPA, standardized test scores (such as ACT or SAT), or rating of high school.
3. Multivariate multiple linear regression: several y 's and several x 's. In the preceding illustration, we may wish to predict several y 's (such as number of years of college the person will complete or GPA in the sciences, arts, and humanities). As another example, suppose the Air Force wishes to predict several measures of pilot efficiency. These response variables could be regressed against independent variables (such as math and science skills, reaction time, eyesight acuity, and manual dexterity).

To further distinguish case 2 from case 3, we could designate case 2 as univariate multiple regression because there is only one y . Thus in case 3, multivariate indicates that there are several y 's and multiple implies several x 's. The term multivariate regression usually refers to case 3.

There are two basic types of independent variables, fixed and random . In the preceding illustrations, all x 's are random variables and are therefore not under the control of the researcher. A person is chosen at random, and all the y 's and x 's are

measured, or observed, for that person. In some experimental situations, the x 's are fixed, that is, under the control of the experimenter. For example, a researcher may wish to relate yield per acre and nutritional value to level of application of various chemical fertilizers. The experimenter can choose the amount of chemicals to be applied and then observe the changes in the yield and nutritional responses.

In order to provide a solid base for multivariate multiple regression, we review several aspects of multiple regression with fixed x 's in Section 10.2. The randomx case for multiple regression is discussed briefly in Section 10.3.

## 10.2 MULTIPLE REGRESSION: FIXED x 's

## 10.2.1 Model for Fixed x 's

In the fixedx regression model, we express each y in a sample of n observations as a linear function of the x 's plus a random error, ε :

<!-- formula-not-decoded -->

The number of x 's is denoted by q . The β 's in (10.1) are called regression coefficients . Additional assumptions that accompany the equations of the model are as follows:

1. E (ε i ) = 0 for all i = 1, 2 , . . . , n .
3. cov (ε i , ε j ) = 0 for all i /negationslash= j .
2. var (ε i ) = σ 2 for all i = 1, 2 , . . . , n .

Assumption 1 states that the model is linear and that no additional terms are needed to predict y ; all remaining variation in y is purely random and unpredictable. Thus if E (ε i ) = 0 and the x 's are fixed, then E ( yi ) = β 0 + β 1 xi 1 + β 2 xi 2 +··· + β q xiq , and the mean of y is expressible in terms of these q x 's with no others needed. In assumption 2, the variance of each ε i is the same, which also implies that var ( yi ) = σ 2 , since the x 's are fixed. Assumption 3 imposes the condition that the error terms be uncorrelated, from which it follows that the y 's are also uncorrelated, that is, cov ( yi , y j ) = 0.

Thus the three assumptions can be restated in terms of y as follows:

1. E ( yi ) = β 0 + β 1 xi 1 + β 2 xi 2 +··· + β q xiq , i = 1, 2 , . . . , n .
3. cov ( yi , y j ) = 0, for all i /negationslash= j .
2. var ( yi ) = σ 2 , i = 1, 2 , . . . , n .

Using matrix notation, the models for the n observations in (10.1) can be written much more concisely in the form or

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

With this notation, the preceding three assumptions become

<!-- formula-not-decoded -->

which can be rewritten in terms of y as

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Note that the second assumption in matrix form incorporates both the second and third assumptions in univariate form; that is, cov ( y ) = σ 2 I implies var ( yi ) = σ 2 and cov ( yi , y j ) = 0.

For estimation and testing purposes, we need to have n &gt; q + 1. Therefore, the matrix expression (10.3) has the following typical pattern:

<!-- image -->

## 10.2.2 Least Squares Estimation in the Fixedx Model

If the first assumption holds, we have E ( yi ) = β 0 + β 1 xi 1 + β 2 xi 2 +··· + β q xiq . We seek to estimate the β 's and thereby estimate E ( yi ) . If the estimates are denoted

by ˆ β 0, ˆ β 1 , . . . , ˆ β q , then ˆ E ( yi ) = ˆ β 0 + ˆ β 1 xi 1 + ˆ β 2 xi 2 +···+ ˆ β q xiq . However, ˆ E ( yi ) is usually designated ˆ yi . Thus ˆ yi estimates E ( yi ) , not yi . We now consider the least squares estimates of the β 's.

The least squares estimates of β 0, β 1 , . . . , β q minimize the sum of squares of deviations of the n observed y 's from their 'modeled' values, that is, from their values ˆ yi predicted by the model. Thus we seek ˆ β 0, ˆ β 1 , . . . , ˆ β q that minimize

<!-- formula-not-decoded -->

The value of ˆ 𝛃 = ( ˆ β 0 , ˆ β 1 , . . . , ˆ β q ) ′ that minimizes SSE in (10.4) is given by

<!-- formula-not-decoded -->

In (10.5), we assume that X ′ X is nonsingular. This will ordinarily hold if n &gt; q + 1 and no x j is a linear combination of other x 's.

We now demonstrate algebraically that ˆ 𝛃 = ( X ′ X ) -1 X ′ y in (10.5) minimizes SSE (this can also be done readily with calculus). If we designate the i th row of X as x ′ i = ( 1 , xi 1 , xi 2 , . . . , xiq ) , we can write (10.4) as

In expression (10.5), we see a characteristic pattern similar to that for ˆ β 1 in simple linear regression given in (3.11), ˆ β 1 = sxy / s 2 x . The product X ′ y can be used to compute the covariances of the x 's with y . The product X ′ X can be used to obtain the covariance matrix of the x 's, which includes the variances and covariances of the x 's [see the comment following (10.16) about variances and covariances involving the fixed x 's]. Since X ′ X is typically not diagonal, each ˆ β j depends on sx j y and s 2 x j as well as the relationship of x j to the other x 's.

<!-- formula-not-decoded -->

The quantity yi -x ′ i ˆ 𝛃 is the i th element of the vector y -X ˆ 𝛃 . Hence, by (2.33),

<!-- formula-not-decoded -->

Let b be an alternative estimate that may lead to a smaller value of SSE than does ˆ 𝛃 . We add X ( ˆ 𝛃 -b ) to see if this reduces SSE.

<!-- formula-not-decoded -->

We now expand this using the two terms y -X ˆ 𝛃 and X ( ˆ 𝛃 -b ) to obtain

<!-- formula-not-decoded -->

The third term vanishes if we substitute ˆ 𝛃 = ( X ′ X ) -1 X ′ y into X ′ X ˆ 𝛃 . The second term is a positive definite quadratic form, and SSE is therefore minimized when b = ˆ 𝛃 . Thus no value of b can reduce SSE from the value given by ˆ 𝛃 . For a review of properties of ˆ 𝛃 and an alternative derivation of ˆ 𝛃 based on the assumption that y is normally distributed, see Rencher (1998, Chapter 7; 2000, Chapter 7).

## 10.2.3 An Estimator for 𝛔 2

It can be shown that

<!-- formula-not-decoded -->

We can therefore obtain an unbiased estimator of σ 2 as

<!-- formula-not-decoded -->

We can also express SSE in the form

<!-- formula-not-decoded -->

and we note that there are n terms in y ′ y and q + 1 terms in ˆ 𝛃 ′ X ′ y . The difference is the denominator of s 2 in (10.8). Thus the degrees of freedom (denominator) for SSE are reduced by q + 1.

<!-- formula-not-decoded -->

The need for an adjustment of q + 1 to the degrees of freedom of SSE can be illustrated with a simple random sample of a random variable y from a population with mean µ and variance σ 2 . The sum of squares ∑ i ( yi -µ) 2 has n degrees of freedom, whereas ∑ i ( yi -y ) 2 has n -1. It is intuitively clear that because y fits the sample better than µ , which is the mean of the population but not of the sample. Thus (squared) deviations from y will tend to be smaller than deviations from µ . In fact, it is easily shown that

whence

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Thus ∑ i ( yi -y ) 2 is expressible as a sum of n squares minus 1 square, which corresponds to n -1 degrees of freedom. More formally, we have

<!-- formula-not-decoded -->

## 10.2.4 The Model Corrected for Means

It is sometimes convenient to 'center' the x 's by subtracting their means, x 1 = ∑ n i = 1 xi 1 / n , x 2 = ∑ n i = 1 xi 2 / n , and so on [ x 1, x 2 , . . . , xq are the means of the columns of X in (10.2)]. In terms of centered x 's, the model for each yi in (10.1) becomes

<!-- formula-not-decoded -->

where

To estimate we use the centered x 's in the matrix

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

where x ′ i = ( xi 1 , xi 2 , . . . , xiq ) and x ′ = ( x 1 , x 2 , . . . , xq ) . Then by analogy to (10.5), the least squares estimate of 𝛃 1 is

<!-- formula-not-decoded -->

If E ( y ) = β 0 + β 1 x 1 +··· + β q xq is evaluated at x 1 = x 1 , x 2 = x 2 , . . . , xq = xq , the result is the same as α in (10.12). Thus, we estimate α by y :

<!-- formula-not-decoded -->

In other words, if the origin of the x 's is shifted to x = ( x 1 , x 2 , . . . , xq ) ′ , then the intercept of the fitted model is y . With ˆ α = y , we obtain

<!-- formula-not-decoded -->

as an estimate of β 0 in (10.12). Together, the estimators ˆ β 0 and ˆ 𝛃 1 in (10.15) and (10.14) are the same as the usual least squares estimator ˆ 𝛃 = ( X ′ X ) -1 X ′ y in (10.5).

We can express ˆ 𝛃 1 in (10.14) in terms of sample variances and covariances. The overall sample covariance matrix of y and the x 's is

<!-- formula-not-decoded -->

To express ˆ 𝛃 1 in terms of S xx and s yx in (10.16), we note first that the diagonal elements of X ′ c X c are corrected sums of squares. For example, in the second diagonal position, we have where syy is the variance of y , syj is the covariance of y and x j , s j j is the variance of x j , s jk is the covariance of x j and xk , and s ′ yx = ( sy 1 , sy 2 , . . . , syq ) . These sample variances and covariances are mathematically equivalent to analogous formulas (3.23) and (3.25) for random variables, where the sample variances and covariances were estimates of population variances and covariances. However, here the x 's are considered to be constants that remain fixed from sample to sample, and a formula such as s 11 = ∑ n i = 1 ( xi 1 -x 1 ) 2 /( n -1 ) summarizes the spread in the n values of x 1 but does not estimate a population variance.

<!-- formula-not-decoded -->

The off-diagonal elements of X ′ c X c are analogous corrected sums of products; for example, the element in the ( 1 , 2 ) position is

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Thus

Similarly,

<!-- formula-not-decoded -->

even though y has not been centered. The second element of X ′ c y , for example, is ∑ i ( xi 2 -x 2 ) yi , which is equal to ( n -1 ) s 2 y :

since

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Now, multiplying and dividing by n -1 in (10.14), we obtain

<!-- formula-not-decoded -->

and substituting this in (10.15) gives

<!-- formula-not-decoded -->

## 10.2.5 Hypothesis Tests

In this section, we review two basic tests on the β 's. For other tests and confidence intervals, see Rencher (1998, Section 7.2.4; 2000, Sections 8.4-8.7). In order to obtain F -tests, we assume that y is Nn ( X 𝛃 , σ 2 I ) .

## 10.2.5a Test of Overall Regression

The overall regression hypothesis that none of the x 's predict y can be expressed as H 0 : 𝛃 1 = 0 , since 𝛃 ′ 1 = (β 1 , β 2 , . . . , β q ) . We do not include β 0 = 0 in the hypothesis so as not to restrict y to have an intercept of zero.

We can write SSE = y ′ y - ˆ 𝛃 ′ X ′ y in (10.9) in the form

<!-- formula-not-decoded -->

which partitions y ′ y into a part due to 𝛃 and a part due to deviations from the fitted model.

To correct y for its mean and thereby avoid inclusion of β 0 = 0, we subtract ny 2 from both sides of (10.22) to obtain

<!-- formula-not-decoded -->

where y ′ y -ny 2 = ∑ i ( yi -y ) 2 is the total sum of squares adjusted for the mean and SSR = ˆ 𝛃 ′ X ′ y -ny 2 is the overall regression sum of squares adjusted for the intercept.

We can test H 0 : 𝛃 1 = 0 by means of

<!-- formula-not-decoded -->

which is distributed as Fq , n -q -1 when H 0 : 𝛃 1 = 0 is true. We reject H 0 if F &gt; F α, q , n -q , -1.

## 10.2.5b Test on a Subset of the 𝛃 's

In an attempt to simplify the model, we may wish to test the hypothesis that some of the β 's are zero. For example, in the model

<!-- formula-not-decoded -->

we may be interested in the hypothesis H 0 : β 3 = β 4 = β 5 = 0. If H 0 is true, the model is linear in x 1 and x 2. In other cases, we may want to ascertain whether a single β j can be deleted.

For convenience of exposition, let the β 's that are candidates for deletion be rearranged to appear last in 𝛃 and denote this subset of β 's by 𝛃 d , where d reminds us that these β 's are to be deleted if H 0 : 𝛃 d = 0 is accepted. Let the subset to be retained in the reduced model be denoted by 𝛃 r . Thus 𝛃 is partitioned into

<!-- formula-not-decoded -->

Let h designate the number of parameters in 𝛃 d . Then there are q + 1 -h parameters in 𝛃 r .

To test the hypothesis H 0 : 𝛃 d = 0 , we fit the full model containing all the β 's in 𝛃 and then fit the reduced model containing only the β 's in 𝛃 r . Let X r be the columns of X corresponding to 𝛃 r . Then the reduced model can be written as

<!-- formula-not-decoded -->

and 𝛃 r is estimated by ˆ 𝛃 r = ( X ′ r X r ) -1 X ′ r y . To compare the fit of the full model and the reduced model, we calculate

<!-- formula-not-decoded -->

where ˆ 𝛃 ′ X ′ y is the regression sum of squares from the full model and ˆ 𝛃 ′ r X ′ r y is the regression sum of squares for the reduced model. The difference in (10.26) shows what 𝛃 d contributes 'above and beyond' 𝛃 r . We can test H 0 : 𝛃 d = 0 with an F -statistic:

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

where SSR f = ˆ 𝛃 ′ X ′ y and SSR r = ˆ 𝛃 ′ r X ′ r y . The F -statistic in (10.27) and (10.28) is distributed as Fh , n -q -1 if H 0 is true. We reject H 0 if F &gt; F α, h , n -q -1.

The test in (10.27) is easy to carry out in practice. We fit the full model and obtain the regression and error sums of squares ˆ 𝛃 ′ X ′ y and y ′ y - ˆ 𝛃 ′ X ′ y , respectively. We then fit the reduced model and obtain its regression sum of squares ˆ 𝛃 ′ r X ′ r y to be subtracted from ˆ 𝛃 ′ X ′ y . If a software package gives the regression sum of squares in corrected form, this can readily be used to obtain ˆ 𝛃 ′ X ′ y - ˆ 𝛃 ′ r X ′ r y , since

<!-- formula-not-decoded -->

Alternatively, we can obtain ˆ 𝛃 ′ X ′ y - ˆ 𝛃 ′ r X ′ r y as the difference between error sums of squares for the two models:

<!-- formula-not-decoded -->

A test for an individual β j above and beyond the other β 's is readily obtained using (10.27). To test H 0 : β j = 0, we arrange β j last in 𝛃 ,

<!-- formula-not-decoded -->

where 𝛃 r = (β 0 , β 1 , . . . , β q -1 ) ′ contains all the β 's except β j . By (10.27), the test statistic is

<!-- formula-not-decoded -->

which is F 1 , n -q -1. Note that h = 1. The test of H 0 : β j = 0 by the F -statistic in (10.29) is called a partial F-test . A detailed breakdown of the effect of each variable in the presence of the others is given by Rencher (1993; 2000, Section 10.5).

Since the F -statistic in (10.29) has 1 and n -q -1 degrees of freedom, it is the square of a t -statistic. The t -statistic equivalent to (10.29) is

<!-- formula-not-decoded -->

where gjj is the j th diagonal element of ( X ′ X ) -1 and s = √ SSE f /( n -q -1 ) (Rencher 2000, Section 8.5.1).

## 10.2.6 R 2 in Fixedx Regression

The proportion of the (corrected) total variation in the y 's that can be attributed to regression on the x 's is denoted by R 2 :

<!-- formula-not-decoded -->

The ratio R 2 is called the coefficient of multiple determination , or more commonly the squared multiple correlation . The multiple correlation R is defined as the positive square root of R 2 .

The F -test for overall regression in (10.24) can be expressed in terms of R 2 as

<!-- formula-not-decoded -->

For the reduced model (10.25), R 2 can be written as

<!-- formula-not-decoded -->

Then in terms of R 2 and R 2 r , the full and reduced model test in (10.27) for H 0 : 𝛃 d = 0 becomes

<!-- formula-not-decoded -->

[see (11.36)].

We can express R 2 in terms of sample variances, covariances, and correlations:

<!-- formula-not-decoded -->

where syy , s yx , and S xx are defined in (10.16) and r yx and R xx are from an analogous partitioning of the sample correlation matrix of y and the x 's:

<!-- formula-not-decoded -->

## 10.2.7 Subset Selection

In practice, one often has more x 's than are needed for predicting y . Some of them may be redundant and could be discarded. In addition to logistical motivations for deleting variables, there are statistical incentives; for example, if an x is deleted from the fitted model, the variances of the ˆ β j 's and of the ˆ yi 's are reduced. Various aspects of model validation are reviewed by Rencher (2000, Section 7.9 and Chapter 9).

The two most popular approaches to subset selection are to (1) examine all possible subsets and (2) use a stepwise technique. We discuss these in the next two sections.

## 10.2.7a All Possible Subsets

The optimal approach to subset selection is to examine all possible subsets of the x 's. This may not be computationally feasible if the sample size and number of variables are large. Some programs take advantage of algorithms that find the optimum subset of each size without examining all of the subsets [see, for example, Furnival and Wilson (1974)].

Wediscuss three criteria for comparing subsets when searching for the best subset. To conform with established notation in the literature, the number of variables in a subset is denoted by p -1, so that with the inclusion of an intercept, there are p parameters in the model. The corresponding total number of available variables from which a subset is to be selected is denoted by k -1, with k parameters in the model.

1. R 2 p . By its definition in (10.30) as the proportion of total (corrected) sum of squares accounted for by regression, R 2 is clearly a measure of model fit. The subscript p is an index of the subset size, since it indicates the number of parameters in the model, including an intercept. However, R 2 p does not reach a maximum for any value of p less than k because it cannot decrease when a variable is added to the model. The usual procedure is to find the subset with largest R 2 p for each of p = 2, 3 , . . . , k and then choose a value of p beyond which the increases in R 2 appear to be unimportant. This judgment is, of course, subjective.
2. s 2 p . Another useful criterion is the variance estimator for each subset as defined in (10.8):

<!-- formula-not-decoded -->

For each of p = 2, 3 , . . . , k , we find the subset with smallest s 2 p . If k is fairly large, a typical pattern as p approaches k is for the minimal s 2 p to decrease to an overall minimum less than s 2 k and then increase. The minimum value of s 2 p can be less than s 2 k if the decrease in SSE p with an additional variable does not offset the loss of a degree of freedom in the denominator. It is often suggested that the researcher choose the subset with absolute minimum s 2 p . However, as Hocking (1976, p. 19) notes, this procedure may fit some noise unique to the sample and thereby include one or more extraneous predictor variables. An alternative suggestion is to choose p such that min p s 2 p = s 2 k or, more precisely, choose the smallest value of p such that min p s 2 p &lt; s 2 k , since there will not be a p &lt; k such that min p s 2 p is exactly equal to s 2 k .

3. Cp . The Cp criterion is due to Mallows (1964, 1973). In the following development, we follow Myers (1990, pp. 180-182). The expected squared error , E [ ˆ yi -E ( yi ) ] 2 , is used in formulating the Cp criterion because it incorporates a variance component and a bias component. The goal is to find a model that achieves a good balance between the bias and variance of the fitted values ˆ yi . Bias arises when the ˆ yi values are based on an incorrect model, in which E ( ˆ yi ) /negationslash= E ( yi ) . If ˆ yi were based on the correct model, so that E ( ˆ yi ) = E ( yi ) , then E [ ˆ yi -E ( yi ) ] 2 would be equal to var ( ˆ yi ) . In general, however, as we examine many competing models, for various values of p , ˆ yi is not based on the correct model, and we have (see Problem 10.4)

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

For a given value of p , the total expected squared error for the n observations in the sample, standardized by dividing by σ 2 , becomes

<!-- formula-not-decoded -->

Before defining Cp as an estimate of (10.39), we can achieve some simplification. We first show that ∑ i var ( ˆ yi )/σ 2 is equal to p . Let the model for all n observations be designated by

<!-- formula-not-decoded -->

We assume that, in general, this prospective model is underspecified and that the true model (which produces σ 2 ) contains additional β 's and additional columns of the X matrix. If we designate the i th row of X p by x ′ pi , then the first term on the right side of (10.39) becomes (see also Problem 10.5)

<!-- formula-not-decoded -->

It can be shown (Myers 1990, pp. 178-179) that

<!-- formula-not-decoded -->

Using (10.41) and (10.42), the final simplified form of the (standardized) total expected squared error in (10.39) is

<!-- formula-not-decoded -->

In practice, σ 2 is usually estimated by s 2 k , the MSE from the full model. We thus estimate (10.43) by

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

In (10.44), we see that if the bias is small for a particular model, Cp will be close to p . For this reason, the line Cp = p is commonly plotted along with the Cp values of several candidate models. We look for small values of Cp that are near this line.

In a Monte Carlo study, Hilton (1983) compared several subset selection criteria based on MSE p and Cp . The three best procedures were to choose (1) the subset with the smallest p such that Cp &lt; p , (2) the subset with the smallest p such that s 2 p &lt; s 2 k , and (3) the subset with minimum s 2 p . The first of these was found to give best results overall, with the second method close behind. The third method performed best in some cases where k was small.

## 10.2.7b Stepwise Selection

For many data sets, it may be impractical to examine all possible subsets, even with an efficient algorithm such as that of Furnival and Wilson (1974). In such cases, we

An alternative form is

can use the familiar stepwise approach, which is widely available and has virtually no limit as to the number of variables or observations. A related stepwise technique was discussed in Sections 6.11.2 and 8.9 in connection with selection of dependent variables to separate groups in a MANOVA or discriminant analysis setting. In this section, we are concerned with selecting the independent variables ( x 's) that best predict the dependent variable ( y ) in regression.

We first review the forward selection procedure, which typically uses an F -test at each step. At the first step, y is regressed on each x j alone, and the x with the largest F -value is 'entered' into the model. At the second step, we search for the variable with the largest partial F -value for testing the significance of each variable in the presence of the variable first entered. Thus, if we denote the first variable to enter as x 1, then at the second step we calculate the partial F -statistic

<!-- formula-not-decoded -->

for each j /negationslash= 1 and choose the variable that maximizes F , where MSR = ( SSR f -SSR r )/ h and MSE = SSE f /( n -q -1 ) are the mean squares for regression and error, respectively, as in (10.28). In this case, SSR f = SSR ( x 1 , x j ) and SSR r = SSR ( x 1 ) . Note also that h = 1 because only one variable is being added, and MSE is calculated using only the variable already entered plus the candidate variable. This procedure continues at each step until the largest partial F for an entering variable falls below a preselected threshold F -value or until the corresponding p -value exceeds some predetermined level.

The stepwise selection procedure similarly seeks the best variable to enter at each step. Then after a variable has entered, each of the variables previously entered is examined by a partial F -test to see if it is no longer significant and can be dropped from the model.

The backward elimination procedure begins with all x 's in the model and deletes one at a time. The partial F -statistic for each variable in the presence of the others is calculated, and the variable with smallest F is eliminated. This continues until the smallest F at some step exceeds a preselected threshold value.

Since these sequential methods do not examine all subsets, they will often fail to find the optimum subset, especially if k is large. However, R 2 p , s 2 p , or Cp may not differ substantially between the optimum subset and the one found by stepwise selection. These sequential methods have been popular for at least a generation, and it is very likely they will continue to be used, even though increased computing power has put the optimal methods within reach for larger data sets.

There are some possible risks in the use of stepwise methods. The stepwise procedure may fail to detect a true predictor (an x j for which β j /negationslash= 0) because s 2 p is biased upward in an underspecified model, thus artificially reducing the partial F -value. On the other hand, a variable that is not a true predictor of y (an x j for which β j = 0) may enter because of chance correlations in a particular sample. In the presence of such 'noise' variables, the partial F -statistic for the entering variable does not have an F -distribution because it is maximized at each step. The calculated p -values

become optimistic. This problem intensifies when the sample size is relatively small compared to the number of variables. Rencher and Pun (1980) found that in such cases some surprisingly large values of R 2 can occur, even when there is no relationship between y and the x 's in the population. In a related study, Flack and Chang (1987) included x 's that were authentic contributors as well as noise variables. They found that 'for most samples, a large percentage of the selected variables is noise, particularly when the number of candidate variables is large relative to the number of observations. The adjusted R 2 of the selected variables is highly inflated' (p. 84).

## 10.3 MULTIPLE REGRESSION: RANDOM x 's

In Section 10.2, it was assumed that the x 's were fixed and would have the same values if another sample were taken; that is, the same X matrix would be used each time a y vector was observed. However, many regression applications involve x 's that are random variables.

Thus in the randomx case, the values of x 1, x 2 , . . . , xq are not under the control of the experimenter. They occur randomly along with y . On each subject we observe y , x 1, x 2 , . . . , xq .

If we assume that ( y , x 1 , x 2 , . . . , xq ) has a multivariate normal distribution, then ˆ 𝛃 , R 2 , and the F -tests have the same formulation as in the fixedx case [for details, see Rencher (1998, Section 7.3; 2000, Section 10.4)]. Thus with the multivariate normal assumption, we can proceed with estimation and testing the same way in the randomx case as with fixed x 's.

## 10.4 MULTIVARIATE MULTIPLE REGRESSION: ESTIMATION

In this section we extend the estimation results of Sections 10.2.2-10.2.4 to the multivariate y case. We assume the x 's are fixed.

## 10.4.1 The Multivariate Linear Model

Weturn now to the multivariate multiple regression model , where multivariate refers to the dependent variables and multiple pertains to the independent variables. In this case, several y 's are measured corresponding to each set of x 's. Each of y 1, y 2 , . . . , yp is to be predicted by all of x 1, x 2 , . . . , xq .

The n observed values of the vector of y 's can be listed as rows in the following matrix:

<!-- formula-not-decoded -->

Thus each row of Y contains the values of the p dependent variables measured on a subject. Each column of Y consists of the n observations on one of the p variables and therefore corresponds to the y vector in the (univariate) regression model (10.3).

The n values of x 1, x 2 , . . . , xq can be placed in a matrix that turns out to be the same as the X matrix in the multiple regression formulation in Section 10.2.1:

<!-- formula-not-decoded -->

We assume that X is fixed from sample to sample.

Since each of the p y 's will depend on the x 's in its own way, each column of Y will need different β 's. Thus we have a column of β 's for each column of Y , and these columns form a matrix B = ( 𝛃 1 , 𝛃 2 , . . . , 𝛃 p ) . Our multivariate model is therefore

<!-- formula-not-decoded -->

where Y is n × p , X is n × ( q + 1 ) , and B is ( q + 1 ) × p . The notation 𝚵 (the uppercase version of 𝛏 ) is adopted here because of its resemblance to ε .

We illustrate the multivariate model with p = 2, q = 3:

<!-- formula-not-decoded -->

The model for the first column of Y is

<!-- formula-not-decoded -->

and for the second column, we have

<!-- formula-not-decoded -->

By analogy with the univariate case in Section 10.2.1, additional assumptions that lead to good estimates are as follows:

1. E ( Y ) = XB or E ( 𝚵 ) = O .
3. cov ( y i , y j ) = O for all i /negationslash= j .
2. cov ( y i ) = 𝚺 for all i = 1, 2 , . . . , n , where y ′ i is the i th row of Y .

Assumption 1 states that the linear model is correct and that no additional x 's are needed to predict the y 's. Assumption 2 asserts that each of the n observation vectors (rows) in Y has the same covariance matrix. Assumption 3 declares that observation vectors (rows of Y ) are uncorrelated with each other. Thus we assume that the y 's within an observation vector (row of Y ) are correlated with each other but independent of the y 's in any other observation vector.

The covariance matrix 𝚺 in assumption 2 contains the variances and covariances of yi 1, yi 2 , . . . , yip in any y i :

<!-- formula-not-decoded -->

The covariance matrix cov ( y i , y j ) = O in assumption 3 contains the covariances of each of yi 1, yi 2 , . . . , yip with each of y j 1, y j 2 , . . . , y j p :

<!-- formula-not-decoded -->

## 10.4.2 Least Squares Estimation in the Multivariate Model

By analogy with the univariate case in (10.5), we estimate B with

<!-- formula-not-decoded -->

We call ˆ B the least squares estimator for B because it 'minimizes' E = ˆ 𝚵 ′ ˆ 𝚵 , a matrix analogous to SSE:

<!-- formula-not-decoded -->

The matrix ˆ B minimizes E in the following sense. If we let B 0 be an estimate that may possibly be better than ˆ B and add X ˆ B -XB 0 to Y -X ˆ B , we find that this adds a positive definite matrix to E = ( Y -X ˆ B ) ′ ( Y -X ˆ B ) (Rencher 1998, Section 7.4.2). Thus we cannot improve on ˆ B . The least squares estimate ˆ B also minimizes the scalar quantities tr ( Y -X ˆ B ) ′ ( Y -X ˆ B ) and | ( Y -X ˆ B ) ′ ( Y -X ˆ B ) | . Note that by (2.98) tr ( Y -X ˆ B ) ′ ( Y -X ˆ B ) = ∑ n i = 1 ∑ p j = 1 ˆ ε 2 i j .

Wenoted earlier that in the model Y = XB + 𝚵 , there is a column of B corresponding to each column of Y ; that is, each y j , j = 1, 2 , . . . , p , is predicted differently by x 1, x 2 , . . . , xq . (This is illustrated in Section 10.4.1 for p = 2.) In the estimate ˆ B = ( X ′ X ) -1 X ′ Y , we have a similar pattern. The matrix product ( X ′ X ) -1 X ′ is multiplied by each column of Y [see (2.48)]. Thus the j th column of ˆ B is the usual least squares estimate ˆ 𝛃 for the j th dependent variable y j . To give this a more precise expression, let us denote the p columns of Y by y ( 1 ) , y ( 2 ) , . . . , y ( p ) to distinguish them from the n rows y ′ i , i = 1, 2 , . . . , n . Then

<!-- formula-not-decoded -->

Example 10.4.2. The results of a planned experiment involving a chemical reaction are given in Table 10.1 (Box and Youle 1955).

The input (independent) variables are

<!-- formula-not-decoded -->

Table 10.1. Chemical Reaction Data

| Experiment   | Yield Variables   | Yield Variables   | Yield Variables   | Input Variables   | Input Variables   | Input Variables   |
|--------------|-------------------|-------------------|-------------------|-------------------|-------------------|-------------------|
| Number       | y 1               | y 2               | y 3               | x 1               | x 2               | x 3               |
| 1            | 41.5              | 45.9              | 11.2              | 162               | 23                | 3                 |
| 2            | 33.8              | 53.3              | 11.2              | 162               | 23                | 8                 |
| 3            | 27.7              | 57.5              | 12.7              | 162               | 30                | 5                 |
| 4            | 21.7              | 58.8              | 16.0              | 162               | 30                | 8                 |
| 5            | 19.9              | 60.6              | 16.2              | 172               | 25                | 5                 |
| 6            | 15.0              | 58.0              | 22.6              | 172               | 25                | 8                 |
| 7            | 12.2              | 58.6              | 24.5              | 172               | 30                | 5                 |
| 8            | 4.3               | 52.4              | 38.0              | 172               | 30                | 8                 |
| 9            | 19.3              | 56.9              | 21.3              | 167               | 27.5              | 6.5               |
| 10           | 6.4               | 55.4              | 30.8              | 177               | 27.5              | 6.5               |
| 11           | 37.6              | 46.9              | 14.7              | 157               | 27.5              | 6.5               |
| 12           | 18.0              | 57.3              | 22.2              | 167               | 32.5              | 6.5               |
| 13           | 26.3              | 55.0              | 18.3              | 167               | 22.5              | 6.5               |
| 14           | 9.9               | 58.9              | 28.0              | 167               | 27.5              | 9.5               |
| 15           | 25.0              | 50.3              | 22.1              | 167               | 27.5              | 3.5               |
| 16           | 14.1              | 61.1              | 23.0              | 177               | 20                | 6.5               |
| 17           | 15.2              | 62.9              | 20.7              | 177               | 20                | 6.5               |
| 18           | 15.9              | 60.0              | 22.1              | 160               | 34                | 7.5               |
| 19           | 19.6              | 60.6              | 19.3              | 160               | 34                | 7.5               |

The yield (dependent) variables are y 1 = percentage of unchanged starting material,

y 3 = percentage of unwanted by-product.

y 2 = percentage converted to the desired product,

Using (10.46), the least squares estimator ˆ B for the regression of ( y 1 , y 2 , y 3 ) on ( x 1 , x 2 , x 3 ) is given by

<!-- formula-not-decoded -->

Note that the first column of ˆ B gives ˆ β 0, ˆ β 1, ˆ β 2, ˆ β 3 for regression of y 1 on x 1, x 2, x 3; the second column of ˆ B gives ˆ β 0, ˆ β 1, ˆ β 2, ˆ β 3 for regression of y 2 on x 1, x 2, x 3, and so on.

## 10.4.3 Properties of Least Squares Estimators ˆ B

The least squares estimator ˆ B can be obtained without imposing the assumptions E ( y ) = XB , cov ( y i ) = 𝚺 , and cov ( y i , y j ) = O . However, when these assumptions hold, ˆ B has the following properties:

1. The estimator ˆ B is unbiased, that is, E ( ˆ B ) = B . This means that if we took repeated random samples from the same population, the average value of ˆ B would be B .
2. The least squares estimators ˆ β j k in ˆ B have minimum variance among all possible linear unbiased estimators. This result is known as the Gauss-Markov theorem. The restriction to unbiased estimators is necessary to exclude trivial estimators such as a constant, which has variance equal to zero, but is of no interest. This minimum variance property of least squares estimators is remarkable for its distributional generality; normality of the y 's is not required.
3. ˆ ˆ
3. All β j k 's in B are correlated with each other. This is due to the correlations among the x 's and among the y 's. The ˆ β 's within a given column of ˆ B are correlated because x 1, x 2 , . . . , xq are correlated. If x 1, x 2 , . . . , xq were orthogonal to each other, the ˆ β 's within each column of ˆ B would be uncorrelated. Thus the relationship of the x 's to each other affects the relationship of the ˆ β 's within each column to each other. On the other hand, the ˆ β 's in each column are correlated with ˆ β 's in other columns because y 1, y 2 , . . . , yp are correlated.

Because of the correlations among the columns of ˆ B , we need multivariate tests for hypotheses about B. We cannot use an F -test from Section 10.2.5 on each column of B , because these F -tests would not take into account the correlations or preserve the α -level. Some appropriate multivariate tests are given in Section 10.5.

## 10.4.4 An Estimator for 𝚺

By analogy with (10.8) and (10.9), an unbiased estimator of cov ( y i ) = 𝚺 is given by

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

With the denominator n -q -1, S e is an unbiased estimator of 𝚺 ; that is, E ( S e ) = 𝚺 .

## 10.4.5 Model Corrected for Means

If the x 's are centered by subtracting their means, we have the centered X matrix as in (10.13),

<!-- formula-not-decoded -->

The B matrix can be partitioned as

<!-- formula-not-decoded -->

By analogy with (10.14) and (10.15), the estimates are

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

where y ′ = ( y 1 , y 2 , . . . , y p ) and x ′ = ( x 1 , x 2 , . . . , xq ) . These estimates give the same results as ˆ B = ( X ′ X ) -1 X ′ Y in (10.46).

As in (10.20), the estimate ˆ B 1 in (10.50) can be expressed in terms of sample covariance matrices. We multiply and divide (10.50) by n -1 to obtain

<!-- formula-not-decoded -->

where S xx and S xy are blocks from the overall sample covariance matrix of y 1, y 2 , . . . , yp , x 1, x 2 , . . . , xq :

<!-- formula-not-decoded -->

## 10.5 MULTIVARIATE MULTIPLE REGRESSION: HYPOTHESIS TESTS

In this section we extend the two tests of Section 10.2.5 to the multivariate y case. We assume the x 's are fixed and the y 's are multivariate normal. For other tests and confidence intervals, see Rencher (1998, Chapter 7).

## 10.5.1 Test of Overall Regression

We first consider the hypothesis that none of the x 's predict any of the y 's, which can be expressed as H 0 : B 1 = O , where B 1 includes all rows of B except the first:

<!-- formula-not-decoded -->

The numerator of (10.49) suggests a partitioning of the total sum of squares and products matrix Y ′ Y ,

We do not wish to include 𝛃 ′ 0 = 0 ′ in the hypothesis, because this would restrict all y 's to have intercepts of zero. The alternative hypothesis is H 1 : B 1 /negationslash= O , which implies that we want to know if even one β j k /negationslash= 0, j = 1, 2 , . . . , q ; k = 1, 2 , . . . , p .

<!-- formula-not-decoded -->

By analogy to (10.23), we subtract n yy ′ from both sides to avoid inclusion of 𝛃 ′ 0 = 0 ′ :

<!-- formula-not-decoded -->

The overall regression sum of squares and products matrix H = ˆ B ′ X ′ Y -n yy ′ can be used to test H 0 : B 1 = O . The notation E = Y ′ Y - ˆ B ′ X ′ Y and H = ˆ B ′ X ′ Y -n yy ′ conforms with usage of E and H in Chapter 6.

As in Chapter 6, we can test H 0 : B 1 = O by means of

<!-- formula-not-decoded -->

which is distributed as /Lambda1 p , q , n -q -1 when H 0 : B 1 = O is true, where p is the number of y 's and q is the number of x 's. We reject H 0 if /Lambda1 ≤ /Lambda1α, p , q , n -q -1. The likelihood ratio approach leads to the same test statistic. If H is 'large' due to large values of the ˆ β j k 's, then | E + H | would be expected to be sufficiently greater than | E | so that /Lambda1 would lead to rejection. By H large, we mean that the regression sums of squares on the diagonal are large. Critical values for /Lambda1 are available in Table A.9 using ν H = q and ν E = n -q -1. Note that these degrees of freedom are the same as in the univariate test for regression of y on x 1, x 2 , . . . , xq in (10.24). The F and χ 2 approximations for /Lambda1 in (6.15) and (6.16) can also be used.

There are two alternative expressions for Wilks' /Lambda1 in (10.55). We can express /Lambda1 in terms of the eigenvalues λ 1, λ 2 , . . . , λ s of E -1 H :

<!-- formula-not-decoded -->

where s = min ( p , q ) . Wilks' /Lambda1 can also be written in the form

<!-- formula-not-decoded -->

where S is partitioned as in (10.53):

<!-- formula-not-decoded -->

The form of /Lambda1 in (10.57) is the same as in the test for independence of y and x given in (7.30), where y and x are both random vectors. In the present section, the y 's are random variables and the x 's are fixed. Thus S yy is the sample covariance matrix of the y 's in the usual sense, whereas S xx consists of an analogous mathematical expression involving the constant x 's (see comments about S xx in Section 10.2.4).

By the symmetry of (10.57) in x and y , /Lambda1 is distributed as /Lambda1 q , p , n -p -1 as well as /Lambda1 p , q , n -q -1. This is equivalent to property 3 in Section 6.1.3. Hence, if we regressed the x 's on the y 's, we would get a different ˆ B but would have the same value of /Lambda1 for the test.

The union-intersection test of H 0 : B 1 = O uses Roy's test statistic analogous to (6.20),

<!-- formula-not-decoded -->

where λ 1 is the largest eigenvalue of E -1 H . Upper percentage points θα are given in Table A.10. The accompanying parameters are

<!-- formula-not-decoded -->

The hypothesis is rejected if θ &gt; θα .

As in Section 6.1.5, Pillai's test statistic is defined as

<!-- formula-not-decoded -->

and the Lawley-Hotelling test statistic is given by

<!-- formula-not-decoded -->

where λ 1, λ 2 , . . . , λ s are the eigenvalues of E -1 H . For V ( s ) , upper percentage points are found in Table A.11, indexed by s , m , and n as defined earlier in connection with Roy's test. Upper percentage points for ν EU ( s ) /ν H (see Section 6.1.5) are provided in Table A.12, where ν H = q and ν E = n -q -1. Alternatively, we can use the F -approximations for V ( s ) and U ( s ) in Section 6.1.5.

When H 0 is true, all four test statistics have probability α of rejecting; that is, they all have the same probability of a Type I error. When H 0 is false, the power ranking of the tests depends on the configuration of the population eigenvalues, as was noted in Section 6.2. (The sample eigenvalues λ 1, λ 2 , . . . , λ s from E -1 H are estimates of the population eigenvalues.) If the population eigenvalues are equal or nearly equal, the power ranking of the tests is V ( s ) ≥ /Lambda1 ≥ U ( s ) ≥ θ . If only one population eigenvalue is nonzero, the powers are reversed: θ ≥ U ( s ) ≥ /Lambda1 ≥ V ( s ) .

In the case of a single nonzero population eigenvalue, the rank of B 1 is 1. There are various ways this could occur; for example, B 1 could have only one nonzero row, which would indicate that only one of the x 's predicts the y 's. On the other hand, a single nonzero column implies that only one of the y 's is predicted by the x 's. Alternatively, B 1 would have rank 1 if all rows were equal or linear combinations of each other, manifesting that all x 's act alike in predicting the y 's. Similarly, all columns equal to each other or linear functions of each other would signify only one dimension in the y 's as they relate to the x 's.

Example 10.5.1. For the chemical data of Table 10.1, we test the overall regression hypothesis H 0 : B 1 = O . The E and H matrices are given by

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

The eigenvalues of E -1 H are 26.3183, .1004, and .0033. The parameters for use in obtaining critical values of the four test statistics are

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Using the eigenvalues, we obtain the test statistics

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

which exceeds the .05 critical value, 8.936 (interpolated), from Table A.11 (see Section 6.1.5). Thus all four tests reject H 0. Note that the critical values given for θ and V ( s ) are conservative, since 0 was used in place of -. 5 for m .

In this case, the first eigenvalue, 26.3183, completely dominates the other two. In Example 10.4.2, we obtained

<!-- formula-not-decoded -->

The columns are approximately proportional to each other, indicating that there is essentially only one dimension in the y 's as they are predicted by the x 's. A similar

statement can be made about the rows and the dimensionality of the x 's as they predict the y 's.

## 10.5.2 Test on a Subset of the x 's

Weconsider the hypothesis that the y 's do not depend on the last h of the x 's, xq -h + 1, xq -h + 2 , . . . , xq . By this we mean that none of the p y 's is predicted by any of these h x 's. To express this hypothesis, write the B matrix in partitioned form,

<!-- formula-not-decoded -->

where, as in Section 10.2.5b, the subscript r denotes the subset of β j k 's to be retained in the reduced model and d represents the subset of β j k 's to be deleted if they are not significant predictors of the y 's. Thus B d has h rows. The hypothesis can be expressed as

<!-- formula-not-decoded -->

If X r contains the columns of X corresponding to B r , then the reduced model is

<!-- formula-not-decoded -->

To compare the fit of the full model and the reduced model, we use the difference between the regression sum of squares and products matrix for the full model, ˆ B ′ X ′ Y , and regression sum of squares and products matrix for the reduced model, ˆ B ′ r X ′ r Y . By analogy to (10.26), this difference becomes our H matrix:

<!-- formula-not-decoded -->

Thus the test of H 0 : B d = O is a full and reduced model test of the significance of xq -h + 1, xq -h + 2 , . . . , xq above and beyond x 1, x 2 , . . . , xq -h .

To make the test, we use the E matrix based on the full model, E = Y ′ Y - ˆ B ′ X ′ Y . Then

<!-- formula-not-decoded -->

and Wilks' /Lambda1 -statistic is given by

<!-- formula-not-decoded -->

which is distributed as /Lambda1 p , h , n -q -1 when H 0 : B d = O is true. Critical values are available in Table A.9 with ν H = h and ν E = n -q -1. Note that these degrees of freedom for the multivariate y case are the same as for the univariate y case (multiple regression) in Section 10.2.5b. The F - and χ 2 -approximations for /Lambda1 in (6.15) and (6.16) can also be used.

As implied in the notation /Lambda1( xq -h + 1 , . . . , xq | x 1 , . . . , xq -h ) , Wilks' /Lambda1 in (10.63) provides a full and reduced model test. We can express it in terms of /Lambda1 for the full model and a similar /Lambda1 for the reduced model. In the denominator of (10.63), we have Y ′ Y - ˆ B ′ r X ′ r Y , which is the error matrix for the reduced model Y = X r B r + 𝚵 in (10.61). This error matrix could be used in a test for the significance of overall regression in the reduced model, as in (10.55),

<!-- formula-not-decoded -->

Since /Lambda1 r in (10.64) has the same denominator as /Lambda1 in (10.55), we recognize (10.63) as the ratio of Wilks' /Lambda1 for the overall regression test in the full model to Wilks' /Lambda1 for the overall regression test in the reduced model,

<!-- formula-not-decoded -->

where /Lambda1 f is given by (10.55). In (10.65), we have a convenient computational device. We run the overall regression test for the full model and again for the reduced model and take the ratio of the resulting /Lambda1 values.

The Wilks' /Lambda1 in (10.65), comparing the full and reduced models, is similar in appearance to (6.127). However, in (6.127), the full and reduced models involve the dependent variables y 1, y 2 , . . . , yp in MANOVA, whereas in (10.65), the reduced model is obtained by deleting a subset of the independent variables x 1, x 2 , . . . , xq in regression. The parameters of the Wilks' /Lambda1 distribution are different in the two cases. Note that in (6.127), some of the dependent variables were denoted by x 1 , . . . , xq for convenience.

Test statistics due to Roy, Pillai, and Lawley-Hotelling can be obtained from the eigenvalues of E -1 H = ( Y ′ Y - ˆ B ′ X ′ Y ) -1 ( ˆ B ′ X ′ Y - ˆ B ′ r X ′ r Y ) . Critical values or approximate tests for these three test statistics are based on ν H = h , ν E = n -q -1, and

<!-- formula-not-decoded -->

Example 10.5.2. The chemical data in Table 10.1 originated from a response surface experiment seeking to locate optimum operating conditions. Therefore, a secondorder model is of interest, and we add x 2 1 , x 2 2 , x 2 3 , x 1 x 2, x 1 x 3, x 2 x 3 to the variables x 1, x 2, x 3 considered in Example 10.5.1. There are now q = 9 independent variables, and we obtain an overall Wilks' /Lambda1 of

<!-- formula-not-decoded -->

where ν H = q = 9 and ν E = n -q -1 = 19 -9 -1 = 9. To see whether the six second-order variables add a significant amount to x 1, x 2, x 3 for predicting the y 's, we calculate

<!-- formula-not-decoded -->

where ν H = h = 6 and ν E = n -q -1 = 19 -9 -1 = 9. In this case, /Lambda1 r = . 0332 is from Example 10.5.1, in which we considered the model with x 1, x 2, and x 3. Thus we reject H 0 : B d = O and conclude that the second-order terms add significant predictability to the model.

## 10.6 MEASURES OF ASSOCIATION BETWEEN THE y 's AND THE x 's

The most widely used measures of association between two sets of variables are the canonical correlations , which are treated in Chapter 11. In this section, we review other measures of association that have been proposed.

In (10.34), we have R 2 = s ′ yx S -1 xx s yx / syy for the univariate y case. By analogy, we define an R 2 -like measure of association between y 1, y 2 , . . . , yp and x 1, x 2 , . . . , xq as

<!-- formula-not-decoded -->

where S yx , S xy , S xx , and S yy are defined in (10.53) and the subscript M indicates multivariate y 's.

By analogy to rxy = sxy / sx s y in (3.13), Robert and Escoufier (1976) suggested

<!-- formula-not-decoded -->

Kabe (1985) discussed the generalized correlation determinant

<!-- formula-not-decoded -->

for various choices of the transformation matrices L and M .

In Section 6.1.8, we introduced several measures of association that quantify the amount of relationship between the y 's and the dummy grouping variables in a MANOVA context. These are even more appropriate here in the multivariate regression setting, where both the x 's and the y 's are continuous variables. The R 2 -like indices given in (6.41), (6.43), (6.46), (6.49), and (6.51) range between 0 and 1 and will be briefly reviewed in the remainder of this section. For more complete commentary, see Section 6.1.8.

The two measures based on Wilks' /Lambda1 are

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

where s = min ( p , q ) . A measure based on Roy's θ is provided by θ itself,

<!-- formula-not-decoded -->

where λ 1 is the largest eigenvalue of E -1 H . This was identified in Section 6.1.8 as the square of the first canonical correlation (see Chapter 11) between the y 's and the grouping variables in MANOVA. In the multivariate regression setting, θ is the square of the first canonical correlation, r 2 1 , between the y 's and the x 's.

Measures of association based on the Lawley-Hotelling and Pillai statistics are given by

<!-- formula-not-decoded -->

By (6.48) and (6.49), AP in (10.68) is the average of the s squared canonical correlations, r 2 1 , r 2 2 , . . . , r 2 s .

Example 10.6. We use the chemical data of Table 10.1 to illustrate some measures of association. For the three dependent variables y 1, y 2, and y 3 and the three independent variables x 1, x 2, and x 3, the partitioned covariance matrix is

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

from which we obtain R 2 M and RV directly using (10.66) and (10.67),

<!-- formula-not-decoded -->

Using the results in Example 10.5.1, we obtain

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

We have general agreement among η 2 /Lambda1 , A /Lambda1 , η 2 θ , and A LH. But R 2 M , RV , and AP do not appear to be measuring the same level of association, especially R 2 M . It appears that more study is needed before one or more of these measures can be universally recommended.

## 10.7 SUBSET SELECTION

As in the univariate y case in Section 10.2.7, there may be more potential predictor variables ( x 's) than are useful in a given situation. Some of the x 's may be redundant in the presence of the other x 's.

Wemayalso be interested in deleting some of the y 's if they are not well predicted by any of the x 's. This would lead to further simplification of the model.

We present two approaches to subset selection: stepwise procedures and methods involving all possible subsets.

## 10.7.1 Stepwise Procedures

Subset selection among the x 's is discussed in Section 10.7.1a, followed by selection among the y 's in Section 10.7.1b.

## 10.7.1a Finding a Subset of the x's

We begin with the forward selection procedure based on Wilks' /Lambda1 . At the first step, we test the regression of the p y 's on each x j . There will be two rows in the ˆ B matrix,

a row containing the intercepts and a row corresponding to x j :

<!-- formula-not-decoded -->

We use the overall regression test statistic (10.55),

<!-- formula-not-decoded -->

which is distributed as /Lambda1 p , 1 , n -2, since ˆ B j has two rows and X j has two columns. After calculating /Lambda1( x j ) for each j , we choose the variable with minimum /Lambda1( x j ) . Note that at the first step, we are not testing each variable in the presence of the others. We are searching for the variable x j that best predicts the p y 's by itself, not above and beyond the other x 's.

At the second step, we seek the variable yielding the smallest partial /Lambda1 for each x adjusted for the variable first entered, where the partial /Lambda1 -statistic is given by (10.65). After one variable has entered, (10.65) becomes

<!-- formula-not-decoded -->

where x 1 denotes the variable entered at the first step. We calculate (10.69) for each x j /negationslash= x 1 and choose the variable that minimizes /Lambda1( x j | x 1 ) .

If we denote the second variable to enter by x 2, then at the third step we seek the x j that minimizes

<!-- formula-not-decoded -->

By property 7 in Section 6.1.3, the partial Wilks' /Lambda1 -statistic transforms to an exact F since ν H = h = 1 at each step.

After m variables have been selected, the partial /Lambda1 would have the following form at the next step:

<!-- formula-not-decoded -->

where the first m variables to enter are denoted x 1, x 2 , . . . , xm , and x j is a candidate variable from among the q -m remaining variables. At this step, we would choose the x j that minimizes (10.71). The partial Wilks' /Lambda1 in (10.71) is distributed as /Lambda1 p , 1 , n -m -1 and, by Table 6.1, transforms to a partial F -statistic distributed as Fp , n -m -p . [These distributions hold for a variable x j chosen before seeing the data but not for the x j that minimizes (10.71) or maximizes the corresponding partial F .]

The procedure continues, bringing in the 'best' variable at each step, until a step is reached at which the minimum partial /Lambda1 exceeds a predetermined threshold value or,

equivalently, the associated partial F falls below a preselected value. Alternatively, the stopping rule can be cast in terms of the p -value of the partial /Lambda1 or F . If the smallest p -value at some step exceeds a predetermined value, the procedure stops.

For each x j , there corresponds an entire row of the ˆ B matrix because x j has a coefficient for each of the p y 's. Thus if a certain x significantly predicts even one of the y 's, it should be retained.

The stepwise procedure is an extension of forward selection. Each time a variable enters, all the variables that have entered previously are checked by a partial /Lambda1 or F to see if the least 'significant' one is now redundant and can be deleted.

The backward elimination procedure begins with all x 's (all rows of ˆ B ) included in the model and deletes one at a time using a partial /Lambda1 or F . At the first step, the partial /Lambda1 for each x j is

<!-- formula-not-decoded -->

which is distributed as /Lambda1 p , 1 , n -q -1 and can be converted to Fp , n -q -p by Table 6.1. The variable with largest /Lambda1 or smallest F is deleted. At the second step, a partial /Lambda1 or F is calculated for each of the q -1 remaining variables, and again the least important variable in the presence of the others is eliminated. This process continues until a step is reached at which the largest /Lambda1 or smallest F is significant, indicating that the corresponding variable is apparently not redundant in the presence of its fellows. Some preselected p -value or threshold value of /Lambda1 or F is used to determine a stopping rule.

If no automated program is available for subset selection in the multivariate case, a forward selection or backward elimination procedure could be carried out by means of a rather simple set of commands based on (10.71) or (10.72).

A sequential procedure such as stepwise selection will often fail to find the optimum subset, especially if a large pool of predictor variables is involved. However, the value of Wilks' /Lambda1 found by stepwise selection may not be far from that for the optimum subset.

The remarks in the final paragraph of Section 10.2.7b are pertinent in the multivariate context as well. True predictors of the y 's in the population may be overlooked because of inflated error variances, or, on the other hand, x 's that are not true predictors may appear to be so in the sample. The latter problem can be severe for small sample sizes (Rencher and Pun 1980).

## 10.7.1b Finding a Subset of the y's

After a subset of x 's has been found, the researcher may wish to know if these x 's predict all p of the y 's. If some of the y 's do not relate to any of the x 's, they could be deleted from the model to achieve a further simplification. The y 's can be checked for redundancy in a manner analogous to the stepwise discriminant approach presented in Sections 6.11.2 and 8.9, which finds subsets of dependent variables using a full and reduced model Wilks' /Lambda1 for the y 's. The partial /Lambda1 -statistic for adding or deleting a y is similar to (10.69), (10.70), or (10.71), except that dependent variables are involved

rather than independent variables. Thus to add a y at the third step of a forward selection procedure , for example, where the first two variables already entered are denoted as y 1 and y 2, we use (6.128) to obtain

<!-- formula-not-decoded -->

for each y j /negationslash= y 1 or y 2, and we choose the y j that minimizes /Lambda1( y j | y 1 , y 2 ) . [In (6.128) the dependent variable of interest was denoted by x instead of y j as here.] Similarly, if three y 's, designated as y 1, y 2, and y 3, were 'in the model' and we were checking the feasibility of adding y j , the partial /Lambda1 -statistic would be

<!-- formula-not-decoded -->

which is distributed as /Lambda1 1 , q , n -q -4, where q is the number of x 's presently in the model and 4 is the number of y 's presently in the model. The two Wilks /Lambda1 values in the numerator and denominator of the right side of (10.74), /Lambda1( y 1 , y 2 , y 3 , y j ) and /Lambda1( y 1 , y 2 , y 3 ) , are obtained from (10.55). Since p = 1, /Lambda1 1 , q , n -q -4 in (10.74) transforms to Fq , n -q -4 (see Table 6.1).

In the first step of a backward elimination procedure , we would delete the y j that maximizes

<!-- formula-not-decoded -->

which is distributed as /Lambda1 1 ,ν H ,ν E -p + 1. In this case, ν H = q and ν E = n -q -1 so that the distribution of (10.75) is /Lambda1 1 , q , n -q -p , which can be transformed to an exact F . Note that q , the number of x 's, may have been reduced in a subset selection on the x 's, as in Section 10.7.1a. Similarly, p is the number of y 's and will decrease in subsequent steps.

Stopping rules for either the forward or backward approach could be defined in terms of p -values or threshold values of /Lambda1 or the equivalent F . A stepwise procedure could be devised as a modification of the forward approach.

If a software program is available that tests the significance of one x as in (10.72), it can be adapted to test one y as in (10.75) by use of property 3 of Section 6.1.3: The distribution of /Lambda1 p ,ν H ,ν E is the same as that of /Lambda1ν H , p ,ν E + ν H -p , which can also be seen from the symmetry of /Lambda1 in (10.57),

<!-- formula-not-decoded -->

which is distributed as /Lambda1 p , q , n -q -1 or, equivalently, as /Lambda1 q , p , n -p -1. Thus we can reverse the y 's and x 's; we list the x 's as dependent variables in the program and the y 's as independent variables. Then the test of a single y in (10.74) or (10.75) can be carried out using (10.72) without any adjustment. The partial /Lambda1 -statistic in (10.72)

is distributed as /Lambda1 p , 1 , n -q -1. If we interchange p and q , because the y 's and x 's are interchanged as dependent and independent variables, this becomes /Lambda1 q , 1 , n -p -1. By property 3 of Section 6.1.3 (repeated above), this is equivalent to /Lambda1 1 , q , n -p -1 + 1 -q = /Lambda1 1 , q , n -p -q , which is the distribution of (10.75).

## 10.7.2 All Possible Subsets

In Section 10.2.7a, we discussed the criteria R 2 p , s 2 p , and Cp for comparing all possible subsets of x 's to predict a univariate y in multiple regression, where p -1 denotes the number of x 's in a subset selected from a pool of k -1 available independent variables. We now discuss some matrix analogues of these criteria for the multivariate y case, as suggested by Mallows (1973) and Sparks, Coutsourides, and Troskie (1983).

In this section, in order to conform with notation in the literature, we will use p for the number of columns in X (and the number of rows in B ), rather than for the number of y 's. The number of y 's will be indicated by m .

We now extend the three criteria R 2 p , s 2 p , and Cp to analogous matrix expressions R 2 p , S p , and C p . These can be reduced to scalar form using trace or determinant.

1. R 2 p . In the univariate y case, R 2 p for a ( p -1 ) -variable subset of x 's is defined by (10.32) as

<!-- formula-not-decoded -->

A direct extension of R 2 p for the multivariate y case is given by the matrix

<!-- formula-not-decoded -->

where p -1 is the number of x 's selected from the k -1 available x 's. To convert R 2 p to scalar form, we can use tr ( R 2 p )/ m , in which we divide by m , the number of y 's, so that 0 ≤ tr ( R 2 p )/ m ≤ 1. As in the univariate case, we identify the subset that maximizes tr ( R 2 p )/ m for each value of p = 2, 3 , . . . , k . The criterion tr ( R 2 p )/ m does not attain its maximum until p reaches k , but we look for the value of p at which further increases are deemed unimportant. We could also use | R 2 p | in place of tr ( R 2 p )/ m .

2. S p . A direct extension of the univariate criterion s 2 p = MSE p = SSE p /( n -p ) is provided by

<!-- formula-not-decoded -->

where E p = Y ′ Y - ˆ B ′ p X ′ p Y . To convert to a scalar, we can use tr ( S p ) or | S p | , either of which will behave in an analogous fashion to s 2 p in the univariate case. The remarks in

Section 10.2.7a apply here as well; one may wish to select the subset with minimum value of tr ( S p ) or perhaps the subset with smallest p such that tr ( S p ) &lt; tr ( S k ) . A similar application could be made with | S p | .

3. C p . To extend the Cp criterion to the multivariate y case, we write the model under consideration as

<!-- formula-not-decoded -->

where p -1 is the number of x 's in the subset and k -1 is the number of x 's in the 'full model.' The predicted values of the y 's are given by

<!-- formula-not-decoded -->

We are interested in predicted values of the observation vectors, namely, ˆ y 1, ˆ y 2 , . . . , ˆ y n , which are given by the rows of ˆ Y :

<!-- formula-not-decoded -->

In general, the predicted vectors ˆ y i are biased estimators of E ( y i ) in the correct model, because we are examining many competing models for which E ( ˆ y i ) /negationslash= E ( y i ) . In place of the univariate expected squared error E [ ˆ yi -E ( yi ) ] 2 in (10.37) and (10.38), we define a matrix of expected squares and products of errors, E [ˆ y i -E ( y i ) ][ˆ y i -E ( y i ) ] ′ . We then add and subtract E ( ˆ y i ) to obtain (see Problem 10.8)

<!-- formula-not-decoded -->

By analogy to (10.39), our C p matrix will be an estimate of the sum of (10.78), multiplied by 𝚺 -1 for standardization.

We first find an expression for cov ( ˆ y i ) , which for convenience we write in row form,

<!-- formula-not-decoded -->

where ˆ B = ( ˆ 𝛃 ( 1 ) , ˆ 𝛃 ( 2 ) , . . . , ˆ 𝛃 ( m )) , as in (10.47). This can be written as

<!-- formula-not-decoded -->

where m is the number of y 's and 𝚺 = cov ( y i ) (see Problem 10.9). As in (10.41) (see also Problem 10.5), we can sum over the n observations and use (3.65) to obtain

<!-- formula-not-decoded -->

To sum the second term on the right of (10.78), we have, by analogy to (10.42),

<!-- formula-not-decoded -->

where S p is given by (10.77).

Now by (10.80) and (10.81), we can sum (10.78) and multiply by 𝚺 -1 to obtain the matrix of total expected squares and products of error standardized by 𝚺 -1 ,

<!-- formula-not-decoded -->

Using S k = E k /( n -k ) , the sample covariance matrix based on all k -1 variables, as an estimate of 𝚺 , we can estimate (10.82) by

<!-- formula-not-decoded -->

which is the form suggested by Mallows (1973). We can use tr ( C p ) or | C p | to reduce this to a scalar. But if 2 p -n is negative, | C p | can be negative, and Sparks, Coutsourides, and Troskie (1983) suggested a modification of | C p | ,

<!-- formula-not-decoded -->

which is always positive.

To find the optimal subset of x 's for each value of p , we could examine all possible subsets [or use a computational scheme such as that of Furnival and Wilson (1974)] and look for the 'smallest' C p matrix for each p . In (10.82), we see that when the bias is O , the 'population C p ' is equal to p I . Thus we seek a C p that is 'small' and near p I . In terms of trace, we seek tr ( C p ) close to pm , where m is the number of y 's in the vector of measurements; that is, tr( I ) = m .

To find a 'target' value for (10.85), we write E -1 k E p in terms of C p from (10.84),

<!-- formula-not-decoded -->

When the bias is O , we have C p = p I , and (10.86) becomes

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Thus we seek subsets such that

<!-- formula-not-decoded -->

In summary, when examining all possible subsets, any or all of the following criteria may be useful in finding the single best subset or the best subset for each p :

<!-- formula-not-decoded -->

## 10.8 MULTIVARIATE REGRESSION: RANDOM x 's

In Sections 10.4 and 10.5, it was assumed that the x 's were fixed and would have the same values in repeated sampling. In many applications, the x 's are random variables. In such a case, the values of x 1, x 2 , . . . , xq are not under the control of the experimenter but occur randomly along with y 1, y 2 , . . . , yp . On each subject, we observe p + q values in the vector ( y 1, y 2 , . . . , yp , x 1, x 2 , . . . , xq ).

If we assume that ( y 1, y 2 , . . . , yp , x 1, x 2 , . . . , xq ) has a multivariate normal distribution, then all estimates and tests have the same formulation as in the fixedx case [for details, see Rencher (1998, Section 7.7)]. Thus there is no essential difference in our procedures between the fixedx case and the randomx case.

whence, by (2.85),

## PROBLEMS

- 10.1 Show that ∑ n i = 1 ( yi -x ′ i ˆ 𝛃 ) 2 = ( y -X ˆ 𝛃 ) ′ ( y -X ˆ 𝛃 ) as in (10.6). 10.2 Show that n i = 1 ( yi -µ) 2 = n i = 1 ( yi -y ) 2 + n ( y -µ) 2 as in (10.10).
- ∑ 0 as in (10.19). 10.4 Show that E [ ˆ yi -E ( yi ) ] 2 = E [ ˆ yi -E ( ˆ yi ) ] 2 +[ E ( ˆ yi ) -E ( yi ) ] 2 as in (10.37).
- ∑ ∑ 10.3 Show that n i = 1 ( xi 2 -x 2 ) y =
- 10.5 Show that n var ( yi )/σ 2 tr X p ( X X p ) 1 X as in (10.40).
- 10.6 Show that the alternative form of Cp in (10.45) is equal to the original form in (10.44).
- ∑ i = 1 ˆ = [ ′ p -′ p ]
- 10.7 Show that (10.48) is the same as (10.49), that is, ( Y -X ˆ B ) ′ ( Y -X ˆ B ) =
- Y ′ Y - ˆ B ′ X ′ Y .
- 10.8 Show that

<!-- formula-not-decoded -->

thus verifying (10.78).

- 10.9 Show that cov ( ˆ y ′ i ) has the form given in (10.79).
- 10.10 Show that the two forms of C p
- in (10.83) and (10.84) are equal.
- 10.11 Explain why | E -1 k E p | &gt; 0, as claimed following (10.85).
- 10.12 Show that E -1 k E p = [ C p + ( n -2 p ) I ] /( n -k ) , as in (10.86), where C p is given in (10.83).
- 10.13 Show that if C p = p I , then E -1 k E p = [ ( n -p )/( n -k ) ] I as in (10.87).
- 10.14 Use the diabetes data of Table 3.4.
- (a) Find the least squares estimate B for the regression of ( y 1, y 2) on ( x 1, x 2, x
- ˆ 3).
- (b) Test the significance of overall regression using all four test statistics.
- (c) Determine what the eigenvalues of E -1 H reveal about the essential rank of ˆ B 1 and the implications of this rank, such as the relative power of the
- four tests.
- (d) Test the significance of each of x 1, x 2, and x 3 adjusted for the other two
- x 's.
- 10.15 Use the sons data of Table 3.7.
- (a) Find the least squares estimate B for the regression of ( y y x x
- (b)
- ˆ 1, 2) on ( 1, 2).
- Test the significance of overall regression using all four test statistics.

- (c) Determine what the eigenvalues of E -1 H reveal about the essential rank of ˆ B 1 and the implications of this rank, such as the relative power of the four tests.
- (d) Test the significance of x 1 adjusted for x 2 and of x 2 adjusted for x 1.

## 10.16 Use the glucose data of Table 3.8.

- (a) Find the least squares estimate ˆ B for the regression of ( y 1, y 2, y 3) on ( x 1, x 2, x 3).
- (b) Test the significance of overall regression using all four test statistics.
- (c) Determine what the eigenvalues of E -1 H reveal about the essential rank of ˆ B 1 and the implications of this rank, such as the relative power of the four tests.
- (d) Test the significance of each of x 1, x 2, and x 3 adjusted for the other two x 's.
- (e) Test the significance of each y adjusted for the other two by using (10.75).

## 10.17 Use the Seishu data of Table 7.1.

- (a) Find the least squares estimate ˆ B for the regression of ( y 1, y 2) on ( x 1, x 2 , . . . , x 8) and test for significance.
- (b) Test the significance of ( x 7, x 8) adjusted for the other x 's.
- (c) Test the significance of ( x 4, x 5, x 6) adjusted for the other x 's.
- (d) Test the significance of ( x 1, x 2, x 3) adjusted for the other x 's.

## 10.18 Use the Seishu data of Table 7.1.

- (a) Do a stepwise regression to select a subset of x 1, x 2 , . . . , x 8 that adequately predicts ( y 1, y 2).
- (b) After selecting a subset of x 's, use the methods of Section 10.7.1b to check if either of the y 's can be deleted.
- 10.19 Use the temperature data of Table 7.2.
- (a) Find the least squares estimate ˆ B for the regression of ( y 4, y 5, y 6) on ( y 1, y 2, y 3) and test for significance.
- (c) Find the least squares estimate ˆ B for the regression of ( y 10, y 11) on ( y 1 , . . . , y 9) and test for significance.
- (b) Find the least squares estimate ˆ B for the regression of ( y 7, y 8, y 9) on ( y 1 , . . . , y 6) and test for significance.
- 10.20 Using the temperature data of Table 7.2, carry out a stepwise regression to select a subset of y 1, y 2 , . . . , y 9 that adequately predicts ( y 10, y 11).

## C H A P T E R 11

## Canonical Correlation

## 11.1 INTRODUCTION

Canonical correlation analysis is concerned with the amount of (linear) relationship between two sets of variables. We often measure two types of variables on each research unit-for example, a set of aptitude variables and a set of achievement variables, a set of personality variables and a set of ability measures, a set of price indices and a set of production indices, a set of student behaviors and a set of teacher behaviors, a set of psychological attributes and a set of physiological attributes, a set of ecological variables and a set of environmental variables, a set of academic achievement variables and a set of measures of job success, a set of closed-book exam scores and a set of open-book exam scores, and a set of personality variables of freshmen students and the same variables on the same students as seniors.

## 11.2 CANONICAL CORRELATIONS AND CANONICAL VARIATES

Weassume that two sets of variables y ′ = ( y 1 , y 2 , . . . , yp ) and x ′ = ( x 1 , x 2 , . . . , xq ) are measured on the same sampling unit. We denote the two sets of variables as y and x to conform to notation in Chapters 3, 7, and 10. In Section 7.4.1, we discussed the hypothesis that y and x were independent. In this chapter, we consider a measure of overall correlation between y and x .

Canonical correlation is an extension of multiple correlation, which is the correlation between one y and several x 's (see Section 10.2.6). Canonical correlation analysis is often a useful complement to a multivariate regression analysis.

We first review multiple correlation. The sample covariances and correlations among y , x 1, x 2 , . . . , xq can be summarized in the matrices

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

where s ′ yx = ( sy 1 , sy 2 , . . . , syq ) contains the sample covariances of y with x 1, x 2 , . . . , xq and S xx is the sample covariance matrix of the x 's [see (10.16)]. The partitioned matrix R is defined analogously; r ′ yx = ( ry 1 , ry 2 , . . . , ryq ) contains the sample correlations of y with x 1, x 2 , . . . , xq , and R xx is the sample correlation matrix of the x 's [see (10.35)].

By (10.34), the squared multiple correlation between y and the x 's can be computed from the partitioned covariance matrix (11.1) or correlation matrix (11.2) as follows:

<!-- formula-not-decoded -->

In R 2 , the q covariances between y and the x 's in s yx or the q correlations between y and the x 's in r yx are channeled into a single measure of linear relationship between y and the x 's. The multiple correlation R can be defined alternatively as the maximum correlation between y and a linear combination of the x 's; that is, R = max b r y , b ′ x .

We now return to the case of several y 's and several x 's. The covariance structure associated with two subvectors y and x was first discussed in Section 3.8.1. By (3.42), the overall sample covariance matrix for ( y 1 , . . . , yp , x 1 , . . . , xq ) can be partitioned as

<!-- formula-not-decoded -->

where S yy is the p × p sample covariance matrix of the y 's, S yx is the p × q matrix of sample covariances between the y 's and the x 's, and S xx is the q × q sample covariance matrix of the x 's.

In Section 10.6, we discussed several measures of association between the y 's and the x 's. The first of these is defined in (10.66) as R 2 M = | S yx S -1 xx S xy | / | S yy | , which is analogous to R 2 = s ′ yx S -1 xx s yx / s 2 y in (11.3). By (2.89) and (2.91), R 2 M can be rewritten as R 2 M = | S -1 yy S yx S -1 xx S xy | . By (2.108), R 2 M can be expressed as

<!-- formula-not-decoded -->

where s = min ( p , q ) and r 2 1 , r 2 2 , . . . , r 2 s are the eigenvalues of S -1 yy S yx S -1 xx S xy . When written in this form, R 2 M is seen to be a poor measure of association because 0 ≤ r 2 i ≤ 1 for all i , and the product will usually be too small to meaningfully reflect the amount of association. (In Example 10.6, R 2 M = . 00029 was a tiny fraction of the other measures of association.) The eigenvalues themselves, on the other hand, provide meaningful measures of association between the y 's and the x 's. The square roots of the eigenvalues, r 1, r 2 , . . . , rs , are called canonical correlations .

The best overall measure of association is the largest squared canonical correlation (maximum eigenvalue) r 2 1 of S -1 yy S yx S -1 xx S xy , but the other eigenvalues (squared

canonical correlations) of S -1 yy S yx S -1 xx S xy provide measures of supplemental dimensions of (linear) relationship between y and x . As an alternative approach, it can be shown that r 2 1 is the maximum squared correlation between a linear combination of the y 's, u = a ′ y , and a linear combination of the x 's, v = b ′ x ; that is,

<!-- formula-not-decoded -->

We denote the coefficient vectors that yield the maximum correlation as a 1 and b 1. Thus r 1 (the positive square root of r 2 1 ) is the correlation between u 1 = a ′ 1 y and v 1 = b ′ 1 x . The coefficient vectors a 1 and b 1 can be found as eigenvectors [see (11.7) and (11.8)]. The linear functions u 1 and v 1 are called the first canonical variates . There are additional canonical variates ui = a ′ i y and v i = b ′ i x corresponding to r 2, r 3 , . . . , rs .

It was noted in Section 2.11.5 that the (nonzero) eigenvalues of AB are the same as those of BA as long as AB and BA are square but that the eigenvectors of AB and BA are not the same. If we let A = S -1 yy S yx and B = S -1 xx S xy , then r 2 1 , r 2 2 , . . . , r 2 s can also be obtained from BA = S -1 xx S xy S -1 yy S yx as well as from AB = S -1 yy S yx S -1 xx S xy . Thus the eigenvalues can be obtained from either of the characteristic equations

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

The coefficient vectors a i and b i in the canonical variates ui = a ′ i y and v i = b ′ i x are the eigenvectors of these same two matrices:

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Thus the two matrices S -1 yy S yx S -1 xx S xy and S -1 xx S xy S -1 yy S yx have the same (nonzero) eigenvalues, as indicated in (11.5) and (11.6), but different eigenvectors, as in (11.7) and (11.8). Since y is p × 1 and x is q × 1, the a i 's are p × 1 and the b i 's are q × 1. This can also be seen in the sizes of the matrices in (11.7) and (11.8); that is, S -1 yy S yx S -1 xx S xy is p × p and S -1 xx S xy S -1 yy S yx is q × q . Since p is typically not equal to q , the matrix that is larger in size will be singular, and the smaller one will be nonsingular. We illustrate for p &lt; q . In this case S -1 xx S xy S -1 yy S yx has the form

<!-- image -->

When p &lt; q , the rank of S -1 xx S xy S -1 yy S yx is p , because S -1 xx has rank q and S xy S -1 yy S yx has rank p . In this case p eigenvalues are nonzero and the remaining q -p eigenvalues are equal to zero. In general, there are s = min ( p , q ) values of the squared canonical correlation r 2 i with s corresponding pairs of canonical variates ui = a ′ i y and v i = b ′ i x . For example, if p = 3 and q = 7, there will be three canonical correlations, r 1, r 2, and r 3.

Thus we have s canonical correlations r 1, r 2 , . . . , rs corresponding to the s pairs of canonical variates ui and v i :

<!-- formula-not-decoded -->

For each i , ri is the (sample) correlation between ui and v i ; that is, ri = rui v i . The pairs ( ui , v i ) , i = 1, 2 , . . . , s , provide the s dimensions of relationship. For simplicity, we would prefer only one dimension of relationship, but this occurs only when s = 1, that is, when p = 1 or q = 1.

We examine the elements of the coefficient vectors a i and b i for the information they provide about the contribution of the y 's and x 's to ri . These coefficients can be standardized, as noted in the last paragraph in the present section and in Section 11.5.1.

The s dimensions of relationship ( ui , v i ) , i = 1, 2 , . . . , s , are nonredundant. The information each pair provides is unavailable in the other pairs because u 1, u 2 , . . . , us are uncorrelated . They are not orthogonal because a 1, a 2 , . . . , a s are eigenvectors of S -1 yy S yx S -1 xx S xy , which is nonsymmetric. Similarly, the v i 's are uncorrelated, and each ui is uncorrelated with all v j , j /negationslash= i , except, of course, v i .

As noted, the matrix S -1 yy S yx S -1 xx S xy is not symmetric. Many algorithms for computation of eigenvalues and eigenvectors accept only symmetric matrices. Since S -1 yy S yx S -1 xx S xy is the product of the two symmetric matrices S -1 yy and S yx S -1 xx S xy , we can proceed as in (6.23) and work with ( U ′ ) -1 S yx S -1 xx S xy U -1 , where U ′ U = S yy is the Cholesky factorization of S yy (see Section 2.7). The symmetric matrix ( U ′ ) -1 S yx S -1 xx S xy U -1 has the same eigenvalues as S -1 yy S yx S -1 xx S xy but has eigenvectors Ua i , where a i is given in (11.7).

In effect, the pq covariances between the y 's and x 's in S yx have been replaced by s = min ( p , q ) canonical correlations. These succinctly summarize the relationships between y and x . In fact, in a typical study, we do not need all s canonical correlations. The smallest eigenvalues can be disregarded to achieve even more simplification. As in (8.13) for discriminant functions, we can judge the importance of each eigenvalue by its relative size:

<!-- formula-not-decoded -->

The canonical correlations can also be obtained from the partitioned correlation matrix of the y 's and x 's,

<!-- formula-not-decoded -->

where R yy is the p × p sample correlation matrix of the y 's, R yx is the p × q matrix of sample correlations between the y 's and the x 's, and R xx is the q × q sample correlation matrix of the x 's. The matrix R -1 yy R yx R -1 xx R xy is analogous to R 2 = r ′ yx R -1 xx r yx in the univariate y case. The characteristic equations corresponding to (11.5) and (11.6),

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

yield the same eigenvalues r 2 1 , r 2 2 , . . . , r 2 s as (11.5) and (11.6) (the canonical correlations are scale invariant; see property 1 in Section 11.3).

If we use the partitioned correlation matrix in place of the covariance matrix in (11.7) and (11.8), we obtain the same eigenvalues (squared canonical correlations) but different eigenvectors:

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

The relationship between the eigenvectors c and d in (11.12) and (11.13) and the eigenvectors a and b in (11.7) and (11.8) is

<!-- formula-not-decoded -->

where D y = diag ( sy 1 , sy 2 , . . . , syp ) and D x = diag ( sx 1 , sx 2 , . . . , sxq ) .

The eigenvectors c and d in (11.12), (11.13), and (11.14) are standardized coefficient vectors . By analogy to (8.15), they would be applied to standardized variables. To show this, note that in terms of centered variables y -y , we have

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Hence c and d are preferred to a and b for interpretation of the canonical variates ui and v i .

## 11.3 PROPERTIES OF CANONICAL CORRELATIONS

Two interesting properties of canonical correlations are the following [for other properties, see Rencher (1998, Section 8.3)]:

1. Canonical correlations are invariant to changes of scale on either the y 's or the x 's. For example, if the measurement scale is changed from inches to centimeters, the canonical correlations will not change (the corresponding eigenvectors will change). This property holds for simple and multiple correlations as well.
2. The first canonical correlation r 1 is the maximum correlation between linear functions of y and x . Therefore, r 1 exceeds (the absolute value of) the simple correlation between any y and any x or the multiple correlation between any y and all the x 's or between any x and all the y 's.

Example 11.3. For the chemical data of Table 10.1, we obtain the canonical correlations and illustrate property 2. We consider the extended set of nine x 's, as in Example 10.5.2. The matrix R yx of correlations between the y 's and the x 's is

| x 1    | x 2    | x 3    | x 1 x 2   | x 1 x 3   | x 2 x 3   | x 2 1   | x 2 2   | x 2 3   |
|--------|--------|--------|-----------|-----------|-----------|---------|---------|---------|
| - . 68 | - . 22 | - . 45 | - . 41    | - . 55    | - . 45    | - . 68  | - . 23  | - . 42  |
| .40    | .08    | .39    | .16       | .44       | .33       | .40     | .12     | .33     |
| .58    | .23    | .36    | .40       | .45       | .39       | .58     | .22     | .36     |

The three canonical correlations and their squares are

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

From the relative sizes of the squared canonical correlations, we would consider only the first two to be important. A hypothesis test for the significance of each is carried out in Example 11.4.2.

To confirm that property 2 holds in this case, we compare r 1 = . 9899 to the individual correlations and the multiple correlations. We first note that .9899 is greater than individual correlations, since (the absolute value of) the largest correlation in R yx is .68. The multiple correlation Ryj | x of each y j with the x 's is given by

<!-- formula-not-decoded -->

and for the multiple correlation of each x with the y 's we have

<!-- formula-not-decoded -->

The first canonical correlation r 1 = . 9899 exceeds all multiple correlations, and property 2 is satisfied.

## 11.4 TESTS OF SIGNIFICANCE

In the following two sections we discuss basic tests of significance associated with canonical correlations. For other aspects of model validation for canonical correlations and variates, see Rencher (1998, Section 8.5).

## 11.4.1 Tests of No Relationship between the y 's and the x 's

In Section 7.4.1, we considered the hypothesis of independence, H 0 : 𝚺 yx = O . If 𝚺 yx = O , the covariance of every yi with every x j is zero, and all corresponding correlations are likewise zero. Hence, under H 0, there is no (linear) relationship between the y 's and the x 's, and H 0 is equivalent to the statement that all canonical correlations r 1, r 2 , . . . , rs are nonsignificant. Furthermore, H 0 is equivalent to the overall regression hypothesis in Section 10.5.1, H 0 : B 1 = O , which also relates all the y 's to all the x 's. Thus by (7.30) or (10.57), the significance of r 1, r 2 , . . . , rs can be tested by

<!-- formula-not-decoded -->

which is distributed as /Lambda1 p , q , n -1 -q . We reject H 0 if /Lambda1 1 ≤ /Lambda1α . Critical values /Lambda1α are available in Table A.9 using ν H = q and ν E = n -1 -q . The statistic /Lambda1 1 in (11.16) is also distributed as /Lambda1 q , p , n -1 -p . As in (7.31), /Lambda1 1 is expressible in terms of the squared canonical correlations:

<!-- formula-not-decoded -->

In this form, we can see that if one or more r 2 i is large, /Lambda1 1 will be small. We have used the notation /Lambda1 1 in (11.16) and (11.17) because in Section 11.4.2 we will define /Lambda1 2, /Lambda1 3 and so on to test the significance of r 2 and succeeding ri 's after the first.

If the parameters exceed the range of critical values for Wilks' /Lambda1 in Table A.9, we can use the χ 2 -approximation in (6.16),

<!-- formula-not-decoded -->

which is approximately distributed as χ 2 with pq degrees of freedom. We reject H 0 if χ 2 ≥ χ 2 α . Alternatively, we can use the F -approximation given in (6.15):

<!-- formula-not-decoded -->

which has an approximate F -distribution with df1 and df2 degrees of freedom, where

<!-- formula-not-decoded -->

We reject H 0 if F &gt; F α . When pq = 2 , t is set equal to 1. If s = min ( p , q ) is equal to either 1 or 2, then the F -approximation in (11.19) has an exact F -distribution. For example, if one of the two sets consists of only two variables, an exact test is afforded by the F -approximation in (11.19). In contrast, the χ 2 -approximation in (11.18) does not reduce to an exact test for any parameter values.

The other three multivariate test statistics in Sections 6.1.4, 6.1.5, and 10.5.1 can also be used. Pillai's test statistic for the significance of canonical correlations is

<!-- formula-not-decoded -->

Upper percentage points of V ( s ) are found in Table A.11, indexed by

<!-- formula-not-decoded -->

For F -approximations for V ( s ) , see Section 6.1.5.

The Lawley-Hotelling statistic for canonical correlations is

<!-- formula-not-decoded -->

Upper percentage points for ν EU ( s ) /ν H (see Section 6.1.5) are given in Table A.12, which is entered with p , ν H = q , and ν E = n -q -1. For F -approximations, see Section 6.1.5.

Roy's largest root statistic is given by

<!-- formula-not-decoded -->

Upper percentage points are found in Table A.10, with s , m , and N defined as before for Pillai's test. An 'upper bound' on F for Roy's test is given in (6.21). Even though this upper bound is routinely calculated in many software packages, it is not a valid approximation.

As noted at the beginning of this section, the following three tests are equivalent:

1. Test of H 0 : 𝚺 yx = O , independence of two sets of variables.
3. Test of significance of the canonical correlations.
2. Test of H 0 : B 1 = O , significance of overall multivariate multiple regression.

Even though these tests are equivalent, we have discussed them separately because each has an extension that is different from the others. The respective extensions are

1. Test of independence of three or more sets of variables (Section 7.4.2),
2. Test of full vs. reduced model in multivariate multiple regression (Section 10.5.2),
3. Test of significance of succeeding canonical correlations after the first (Section 11.4.2).

Example 11.4.1. For the chemical data of Table 10.1, with the extended set of nine x 's, we obtained canonical correlations .9899, .9528, and .4625 in Example 11.3. To test the significance of these, we calculate the following four test statistics and associated approximate F 's.

| Statistic                          |   Approximate F |   df 1 |   df 2 | p -Value for F   |
|------------------------------------|-----------------|--------|--------|------------------|
| Wilks' /Lambda1 = . 00145          |           6.537 |     27 |  21.09 | < . 0001         |
| Pillai's V ( s ) = 2 . 10          |           2.34  |     27 |  27    | .0155            |
| Lawley-Hotelling U ( s ) = 59 . 03 |          12.388 |     27 |  17    | < . 0001         |
| Roy's θ . 980                      |          48.908 |      9 |   9    | < . 0001         |

=

The F approximation for Roy's test is, of course, an 'upper bound.' Rejection of H 0 in these tests implies that at least r 2 1 is significantly different from zero. The question of how many r 2 i 's are significant is treated in the next section.

## 11.4.2 Test of Significance of Succeeding Canonical Correlations after the First

If the test in (11.17) based on all s canonical correlations rejects H 0, we are not sure if the canonical correlations beyond the first are significant. To test the significance of r 2 , . . . , rs , we delete r 2 1 from /Lambda1 1 in (11.17) to obtain

<!-- formula-not-decoded -->

If this test rejects the hypothesis, we conclude that at least r 2 is significantly different from zero. We can continue in this manner, testing each ri in turn, until a test fails to reject the hypothesis. At the k th step, the test statistic is

<!-- formula-not-decoded -->

which is distributed as /Lambda1 p -k + 1 , q -k + 1 , n -k -q and tests the significance of rk , rk + 1 , . . . , rs . (These test statistics are analogous to those for discriminant functions in

Section 11.4.2.) Note that each parameter is reduced by k -1 from the parameter values p , q , and n -1 -q for /Lambda1 1 in (11.16) or (11.17).

The usual χ 2 -and F -approximations can also be applied to /Lambda1 k . The χ 2 -approximation analogous to (11.18) is given by

<!-- formula-not-decoded -->

which has ( p -k + 1 )( q -k + 1 ) degrees of freedom. The F -approximation for /Lambda1 k is a simple modification of (11.19) and the accompanying parameter definitions. In place of p , q , and n , we use p -k + 1, q -k + 1, and n -k + 1 to obtain

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Example 11.4.2. We continue our analysis of the canonical correlations for the chemical data in Table 10.1 with three y 's and nine x 's. The tests are summarized in Table 11.1.

In the case of /Lambda1 2, we have a discrepancy between the exact Wilks /Lambda1 -test and the approximate F -test. The test based on /Lambda1 is not significant, whereas the F -test does reach significance. This illustrates the need to check critical values for exact tests whenever p -values for approximate tests are close to the nominal value of α . From the test using /Lambda1 , we conclude that only r 1 = . 9899 is significant. The relative sizes of the squared canonical correlations, .980, .908, and .214, would indicate two dimensions of relationship, but this is not confirmed by the Wilks' test, perhaps because of the small sample size relative to the number of variables ( p + q = 12 and n = 19).

Table 11.1. Tests of Three Canonical Correlations of the Chemical Data

|   k |   /Lambda1 k |   /Lambda1 . 05 |   Approximate F |   df 1 |   df 2 | p -Value for F   |
|-----|--------------|-----------------|-----------------|--------|--------|------------------|
|   1 |      0.00145 |           0.024 |           6.537 |     27 |   21.1 | < . 0001         |
|   2 |      0.0725  |           0.069 |           2.714 |     16 |   16   | .0269            |
|   3 |      0.786   |           0.209 |           0.35  |      7 |    9   | .91              |

where

<!-- formula-not-decoded -->

To illustrate the computations, we obtain the values in Table 11.1 for k = 2. Using (11.24), the computation for /Lambda1 2 is

<!-- formula-not-decoded -->

With k = 2, p = 3, q = 9, and n = 19, the critical value for /Lambda1 2 is obtained from Table A.9 as

<!-- formula-not-decoded -->

For the approximate F for /Lambda1 2, we have

<!-- formula-not-decoded -->

## 11.5 INTERPRETATION

We now turn to an assessment of the information contained in the canonical correlations and canonical variates. As was done for discriminant functions in Section 8.7, a distinction can be made between interpretation of the canonical variates and assessing the contribution of each variable. In the former, the signs of the coefficients are considered; in the latter, the signs are ignored and the coefficients are ranked in order of absolute value.

In Sections 11.5.1-11.5.3, we discuss three common tools for interpretation of canonical variates: (1) standardized coefficients, (2) the correlation between each variable and the canonical variate, and (3) rotation of the canonical variate coefficients. The second of these is the most widely recommended, but we note in Section 11.5.2 that it is the least useful. In fact, for reasons to be outlined, we recommend only the first, standardized coefficients. In Section 11.5.4, we describe redundancy analysis and discuss its shortcomings as a measure of association between two sets of variables.

## 11.5.1 Standardized Coefficients

The coefficients in the canonical variates ui = a ′ i y and v i = b ′ i x reflect differences in scaling of the variables as well as differences in contribution of the variables to

canonical correlation. To remove the effect of scaling, a i and b i can be standardized by multiplying by the standard deviations of the corresponding variables as in (11.14):

<!-- formula-not-decoded -->

where D y = diag ( sy 1 , sy 2 , . . . , syp ) and D x = diag ( sx 1 , sx 2 , . . . , sxq ) . Alternatively, c i and d i can be obtained directly from (11.12) and (11.13) as eigenvectors of R -1 yy R yx R -1 xx R xy and R -1 xx R xy R -1 yy R yx , respectively. It was noted at the end of Section 11.2 that the coefficients in c i are applied to standardized variables [see (11.15)]. Thus the effect of differences in size or scaling of the variables is removed, and the coefficients ci 1, ci 2 , . . . , cip in c i reflect the relative contribution of each of y 1, y 2 , . . . , yp to ui . A similar statement can be made about d i .

The standardized coefficients show the contribution of the variables in the presence of each other. Thus if some of the variables are deleted and others added, the coefficients will change. This is precisely the behavior we desire from the coefficients in a multivariate setting.

Example 11.5.1. For the chemical data in Table 10.1 with the extended set of nine x 's, we obtain the following standardized coefficients for the three canonical variates:

|         | c 1        | c 2         | c 3         |
|---------|------------|-------------|-------------|
| y 1     | 1.5360     | 4.4704      | 5.7961      |
| y 2     | .2108      | 2.8291      | 2.2280      |
| y 3     | .4676      | 3.1309      | 5.1442      |
|         | d 1        | d 2         | d 3         |
| x 1     | 5.0125     | - 38 . 3053 | - 12 . 5072 |
| x 2     | 5.8551     | - 17 . 7390 | - 24 . 2290 |
| x 3     | 1.6500     | - 7 . 9699  | - 32 . 7392 |
| x 1 x 2 | - 3 . 9209 | 19.2937     | 11.6420     |
| x 1 x 3 | - 2 . 2968 | 6.4001      | 31.2189     |
| x 2 x 3 | .5316      | .8096       | 1.2988      |
| x 2 1   | - 2 . 6655 | 32.7933     | 4.8454      |
| x 2 2   | - 1 . 2346 | - 3 . 3641  | 10.7979     |
| x 2     | .5703      | .8733       | .9706       |

3

<!-- formula-not-decoded -->

Thus

<!-- formula-not-decoded -->

The variables that contribute most to the correlation between u 1 and v 1 are y 1 and x 1, x 2, x 1 x 2, x 1 x 3, x 2 1 . The correlation between u 2 and v 2 is due largely to all three y 's and x 1, x 2, x 1 x 2, x 2 1 .

## 11.5.2 Correlations between Variables and Canonical Variates

Many writers recommend the additional step of converting the standardized coefficients to correlations. Thus, for example, in c ′ 1 = ( c 11 , c 12 , . . . , c 1 p ) , instead of the second coefficient c 12 we could examine ry 2 u 1 , the correlation between y 2 and the first canonical variate u 1. Such correlations are sometimes referred to as loadings or structure coefficients , and it is widely claimed that they provide a more valid interpretation of the canonical variates. Rencher (1988; 1992b; 1998, Section 8.6.3) has shown, however, that a weighted sum of the correlations between y j and the canonical variates u 1, u 2 , . . . , us is equal to R 2 y j | x , the squared multiple correlation between y j and the x 's. There is no information about how the y 's contribute jointly to canonical correlation with the x 's. Therefore, the correlations are useless in gauging the importance of a given variable in the context of the others. The researcher who uses these correlations for interpretation is unknowingly reducing the multivariate setting to a univariate one.

## 11.5.3 Rotation

In an attempt to improve interpretability, the canonical variate coefficients can be rotated (see Section 13.5) to increase the number of high and low coefficients and reduce the number of intermediate ones.

We do not recommend rotation of the canonical variate coefficients for two reasons [for proof and further discussion, see Rencher (1992b)]:

1. Rotation destroys the optimality of the canonical correlations. For example, the first canonical correlation is reduced and is no longer equal to max a , b r a ′ y , b ′ x as in (11.4).
2. Rotation introduces correlations among succeeding canonical variates. Thus, for example, u 1 and u 2 are correlated after rotation. Hence even though the resulting coefficients may offer a subjectively more interpretable pattern, this gain is offset by the increased complexity due to interrelationships among the canonical variates. For example, u 2 and v 2 no longer offer a new dimension of relationship uncorrelated with u 1 and v 1. The dimensions now overlap, and some of the information in u 2 and v 2 is already available in u 1 and v 1.

## 11.5.4 Redundancy Analysis

The redundancy is a measure of association between the y 's and the x 's based on the correlations between variables and canonical variates discussed in Section 11.5.2. Since these correlations provide only univariate information, the redundancy turns

out to be a univariate rather than a multivariate measure of relationship. If the squared multiple correlation of y j regressed on the x 's is denoted by R 2 y j | x , then the redundancy of the y 's given the v 's is the average squared multiple correlation:

<!-- formula-not-decoded -->

Similarly, the redundancy of the x 's given the u 's is the average

<!-- formula-not-decoded -->

where R 2 x j | y is the squared multiple correlation of x j regressed on the y 's. Since Rd( y | v ) in (11.26) is the average squared multiple correlation of each y j regressed on the x 's, it does not take into account the correlations among the y 's. It is thus an average univariate measure of relationship between the y 's and the x 's, not a multivariate measure at all. The two redundancy measures in (11.26) and (11.27) are not symmetric; that is, Rd ( y | v ) /negationslash= Rd ( x | u ) .

Thus the so-called redundancy does not really quantify the redundancy among the y 's and x 's and is, therefore, not a useful measure of association between two sets of variables. For a measure of association we recommend r 2 1 itself.

## 11.6 RELATIONSHIPS OF CANONICAL CORRELATION ANALYSIS TO OTHER MULTIVARIATE TECHNIQUES

In Section 11.4.1, we noted the equivalence of the test for significance of the canonical correlations and the test for significance of overall regression, H 0 : B 1 = O . Additional relationships between canonical correlation and multivariate regression are developed in Section 11.6.1. The relationship of canonical correlation analysis to MANOVA and discriminant analysis is discussed in Section 11.6.2.

## 11.6.1 Regression

There is a direct link between canonical variate coefficients and multivariate multiple regression coefficients. The matrix of regression coefficients of the y 's regressed on the x 's (corrected for their means) is given in (10.52) as ˆ B 1 = S -1 xx S xy . This matrix can be used to relate a i and b i :

<!-- formula-not-decoded -->

[Since a i and b i are eigenvectors, (11.28) could also be written as b i = c ˆ B 1 a i , where c is an arbitrary scale factor.] By (2.67) and (11.28), the canonical variate coefficient

vector b i is expressible as a linear combination of the columns of ˆ B 1. A similar expression for a i can be obtained from the regression of x on y : a i = S -1 yy S yx b i .

The two Wilks' test statistics in multivariate regression in Sections 10.5.1 and 10.5.2, namely, the test for overall regression and the test on a subset of the x 's, can both be expressed in terms of canonical correlations. By (10.55) and (11.17), the test statistic for the overall regression hypothesis H 0 : B 1 = O can be written as

In Section 11.2, canonical correlation was defined as an extension of multiple correlation. Correspondingly, canonical correlation reduces to multiple correlation when one of the two sets of variables has only one variable. When p = 1, for example, R yy becomes 1, and by (11.10), the single squared canonical correlation reduces to r 2 = r ′ yx R -1 xx r yx , which we recognize from (10.34) as R 2 .

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

where r 2 i is the i th squared canonical correlation.

A test statistic for H 0 : B d = O , the hypothesis that the y 's do not depend on the last h of the x 's, is given by (10.65) as

<!-- formula-not-decoded -->

where /Lambda1 f is given in (11.29) and /Lambda1 r is given in (10.64) as

<!-- formula-not-decoded -->

By analogy with (11.30), /Lambda1 r can be expressed in terms of the squared canonical correlations c 2 1 , c 2 2 , . . . , c 2 t between y 1, y 2 , . . . , yp and x 1, x 2 , . . . , xq -h :

<!-- formula-not-decoded -->

where t = min ( p , q -h ) . We have used the notation c 2 i instead of r 2 i to emphasize that the canonical correlations in the reduced model differ from those in the full model. By (11.30) and (11.33), the full and reduced model test of H 0 : B d = O in (11.31) can now be expressed in terms of canonical correlations as

<!-- formula-not-decoded -->

If p = 1, as in multiple regression, then s = t = 1, and (11.34) reduces to

<!-- formula-not-decoded -->

where R 2 f and R 2 r are the squared multiple correlations for the full model and for the reduced model. The distribution of /Lambda1 in (11.35) is /Lambda1 1 , h , n -q -1 when H 0 is true. Since p = 1, there is an exact F -transformation from Table 6.1,

<!-- formula-not-decoded -->

which is distributed as Fh , n -q -1 when H 0 is true. Substitution of /Lambda1 = ( 1 -R 2 f )/( 1 -R 2 r ) from (11.35) yields the F -statistic expressed in terms of R 2 ,

<!-- formula-not-decoded -->

as given in (10.33).

Subset selection in canonical correlation analysis can be handled by the methods for multivariate regression given in Section 10.7. A subset of x 's can be found by the procedure of Section 10.7.1a. After a subset of x 's is found, the approach in Section 10.7.1b can be used to select a subset of y 's.

Muller (1982) discussed the relationship of canonical correlation analysis to multivariate regression and principal components. (Principal components are treated in Chapter 12.)

## 11.6.2 MANOVAand Discriminant Analysis

In Sections 6.1.8 and 8.4.2, it was noted that in a one-way MANOV A or discriminant analysis setting, λ i /( 1 + λ i ) is equal to r 2 i , where λ i is the i th eigenvalue of E -1 H and r 2 i is the i th squared canonical correlation between the p dependent variables and the k -1 grouping variables. We now give a justification of this assertion.

Let the dependent variables be denoted by y 1, y 2 , . . . , yp , as usual. We represent the k groups by k -1 dummy variables, x 1, x 2 , . . . , xk -1, defined for each member of the i th group, i ≤ k -1, as x 1 = 0 , . . . , xi -1 = 0, xi = 1, xi + 1 = 0 , . . . , xk -1 = 0. For the k th group, all x 's are zero. (See Section 6.1.8 for an introduction to dummy variables.) To illustrate with k = 4, the x 's are defined as follows in each group:

|   Group |   x 1 |   x 2 |   x 3 |
|---------|-------|-------|-------|
|       1 |     1 |     0 |     0 |
|       2 |     0 |     1 |     0 |
|       3 |     0 |     0 |     1 |
|       4 |     0 |     0 |     0 |

The MANOVA model is equivalent to multivariate regression of y 1, y 2 , . . . , yp on the dummy grouping variables x 1, x 2 , . . . , xk -1. The MANOVA test of H 0 : 𝛍 1 = 𝛍 2 = · · · = 𝛍 k is equivalent to the multivariate regression test of H 0 : B 1 = O , as given by (11.17),

<!-- formula-not-decoded -->

When we compare this form of /Lambda1 to the MANOVA test statistic (6.14),

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

To establish this relationship more formally, we write (6.22) as

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

We multiply (11.42) on the left by S yy to obtain

<!-- formula-not-decoded -->

Using the centered matrix X c in (10.14), with an analogous definition for Y c , we can write B 1 in the form [see (10.52)]

<!-- formula-not-decoded -->

In terms of centered matrices, E = Y ′ Y - ˆ B ′ X ′ Y in (10.49) can be written as

<!-- formula-not-decoded -->

we obtain the relationships and (11.7) as

since S ′ xy = S yx . Similarly,

<!-- formula-not-decoded -->

Since MANOVA is equivalent to multivariate regression on dummy grouping variables, we can substitute these values of E and H into (11.41) to obtain

<!-- formula-not-decoded -->

Subtracting r 2 S yx S -1 xx S xy a from both sides of (11.43) gives

<!-- formula-not-decoded -->

A comparison of (11.46) and (11.47) shows that

<!-- formula-not-decoded -->

as in (11.40). Lindsey, Webster, and Halpern (1985) discussed some advantages of using canonical correlation analysis in place of discriminant analysis in the severalgroup case.

## PROBLEMS

- 11.1 Show that the expression for canonical correlations in (11.12) can be obtained from the analogous expression in terms of variances and covariances in (11.7).
- 11.2 Verify (11.28), b i = ˆ B 1 a i .
- 11.4 Verify the expression in (11.36) for F in terms of R 2 f and R 2 r .
- 11.3 Verify (11.35) for /Lambda1 when p = s = t = 1.
- 11.5 Solve (11.39), r 2 i = λ i /( 1 + λ i ) , for λ i to obtain (11.40).
- 11.7 Show that (11.47) can be obtained by subtracting r 2 S yx S -1 xx S xy a from both sides of (11.43).
- 11.6 Verify (11.46), S yx S -1 xx S xy a = λ( S yy -S yx S -1 xx S xy ) a .
- 11.8 Use the diabetes data of Table 3.4.
- (a) Find the canonical correlations between ( y 1, y 2 ) and ( x 1, x 2, x 3 ) .
- (b) Find the standardized coefficients for the canonical variates.
- (c) Test the significance of each canonical correlation.

- 11.9 Use the sons data of Table 3.7.
- (a) Find the canonical correlations between ( y 1 , y 2 ) and ( x 1 , x 2 ) .
- (b) Find the standardized coefficients for the canonical variates.
- (c) Test the significance of each canonical correlation.
- 11.10 Use the glucose data of Table 3.8.
- (a) Find the canonical correlations between ( y 1, y 2, y 3 ) and ( x 1, x 2, x 3 ) .
- (b) Find the standardized coefficients for the canonical variates.
- (c) Test the significance of each canonical correlation.
- 11.11 Use the Seishu data of Table 7.1.
- (a) Find the canonical correlations between ( y 1 , y 2 ) and ( x 1, x 2 , . . . , x 8 ) .
- (b) Find the standardized coefficients for the canonical variates.
- (c) Test the significance of each canonical correlation.
- 11.12 Use canonical correlation to carry out the tests in parts (b), (c), and (d) of Problem 10.17, using the Seishu data. You will need to find the canonical correlations between ( y 1 , y 2 ) and the x 's in the indicated reduced models and use (11.34).
- 11.13 Using the temperature data of Table 7.2, find the canonical correlations and the standardized coefficients and carry out significance tests for the following:
- (a) ( y 1 , y 2 , y 3 ) and ( y 4 , y 5 , y 6 )
- (b) ( y 1 , y 2 , . . . , y 6 ) and ( y 7 , y 8 , y 9
- )
- (c) ( y 1 , y 2 , . . . , y 9 ) and ( y 10 , y 11 )
- (d) ( y 1 , y 2 , . . . , y 6 ) and ( y 7 , y 8 , . . . , y 11 ) .

## C H A P T E R 12

## Principal Component Analysis

## 12.1 INTRODUCTION

In principal component analysis, we seek to maximize the variance of a linear combination of the variables. For example, we might want to rank students on the basis of their scores on achievement tests in English, mathematics, reading, and so on. An average score would provide a single scale on which to compare the students, but with unequal weights we can spread the students out further on the scale and obtain a better ranking.

Essentially, principal component analysis is a one-sample technique applied to data with no groupings among the observations as in Chapters 8 and 9 and no partitioning of the variables into subsets y and x , as in Chapters 10 and 11. All the linear combinations that we have considered previously were related to other variables or to the data structure. In regression, we have linear combinations of the independent variables that best predict the dependent variable(s); in canonical correlation, we have linear combinations of a subset of variables that maximally correlate with linear combinations of another subset of variables; and discriminant analysis involves linear combinations that maximally separate groups of observations. Principal components, on the other hand, are concerned only with the core structure of a single sample of observations on p variables. None of the variables is designated as dependent, and no grouping of observations is assumed. [For a discussion of the use of principal components with data consisting of several samples or groups, see Rencher (1998, Section 9.9)].

The first principal component is the linear combination with maximal variance; we are essentially searching for a dimension along which the observations are maximally separated or spread out. The second principal component is the linear combination with maximal variance in a direction orthogonal to the first principal component, and so on. In general, the principal components define different dimensions from those defined by discriminant functions or canonical variates.

In some applications, the principal components are an end in themselves and may be amenable to interpretation. More often they are obtained for use as input to another analysis. For example, two situations in regression where principal components may be useful are (1) if the number of independent variables is large relative to

the number of observations, a test may be ineffective or even impossible, and (2) if the independent variables are highly correlated, the estimates of repression coefficients may be unstable. In such cases, the independent variables can be reduced to a smaller number of principal components that will yield a better test or more stable estimates of the regression coefficients. For details of this application, see Rencher (1998, Section 9.8).

As another illustration, suppose that in a MANOVA application p is close to ν E , so that a test has low power, or that p &gt; ν E , in which case we have so many dependent variables that a test cannot be made. In such cases, we can replace the dependent variables with a smaller set of principal components and then carry out the test.

In these illustrations, principal components are used to reduce the number of dimensions. Another useful dimension reduction device is to evaluate the first two principal components for each observation vector and construct a scatter plot to check for multivariate normality, outliers, and so on.

Finally, we note that in the term principal components , we use the adjective principal , describing what kind of components-main, primary, fundamental, major, and so on. We do not use the noun principle as a modifier for components .

## 12.2 GEOMETRIC AND ALGEBRAIC BASES OF PRINCIPAL COMPONENTS

## 12.2.1 Geometric Approach

As noted in Section 12.1, principal component analysis deals with a single sample of n observation vectors y 1, y 2 , . . . , y n that form a swarm of points in a p -dimensional space. Principal component analysis can be applied to any distribution of y , but it will be easier to visualize geometrically if the swarm of points is ellipsoidal.

If the variables y 1, y 2 , . . . , yp in y are correlated, the ellipsoidal swarm of points is not oriented parallel to any of the axes represented by y 1, y 2 , . . . , yp . We wish to find the natural axes of the swarm of points (the axes of the ellipsoid) with origin at y , the mean vector of y 1 , y 2 , . . . , y n . This is done by translating the origin to y and then rotating the axes. After rotation so that the axes become the natural axes of the ellipsoid, the new variables (principal components) will be uncorrelated.

We could indicate the translation of the origin to y by writing y i -y , but we will not usually do so for economy of notation. We will write y i -y when there is an explicit need; otherwise we assume that y i has been centered.

The axes can be rotated by multiplying each y i by an orthogonal matrix A [see (2.101), where the orthogonal matrix was denoted by C ]:

<!-- formula-not-decoded -->

Since A is orthogonal, A ′ A = I , and the distance to the origin is unchanged:

<!-- formula-not-decoded -->

[see (2.103)]. Thus an orthogonal matrix transforms y i to a point z i that is the same distance from the origin, and the axes are effectively rotated.

Finding the axes of the ellipsoid is equivalent to finding the orthogonal matrix A that rotates the axes to line up with the natural extensions of the swarm of points so that the new variables (principal components) z 1, z 2 , . . . , z p in z = Ay are uncorrelated. Thus we want the sample covariance matrix of z , S z = ASA ′ [see (3.64)], to be diagonal:

<!-- formula-not-decoded -->

where S is the sample covariance matrix of y 1 , y 2 , . . . , y n . By (2.111), C ′ SC = D = diag (λ 1 , λ 2 , . . . , λ p ) , where the λ i 's are eigenvalues of S and C is an orthogonal matrix whose columns are normalized eigenvectors of S . Thus the orthogonal matrix A that diagonalizes S is the transpose of the matrix C :

<!-- formula-not-decoded -->

where a i is the i th normalized ( a ′ i a i = 1 ) eigenvector of S . The principal components are the transformed variables z 1 = a ′ 1 y , z 2 = a ′ 2 y , . . . , z p = a ′ p y in z = Ay . For example, z 1 = a 11 y 1 + a 12 y 2 +··· + a 1 p y p .

By (2.111), the diagonal elements of ASA ′ on the right side of (12.2) are eigenvalues of S . Hence the eigenvalues λ 1, λ 2 , . . . , λ p of S are the (sample) variances of the principal components zi = a ′ i y :

<!-- formula-not-decoded -->

Since the rotation lines up with the natural extensions of the swarm of points, z 1 = a ′ 1 y has the largest (sample) variance and z p = a ′ p y has the smallest variance. This also follows from (12.4), because the variance of z 1 is λ 1, the largest eigenvalue, and the variance of z p is λ p , the smallest eigenvalue. If some of the eigenvalues are small, we can neglect them and represent the points fairly well with fewer than p dimensions. For example, if p = 3 and λ 3 is small, then the swarm of points is an 'elliptical pancake,' and a two-dimensional representation will adequately portray the configuration of points.

Because the eigenvalues are variances of the principal components, we can speak of 'the proportion of variance explained' by the first k components:

<!-- formula-not-decoded -->

since ∑ p i = 1 λ i = tr ( S ) by (2.107). Thus we try to represent the p -dimensional points ( yi 1 , yi 2 , . . . , yip ) with a few principal components ( zi 1, zi 2 , . . . , zik ) that account for a large proportion of the total variance. If a few variables have relatively large variances, they will figure disproportionately in ∑ j s j j and in the principal components. For example, if s 22 is strikingly larger than the other variances, then in z 1 = a 11 y 1 + a 12 y 2 +··· + a 1 p y p , the coefficient a 12 will be large and all other a 1 j will be small.

When a ratio analogous to (12.5) is used for discriminant functions and canonical variates [see (8.13) and (11.9)], it is frequently referred to as percent of variance . However, in the case of discriminant functions and canonical variates, the eigenvalues are not variances, as they are in principal components.

If the variables are highly correlated, the essential dimensionality is much smaller than p . In this case, the first few eigenvalues will be large, and (12.5) will be close to 1 for a small value of k . On the other hand, if the correlations among the variables are all small, the dimensionality is close to p and the eigenvalues will be nearly equal. In this case, the principal components essentially duplicate the variables, and no useful reduction in dimension is achieved.

Any two principal components zi = a ′ i y and z j = a ′ j y are orthogonal for i /negationslash= j ; that is, a ′ i a j = 0, because a i and a j are eigenvectors of the symmetric matrix S (see Section 2.11.6). Principal components also have the secondary property of being uncorrelated in the sample [see (12.2) and (3.63)]; that is, the covariance of zi and z j is zero:

<!-- formula-not-decoded -->

Discriminant functions and canonical variates, on the other hand, have the weaker property of being uncorrelated but not the stronger property of orthogonality. Thus when we plot the first two discriminant functions or canonical variates on perpendicular coordinate axes, there is some distortion of their true relationship because the actual angle between their axes is not 90 ◦ .

If we change the scale on one or more of the y 's, the shape of the swarm of points will change, and we will need different components to represent the new points. Hence the principal components are not scale invariant. We therefore need to be concerned with the units in which the variables are measured. If possible, all variables should be expressed in the same units. If the variables have widely disparate variances, we could standardize them before extracting eigenvalues and eigenvectors. This is equivalent to finding principal components of the correlation matrix R and is treated in Section 12.5.

If one variable has a much greater variance than the other variables, the swarm of points will be elongated and will be nearly parallel to the axis corresponding to the

variable with large variance. The first principal component will largely represent that variable, and the other principal components will have negligibly small variances. Such principal components (based on S ) do not involve the other p -1 variables, and we may prefer to analyze the correlation matrix R .

Example 12.2.1. To illustrate principal components as a rotation when p = 2, we use two variables from the sons data of Table 3.7: y 1 is head length and y 2 is head width for the first son. The mean vector and covariance matrix are

<!-- formula-not-decoded -->

The eigenvalues and eigenvectors of S are

<!-- formula-not-decoded -->

The symmetric pattern in the eigenvectors is due to their orthogonality: a ′ 1 a 2 = a 11 a 21 + a 12 a 22 = 0.

The observations are plotted in Figure 12.1, along with the (translated and) rotated axes. The major axis is the line passing through y ′ = ( 185 . 7 , 151 . 1 ) in the direction determined by a ′ 1 = (. 825 , . 565 ) ; the slope is a 12 / a 11 = . 565 /. 825. Alternatively, the equation of the major axis can be obtained by setting z 2 = 0:

<!-- formula-not-decoded -->

Figure 12.1. Principal component transformation for the sons data.

<!-- image -->

The lengths of the semimajor and semiminor axes are proportional to √ λ 1 = 11 . 5 and √ λ 2 = 4 . 3, respectively.

Note that the line formed by the major axis can be considered to be a regression line. It is fit to the points so that the perpendicular distance of the points to the line is minimized, rather than the usual vertical distance (see Section 12.3).

## 12.2.2 Algebraic Approach

An algebraic approach to principal components can be briefly described as follows. As noted in Section 12.1, we seek a linear combination with maximal variance. By (3.55), the sample variance of z = a ′ y is a ′ Sa . Since a ′ Sa has no maximum if a is unrestricted, we seek the maximum of

<!-- formula-not-decoded -->

By an argument similar to that used in (8.8)-(8.12), the maximum value of λ is given by the largest eigenvalue in the expression

<!-- formula-not-decoded -->

(see Problem 12.1). The eigenvector a 1 corresponding to the largest eigenvalue λ 1 is the coefficient vector in z 1 = a ′ 1 y , the linear combination with maximum variance.

Unlike discriminant analysis or canonical correlation, there is no inverse involved before obtaining eigenvectors for principal components. Therefore, S can be singular, in which case some of the eigenvalues are zero and can be ignored. A singular S would arise, for example, when n &lt; p , that is, when the sample size is less than the number of variables.

This tolerance of principal component analysis for a singular S is important in certain research situations. For example, suppose that one has a one-way MANOVA with 10 observations in each of three groups and that p = 50, so that there are 50 variables in each of these 30 observation vectors. A MANOVA test involving E -1 H cannot be carried out directly in this case because E is singular, but we could reduce the 50 variables to a small number of principal components and then do a MANOVA test on the components. The principal components would be based on S obtained from the 30 observations ignoring groups. For entry into the MANOVA program, we would evaluate the principal components for each observation vector. If we are retaining k components, we calculate

<!-- formula-not-decoded -->

for i = 1, 2 , . . . , 30. These are sometimes referred to as component scores . In vector form, (12.9) can be rewritten as

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

where

We then use z 1, z 2 , . . . , z 30 as input to the MANOVA program.

Note that in this case with p &gt; n , the k components would not likely be stable; that is, they would be different in a new sample. However, this is of no concern here because we are using the components only to extract information from the sample at hand in order to compare the three groups.

Example 12.2.2. Consider the football data of Table 8.3. In Example 8.8, we saw that high school football players (group 1) differed from the other two groups, college football players and college-age nonfootball players. Therefore, to obtain a homogeneous group of observations, we delete group 1 and use groups 2 and 3 combined. The covariance matrix is as follows:

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

The eigenvalues of S are as follows:

|   Eigenvalue |   Proportion of Variance |   Cumulative Proportion |
|--------------|--------------------------|-------------------------|
|        3.323 |                    0.579 |                   0.579 |
|        1.374 |                    0.239 |                   0.818 |
|        0.476 |                    0.083 |                   0.901 |
|        0.325 |                    0.057 |                   0.957 |
|        0.157 |                    0.027 |                   0.985 |
|        0.088 |                    0.015 |                   1     |

The total variance is

The first two principal components account for 81.8% of the total variance. The corresponding eigenvectors are as follows:

|        |   a 1 | a 2    |
|--------|-------|--------|
| WDIM   | 0.207 | - .142 |
| CIRCUM | 0.873 | - .219 |
| FBEYE  | 0.261 | - .231 |
| EYEHD  | 0.326 | .891   |
| EARHD  | 0.066 | .222   |
| JAW    | 0.128 | .187   |

Thus the first two principal components are

<!-- formula-not-decoded -->

Notice that the large coefficient in z 1 and the large coefficient in z 2, .873 and .891, respectively, correspond to the two largest variances on the diagonal of S . The two variables with large variances, y 2 and y 4, have a notable influence on the first two principal components. However, z 1 and z 2 are still meaningful linear functions. If the six variances were closer in size, the six variables would enter more evenly into the first two principal components. On the other hand, if the variances of y 2 and y 4 were substantially larger, z 1 and z 2 would be essentially equal to y 2 and y 4, respectively.

Note that y 2 and y 3 did not contribute at all when this data set was used to separate groups in Examples 8.5, 8.9, 9.3.1, and 9.6(a). However, these two variables are very useful here in the first two dimensions showing the spread of individual observations.

## 12.3 PRINCIPAL COMPONENTS AND PERPENDICULAR REGRESSION

It was noted in Section 12.2.1 that principal components constitute a rotation of axes. Another geometric property of the line formed by the first principal component is that it minimizes the total sum of squared perpendicular distances from the points to the line. This is easily demonstrated in the bivariate case. The first principal component line is plotted in Figure 12.2 for the first two variables of the sons data, as in Example 12.2.1. The perpendicular distance from each point to the line is simply z 2, the second coordinate in the transformed coordinates ( z 1 , z 2 ) . Hence the sum of squares of perpendicular distances is

<!-- formula-not-decoded -->

-

Figure 12.2. The first principal component as a perpendicular regression line.

<!-- image -->

where a 2 is the second eigenvector of S , and we use y i -y because the axes have been translated to the new origin y . Since a ′ 2 ( y i -y ) = ( y i -y ) ′ a 2, we can write (12.11) in the form

<!-- formula-not-decoded -->

which is a minimum by a remark following (12.4).

For the two variables y 1 and y 2, as plotted in Figure 12.2, the ordinary regression line of y 2 on y 1 minimizes the sum of squares of vertical distances from the points to the line. Similarly, the regression of y 1 on y 2 minimizes the sum of squares of horizontal distances from the points to the line. The first principal component line represents a 'perpendicular' regression line that lies between the other two. The three lines are compared in Figure 12.3 for the partial sons data. The equation of the first principal component line is easily obtained by setting z 2 = 0:

<!-- formula-not-decoded -->

Figure 12.3. Regression lines compared with first principal component line.

<!-- image -->

## 12.4 PLOTTING OF PRINCIPAL COMPONENTS

The plots in Figures 12.1 and 12.2 were illustrations of principal components as a rotation of axes when p = 2. When p &gt; 2, we can plot the first two components as a dimension reduction device. We simply evaluate the first two components ( z 1 , z 2 ) for each observation vector and plot these n points. The plot is equivalent to a projection of the p -dimensional data swarm onto the plane that shows the greatest spread of the points.

The plot of the first two components may reveal some important features of the data set. In Example 12.4(a), we show a principal component plot that exhibits a pattern typical of a sample from a multivariate normal distribution. One of the objectives of plotting is to check for departures from normality, such as outliers or nonlinearity. In Examples 12.4(b) and 12.4(c), we illustrate principal component plots showing a nonnormal pattern characterized by the presence of outliers. Jackson (1980) provided a test for adequacy of representation of observation vectors in terms of principal components.

Gnanadesikan (1997, p. 308) pointed out that, in general, the first few principal components are sensitive to outliers that inflate variances or distort covariances, and the last few are sensitive to outliers that introduce artificial dimensions or mask singularities. We could examine the bivariate plots of at least the first two and the last two principal components in a search for outliers that may exert undue influence.

Devlin, Gnanadesikan, and Kettenring (1981) recommended the extraction of principal components from robust estimates of S or R that reduce the influence of outliers. Campbell (1980) and Ruymgaart (1981) discussed direct robust estimation of principal components. Critchley (1985) developed methods for detection of influential observations in principal component analysis.

Figure 12.4. Plot of first two components for the modified football data.

<!-- image -->

Another feature of the data that a plot of the first two components may reveal is a tendency of the points to cluster. The plot may reveal groupings of points; this is illustrated in Example 12.4(d).

Example 12.4(a). For the modified football data in Example 12.2.2, the first two principal components were given as follows:

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

These are evaluated for each observation vector and plotted in Figure 12.4. (For convenience in scaling, y -y was used in the computations.) The pattern is typical of that from a multivariate normal distribution. Note that the variance along the z 1 axis is greater than the variance in the z 2 direction, as expected.

Example 12.4(b). In Figures 4.9 and 4.10, the Q -Q plot and bivariate scatter plots for the ramus bone data of Table 3.6 exhibit a nonnormal pattern. A principal component analysis using the covariance matrix is given in Table 12.1, and the first two

Table 12.1. Principal Components for the Ramus Bone Data of Table 3.6

| Eigenvalues   | Eigenvalues   | First Two Eigenvectors   | First Two Eigenvectors   | First Two Eigenvectors   |
|---------------|---------------|--------------------------|--------------------------|--------------------------|
| Number        | Value         | Variable                 | a 1                      | a 2                      |
| 1             | 25.05         | AGE 8                    | .474                     | .592                     |
| 2             | 1.74          | AGE 8.5                  | .492                     | .406                     |
| 3             | .22           | AGE 9                    | .515                     | - . 304                  |
| 4             | .11           | AGE 9.5                  | .517                     | - . 627                  |

Figure 12.5. First two principal components for the ramus bone data in Table 3.6.

<!-- image -->

principal components are plotted in Figure 12.5. The presence of three outliers that cause a nonnormal pattern is evident. These outliers do not appear when the four variables are examined individually.

Example 12.4(c). A rather extreme example of the effect of an outlier is given by Devlin, Gnanadesikan, and Kettenring (1981). The data set involved p = 14 economical variables for n = 29 chemical companies. The first two principal components are plotted in Figure 12.6. The sample correlation rz 1 z 2 is indeed zero for all 29 points, as it must be [see (12.6)], but if the apparent outlier is excluded from the computation, then rz 1 z 2 = . 99 for the remaining 28 points. If the outlier were deleted from the data set, the axes of the principal components would pass through the natural extensions of the data swarm.

Example 12.4(d). Jeffers (1967) applied principal component analysis to a sample of 40 alate adelges (winged aphids) on which the following 19 variables had been measured:

| LENGTH   | body length                    |
|----------|--------------------------------|
| WIDTH    | body width                     |
| FORWING  | forewing length                |
| HINWING  | hind-wing length               |
| SPIRAC   | number of spiracles            |
| ANTSEG 1 | length of antennal segment I   |
| ANTSEG 2 | length of antennal segment II  |
| ANTSEG 3 | length of antennal segment III |
| ANTSEG 4 | length of antennal segment IV  |
| ANTSEG 5 | length of antennal segment V   |

<!-- image -->

Figure 12.6. First two principal components for economics data.

```
ANTSPIN number of antennal spines TARSUS 3 leg length, tarsus III TIBIA 3 leg length, tibia III FEMUR 3 leg length, femur III ROSTRUM rostrum OVIPOS ovipositor OVSPIN number of ovipositor spines FOLD anal fold HOOKS number of hind-wing hooks
```

An objective in the study was to determine the number of distinct taxa present in the habitat where the sample was taken. Since adelges are difficult to identify by the usual taxonomic methods, principal component analysis was used to search for groupings among the 40 individuals in the sample.

The correlation matrix is given in Table 12.2, and the eigenvalues and first four eigenvectors are in Tables 12.3 and 12.4, respectively. The eigenvectors are scaled so that the largest value in each is 1. The first principal component is largely an index of size. The second component is associated with SPIRAC, OVIPOS, OVSPIN, and FOLD.

The first two components were computed for each of the 40 individuals and plotted in Figure 12.7. Since the first two components account for 85% of the total variance, the plot represents the data with very little distortion. There are four major

Table 12.2. Correlation Matrix for Winged Aphid Variables (Lower Triangle)

| y    | y 1     | y       |         |         |         |         |         |         |         |         |
|------|---------|---------|---------|---------|---------|---------|---------|---------|---------|---------|
| 1    |         | 2       |         |         |         |         |         |         |         |         |
| y 2  | .934    | 1.000   | y 3     |         |         |         |         |         |         |         |
| y 3  | .927    | .941    | 1.000   | y 4     |         |         |         |         |         |         |
| y 4  | .909    | .944    | .933    | 1.000   | y 5     |         |         |         |         |         |
| y 5  | .524    | .487    | .543    | .499    | 1.000   | y 6     |         |         |         |         |
| y 6  | .799    | .821    | .856    | .833    | .703    | 1.000   | y 7     |         |         |         |
| y 7  | .854    | .865    | .886    | .889    | .719    | .923    | 1.000   | y 8     |         |         |
| y 8  | .789    | .834    | .846    | .885    | .253    | .699    | .751    | 1.000   | y 9     |         |
| y 9  | .835    | .863    | .862    | .850    | .462    | .752    | .793    | .745    | 1.000   | y 10    |
| y 10 | .845    | .878    | .863    | .881    | .567    | .836    | .913    | .787    | .805    | 1.000   |
| y 11 | - . 458 | - . 496 | - . 522 | - . 488 | - . 174 | - . 317 | - . 383 | - . 497 | - . 356 | - . 371 |
| y 12 | .917    | .942    | .940    | .945    | .516    | .846    | .907    | .861    | .848    | .902    |
| y 13 | .939    | .961    | .956    | .952    | .494    | .849    | .914    | .876    | .877    | .901    |
| y 14 | .953    | .954    | .946    | .949    | .452    | .823    | .886    | .878    | .883    | .891    |
| y 15 | .895    | .899    | .882    | .908    | .551    | .831    | .891    | .794    | .818    | .848    |
| y 16 | .691    | .652    | .694    | .623    | .815    | .812    | .855    | .410    | .620    | .712    |
| y 17 | .327    | .305    | .356    | .272    | .746    | .553    | .567    | .067    | .300    | .384    |
| y 18 | . 676   | - . 712 | - . 667 | - . 736 | - . 233 | - . 504 | - . 502 | - . 758 | - . 666 | - . 629 |
| y 19 | .702    | .729    | .746    | .777    | .285    | .499    | .592    | .793    | .671    | .668    |
|      | y 11    |         |         |         |         |         |         |         |         |         |
| y 11 | 1.000   | y 12    |         |         |         |         |         |         |         |         |
| y 12 | - . 465 | 1.000   | y 13    |         |         |         |         |         |         |         |
| y 13 | - . 447 | .981    | 1.000   | y 14    |         |         |         |         |         |         |
| y 14 | - . 439 | .971    | .991    | 1.000   | y 15    |         |         |         |         |         |
| y 15 | - . 405 | .908    | .920    | .921    | 1.000   | y 16    |         |         |         |         |
| y 16 | - . 198 | .725    | .714    | .676    | .720    | 1.000   | y 17    |         |         |         |
| y 17 | - . 032 | .396    | .360    | .298    | .378    | .781    | 1.000   | y 18    |         |         |
| y 18 | .492    | - . 657 | - . 655 | - . 678 | - . 633 | - . 186 | .169    | 1.000   | y 19    |         |
| y 19 | - . 425 | .696    | .724    | .731    | .694    | .287    | . 026   | - . 775 | 1.000   |         |

groups, apparently corresponding to species. The groupings form an interesting S-shape.

## 12.5 PRINCIPAL COMPONENTS FROM THE CORRELATION MATRIX

Generally, extracting components from S rather than R remains closer to the spirit and intent of principal component analysis, especially if the components are to be used in further computations. However, in some cases, the principal components will be more interpretable if R is used. For example, if the variances differ widely or if the measurement units are not commensurate, the components of S will be dominated by the variables with large variances. The other variables will contribute very little. For a more balanced representation in such cases, components of R may be used (see, for example, Problem 12.9).

Table 12.3. Eigenvalues of the Correlation Matrix of the Winged Aphid Data

| Component   |   Eigenvalue | Percent of Variance   | Cumulative Percent   |
|-------------|--------------|-----------------------|----------------------|
| 1           |       13.861 | 73.0                  | 73.0                 |
| 2           |        2.37  | 12.5                  | 85.4                 |
| 3           |        0.748 | 3.9                   | 89.4                 |
| 4           |        0.502 | 2.6                   | 92.0                 |
| 5           |        0.278 | 1.4                   | 93.5                 |
| 6           |        0.266 | 1.4                   | 94.9                 |
| 7           |        0.193 | 1.0                   | 95.9                 |
| 8           |        0.157 | .8                    | 96.7                 |
| 9           |        0.14  | .7                    | 97.4                 |
| 10          |        0.123 | .6                    | 98.1                 |
| 11          |        0.092 | .4                    | 98.6                 |
| 12          |        0.074 | .4                    | 99.0                 |
| 13          |        0.06  | .3                    | 99.3                 |
| 14          |        0.042 | .2                    | 99.5                 |
| 15          |        0.036 | .2                    | 99.7                 |
| 16          |        0.024 | .1                    | 99.8                 |
| 17          |        0.02  | .1                    | 99.9                 |
| 18          |        0.011 | .1                    | 100.0                |
| 19          |        0.003 | .0                    | 100.0                |
|             |       19     |                       |                      |

Table 12.4. Eigenvectors for the First Four Components of the Winged Aphid Data

|          | Eigenvectors   | Eigenvectors   | Eigenvectors   | Eigenvectors   |
|----------|----------------|----------------|----------------|----------------|
| Variable | 1              | 2              | 3              | 4              |
| LENGTH   | .96            | - . 06         | .03            | - . 12         |
| WIDTH    | .98            | - . 12         | .01            | - . 16         |
| FORWING  | .99            | - . 06         | - . 06         | - . 11         |
| HINWING  | .98            | - . 16         | .03            | - . 00         |
| SPIRAC   | .61            | .74            | - . 20         | 1.00           |
| ANTSEG 1 | .91            | .33            | .04            | .02            |
| ANTSEG 2 | .96            | .30            | .00            | - . 04         |
| ANTSEG 3 | .88            | - . 43         | .06            | - . 18         |
| ANTSEG 4 | .90            | - . 08         | .18            | - . 01         |
| ANTSEG 5 | .94            | .05            | .11            | .03            |
| ANTSPIN  | - . 49         | .37            | 1.00           | .27            |
| TARSUS 3 | .99            | - . 02         | .03            | - . 29         |
| TIBIA 3  | 1.00           | - . 05         | .09            | - . 31         |
| FEMUR 3  | .99            | - . 12         | .12            | - . 31         |
| ROSTRUM  | .96            | .02            | .08            | - . 06         |
| OVIPOS   | .76            | .73            | - . 03         | - . 09         |
| OVSPIN   | .41            | 1.00           | - . 16         | - . 06         |
| FOLD     | - . 71         | .64            | .04            | - . 80         |
| HOOKS    | .76            | - . 52         | .06            | .72            |

Figure 12.7. Plotted values of the first two components for individual insects.

<!-- image -->

As with any change of scale, when the variables are standardized in transforming from S to R , the shape of the swarm of points will change. Note, however, that after transforming to R , any further changes of scale on the variables would not affect the components because changes of scale do not change R . Thus the principal components from R are scale invariant.

To illustrate how the eigenvalues and eigenvectors change when converting from S to R , we use a simple bivariate example in which one variance is substantially larger than the other. Suppose that S and the corresponding R have the values

<!-- formula-not-decoded -->

The eigenvalues and eigenvectors from S are

<!-- formula-not-decoded -->

The patterns we see in λ 1, λ 2 , a 1, and a 2 are quite predictable. The symmetry in a 1 and a 2 is due to their orthogonality, a ′ 1 a 2 = 0. The large variance of y 2 in S is reflected in the first principal component z 1 = . 160 y 1 + . 987 y 2, where y 2 is weighted heavily. Thus the first principal component z 1 essentially duplicates y 2 and does not show the mutual effect of y 1 and y 2. As expected, z 1 accounts for virtually all of the

total variance:

<!-- formula-not-decoded -->

The eigenvalues and eigenvectors of R are

<!-- formula-not-decoded -->

The first principal component of R ,

<!-- formula-not-decoded -->

accounts for a high proportion of variance,

<!-- formula-not-decoded -->

because the variables are fairly highly correlated ( r = . 8 ) . But the standardized variables ( y 1 -y 1 )/ 1 and ( y 2 -y 2 )/ 5 are equally weighted in z 1, due to the equality of the diagonal elements ('variances') of R .

Wenowlist some general comparisons of principal components from R with those from S :

1. The percent of variance in (12.5) accounted for by the components of R will differ from the percent for S , as illustrated above.
2. The coefficients of the principal components from R differ from those obtained from S , as illustrated above.
3. If we express the components from R in terms of the original variables, they still will not agree with the components from S . By transforming the standardized variables back to the original variables in the above illustration, the components of R become

<!-- formula-not-decoded -->

As expected, these are very different from the components extracted directly from S . This problem arises, of course, because of the lack of scale invariance of the components of S .

4. The principal components from R are scale invariant, because R itself is scale invariant.
5. The components from a given matrix R are not unique to that R . For example, in the bivariate case, the eigenvalues of

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

and the eigenvectors are a ′ 1 = (. 707 , . 707 ) and a ′ 2 = (. 707 , -. 707 ) , which give principal components

<!-- formula-not-decoded -->

The components in (12.14) do not depend on r . For example, they serve equally well for r = . 01 and for r = . 99. For r = . 01, the proportion of variance explained by z 1 is λ 1 /(λ 1 + λ 2 ) = ( 1 + . 01 )/( 1 + . 01 + 1 -. 01 ) = 1 . 01 / 2 = . 505. For r = . 99, the ratio is 1 . 99 / 2 = . 995. Thus the statement that the first component from a correlation matrix accounts for, say, 90% of the variance is not very meaningful. In general, for p &gt; 2, the components from R depend only on the ratios (relative values) of the correlations, not on their actual values, and components of a given R matrix will serve for other R matrices [see Rencher (1998, Section 9.4)].

## 12.6 DECIDING HOW MANY COMPONENTS TO RETAIN

In every application, a decision must be made on how many principal components should be retained in order to effectively summarize the data. The following guidelines have been proposed:

1. Retain sufficient components to account for a specified percentage of the total variance, say, 80%.
2. Retain the components whose eigenvalues are greater than the average of the eigenvalues, p i = 1 λ i / p . For a correlation matrix, this average is 1.
3. ∑ 3. Use the scree graph , a plot of λ i versus i , and look for a natural break between the 'large' eigenvalues and the 'small' eigenvalues.
4. Test the significance of the 'larger' components, that is, the components corresponding to the larger eigenvalues.

are given by

We now discuss these four criteria for choosing the components to keep. Note, however, that the smallest components may carry valuable information that should not be routinely ignored (see Section 12.7).

In method 1, the challenge lies in selecting an appropriate threshold percentage. If we aim too high, we run the risk of including components that are either sample specific or variable specific . By sample specific we mean that a component may not generalize to the population or to other samples. A variable-specific component is dominated by a single variable and does not represent a composite summary of several variables.

Method 2 is widely used and is the default in many software packages. By (2.107), ∑ i λ i = tr ( S ) , and the average eigenvalue is also the average variance of the individual variables. Thus method 2 retains those components that account for more variance than the average variance of the variables. In cases where the data can be successfully summarized in a relatively small number of dimensions, there is often a wide gap between the two eigenvalues that fall on both sides of the average. In Example 12.2.2, the average eigenvalue (of S ) for the football data is .957, which is amply bracketed by λ 2 = 1 . 37 and λ 3 = . 48. In the winged aphid data in Example 12.4(d), the second and third eigenvalues (of R ) are 2.370 and .748, leaving a comfortable margin on both sides of 1. In some cases, one may wish to move the cutoff point slightly to accommodate a visible gap in eigenvalues.

The scree graph in method 3 is named for its similarity in appearance to a cliff with rocky debris at its bottom. The scree graph for the modified football data of Example 12.2.2 exhibits an ideal pattern, as shown in Figure 12.8. The first two eigenvalues form a steep curve followed by a bend and then a straight-line trend with shallow slope. The recommendation is to retain those eigenvalues in the steep curve before the first one on the straight line. Thus in Figure 12.8, two components would be retained. In practice, the turning point between the steep curve and the straight line may not be as distinct as this or there may be more than one discernible bend. In such cases, this approach is not as conclusive. The scree graph for the winged aphid

Figure 12.8. Scree graph for eigenvalues of modified football data.

<!-- image -->

Figure 12.9. Scree graph for eigenvalues of winged aphid data.

<!-- image -->

data in Example 12.4(d) is plotted in Figure 12.9. The plot would suggest that two components be retained (possibly four).

The remainder of this section is devoted to method 4, tests of significance. The tests assume multivariate normality, which is not required for estimation of principal components.

It may be useful to make a preliminary test of complete independence of the variables, as in Section 7.4.3: H 0 : 𝚺 = diag (σ 11 , σ 22 , . . . , σ pp ) , or equivalently, H 0 : P ρ = I . The test statistic is given in (7.37) and (7.38). If the results indicate that the variables are independent, there is no point in extracting principal components, since (except for sampling fluctuation) the variables themselves already form the principal components.

To test the significance of the 'larger' components, we test the hypothesis that the last k population eigenvalues are small and equal, H 0 k : γ p -k + 1 = γ p -k + 2 = · · · = γ p , where γ 1, γ 2 , . . . , γ p denote the population eigenvalues, namely, the eigenvalues of 𝚺 . The implication is that the first sample components capture all the essential dimensions, whereas the last components reflect noise. If H 0 is true, the last k sample eigenvalues will tend to have the pattern shown by the straight line with small slope in the ideal scree graph, such as in Figure 12.8 or 12.9.

To test H 0 k : γ p -k + 1 = · · · = γ p using a likelihood ratio approach, we calculate the average of the last k eigenvalues of S ,

<!-- formula-not-decoded -->

and use the test statistic

<!-- formula-not-decoded -->

which has an approximate χ 2 -distribution. We reject H 0 if u ≥ χ 2 α,ν , where ν = 1 2 ( k -1 )( k + 2 ) .

To carry out this procedure, we could begin by testing H 02 : γ p -1 = γ p . If this is accepted, we could then test H 03 : γ p -2 = γ p -1 = γ p and continue testing in this fashion until H 0 k is rejected for some value of k .

In practice, when the variables are fairly highly correlated and the data can be successfully represented by a small number of principal components, the first three methods will typically agree on the number of components to retain, and the test in method 4 will often indicate a larger number of components.

Example 12.6. We apply the preceding four criteria to the modified football data of Example 12.2.2.

For method 1, we simply examine the eigenvalues and their proportion of variance explained, as obtained in Example 12.2.2:

|   Eigenvalue |   Proportion of Variance |   Cumulative Proportion |
|--------------|--------------------------|-------------------------|
|        3.323 |                    0.579 |                   0.579 |
|        1.374 |                    0.239 |                   0.818 |
|        0.476 |                    0.083 |                   0.901 |
|        0.325 |                    0.057 |                   0.957 |
|        0.157 |                    0.027 |                   0.985 |
|        0.088 |                    0.015 |                   1     |

To account for 82% of the variance, we would keep two components. This percent of variance is high enough for most descriptive purposes. For certain other applications, such as input to another analysis, we might wish to retain three components, which would account for 90% of the variance.

To apply method 2, we find the average eigenvalue to be

<!-- formula-not-decoded -->

Since only λ 1 and λ 2 exceed .957, we would retain two components.

For method 3, the scree graph in Figure 12.8 indicates conclusively that two components should be retained.

To implement method 4, we carry out the significance tests in (12.15). The values of the test statistic u for k = 2, 3 , . . . , 6 are as follows:

|   Eigenvalue |   k | u      | df   | χ 2 . 05   |
|--------------|-----|--------|------|------------|
|      3.32341 |   6 | 245.57 | 20   | 31.41      |
|      1.37431 |   5 | 123.93 | 14   | 23.68      |
|      0.47607 |   4 | 44.10  | 9    | 16.92      |
|      0.32468 |   3 | 23.84  | 5    | 11.07      |
|      0.1565  |   2 | 4.62   | 2    | 5.99       |
|      0.08785 |   1 |        |      |            |

The tests indicate that only the last two (population) eigenvalues are equal, and we should retain the first four. This differs from the results of the other three criteria, which are in close agreement that two components should be retained.

## 12.7 INFORMATION IN THE LAST FEW PRINCIPAL COMPONENTS

Up to this point, we have focused on using the first few principal components to summarize and simplify the data. However, the last few components may carry useful information in some applications.

Since the eigenvalues serve as variances of the principal components, the last few principal components have smaller variances. If the variance of a component is zero or close to zero, the component represents a linear relationship among the variables that is essentially constant; that is, the relationship holds for all y i 's in the sample. Thus if the last eigenvalue is near zero, it signifies the presence of a collinearity that may provide new information for the researcher. Suppose, for example, that there are five variables and y 5 = ∑ 4 j = 1 y j / 4. Then S is singular, and barring round-off error, λ 5 will be zero. Thus s 2 z 5 = 0, and z 5 is constant. As noted early in Section 12.2, the y i 's are centered, because the origin of the principal components is translated to y . Hence the constant value of z 5 is its mean, which is zero:

<!-- formula-not-decoded -->

Since this must reflect the dependency of y 5 on y 1, y 2, y 3, and y 4, the eigenvector a ′ 5 will be proportional to ( 1 , 1 , 1 , 1 , -4 ) .

## 12.8 INTERPRETATION OF PRINCIPAL COMPONENTS

In Section 12.5, we noted that principal components obtained from R are not compatible with those obtained from S . Because of this lack of scale invariance of principal components from S , the coefficients cannot be converted to standardized form, as can be done with coefficients in discriminant functions in Chapter 8 and canonical variates in Chapter 11. Hence interpretation of principal components is not as clear-cut as with previous linear functions that we have discussed. We must choose between components of S or R , knowing they will have a different interpretation. If the variables have widely disparate variances, we can use R instead of S to improve interpretation.

For certain patterns of elements in S or R , the form of the principal components can be predicted. This aid to interpretation is discussed in Section 12.8.1. As with discriminant functions and canonical variates, some writers have advocated rotation and the use of correlations between the variables and the principal components. We argue against the use of these two approaches to interpretation in Sections 12.8.2 and 12.8.3.

## 12.8.1 Special Patterns in S or R

In the covariance or correlation matrix, we may recognize a distinguishing pattern from which the structure of the principal components can be deduced. For example, we noted in Section 12.2 that if one variable has a much larger variance than the other variables, this variable will dominate the first component, which will account for most of the variance. Another case in which a component will duplicate a variable occurs when the variable is uncorrelated with the other variables. We now demonstrate this by showing that if all p variables are uncorrelated, the variables themselves are the principal components. If the variables were uncorrelated (orthogonal), S would have the form

<!-- formula-not-decoded -->

and the characteristic equation would be

<!-- formula-not-decoded -->

which has solutions

Thus the i th component is

<!-- formula-not-decoded -->

The corresponding normalized eigenvectors have a 1 in the i th position and 0's elsewhere:

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

In practice, the sample correlations (of continuous random variables) will not be zero, but if the correlations are all small, the principal components will largely duplicate the variables.

By the Perron-Forbenius theorem in Section 2.11.4, if all correlations or covariances are positive, all elements of the first eigenvector a 1 are positive. Since the remaining eigenvectors a 2, a 3 , . . . , a p are orthogonal to a 1, they must have both positive and negative elements. When all elements of a 1 are positive, the first component is a weighted average of the variables and is sometimes referred to as a measure of size . Likewise, the positive and negative coefficients in subsequent components

may be regarded as defining shape . This pattern is often seen when the variables are various measurements of an organism.

Example 12.8.1. In the modified football data of Example 12.2.2, there are a few negative covariances in S , but they are small, and all elements of the first eigenvector remain positive. The second eigenvector therefore has positive and negative elements:

| First Two Eigenvectors   | First Two Eigenvectors   | First Two Eigenvectors   |
|--------------------------|--------------------------|--------------------------|
|                          | a 1                      | a 2                      |
| WDIM                     | .207                     | - . 142                  |
| CIRCUM                   | .873                     | - . 219                  |
| FBEYE                    | .261                     | - . 231                  |
| EYEHD                    | .326                     | .891                     |
| EARHD                    | .066                     | .222                     |
| JAW                      | .128                     | . 187                    |

-

With all positive coefficients, the first component z 1 is an overall measure of head size ( z 1 increases if all six variables increase). The second component z 2 is a shape component that contrasts the vertical measurements EYEHD and EARHD with the three lateral measurements and CIRCUM ( z 2 increases if EYEHD and EARHD increase and the other four variables decrease).

## 12.8.2 Rotation

The principal components are initially obtained by rotating axes in order to line up with the natural extensions of the system, whereupon the new variables become uncorrelated and reflect the directions of maximum variance. If the resulting components do not have a satisfactory interpretation, they can be further rotated, seeking dimensions in which many of the coefficients of the linear combinations are near zero to simplify interpretation.

However, the new rotated components are correlated, and they do not successively account for maximum variance. They are, therefore, no longer principal components in the usual sense, and their routine use is questionable. For improved interpretation, you may wish to try factor analysis (Chapter 13), in which rotation does not destroy any properties. (In factor analysis, the rotation does not involve the space of the variables y 1, y 2 , . . . , yp , but another space, that of the factor loadings.)

## 12.8.3 Correlations between Variables and Principal Components

The use of correlations between variables and principal components is widely recommended as an aid to interpretation. It was noted in Sections 8.7.3 and 11.5.2 that analogous correlations for discriminant functions and canonical variates are not useful in a multivariate context because they provide only univariate information about how each variable operates by itself, ignoring the other variables. Rencher (1992b) obtained a similar result for principal components.

We denote the correlation between the i th variable yi and the j th principal component z j by ryi z j . Because of the orthogonality of the z j 's, we have the simple relationship

<!-- formula-not-decoded -->

where k is the number of components retained and R 2 yi | z 1 ,... , zk is the squared multiple correlation of yi with the z j 's. Thus r 2 yi z j forms part of R 2 yi | z 1 ,... , zk , which shows how yi relates to the z 's by itself, not what it contributes in the presence of the other y 's. The correlations are, therefore, not informative about the joint contribution of the y 's in a principal component.

Note that the simple partitioning of R 2 into the sum of squares of correlations in (12.19) does not happen in practice when the independent variables ( x 's) are correlated. However, here the z 's are principal components and are, therefore, orthogonal.

Since we do not recommend rotation or correlations for interpretation, we are left with the coefficients themselves, obtained from the eigenvectors of either S or R .

Example 12.8.3. In Example 12.8.1, the eigenvectors of S from the modified football data gave a satisfactory interpretation of the first two principal components as head size and shape. We give these in Table 12.5, along with the correlations between each of the variables y 1, y 2 , . . . , y 6 and the first two principal components z 1 and z 2. For comparison we also give R 2 yi | z 1 , z 2 for each variable.

The correlations rank the variables somewhat differently in their contribution to the components, since they form part of the univariate information provided by R 2 for each variable by itself. For example, for the first component, the correlations rank the variables in the order 2, 3, 1, 4, 6, 5, whereas the coefficients (first eigenvector) from S rank them in the order 2, 4, 3, 1, 6, 5.

## 12.9 SELECTION OF VARIABLES

We have previously discussed subset selection in connection with Wilks' /Lambda1 (Section 6.11.2), discriminant analysis (Section 8.9), classification analysis (Section 9.6),

Table 12.5. Eigenvectors Obtained from S, Correlations between Variables and Principal Components, and R 2 for the First Two Principal Components

|          | Eigenvectors from S   | Eigenvectors from S   | Correlations   | Correlations   |                     |
|----------|-----------------------|-----------------------|----------------|----------------|---------------------|
| Variable | a 1                   | a 2                   | r y i z 1      | r y i z 2      | R 2 y i | z 1 , z 2 |
| 1        | .21                   | - . 14                | .62            | - . 27         | .46                 |
| 2        | .87                   | - . 22                | .98            | - . 16         | .99                 |
| 3        | .26                   | - . 23                | .70            | - . 40         | .66                 |
| 4        | .33                   | .89                   | .49            | .86            | .98                 |
| 5        | .07                   | .22                   | .17            | .37            | .17                 |
| 6        | .13                   | - . 19                | .41            | - . 39         | .32                 |

and regression (Sections 10.2.7 and 10.7). In each case the criterion for selection of variables was the relationship of the variables to some external factor, such as dependent variable(s), separation of groups, or correct classification rates. In the context of principal components, we have no dependent variable, as in regression, and no groupings among the observations, as in discriminant analysis. With no external influence, we simply wish to find the subset that best captures the internal variation (and covariation) of the variables.

Jolliffe (1972, 1973) discussed eight selection methods and referred to the process as discarding variables . The eight methods were based on three basic approaches: multiple correlation, clustering of variables, and principal components. One of the correlation methods, for example, proceeds in a stepwise fashion, deleting at each step the variable that has the largest multiple correlation with the other variables. The clustering methods partition the variables into groups or clusters (see Chapter 14) and select a variable from each cluster.

We describe Jolliffe's principal component methods in the context of selecting a subset of 10 variables out of 50 variables. One of his techniques associates a variable with each of the first 10 principal components and retains these 10 variables. Another approach is to associate a variable with each of the last 40 principal components and delete the 40 variables. To associate a variable with a principal component, we choose the variable corresponding to the largest coefficient (in absolute value) in the component, providing the variable has not previously been selected. We can use components extracted from either S or R . For example, in the two principal components for the football data in Example 12.2.2, we would choose variables 2 and 4, which clearly have the largest coefficients in the two components. Jolliffe's methods could also be applied iteratively, with the principal components being recomputed after a variable is retained or deleted.

Jolliffe (1972) compared the eight methods using both real and simulated data and found that the methods based on principal components performed well in comparison to the regression and cluster-based methods. But he concluded that no single method was uniformly best.

McCabe (1984) suggested several criteria for selection, most of which are based on the conditional covariance matrix of the variables not selected, given those selected. He denoted the selected variables as principal variables . Let y be partitioned as

<!-- formula-not-decoded -->

where y 1 contains the selected variables and y 2 consists of the variables not selected. The corresponding covariance matrix is

<!-- formula-not-decoded -->

By (4.8), the conditional covariance matrix is given by (assuming normality)

<!-- formula-not-decoded -->

which is estimated by S 22 -S 21 S -1 11 S 12. To find a subset y 1 of size m , two of McCabe's criteria are to choose the subset y 1 that

1. minimizes | S 22 -S 21 S -1 11 S 12 | and
2. maximizes ∑ m ∗ i = 1 r 2 i , where ri , i = 1, 2 , . . . , m ∗ = min ( m , p -m ) are the canonical correlations between the m selected variables in y 1 and the p -m deleted variables in y 2.

Ideally, these criteria would be evaluated for all possible subsets so as to obtain the best subset of each size. McCabe suggested a regression approach for obtaining a percent of variance explained by a subset of variables to be compared with the percent of variance accounted for by the same number of principal components.

## PROBLEMS

- 12.1 Show that the solutions to λ = a ′ Sa / a ′ a in (12.7) are given by the eigenvalues and eigenvectors in (12.8), so that λ in (12.7) is maximized by the largest eigenvalue of S .
- 12.2 Show that the eigenvalues of

<!-- formula-not-decoded -->

are 1 ± r , as in (12.13), and that the eigenvectors are as given in (12.14).

- 12.3 (a) Give a justification based on the likelihood ratio for the test statistic u in (12.15).
- (b) Give a justification for the degrees of freedom ν = 1 2 ( k -1 )( k + 2 ) for the test statistic in (12.15).
- 12.4 Show that when S is diagonal as in (12.16), the eigenvectors have the form a ′ i = ( 0 , . . . , 0 , 1 , 0 , . . . , 0 ) , as given in (12.18).
- 12.5 Show that r 2 yi z 1 + r 2 yi z 2 +··· + r 2 yi zk = R 2 yi | z 1 ,... , zk , as in (12.19).
- 12.6 Carry out a principal component analysis of the diabetes data of Table 3.4. Use all five variables, including y 's and x 's. Use both S and R . Which do you think is more appropriate here? Show the percent of variance explained.

- Based on the average eigenvalue or a scree plot, decide how many components to retain. Can you interpret the components of either S or R ?
- 12.7 Do a principal component analysis of the probe word data of Table 3.5. Use both S and R . Which do you think is more appropriate here? Show the percent of variance explained. Based on the average eigenvalue or a scree plot, decide how many components to retain. Can you interpret the components of either S or R ?
- 12.8 Carry out a principal component analysis on all six variables of the glucose data of Table 3.8. Use both S and R . Which do you think is more appropriate here? Show the percent of variance explained. Based on the average eigenvalue or a scree plot, decide how many components to retain. Can you interpret the components of either S or R ?
- 12.9 Carry out a principal component analysis on the hematology data of Table 4.3. Use both S and R . Which do you think is more appropriate here? Show the percent of variance explained. Based on the average eigenvalue or a scree plot, decide how many components to retain. Can you interpret the components of either S or R ? Does the large variance of y 3 affect the pattern of the components of S ?
- 12.10 Carry out a principal component analysis separately for males and females in the psychological data of Table 5.1. Compare the results for the two groups. Use S .
- 12.11 Carry out a principal component analysis separately for the two species in the beetle data of Table 5.5. Compare the results for the two groups. Use S .
- 12.12 Carry out a principal component analysis on the engineer data of Table 5.6 as follows:
- (a) Use the pooled covariance matrix.
- (b) Ignore groups and use a covariance matrix based on all 40 observations.
- (c) Which of the approaches in (a) or (b) appears to be more successful?
- 12.13 Repeat the previous problem for the dystrophy data of Table 5.7.
- 12.14 Carry out a principal component analysis on all 10 variables of the Seishu data of Table 7.1. Use both S and R . Which do you think is more appropriate here? Show the percent of variance explained. Based on the average eigenvalue or a scree plot, decide how many components to retain. Can you interpret the components of either S or R ?
- 12.15 Carry out a principal component analysis on the temperature data of Table 7.2. Use both S and R . Which do you think is more appropriate here? Show the percent of variance explained. Based on the average eigenvalue or a scree plot, decide how many components to retain. Can you interpret the components of either S or R ?

## C H A P T E R 13

## Factor Analysis

## 13.1 INTRODUCTION

In factor analysis we represent the variables y 1, y 2 , . . . , yp as linear combinations of a few random variables f 1, f 2 , . . . , fm ( m &lt; p ) called factors . The factors are underlying constructs or latent variables that 'generate' the y 's. Like the original variables, the factors vary from individual to individual; but unlike the variables, the factors cannot be measured or observed. The existence of these hypothetical variables is therefore open to question.

If the original variables y 1, y 2 , . . . , yp are at least moderately correlated, the basic dimensionality of the system is less than p . The goal of factor analysis is to reduce the redundancy among the variables by using a smaller number of factors.

Suppose the pattern of the high and low correlations in the correlation matrix is such that the variables in a particular subset have high correlations among themselves but low correlations with all the other variables. Then there may be a single underlying factor that gave rise to the variables in the subset. If the other variables can be similarly grouped into subsets with a like pattern of correlations, then a few factors can represent these groups of variables. In this case the pattern in the correlation matrix corresponds directly to the factors. For example, suppose the correlation matrix has the form

|  1 . 00   | . 90   | . 05   | . 05   | . 05        |
|------------|--------|--------|--------|--------------|
| . 90       | 1 . 00 | . 05   | . 05   | . 05         |
| . 05       | . 05   | 1 . 00 | . 90   | . 90         |
|  . 05     | . 05   | . 90   | 1 . 00 | . 90        |
|   . 05   | . 05   | . 90   | . 90   | 1 . 00    |

Then variables 1 and 2 correspond to a factor, and variables 3, 4, and 5 correspond to another factor. In some cases where the correlation matrix does not have such a simple pattern, factor analysis will still partition the variables into clusters.

Factor analysis is related to principal component analysis in that both seek a simpler structure in a set of variables but they differ in many respects (see Section 13.8). For example, two differences in basic approach are as follows:

1. Principal components are defined as linear combinations of the original variables. In factor analysis, the original variables are expressed as linear combinations of the factors.
2. In principal component analysis, we explain a large part of the total variance of the variables, ∑ i sii . In factor analysis, we seek to account for the covariances or correlations among the variables.

In practice, there are some data sets for which the factor analysis model does not provide a satisfactory fit. Thus, factor analysis remains somewhat subjective in many applications, and it is considered controversial by some statisticians. Sometimes a few easily interpretable factors emerge, but for other data sets, neither the number of factors nor the interpretation is clear. Some possible reasons for these failures are discussed in Section 13.7.

## 13.2 ORTHOGONALFACTOR MODEL

## 13.2.1 Model Definition and Assumptions

Factor analysis is basically a one-sample procedure [for possible applications to data with groups, see Rencher (1998, Section 10.8)]. We assume a random sample y 1, y 2 , . . . , y n from a homogeneous population with mean vector 𝛍 and covariance matrix 𝚺 .

The factor analysis model expresses each variable as a linear combination of underlying common factors f 1, f 2 , . . . , fm , with an accompanying error term to account for that part of the variable that is unique (not in common with the other variables). For y 1, y 2 , . . . , yp in any observation vector y , the model is as follows:

<!-- formula-not-decoded -->

Ideally, m should be substantially smaller than p ; otherwise we have not achieved a parsimonious description of the variables as functions of a few underlying factors. We might regard the f 's in (13.1) as random variables that engender the y 's. The coefficients λ i j are called loadings and serve as weights, showing how each yi individually depends on the f 's. (In this chapter, we defer to common usage in the factor analysis literature and use the notation λ i j for loadings rather than eigenvalues.) With appropriate assumptions, λ i j indicates the importance of the j th factor f j to the i th variable yi and can be used in interpretation of f j . We describe or interpret f 2, for example, by examining its coefficients, λ 12, λ 22 , . . . , λ p 2. The larger loadings relate f 2 to the corresponding y 's. From these y 's, we infer a meaning or description of f 2. After estimating the λ i j 's (and rotating them; see Sections 13.2.2 and 13.5), it is hoped they will partition the variables into groups corresponding to factors.

The system of equations (13.1) bears a superficial resemblance to the multiple regression model (10.1), but there are fundamental differences. For example, (1) the f 's are unobserved and (2) the model in (13.1) represents only one observation vector, whereas (10.1) depicts all n observations.

It is assumed that for j = 1, 2 , . . . , m , E ( f j ) = 0, var ( f j ) = 1, and cov ( f j , fk ) = 0, j /negationslash= k . The assumptions for ε i , i = 1, 2 , . . . , p , are similar, except that we must allow each ε i to have a different variance, since it shows the residual part of yi that is not in common with the other variables. Thus we assume that E (ε i ) = 0, var (ε i ) = ψ i , and cov (ε i , ε k ) = 0, i /negationslash= k . In addition, we assume that cov (ε i , f j ) = 0 for all i and j . We refer to ψ i as the specific variance .

These assumptions are natural consequences of the basic model (13.1) and the goals of factor analysis. Since E ( yi -µ i ) = 0, we need E ( f j ) = 0, j = 1, 2 , . . . , m . The assumption cov ( f j , fk ) = 0 is made for parsimony in expressing the y 's as functions of as few factors as possible. The assumptions var ( f j ) = 1, var (ε i ) = ψ i , cov ( f j , fk ) = 0, and cov (ε i , f j ) = 0 yield a simple expression for the variance of yi ,

<!-- formula-not-decoded -->

which plays an important role in our development. Note that the assumption cov (ε i , ε k ) = 0 implies that the factors account for all the correlations among the y 's, that is, all that the y 's have in common. Thus the emphasis in factor analysis is on modeling the covariances or correlations among the y 's.

Model (13.1) can be written in matrix notation as

<!-- formula-not-decoded -->

where y = ( y 1 , y 2 , . . . , yp ) ′ , 𝛍 = (µ 1 , µ 2 , . . . , µ p ) ′ , f = ( f 1 , f 2 , . . . , fm ) ′ , 𝛆 = (ε 1 , ε 2 , . . . , ε p ) ′ , and

<!-- formula-not-decoded -->

We illustrate the model in (13.1) and (13.3) with p = 5 and m = 2. The model for each variable in (13.1) becomes

<!-- formula-not-decoded -->

In matrix notation as in (13.3), this becomes

<!-- formula-not-decoded -->

or y -𝛍 = 𝚲 f + 𝛆 .

The assumptions listed between (13.1) and (13.2) can be expressed concisely using vector and matrix notation: E ( f j ) = 0, j = 1, 2 , . . . , m , becomes

<!-- formula-not-decoded -->

var ( f j ) = 1, j = 1, 2 , . . . , m , and cov ( f j , fk ) = 0, j /negationslash= k , become

<!-- formula-not-decoded -->

E (ε i ) = 0, i = 1, 2 , . . . , p , becomes

<!-- formula-not-decoded -->

var (ε i ) = ψ i , i = 1, 2 , . . . , p , and cov (ε i , ε k ) = 0 , i /negationslash= k , become

<!-- formula-not-decoded -->

and cov (ε i , f j ) = 0 for all i and j becomes

<!-- formula-not-decoded -->

The notation cov ( f , 𝛆 ) indicates a rectangular matrix containing the covariances of the f 's with the ε 's:

<!-- formula-not-decoded -->

It was noted following (13.2) that the emphasis in factor analysis is on modeling the covariances among the y 's. We wish to express the 1 2 p ( p -1 ) covariances (and the p variances) of the variables y 1, y 2 , . . . , yp in terms of a simplified structure involving the pm loadings λ i j and the p specific variances ψ i ; that is, we wish to express 𝚺 in terms of 𝚲 and 𝚿 . We can do this using the model (13.3) and the assumptions (13.7), (13.9), and (13.10). Since 𝛍 does not affect variances and covariances of y , we have, from (13.3),

<!-- formula-not-decoded -->

By (13.10), 𝚲 f and 𝛆 are uncorrelated; therefore, the covariance matrix of their sum is the sum of their covariance matrices:

<!-- formula-not-decoded -->

If 𝚲 has only a few columns, say two or three, then 𝚺 = 𝚲𝚲 ′ + 𝚿 in (13.11) represents a simplified structure for 𝚺 , in which the covariances are modeled by the λ i j 's alone since 𝚿 is diagonal. For example, in the illustration in (13.5) with m = 2 factors, σ 12 would be the product of the first two rows of 𝚲 , that is,

<!-- formula-not-decoded -->

where (λ 11 , λ 12 ) is the first row of 𝚲 and (λ 21 , λ 22 ) is the second row of 𝚲 . If y 1 and y 2 have a great deal in common, they will have similar loadings on the common factors f 1 and f 2; that is, (λ 11 , λ 12 ) will be similar to (λ 21 , λ 22 ) . In this case, either λ 11 λ 21 or λ 12 λ 22 is likely to be high. On the other hand, if y 1 and y 2 have little in common, then their loadings λ 11 and λ 21 on f 1 will be different and their loadings λ 12 and λ 22 on f 2 will likewise differ. In this case, the products λ 11 λ 21 and λ 12 λ 22 will tend to be small.

We can also find the covariances of the y 's with the f 's in terms of the λ 's. Consider, for example, cov ( y 1 , f 2 ) . By (13.1), y 1 -µ 1 = λ 11 f 1 + λ 12 f 2 + · · · + λ 1 m fm + ε 1. From (13.7), f 2 is uncorrelated with all other f j 's, and by (13.10), f 2 is uncorrelated with ε 1. Thus

where

<!-- formula-not-decoded -->

since var ( f 2 ) = 1. Hence the loadings themselves represent covariances of the variables with the factors. In general,

<!-- formula-not-decoded -->

Since λ i j is the ( i j ) th element of 𝚲 , we can write (13.12) in the form

<!-- formula-not-decoded -->

If standardized variables are used, (13.11) is replaced by P ρ = 𝚲𝚲 ′ + 𝚿 , and the loadings become correlations:

<!-- formula-not-decoded -->

In (13.2), we have a partitioning of the variance of yi into a component due to the common factors, called the communality , and a component unique to yi , called the specific variance :

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Specific variance ψ

<!-- formula-not-decoded -->

The communality h 2 i is also referred to as common variance , and the specific variance ψ i has been called specificity, unique variance , or residual variance .

Assumptions (13.6)-(13.10) lead to the simple covariance structure of (13.11), 𝚺 = 𝚲𝚲 ′ + 𝚿 , which is an essential part of the factor analysis model. In schematic form, 𝚺 = 𝚲𝚲 ′ + 𝚿 has the following appearance:

where

<!-- image -->

The diagonal elements of 𝚺 can be easily modeled by adjusting the diagonal elements of 𝚿 , but 𝚲𝚲 ′ is a simplified configuration for the off-diagonal elements. Hence the critical aspect of the model involves the covariances, and this is the major emphasis of factor analysis, as noted in Section 13.1 and in comments following (13.2) and (13.10).

It is a rare population covariance matrix 𝚺 that can be expressed exactly as 𝚺 = 𝚲𝚲 ′ + 𝚿 , where 𝚿 is diagonal and 𝚲 is p × m , with m relatively small. In practice, many sample covariance matrices do not come satisfactorily close to this ideal pattern. However, we do not relax the assumptions because the structure 𝚺 = 𝚲𝚲 ′ + 𝚿 is essential for estimation of 𝚲 .

One advantage of the factor analysis model is that when it does not fit the data, the estimate of 𝚲 clearly reflects this failure. In such cases, there are two problems in the estimates: (1) it is unclear how many factors there should be, and (2) it is unclear what the factors are. In other statistical procedures, failure of assumptions may not lead to such obvious consequences in the estimates or tests. In factor analysis, the assumptions are essentially self-checking, whereas in other procedures, we typically have to check the assumptions with residual plots, tests, and so on.

## 13.2.2 Nonuniqueness of Factor Loadings

The loadings in the model (13.3) can be multiplied by an orthogonal matrix without impairing their ability to reproduce the covariance matrix in 𝚺 = 𝚲𝚲 ′ + 𝚿 . To see this, let T be an arbitrary orthogonal matrix. Then by (2.102), TT ′ = I , and we can insert TT ′ into the basic model (13.3) to obtain

<!-- formula-not-decoded -->

We then associate T with 𝚲 and associate T ′ with f so that the model becomes

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

If 𝚲 in 𝚺 = 𝚲𝚲 ′ + 𝚿 is replaced by 𝚲 ∗ = 𝚲 T , we have

<!-- formula-not-decoded -->

since TT ′ = I . Thus the new loadings 𝚲 ∗ = 𝚲 T in (13.17) reproduce the covariance matrix, just as 𝚲 does in (13.11):

<!-- formula-not-decoded -->

The new factors f ∗ = T ′ f in (13.18) satisfy the assumptions (13.6), (13.7), and (13.10); that is, E ( f ∗ ) = 0 , cov ( f ∗ ) = I , and cov ( f ∗ , 𝛆 ) = O .

The communalitites h 2 i = λ 2 i 1 + λ 2 i 2 +··· + λ 2 i m , i = 1, 2 , . . . , p , as defined in (13.15), are also unaffected by the transformation 𝚲 ∗ = 𝚲 T . This can be seen as follows. The communality h 2 i is the sum of squares of the i th row of 𝚲 . If we denote the i th row of 𝚲 by 𝛌 ′ i , then the sum of squares in vector notation is h 2 i = 𝛌 ′ i 𝛌 i . The i th row of 𝚲 ∗ = 𝚲 T is 𝛌 ∗ ′ i = 𝛌 ′ i T , and the corresponding communality is

<!-- formula-not-decoded -->

Thus the communalities remain the same for the new loadings. Note that h 2 i = λ 2 i 1 + λ 2 i 2 + · · · + λ 2 i m = 𝛌 ′ i 𝛌 i is the distance from the origin to the point 𝛌 ′ i = (λ i 1 , λ i 2 , . . . , λ i m ) in the m -dimensional space of the factor loadings. Since the distance 𝛌 ′ i 𝛌 i is the same as 𝛌 ∗ ′ i 𝛌 ∗ i , the points 𝛌 ∗ i are rotated from the points 𝛌 i . [This also follows because 𝛌 ∗ ′ i = 𝛌 ′ i T , where T is orthogonal. Multiplication of a vector by an orthogonal matrix is equivalent to a rotation of axes; see (2.103).]

The inherent potential to rotate the loadings to a new frame of reference without affecting any assumptions or properties is very useful in interpretation of the factors and will be exploited in Section 13.5.

Note that the coefficients (loadings) in (13.1) are applied to the factors, not to the variables, as they are in discriminant functions and principal components. Thus in factor analysis, the observed variables are not involved in the rotation, as they are in discriminant functions and principal components.

## 13.3 ESTIMATION OF LOADINGS AND COMMUNALITIES

In the Sections 13.3.1-13.3.4, we discuss four approaches to estimation of the loadings and communalities.

## 13.3.1 Principal Component Method

The first technique we consider is commonly called the principal component method. This name is perhaps unfortunate in that it adds to the confusion between factor

analysis and principal component analysis. In the principal component method for estimation of loadings, we do not actually calculate any principal components. The reason for the name is given following (13.25).

From a random sample y 1, y 2 , . . . , y n , we obtain the sample covariance matrix S and then attempt to find an estimator ˆ 𝚲 that will approximate the fundamental expression (13.11) with S in place of 𝚺 :

<!-- formula-not-decoded -->

In the principal component approach, we neglect ˆ 𝚿 and factor S into S = ˆ 𝚲 ˆ 𝚲 ′ . In order to factor S , we use the spectral decomposition in (2.109),

<!-- formula-not-decoded -->

where C is an orthogonal matrix constructed with normalized eigenvectors ( c ′ i c i = 1 ) of S as columns and D is a diagonal matrix with the eigenvalues θ 1, θ 2 , . . . , θ p of S on the diagonal:

<!-- formula-not-decoded -->

We use the notation θ i for eigenvalues instead of the usual λ i in order to avoid confusion with the notation λ i j used for the loadings.

To finish factoring CDC ′ in (13.21) into the form ˆ 𝚲 ˆ 𝚲 ′ , we observe that since the eigenvalues θ i of the positive semidefinite matrix S are all positive or zero, we can factor D into

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

where

With this factoring of D , (13.21) becomes

<!-- formula-not-decoded -->

This is of the form S = ˆ 𝚲 ˆ 𝚲 ′ , but we do not define ˆ 𝚲 to be CD 1 / 2 because CD 1 / 2 is p × p , and we are seeking a ˆ 𝚲 that is p × m with m &lt; p . We therefore define D 1 = diag (θ 1 , θ 2 , . . . , θ m ) with the m largest eigenvalues θ 1 &gt; θ 2 &gt; · · · &gt; θ m and C 1 = ( c 1 , c 2 , . . . , c m ) containing the corresponding eigenvectors. We then estimate 𝚲 by the first m columns of CD 1 / 2 ,

[see (2.56)], where

<!-- formula-not-decoded -->

We illustrate the structure of the λ i j 's in (13.24) for p 5 and m 2:

ˆ 𝚲 is p × m , C 1 is p × m , and D 1 / 2 1 is m × m . ˆ = =

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

We can see in (13.25) the source of the term principal component solution. The columns of ˆ 𝚲 are proportional to the eigenvectors of S , so that the loadings on the j th factor are proportional to coefficients in the j th principal component. The factors are thus related to the first m principal components, and it would seem that interpretation would be the same as for principal components. But after rotation of the loadings, the interpretation of the factors is usually different. The researcher will ordinarily prefer the rotated factors for reasons to be treated in Section 13.5.

By (2.52), the i th diagonal element of ˆ 𝚲 ˆ 𝚲 ′ is the sum of squares of the i th row of ˆ 𝚲 , or ˆ 𝛌 ′ i ˆ 𝛌 i = ∑ m j = 1 ˆ λ 2 i j . Hence to complete the approximation of S in (13.20), we define

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

where ˆ 𝚿 = diag ( ˆ ψ 1 , ˆ ψ 2 , . . . , ˆ ψ p ) . Thus in (13.27) the variances on the diagonal of S are modeled exactly, but the off-diagonal covariances are only approximate. Again, this is the challenge of factor analysis.

and write

In this method of estimation, the sums of squares of the rows and columns of ˆ 𝚲 are equal to communalities and eigenvalues, respectively. This is easily shown. By (13.26) and by analogy with (13.15), the i th communality is estimated by

<!-- formula-not-decoded -->

which is the sum of squares of the i th row of ˆ 𝚲 . The sum of squares of the j th column of ˆ 𝚲 is the j th eigenvalue of S :

<!-- formula-not-decoded -->

since the normalized eigenvectors (columns of C ) have length 1.

By (13.26) and (13.28), the variance of the i th variable is partitioned into a part due to the factors and a part due uniquely to the variable:

<!-- formula-not-decoded -->

Thus the j th factor contributes ˆ λ 2 i j to sii . The contribution of the j th factor to the total sample variance, tr ( S ) = s 11 + s 22 +··· + s pp , is, therefore,

<!-- formula-not-decoded -->

which is the sum of squares of loadings in the j th column of ˆ 𝚲 . By (13.29), this is equal to the j th eigenvalue, θ j . The proportion of total sample variance due to the j th factor is, therefore,

<!-- formula-not-decoded -->

If the variables are not commensurate, we can use standardized variables and work with the correlation matrix R . The eigenvalues and eigenvectors of R are then used in place of those of S in (13.24) to obtain estimates of the loadings. In practice, R is used more often than S and is the default in most software packages. Since the emphasis in factor analysis is on reproducing the covariances or correlations rather

than the variances, use of R is more appropriate in factor analysis than in principal components. In applications, R often gives better results than S .

If we are factoring R , the proportion corresponding to (13.32) is

<!-- formula-not-decoded -->

where p is the number of variables.

We can assess the fit of the factor analysis model by comparing the left and right sides of (13.27). The error matrix

<!-- formula-not-decoded -->

has zeros on the diagonal but nonzero off-diagonal elements. The following inequality gives a bound on the size of the elements in E :

<!-- formula-not-decoded -->

that is, the sum of squared entries in the matrix E = S -( ˆ 𝚲 ˆ 𝚲 ′ + ˆ 𝚿 ) is at most equal to the sum of squares of the deleted eigenvalues of S . If the eigenvalues are small, the residuals in the error matrix S -( ˆ 𝚲 ˆ 𝚲 ′ + ˆ 𝚿 ) are small and the fit is good.

Example 13.3.1. To illustrate the principal component method of estimation, we use a simple data set collected by Brown, Williams, and Barlow (1984). A 12-year-old girl made five ratings on a 9-point semantic differential scale for each of seven of her acquaintances. The ratings were based on the five adjectives kind, intelligent, happy, likeable, and just. Her ratings are given in Table 13.1.

Table 13.1. Perception Data: Ratings on Five Adjectives for Seven People

| People   |   Kind |   Intelligent |   Happy |   Likeable |   Just |
|----------|--------|---------------|---------|------------|--------|
| FSM1 a   |      1 |             5 |       5 |          1 |      1 |
| SISTER   |      8 |             9 |       7 |          9 |      8 |
| FSM2     |      9 |             8 |       9 |          9 |      8 |
| FATHER   |      9 |             9 |       9 |          9 |      9 |
| TEACHER  |      1 |             9 |       1 |          1 |      9 |
| MSM b    |      9 |             7 |       7 |          9 |      9 |
| FSM3     |      9 |             7 |       9 |          9 |      7 |

a Female schoolmate 1.

b Male schoolmate.

The correlation matrix for the five variables (adjectives) is as follows, with the larger values bolded:

<!-- formula-not-decoded -->

The boldface values indicate two groups of variables: { 1 , 3 , 4 } and { 2 , 5 } . We would therefore expect that the correlations among the variables can be explained fairly well by two factors.

The eigenvalues of R are 3.263, 1.538, .168, .031, and 0. Thus R is singular, which is possible in a situation such as this with only seven observations on five variables recorded in a single-digit scale. The multicollinearity among the variables induced by the fifth eigenvalue, 0, could be ascertained from the corresponding eigenvector, as noted in Section 12.7 (see Problem 13.6).

By (13.33), the first two factors account for ( 3 . 263 + 1 . 538 )/ 5 = . 96 of the total sample variance. We therefore extract two factors. The first two eigenvectors are

<!-- formula-not-decoded -->

Table 13.2. Factor Loadings by the Principal Component Method for the Perception Data of Table 13.1

|                              | Loadings   | Loadings   |                        |                           |
|------------------------------|------------|------------|------------------------|---------------------------|
| Variables                    | ˆ λ 1 j    | ˆ λ 2 j    | Communalities, ˆ h 2 i | Specific Variances, ˆ ψ i |
| Kind                         | .969       | - . 231    | .993                   | .007                      |
| Intelligent                  | .519       | .807       | .921                   | .079                      |
| Happy                        | .785       | - . 587    | .960                   | .040                      |
| Likeable                     | .971       | - . 210    | .987                   | .013                      |
| Just                         | .704       | .667       | .940                   | .060                      |
| Variance accounted for       | 3.263      | 1.538      | 4.802                  |                           |
| Proportion of total variance | .653       | .308       | .960                   |                           |
| Cumulative proportion        | .653       | .960       | .960                   |                           |

When these are multiplied by the square roots of the respective eigenvalues 3.263 and 1.538 as in (13.25), we obtain the loadings in Table 13.2.

The communalities in Table 13.2 are obtained from the sum of squares of the rows of the loadings, as in (13.28). The first one, for example, is (. 969 ) 2 + ( -. 231 ) 2 = . 993. The specific variances are obtained from (13.26) as ˆ ψ i = 1 - ˆ h 2 i using 1 in place of sii because we are factoring R rather than S . The variance accounted for by each factor is the sum of squares of the corresponding column of the loadings, as in (13.31). By (13.29), the variance accounted for is also equal to the eigenvalue in each case. Notice that the variance accounted for by the two factors adds to the sum of the communalities, since the latter is the sum of all squared loadings. By (13.33), the proportion of total variance for each factor is the variance accounted for divided by 5.

The two factors account for 96% of the total variance and therefore represent the five variables very well. To see how well the two-factor model reproduces the correlation matrix, we examine

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

which is very close to the original R . We will not attempt to interpret the factors at this point but will wait until they have been rotated in Section 13.5.2.

## 13.3.2 Principal Factor Method

In the principal component approach to estimation of the loadings, we neglected 𝚿 and factored S or R . The principal factor method (also called the principal axis method) uses an initial estimate ˆ 𝚿 and factors S - ˆ 𝚿 or R - ˆ 𝚿 to obtain

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

where ˆ 𝚲 is p × m and is calculated as in (13.24) using eigenvalues and eigenvectors of S - ˆ 𝚿 or R - ˆ 𝚿 .

The i th diagonal element of S - ˆ 𝚿 is given by sii - ˆ ψ i , which is the i th communality, ˆ h 2 i = sii - ˆ ψ i [see (13.30)]. Likewise, the diagonal elements of R - ˆ 𝚿 are the communalities ˆ h 2 i = 1 - ˆ ψ i . (Clearly, ˆ ψ i and ˆ h 2 i have different values for S than for R .) With these diagonal values, S - ˆ 𝚿 and R - ˆ 𝚿 have the form

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

A popular initial estimate for a communality in R - ˆ 𝚿 is ˆ h 2 i = R 2 i , the squared multiple correlation between yi and the other p -1 variables. This can be found as

<!-- formula-not-decoded -->

where r ii is the i th diagonal element of R -1 .

For S - ˆ 𝚿 , an initial estimate of communality analogous to (13.40) is

<!-- formula-not-decoded -->

where sii is the i th diagonal element of S and s ii is the i th diagonal element of S -1 . It can be shown that (13.41) is equivalent to

<!-- formula-not-decoded -->

which is a reasonable estimate of the amount of variance that yi has in common with the other y 's.

To use (13.40) or (13.41), R or S must be nonsingular. If R is singular, we can use the absolute value or the square of the largest correlation in the i th row of R as an estimate of communality.

After obtaining communality estimates, we calculate eigenvalues and eigenvectors of S - ˆ 𝚿 or R - ˆ 𝚿 and use (13.24) to obtain estimates of factor loadings, ˆ 𝚲 . Then the columns and rows of ˆ 𝚲 can be used to obtain new eigenvalues (variance

explained) and communalities, respectively. The sum of squares of the j th column of ˆ 𝚲 is the j th eigenvalue of S - ˆ 𝚿 or R - ˆ 𝚿 , and the sum of squares of the i th row of ˆ 𝚲 is the communality of yi . The proportion of variance explained by the j th factor is or

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

where θ j is the j th eigenvalue of S - ˆ 𝚿 or R - ˆ 𝚿 . The matrices S - ˆ 𝚿 and R - ˆ 𝚿 are not necessarily positive semidefinite and will often have some small negative eigenvalues. In such a case, the cumulative proportion of variance will exceed 1 and then decline to 1 as the negative eigenvalues are added. [Note that loadings cannot be obtained by (13.24) for the negative eigenvalues.]

Example 13.3.2. To illustrate the principal factor method, we use the perception data from Table 13.1. The correlation matrix as given in Example 13.3.1 is singular. Hence in place of multiple correlations as communality estimates, we use (the absolute value of) the largest correlation in each row of R . [The multiple correlation of y with several variables is greater than the simple correlation of y with any of the individual variables; see, for example, Rencher (2000, p. 240).] The diagonal elements of R - ˆ 𝚿 as given by (13.39) are, therefore, .995, .837, .881, .995, and .837, which are obtained from R in (13.35). The eigenvalues of R - ˆ 𝚿 are 3.202, 1.395, .030, -. 0002, and -. 080, whose sum is 4.546. The first two eigenvectors of R - ˆ 𝚿 are

<!-- formula-not-decoded -->

When these are multiplied by the square roots of the respective eigenvalues, we obtain the principal factor loadings. In Table 13.3, these are compared with the loadings obtained by the principal component method in Example 13.3.1. The two sets of loadings are very similar, as we would have expected because of the large size of the communalities. The communalities in Table 13.3 are for the principal factor loadings, as noted above. The proportion of variance in each case for the principal factor loadings is obtained by dividing the variance accounted for (eigenvalue) by the sum of the eigenvalues, 4.546; for example, 3 . 202 / 4 . 546 = . 704.

Table 13.3. Loadings Obtained by Two Different Methods for Perception Data of Table 13.1

|                              | Principal Component Loadings   | Principal Component Loadings   | Principal Factor Loadings   | Principal Factor Loadings   |               |
|------------------------------|--------------------------------|--------------------------------|-----------------------------|-----------------------------|---------------|
| Variables                    | f 1                            | f 2                            | f 1                         | f 2                         | Communalities |
| Kind                         | .969                           | - . 231                        | .981                        | - . 210                     | .995          |
| Intelligent                  | .519                           | .807                           | .487                        | .774                        | .837          |
| Happy                        | .785                           | - . 587                        | .771                        | - . 544                     | .881          |
| Likeable                     | .971                           | - . 210                        | .982                        | - . 188                     | .995          |
| Just                         | .704                           | .667                           | .667                        | .648                        | .837          |
| Variance accounted for       | 3.263                          | 1.538                          | 3.202                       | 1.395                       |               |
| Proportion of total variance | .653                           | .308                           | .704                        | .307                        |               |
| Cumulative proportion        | .653                           | .960                           | .704                        | 1.01                        |               |

## 13.3.3 Iterated Principal Factor Method

The principal factor method can easily be iterated to improve the estimates of communality. After obtaining ˆ 𝚲 from S - ˆ 𝚿 or R - ˆ 𝚿 in (13.36) or (13.37) using initial communality estimates, we can obtain new communality estimates from the loadings in ˆ 𝚲 using (13.28),

<!-- formula-not-decoded -->

These values of ˆ h 2 i are substituted into the diagonal of S - ˆ 𝚿 or R - ˆ 𝚿 , from which we obtain a new value of ˆ 𝚲 using (13.24). This process is continued until the communality estimates converge. (For some data sets, the iterative procedure does not converge.) Then the eigenvalues and eigenvectors of the final version of S - ˆ 𝚿 or R - ˆ 𝚿 are used in (13.24) to obtain the loadings.

The principal factor method and iterated principal factor method will typically yield results very close to those from the principal component method when either of the following is true.

1. The correlations are fairly large, with a resulting small value of m .
2. The number of variables, p , is large.

A shortcoming of the iterative approach is that sometimes it leads to a communality estimate ˆ h 2 i exceeding 1 (when factoring R ). Such a result is known as a Hey-

wood case (Heywood 1931). If ˆ h 2 i &gt; 1, then ˆ ψ i &lt; 0 by (13.26) and (13.28), which is clearly improper, since we cannot have a negative specific variance. Thus when a communality exceeds 1, the iterative process should stop, with the program reporting that a solution cannot be reached. Some software programs have an option of continuing the iterations by setting the communality equal to 1 in all subsequent iterations. The resulting solution with ˆ ψ i = 0 is somewhat questionable because it implies exact dependence of a variable on the factors, a possible but unlikely outcome.

Example 13.3.3. We illustrate the iterated principal factor method using the Seishu data in Table 7.1. The correlation matrix is as follows:

<!-- formula-not-decoded -->

The eigenvalues of R are 3.17, 2.56, 1.43, 1.28, .54, .47, .25, .12, .10, and .06. There is a notable gap between 1.28 and .54, and we therefore extract four factors (see Section 13.4). The first four eigenvalues account for a proportion

<!-- formula-not-decoded -->

of tr ( R ) .

For initial communality estimates, we use the squared multiple correlation between each variable and the other nine variables. These are given in Table 13.4, along with the final communalities after iteration. We multiply the first four eigenvectors of the final iterated version of R - ˆ 𝚿 by the square roots of the respective eigenvalues, as in (13.24), to obtain the factor loadings given in Table 13.4. We will not attempt to interpret the factors until after they have been rotated in Example 13.5.2(b).

## 13.3.4 Maximum Likelihood Method

If we assume that the observations y 1, y 2 , . . . , y n constitute a random sample from Np ( 𝛍 , 𝚺 ) , then 𝚲 and 𝚿 can be estimated by the method of maximum likelihood. It can be shown that the estimates ˆ 𝚲 and ˆ 𝚿 satisfy the following:

<!-- formula-not-decoded -->

Table 13.4. Iterated Principal Factor Loadings and Communalities for the Seishu Data

|                        | Loadings   | Loadings   | Loadings   | Loadings   | Initial       | Final         |
|------------------------|------------|------------|------------|------------|---------------|---------------|
| Variable               | f 1        | f 2        | f 3        | f 4        | Communalities | Communalities |
| Taste                  | .22        | .31        | .92        | .12        | .57           | 1.00          |
| Odor                   | .07        | .40        | .43        | - . 20     | .54           | .38           |
| pH                     | .80        | .04        | .05        | - . 40     | .78           | .79           |
| Acidity 1              | .41        | .22        | - . 11     | .37        | .40           | .36           |
| Acidity 2              | .94        | .28        | - . 07     | .05        | .88           | .98           |
| Sake-meter             | - . 13     | - . 67     | .10        | .56        | .77           | .79           |
| Reducing sugar         | - . 55     | .66        | .03        | - . 11     | .79           | .75           |
| Total sugar            | - . 45     | .88        | - . 14     | - . 07     | .87           | .99           |
| Alcohol                | .13        | .54        | - . 37     | .54        | .66           | .74           |
| Formyl-nitrogen        | .84        | .21        | - . 17     | - . 02     | .80           | .78           |
| Variance accounted for | 3.00       | 2.37       | 1.25       | .96        | 7.06          | 7.57          |

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

These equations must be solved iteratively, and in practice the procedure may fail to converge or may yield a Heywood case (Section 13.3.3).

We note that the proportion of variance accounted for by the factors, as given by (13.32) or (13.33), will not necessarily be in descending order for maximum likelihood factors, as it is for factors obtained from the principal component or principal factor method.

Example 13.3.4. Weillustrate the maximum likelihood method with the Seishu data of Table 7.1. The correlation matrix and its eigenvalues were given in Example 13.3.3. We extract four factors, as in Example 13.3.3. The iterative solution of (13.43), (13.44), and (13.45) yielded the loadings and communalities given in Table 13.5.

The pattern of the loadings is different from that obtained using the iterated principal factor method in Example 13.3.3, but we will not compare them until after rotation in Example 13.5.2(b). Note that the four values of variance accounted for are not in descending order.

## 13.4 CHOOSING THE NUMBER OF FACTORS, m

Several criteria have been proposed for choosing m , the number of factors. We consider four criteria, which are similar to those given in Section 12.6 for choosing the number of principal components to retain.

Table 13.5. Maximum Likelihood Loadings and Communalities for the Seishu Data

|                        | Loadings   | Loadings   | Loadings   | Loadings   |               |
|------------------------|------------|------------|------------|------------|---------------|
| Variables              | f 1        | f 2        | f 3        | f 4        | Communalities |
| Taste                  | 1.00       | 0          | 0          | 0          | 1.00          |
| Odor                   | .45        | - . 05     | .22        | .19        | .29           |
| pH                     | .22        | .68        | - . 20     | - . 40     | .71           |
| Acidity 1              | .10        | .47        | .10        | .37        | .38           |
| Acidity 2              | .20        | .98        | .02        | .00        | 1.00          |
| Sake-meter             | - . 04     | - . 31     | - . 68     | .55        | .86           |
| Reducing sugar         | .13        | - . 39     | .76        | - . 02     | .75           |
| Total sugar            | .03        | - . 22     | .96        | .02        | .98           |
| Alcohol                | - . 07     | .31        | .52        | .60        | .72           |
| Formyl-nitrogen        | .02        | .79        | - . 05     | - . 10     | .63           |
| Variance accounted for | 1.33       | 2.66       | 2.34       | 1.00       | 7.32          |

1. Choose m equal to the number of factors necessary for the variance accounted for to achieve a predetermined percentage, say 80%, of the total variance tr ( S ) or tr ( R ) .
2. Choose m equal to the number of eigenvalues greater than the average eigenvalue. For R the average is 1; for S it is p j = 1 θ j / p .
3. ∑ 3. Use the scree test based on a plot of the eigenvalues of S or R . If the graph drops sharply, followed by a straight line with much smaller slope, choose m equal to the number of eigenvalues before the straight line begins.
4. Test the hypothesis that m is the correct number of factors, H 0 : 𝚺 = 𝚲𝚲 ′ + 𝚿 , where 𝚲 is p × m .

Method 1 applies particularly to the principal component method. By (13.32), the proportion of total sample variance (variance accounted for) due to the j th factor from S is ∑ p i = 1 ˆ λ 2 i j / tr ( S ) . The corresponding proportion from R is ∑ p i = 1 ˆ λ 2 i j / p , as in (13.33). The contribution of all m factors to tr ( S ) or p is therefore ∑ p i = 1 ∑ m j = 1 ˆ λ 2 i j , which is the sum of squares of all elements of ˆ 𝚲 . For the principal component method, we see by (13.28) and (13.29) that this sum is also equal to the sum of the first m eigenvalues or to the sum of all p communalities:

<!-- formula-not-decoded -->

Thus we choose m sufficiently large so that the sum of the communalities or the sum of the eigenvalues (variance accounted for) constitutes a relatively large portion of tr ( S ) or p .

Method 1 can be extended to the principal factor method, where prior estimates of communalities are used to form S - ˆ 𝚿 or R - ˆ 𝚿 . However, S - ˆ 𝚿 or R - ˆ 𝚿 will often have some negative eigenvalues. Therefore, as values of m range from 1 to p , the cumulative proportion of eigenvalues, ∑ m j = 1 θ j / ∑ p j = 1 θ j , will exceed 1.0 and then reduce to 1.0 as the negative eigenvalues are added. Hence a percentage such as 80% will be reached for a lower value of m than would be the case for S or R , and a better strategy might be to choose m equal to the value for which the percentage first exceeds 100%.

In the iterated principal factor method, m is specified before iteration, and ∑ i ˆ h 2 i is obtained after iteration as ∑ i ˆ h 2 i = tr ( S - ˆ 𝚿 ) . To choose m before iterating, one could use a priori considerations or the eigenvalues of S or R , as in the principal component method.

Method 2 is a popular criterion of long standing and is the default in many software packages. Although heuristically based, it often works well in practice. A variation to method 2 that has been suggested for use with R - ˆ 𝚿 is to let m equal the number of positive eigenvalues. (There will typically be some negative eigenvalues of R - ˆ 𝚿 .) However, this criterion will often result in too many factors, since the sum of the positive eigenvalues will exceed the sum of the communalities.

The scree test in method 3 was named after the geological term scree , referring to the debris at the bottom of a rocky cliff. It also performs well in practice.

In method 4 we wish to test

<!-- formula-not-decoded -->

where 𝚲 is p × m . The test statistic, a function of the likelihood ratio, is

<!-- formula-not-decoded -->

which is approximately χ 2 ν when H 0 is true, where ν = 1 2 [ ( p -m ) 2 -p -m ] and ˆ 𝚲 and ˆ 𝚿 are the maximum likelihood estimators. Rejection of H 0 implies that m is too small and more factors are needed.

In practice, when n is large, the test in method 4 often shows more factors to be significant than do the other three methods. We may therefore consider the value of m indicated by the test to be an upper bound on the number of factors with practical importance.

For many data sets, the choice of m will not be obvious. This indeterminacy leaves many statisticians skeptical as to the validity of factor analysis. A researcher may begin with one of the methods (say, method 2) for an initial choice of m , will inspect the resulting percent of tr ( R ) or tr ( S ) , and will then examine the rotated loadings for interpretability. If the percent of variance or the interpretation does not seem satisfactory, the experimenter will try other values of m in a search for an acceptable compromise between percent of tr ( R ) and interpretability of the factors. Admittedly, this is a subjective procedure, and for such data sets one could well question the outcome (see Section 13.7).

When a data set is successfully fitted by a factor analysis model, the first three methods will almost always give the same value of m , and there will be little question as to what this value should be. Thus for a 'good' data set, the entire procedure becomes much more objective.

Example 13.4(a). We compare the four methods of choosing m for the perception data used in Examples 13.3.1 and 13.3.2.

Method 1 gives m = 2, because one eigenvalue accounts for 65% of tr ( R ) , and two eigenvalues account for 96%.

Method 2 gives m = 2, since λ 2 = 1 . 54 and λ 3 = . 17.

For method 3, we examine the scree plot in Figure 13.1. It is clear that m = 2 is indicated.

Method 4 is not available for the perception data because R is singular (fifth eigenvalue is zero), and the test involves | R | .

Hence for the perception data, all three available methods agree on m = 2.

Example 13.4(b). We compare the four methods of choosing m for the Seishu data used in Examples 13.3.3 and 13.3.4.

Method 1 gives m = 4 for the principal component method, because four eigenvalues of R account for 82% of tr ( R ) . For the principal factor method with initial communality estimates R 2 i , the eigenvalues of R - ˆ 𝚿 and corresponding proportions are as follows:

Figure 13.1. Scree graph for the perception data.

| Eigenvalues            |   2.86 |   2.17 |   .94 |   .88 |   .12 |   .08 |   .01 | - . 06   | - . 13   | - . 22   |
|------------------------|--------|--------|-------|-------|-------|-------|-------|----------|----------|----------|
| Proportions            |   0.43 |   0.33 |  0.14 |  0.16 |  0.02 |  0.01 |  0    | - . 01   | - . 02   | - . 03   |
| Cumulative proportions |   0.43 |   0.76 |  0.9  |  1.03 |  1.05 |  1.06 |  1.06 | 1.06     | 1.03     | 1.00     |

<!-- image -->

Figure 13.2. Scree graph for the Seishu data.

<!-- image -->

The proportions are obtained by dividing the eigenvalues by their sum, 6.63. Thus the cumulative proportion first exceeds 1.00 for m = 4.

Method 2 gives m = 4, since λ 4 = 1 . 31 and λ 5 = . 61, where λ 4 and λ 5 are eigenvalues of R .

For method 3, we examine the scree plot in Figure 13.2. There is a discernible bend in slope at the fifth eigenvalue.

For method 4, we use m = 4 in the approximate chi-squared statistic in (13.47) and obtain χ 2 = 9 . 039, with degrees of freedom

<!-- formula-not-decoded -->

Since 9 . 039 &lt; χ 2 . 05 , 11 = 19 . 68, we do not reject the hypothesis that four factors are adequate.

Thus for the Seishu data, all four methods agree on m = 4.

## 13.5 ROTATION

## 13.5.1 Introduction

As noted in Section 13.2.2, the factor loadings (rows of 𝚲 ) in the population model are unique only up to multiplication by an orthogonal matrix that rotates the loadings. The rotated loadings preserve the essential properties of the original loadings; they reproduce the covariance matrix and satisfy all basic assumptions. The estimated loading matrix ˆ 𝚲 can likewise be rotated to obtain ˆ 𝚲 ∗ = ˆ 𝚲 T , where T is orthogonal. Since TT ′ = I by (2.102), the rotated loadings provide the same estimate of the covariance matrix as before:

<!-- formula-not-decoded -->

Geometrically, the loadings in the i th row of ˆ 𝚲 constitute the coordinates of a point in the loading space corresponding to yi . Rotation of the p points gives their coordinates with respect to new axes (factors) but otherwise leaves their basic geometric configuration intact. We hope to find a new frame of reference in which the factors are more interpretable. To this end, the goal of rotation is to place the axes close to as many points as possible. If there are clusters of points (corresponding to groupings of y 's), we seek to move the axes in order to pass through or near these clusters. This would associate each group of variables with a factor (axis) and make interpretation more objective. The resulting axes then represent the natural factors.

If we can achieve a rotation in which every point is close to an axis, then each variable loads highly on the factor corresponding to the axis and has small loadings on the remaining factors. In this case, there is no ambiguity. Such a happy state of affairs is called simple structure , and interpretation is greatly simplified. We merely observe which variables are associated with each factor, and the factor is defined or named accordingly.

In order to identify the natural groupings of variables, we seek a rotation to an interpretable pattern for the loadings, in which the variables load highly on only one factor. The number of factors on which a variable has moderate or high loadings is called the complexity of the variable. In the ideal situation referred to previously as simple structure, the variables all have a complexity of 1. In this case, the variables have been clearly clustered into groups corresponding to the factors.

We consider two basic types of rotation: orthogonal and oblique . The rotation in (13.48) involving an orthogonal matrix is an orthogonal rotation; the original perpendicular axes are rotated rigidly and remain perpendicular. In an orthogonal rotation, angles and distances are preserved, communalities are unchanged, and the basic configuration of the points remains the same. Only the reference axes differ. In an oblique 'rotation' (transformation), the axes are not required to remain perpendicular and are thus free to pass closer to clusters of points.

In Sections 13.5.2 and 13.5.3, we discuss orthogonal and oblique rotations, followed by some guidelines for interpretation in Section 13.5.4.

## 13.5.2 Orthogonal Rotation

It was noted above in Section 13.5.1 that orthogonal rotations preserve communalities. This is because the rows of ˆ 𝚲 are rotated, and the distance to the origin is unchanged, which, by (13.28), is the communality. However, the variance accounted for by each factor as given in (13.31) will change, as will the corresponding proportion in (13.32) or (13.33). The proportions due to the rotated loadings will not necessarily be in descending order.

In Sections 13.5.2a and 13.5.2b, we consider two approaches to orthogonal rotation.

## 13.5.2a Graphical Approach

If there are only two factors ( m = 2 ) , we can use a graphical rotation based on a visual inspection of a plot of factor loadings. In this case, the rows of ˆ 𝚲 are pairs of

loadings, ( ˆ λ i 1 , ˆ λ i 2 ) , i = 1, 2 , . . . , p , corresponding to y 1, y 2 , . . . , yp . We choose an angle φ through which the axes can be rotated to move them closer to groupings of points. The new rotated loadings ( ˆ λ ∗ i 1 , ˆ λ ∗ i 2 ) can be measured directly on the graph as coordinates of the axes or calculated from ˆ 𝚲 ∗ = ˆ 𝚲 T using

<!-- formula-not-decoded -->

Example 13.5.2a. In Example 13.3.1, the initial factor loadings for the perception data did not provide an interpretation consistent with the two groupings of variables apparent in the pattern of correlations in R . The five pairs of loadings ( ˆ λ i 1 , ˆ λ i 2 ) corresponding to the five variables are plotted in Figure 13.3. An orthogonal rotation through -35 ◦ would bring the axes (factors) closer to the two clusters of points (variables) identified in Example 13.3.1. With the rotation, each cluster of variables corresponds much more closely to a factor. Using ˆ 𝚲 from Example 13.3.1 and -35 ◦ in T as given in (13.49), we obtain the following rotated loadings:

Figure 13.3. Plot of the two loadings for each of the five variables in the perception data of Table 13.1.

<!-- image -->

<!-- formula-not-decoded -->

In Table 13.6, we compare the rotated loadings in ˆ 𝚲 ∗ with the original loadings in ˆ 𝚲 .

<!-- formula-not-decoded -->

The interpretation of the rotated loadings is clear. As indicated by the boldface loadings in Table 13.6, the first factor is associated with variables 1, 3, and 4: kind, happy, and likeable. The second factor is associated with the other two variables: intelligent and just. This same grouping of variables is indicated by the pattern in the correlation matrix in (13.35) and can also be seen in the two clusters of points in Figure 13.3. The first factor might be described as representing a person's perceived humanity or amiability, while the second involves more logical or rational practices.

Note that if the angle between the rotated axes is allowed to be less than 90 ◦ (an oblique rotation), the lower axis representing f ∗ 1 could come closer to the points corresponding to variables 1 and 4 so that the coordinates on f ∗ 2 , .367 and .385, could be reduced. However, the basic interpretation would not change; variables 1 and 4 would still be associated with f ∗ 1 .

Table 13.6. Graphically Rotated Loadings for the Perception Data of Table 13.1

|                              | Principal Component Loadings   | Principal Component Loadings   | Graphically Rotated Loadings   | Graphically Rotated Loadings   | Communalities,   |
|------------------------------|--------------------------------|--------------------------------|--------------------------------|--------------------------------|------------------|
| Variables                    | f 1                            | f 2                            | f 1                            | f 2                            | ˆ h 2 i          |
| Kind                         | .969                           | - . 231                        | . 927                          | .367                           | .993             |
| Intelligent                  | .519                           | .807                           | - . 037                        | . 959                          | .921             |
| Happy                        | .785                           | - . 587                        | . 980                          | - . 031                        | .960             |
| Likeable                     | .971                           | - . 210                        | . 916                          | .385                           | .987             |
| Just                         | .704                           | .667                           | .194                           | . 950                          | .940             |
| Variance accounted for       | 3.263                          | 1.538                          | 2.696                          | 2.106                          | 4.802            |
| Proportion of total variance | .653                           | .308                           | .539                           | .421                           | .960             |
| Cumulative proportion        | .653                           | .960                           | .539                           | .960                           | .960             |

## 13.5.2b Varimax Rotation

The graphical approach to rotation is generally limited to m = 2. For m &gt; 2, various analytical methods have been proposed. The most popular of these is the varimax technique, which seeks rotated loadings that maximize the variance of the squared loadings in each column of ˆ 𝚲 ∗ . If the loadings in a column were nearly equal, the variance would be close to 0. As the squared loadings approach 0 and 1 (for factoring R ), the variance will approach a maximum. Thus the varimax method attempts to make the loadings either large or small to facilitate interpretation.

The varimax procedure cannot guarantee that all variables will load highly on only one factor. In fact, no procedure could do this for all possible data sets. The configuration of the points in the loading space remains fixed; we merely rotate the axes to be as close to as many points as possible. In many cases, the points are not well clustered, and the axes simply cannot be rotated so as to be near all of them. This problem is compounded by having to choose m . If m is changed, the coordinates ( ˆ λ i 1 , ˆ λ i 2 , . . . , ˆ λ i m ) change, and the relative position of the points is altered.

The varimax rotation is available in virtually all factor analysis software programs. The output typically includes the rotated loading matrix ˆ 𝚲 ∗ , the variance accounted for (sum of squares of each column of ˆ 𝚲 ∗ ), the communalities (sum of squares of each row of ˆ 𝚲 ∗ ), and the orthogonal matrix T used to obtain ˆ 𝚲 ∗ = ˆ 𝚲 T .

Example 13.5.2b(a). In Example 13.5.2a, a graphical rotation was devised visually to achieve interpretable loadings for the perception data of Table 13.1. As we would expect, the varimax method yields a similar result. The varimax rotated loadings are given in Table 13.7. For comparison, we have included the original unrotated loadings from Table 13.3 and the graphically rotated loadings from Table 13.6.

Table 13.7. Varimax Rotated Factor Loadings for the Perception Data of Table 13.1

|                              | Principal Component Loadings   | Principal Component Loadings   | Graphically Rotated Loadings   | Graphically Rotated Loadings   | Varimax Rotated Loadings   | Varimax Rotated Loadings   | Communalities   |
|------------------------------|--------------------------------|--------------------------------|--------------------------------|--------------------------------|----------------------------|----------------------------|-----------------|
| Variables                    | f 1                            | f 2                            | f 1                            | f 2                            | f 1                        | f 2                        | ˆ h 2 i         |
| Kind                         | .969                           | - .231                         | . 927                          | .367                           | . 951                      | .298                       | .993            |
| Intelligent                  | .519                           | .807                           | - .037                         | . 959                          | .033                       | . 959                      | .921            |
| Happy                        | .785                           | - .587                         | . 980                          | - .031                         | . 975                      | - .103                     | .960            |
| Likeable                     | .971                           | - .210                         | . 916                          | .385                           | . 941                      | .317                       | .987            |
| Just                         | .704                           | .667                           | .194                           | . 950                          | .263                       | . 933                      | .940            |
| Variance accounted for       | 3.263                          | 1.538                          | 2.696                          | 2.106                          | 2.811                      | 1.991                      | 4.802           |
| Proportion of total variance | .653                           | .308                           | .539                           | .421                           | .562                       | .398                       | .960            |
| Cumulative proportion        | .653                           | .960                           | .539                           | .960                           | .562                       | .960                       | .960            |

The orthogonal matrix T for the varimax rotation is

<!-- formula-not-decoded -->

By(13.49), -sin φ = . 512, and the angle of rotation is given by φ = -sin -1 (. 512 ) = -30 . 8 ◦ . Thus the varimax rotation chose an angle of rotation of -30 . 8 ◦ as compared to the -35 ◦ we selected visually, but the results are very close and the interpretation is exactly the same.

Example 13.5.2b(b). In Examples 13.3.3 and 13.3.4, we obtained the iterated principal factor loadings and maximum likelihood loadings for the Seishu data. In Table 13.8, we show the varimax rotation of these two sets of loadings. The similarities in the two sets of rotated loadings are striking. The interpretation in each case is the same. The variances accounted for are virtually identical.

The rotation in each case has achieved a satisfactory simple structure and most variables show a complexity of 1. The boldface loadings indicate the variables associated with each factor for interpretation purposes. These may be meaningful to the researcher. For example, factor 2 is associated with sake-meter, reducing sugar, and total sugar, whereas factor 3 is aligned with taste and odor.

## 13.5.3 Oblique Rotation

The term oblique rotation refers to a transformation in which the axes do not remain perpendicular. Technically, the term oblique rotation is a misnomer, since rotation implies an orthogonal transformation that preserves distances. A more accurate char-

Table 13.8. Varimax Rotated Loadings for the Seishu Data

|                        | Iterated Principal Factor Rotated Loadings   | Iterated Principal Factor Rotated Loadings   | Iterated Principal Factor Rotated Loadings   | Iterated Principal Factor Rotated Loadings   | Maximum Likelihood Rotated Loadings   | Maximum Likelihood Rotated Loadings   | Maximum Likelihood Rotated Loadings   | Maximum Likelihood Rotated Loadings   |
|------------------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|---------------------------------------|---------------------------------------|---------------------------------------|---------------------------------------|
| Variables              | f 1                                          | f 2                                          | f 3                                          | f 4                                          | f 1                                   | f 2                                   | f 3                                   | f 4                                   |
| Taste                  | .16                                          | - . 01                                       | . 99                                         | - . 09                                       | .16                                   | - . 00                                | . 98                                  | - . 10                                |
| Odor                   | - . 11                                       | .14                                          | . 48                                         | .14                                          | - . 07                                | .14                                   | . 49                                  | .17                                   |
| pH                     | . 88                                         | - . 12                                       | .02                                          | - . 13                                       | . 82                                  | - . 10                                | .08                                   | - . 15                                |
| Acidity 1              | .26                                          | - . 09                                       | .09                                          | . 54                                         | .29                                   | - . 08                                | .11                                   | . 53                                  |
| Acidity 2              | . 89                                         | - . 06                                       | .10                                          | .43                                          | . 91                                  | - . 06                                | .10                                   | .39                                   |
| Sake-meter             | - . 43                                       | - . 76                                       | .01                                          | .07                                          | - . 46                                | - . 80                                | .04                                   | .10                                   |
| Reducing sugar         | - . 37                                       | . 76                                         | .18                                          | .03                                          | - . 37                                | . 75                                  | .20                                   | .08                                   |
| Total sugar            | - . 26                                       | . 92                                         | .10                                          | .25                                          | - . 27                                | . 91                                  | .11                                   | .26                                   |
| Alcohol                | - . 01                                       | .25                                          | .00                                          | . 80                                         | - . 00                                | .25                                   | .01                                   | . 81                                  |
| Formyl-nitrogen        | . 74                                         | - . 07                                       | - . 08                                       | .20                                          | . 76                                  | - . 07                                | - . 08                                | .22                                   |
| Variance accounted for | 2.62                                         | 2.12                                         | 1.27                                         | 1.27                                         | 2.61                                  | 2.14                                  | 1.29                                  | 1.28                                  |

acterization would be oblique transformation , but the term oblique rotation is well established in the literature.

Instead of the orthogonal transformation matrix T used in (13.16), (13.17), and (13.18), an oblique rotation uses a general nonsingular transformation matrix Q to obtain f ∗ = Q ′ f , and by (3.74),

<!-- formula-not-decoded -->

Thus the new factors are correlated. Since distances and angles are not preserved, the communalities for f ∗ are different from those for f . Some program packages report communalities obtained from the original loadings, rather than the oblique loadings.

When the axes are not required to be perpendicular, they can more easily pass through the major clusters of points in the loading space (assuming there are such clusters). For example, in Figure 13.4, we have plotted the varimax rotated loadings for two factors extracted from the sons data of Table 3.7 (see Example 13.5.3 at the end of this section). Oblique axes with an angle of 38 ◦ would pass much closer to the points, and the resulting loadings would be very close to 0 and 1. However, the interpretation would not change, since the same points (variables) would be associated with the oblique axes as with the orthogonal axes.

Various analytical methods for achieving oblique rotations have been proposed and are available in program packages. Typically, the output of one of these procedures includes a pattern matrix , a structure matrix , and a matrix of correlations among the oblique factors. For interpretation, we would usually prefer the pattern

Figure 13.4. Orthogonal and oblique rotations for the sons data.

<!-- image -->

matrix rather than the structure matrix. The loadings in a row of the pattern matrix are the natural coordinates of the point (variable) on the oblique axes and serve as coefficients in the model relating the variable to the factors.

One use for an oblique rotation is to check on the orthogonality of the factors. The orthogonality in the original factors is imposed by the model and maintained by an orthogonal rotation. If an oblique rotation produces a correlation matrix that is nearly diagonal, we can be more confident that the factors are indeed orthogonal.

## Example 13.5.3. The correlation matrix for the sons data of Table 3.7 is

<!-- formula-not-decoded -->

The varimax rotated loadings for two factors obtained by the principal component method are given in Table 13.9 and plotted in Figure 13.4. An analytical oblique rotation (Harris-Kaiser orthoblique method in SAS) produced oblique axes with an angle of 38 ◦ , the same as obtained by a graphical approach. The correlation between the two factors is .79 [obtained from Q ′ Q in (13.50)], which is related to the angle by (3.15), . 79 = cos 38 ◦ . The pattern loadings are given in Table 13.9.

The oblique loadings give a much cleaner simple structure than the varimax loadings, but the interpretation is essentially the same if we neglect loadings below .45 on the varimax rotation.

In Figure 13.4, it is evident that a single factor would be adequate since the angle between axes is less than 45 ◦ . The suggestion to let m = 1 is also supported by the first three criteria in Section 13.4: the eigenvalues of R are 3.20, .38, .27, and .16. The first accounts for 80%; the second for an additional 9%. The large correlation, .79, between the two oblique factors constitutes additional evidence that a single-factor model would suffice here. In fact, the pattern in R itself indicates the presence of only one factor. The four variables form only one cluster, since all are highly correlated. There are no small correlations between groupings of variables.

Table 13.9. Varimax and Orthoblique Loadings for the Sons Data

|          | Varimax Loadings   | Varimax Loadings   | Orthoblique Pattern matrix   | Orthoblique Pattern matrix   |
|----------|--------------------|--------------------|------------------------------|------------------------------|
| Variable | f 1                | f 2                | f 1                          | f 2                          |
| 1        | .42                | . 82               | .03                          | . 90                         |
| 2        | .40                | . 85               | - .03                        | . 96                         |
| 3        | . 87               | .41                | . 97                         | - .01                        |
| 4        | . 86               | .43                | . 95                         | .01                          |

## 13.5.4 Interpretation

In Sections 13.5.1, 13.5.2, and 13.5.3, we have discussed the usefulness of rotation as an aid to interpretation. Our goal is to achieve a simple structure in which each variable loads highly on only one factor, with small loadings on all other factors. In practice, we often fail to achieve this goal, but rotation usually produces loadings that are closer to the desired simple structure.

We now suggest general guidelines for interpreting the factors by examination of the matrix of rotated factor loadings. Moving horizontally from left to right across the m loadings in each row, identify the highest loading (in absolute value). If the highest loading is of a significant size (a subjective determination, see the next paragraph), circle or underline it. This is done for each of the p variables. There may be other significant loadings in a row besides the one circled. If these are considered, the interpretation is less simple. On the other hand, there may be variables with such small communalities that no significant loading appears on any factor. In this case, the researcher may wish to increase the number of factors and run the program again so that these variables might associate with a new factor.

After identifying potentially significant loadings, the experimenter then attempts to discover some meaning in the factors and, ideally, to label or name them. This can readily be done if the group of variables associated with each factor makes sense to the researcher. But in many situations, the groupings are not so logical, and a revision can be tried, such as adjusting the size of loading deemed to be important, changing m , using a different method of estimating the loadings, or employing another type of rotation.

To assess significance of factor loadings ˆ λ i j obtained from R , a threshold value of .3 has been advocated by many writers. For most successful applications, however, a critical value of .3 is too low and will result in variables of complexity greater than 1. A target value of .5 or .6 is typically more useful. The .3 criterion is loosely based on the critical value for significance of an ordinary correlation coefficient, r . However, the distribution of the sample loadings is not the same as that of r arising from the bivariate normal. In addition, the critical value should be increased because mp values of ˆ λ i j are being tested. On the other hand, if m is large, the critical value might possibly need to be reduced somewhat. Since ˆ h 2 i = ∑ m j = 1 ˆ λ 2 i j is bounded by 1, an increase in m reduces the average squared loading in a row.

## 13.6 FACTOR SCORES

In many applications, the researcher wishes only to ascertain whether a factor analysis model fits the data and to identify the factors. In other applications, the experimenter wishes to obtain factor scores , ˆ f i = ( ˆ f i 1 , ˆ f i 2 , . . . , ˆ f i m ) ′ , i = 1, 2 , . . . , n , which are defined as estimates of the underlying factor values for each observation. There are two potential uses for such scores: (1) the behavior of the observations in terms of the factors may be of interest and (2) we may wish to use the factor scores

as input to another analysis, such as MANOVA. The latter usage resembles a similar application of principal components.

Since the f 's are not observed, we must estimate them as functions of the observed y 's. The most popular approach to estimating the factors is based on regression (Thomson 1951). We will discuss this method and also briefly describe an informal technique that can be used when R (or S ) is singular. For other approaches see Harman (1976, Chapter 16).

Since E ( f i ) = 0, we relate the f 's to the y 's by a centered regression model

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

which can be written in matrix form as

<!-- formula-not-decoded -->

We have used the notation 𝛜 to distinguish this error from 𝛆 in the original factor model y -𝛍 = 𝚲 f + 𝛆 given in (13.3). Our approach is to estimate B 1 and use the predicted value ˆ f = ˆ B ′ 1 ( y -y ) to estimate f .

The model (13.52) holds for each observation:

<!-- formula-not-decoded -->

In transposed form, the model becomes

<!-- formula-not-decoded -->

and these n equations can be combined into a single model,

<!-- formula-not-decoded -->

The model (13.53) has the appearance of a centered multivariate multiple regression model as in Section 10.4.5, with Y c in place of X c . By (10.50), the estimate for B 1 would be

<!-- formula-not-decoded -->

However, F is unobserved. To evaluate ˆ B 1 in spite of this, we first use (10.52) to rewrite (13.54) in terms of covariance matrices,

<!-- formula-not-decoded -->

In the notation of the present chapter, S yy is represented by S ; for S yf we use ˆ 𝚲 , since ˆ 𝚲 estimates cov ( y , f ) = 𝚲 in (13.13). Thus, based on the assumptions in Section 13.2.1, we can write (13.55) as

<!-- formula-not-decoded -->

Then from model (13.53), the estimated (predicted) f i values are given by

<!-- formula-not-decoded -->

If R is factored instead of S , (13.56) and (13.57) become

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

respectively, where Y s is the observed matrix of standardized variables, ( yi j -y j )/ s j .

In order to obtain factor scores by (13.57) or (13.59), S or R must be nonsingular. When R (or S ) is singular, we can obtain factor scores by a simple method based directly on the rotated loadings. We cluster the variables into groups (factors) according to the loadings and find a score for each factor by averaging the variables associated with the factor. If the variables are not commensurate, the variables should be standardized before averaging. An alternative approach would be to weight the variables by their loadings when averaging.

We would ordinarily obtain factor scores for the rotated factors rather than the original factors. Thus ˆ 𝚲 in (13.57) or (13.59) would be replaced by ˆ 𝚲 ∗ .

Example 13.6. The speaking rate of four voices was artificially manipulated by means of a rate changer without altering the pitch (Brown, Strong, and Rencher 1973). There were five rates for each voice:

<!-- formula-not-decoded -->

The resulting 20 voices were played to 30 judges, who rated them on 15 pairedopposite adjectives (variables) with a 14-point scale between poles. The following adjectives were used: intelligent, ambitious, polite, active, confident, happy, just, likeable, kind, sincere, dependable, religious, good-looking, sociable, and strong. The results were averaged over the 30 judges to produce 20 observation vectors of 15 variables each. The averaging produced very reliable data, so that even though there were only 20 observations on 15 variables, the factor analysis model fit very well. The correlation matrix is as follows:

```
R =                      1 . 00 . 90 -. 17 . 88 . 92 . 88 . 15 . 39 -. 02 -. 16 . 52 -. 15 -. 79 -. 78 . 73 . 90 1 . 00 -. 46 . 93 . 87 . 79 -. 16 . 10 -. 35 -. 42 . 25 -. 40 . 68 -. 60 . 62 -. 17 -. 46 1 . 00 -. 56 -. 13 . 07 . 85 . 75 . 88 . 91 . 68 . 88 . 21 . 31 . 25 . 88 . 93 -. 56 1 . 00 . 85 . 73 -. 25 -. 02 -. 45 -. 57 . 10 -. 53 . 58 . 84 . 50 . 92 . 87 -. 13 . 85 1 . 00 . 91 . 20 . 39 -. 09 -. 16 . 49 -. 10 . 85 . 80 . 81 . 88 . 79 . 07 . 73 . 91 1 . 00 . 27 . 53 . 12 . 06 . 66 . 08 . 90 . 85 . 78 . 15 -. 16 . 85 -. 25 . 20 . 27 1 . 00 . 85 . 81 . 79 . 79 . 81 . 43 . 54 . 53 . 39 . 10 . 75 -. 02 . 39 . 53 . 85 1 . 00 . 84 . 79 . 93 . 77 . 71 . 69 . 76 -. 02 -. 35 . 88 -. 45 -. 09 . 12 . 81 . 84 1 . 00 . 91 . 76 . 85 . 28 . 36 . 35 -. 16 -. 42 . 91 -. 57 -. 16 . 06 . 79 . 79 . 91 1 . 00 . 72 . 96 . 26 . 28 . 29 . 52 . 25 . 67 . 10 . 49 . 66 . 79 . 93 . 76 . 72 1 . 00 . 72 . 75 . 77 . 78 -. 15 -. 40 . 88 -. 53 -. 10 . 08 . 81 . 77 . 85 . 96 . 72 1 . 00 . 33 . 32 . 34 . 79 . 68 . 21 . 58 . 85 . 90 . 43 . 71 . 28 . 26 . 75 . 33 1 . 00 . 86 . 92 . 78 . 60 . 31 . 54 . 80 . 85 . 54 . 69 . 36 . 28 . 77 . 32 . 86 1 . 00 . 82 . 73 . 62 . 25 . 50 . 81 . 78 . 53 . 76 . 35 . 29 . 78 . 34 . 92 . 82 1 . 00                     
```

The eigenvalues of R are 7.91, 5.85, .31, . 26 , . . . , . 002, with the scree plot in Figure 13.5. Clearly, by any criterion for choosing m , there are two factors.

All four major methods of factor extraction discussed in Section 13.3 produced nearly identical results (after rotation). We give the initial and rotated loadings obtained from the principal component method in Table 13.10.

The two rotated factors were labeled competence and benevolence . The same two factors emerged consistently in similar studies with different voices and different judges.

The two groupings of variables can also be seen in the correlation matrix. For example, in the first row, the large correlations correspond to the boldface rotated

Figure 13.5. Scree graph for voice data.

<!-- image -->

Table 13.10. Initial and Varimax Rotated Loadings for the Voice Data

|                              | Initial Loadings   | Initial Loadings   | Rotated Loadings   | Rotated Loadings   |               |
|------------------------------|--------------------|--------------------|--------------------|--------------------|---------------|
| Variable                     | f 1                | f 2                | f 1                | f 2                | Communalities |
| Intelligent                  | .71                | - .65              | . 96               | - .06              | .93           |
| Ambitious                    | .48                | - .84              | . 90               | - .36              | .94           |
| Polite                       | .50                | .81                | - .12              | . 95               | .92           |
| Active                       | .37                | - .91              | . 86               | - .48              | .97           |
| Confident                    | .73                | - .64              | . 97               | - .04              | .95           |
| Happy                        | .83                | - .47              | . 94               | .15                | .91           |
| Just                         | .71                | .58                | .20                | . 89               | .84           |
| Likeable                     | .89                | .39                | .45                | . 87               | .95           |
| Kind                         | .58                | .75                | - .02              | . 95               | .89           |
| Sincere                      | .52                | .82                | - .11              | . 97               | .95           |
| Dependable                   | .93                | .27                | .56                | . 79               | .94           |
| Religious                    | .55                | .79                | - .07              | . 96               | .92           |
| Good looking                 | .91                | - .29              | . 89               | .35                | .91           |
| Sociable                     | .91                | - .22              | . 84               | .40                | .87           |
| Strong                       | .91                | - .21              | . 84               | .41                | .86           |
| Variance accounted for       | 7.91               | 5.85               | 7.11               | 6.65               | 13.76         |
| Proportion of total variance | .53                | .39                | .47                | .44                | .92           |
| Cumulative proportion        | .53                | .92                | .47                | .92                | .92           |

Figure 13.6. Factor scores of adjective rating of voices with five levels of manipulated rate.

<!-- image -->

loadings for f 1, whereas in the third row, the large correlations correspond to the boldface rotated loadings for f 2.

The factor scores were of primary interest in this study. The goal was to ascertain the effect of the rate manipulations on the two factors, that is, to determine the perceived change in competence and benevolence when the speaking rate is increased or decreased.

The two factor scores were obtained for each of the 20 voices; these are plotted in Figure 13.6, where a consistent effect of the manipulation of speaking rate on all four voices can clearly be seen. Decreasing the speaking rate causes the speaker to be rated less competent; increasing the rate causes the speaker to be rated less benevolent. The mean vectors (centroids) are also given in Figure 13.6 for the four speakers.

## 13.7 VALIDITY OF THE FACTOR ANALYSIS MODEL

For many statisticians, factor analysis is controversial and does not belong in a toolkit of legitimate multivariate techniques. The reasons for this mistrust include the following: the difficulty in choosing m , the many methods of extracting factors, the many rotation techniques, and the subjectivity in interpretation. Some statisticians also criticize factor analysis because of the indeterminacy of the factor loading matrix 𝚲 or ˆ 𝚲 , first noted in Section 13.2.2. However, it is the ability to rotate that gives factor analysis its utility, if not its charm.

The basic question is whether the factors really exist. The model (13.11) for the covariance matrix is 𝚺 = 𝚲𝚲 ′ + 𝚿 or 𝚺 -𝚿 = 𝚲𝚲 ′ , where 𝚲𝚲 ′ is of rank m . Many populations have covariance matrices that do not approach this pattern unless m is large. Thus the model will not fit data from such a population when we try to impose a small value of m . On the other hand, for a population in which 𝚺 is reasonably close to 𝚲𝚲 ′ + 𝚿 for small m , the sampling procedure leading to S may obscure this pattern. The researcher may believe there are underlying factors but has difficulty collecting data that will reveal them. In many cases, the basic problem is that S (or R ) contains both structure and error, and the methods of factor analysis cannot separate the two.

A statistical consultant in a university setting or elsewhere all too often sees the following scenario. A researcher designs a long questionnaire, with answers to be given in, say, a five-point semantic differential scale or Likert scale. The respondents, who vary in attitude from uninterested to resentful, hurriedly mark answers that in many cases are not even good subjective responses to the questions. Then the researcher submits the results to a handy factor analysis program. Being disappointed in the results, he or she appeals to a statistician for help. They attempt to improve the results by trying different methods of extraction, different rotations, different values of m , and so on. But it is all to no avail. The scree plot looks more like the foothills than a steep cliff with gently sloping debris at the bottom. There is no clear value of m . They have to extract 10 or 12 factors to account for, say, 60% of the variance, and interpretation of this large number of factors is hopeless. If a few underlying dimensions exist, they are totally obscured by both systematic and random errors in marking the questionnaire. A factor analysis model simply does not fit such a data set, unless a large value of m is used, which gives useless results.

It is not necessarily the 'discreteness' of the data that causes the problem, but the 'noisiness' of the data. The specified variables are not measured accurately. In some cases, discrete variables yield satisfactory results, such as in Examples 13.3.1, 13.3.2, 13.5.2a, and 13.5.2b(a), where a 12-year-old girl, responding carefully to a semantic differential scale, produced data leading to an unambiguous factor analysis. On the other hand, continuous variables do not guarantee good results [see Example 13.7(a)].

In cases in which some factors are found that provide a satisfactory fit to the data, we should still be tentative in interpretation until we can independently establish the existence of the factors. If the same factors emerge in repeated sampling from the same population or a similar one, then we can have confidence that application of the model has uncovered some real factors. Thus it is good practice to repeat the experiment to check the stability of the factors. If the data set is large enough, it could be split in half and a factor analysis performed on each half. The two solutions could be compared with each other and with the solution for the complete set.

If there is replication in the data set, it may be helpful to average over the replications. This was done to great advantage in Example 13.6, where several judges rated the same voices. Averaging over the judges produced variables that apparently possessed very low noise. Similar experimentation with different judges always pro-

duced the same factors. Unfortunately, replication of this type is unavailable in most situations.

As with other techniques in this book, factor analysis assumes that the variables are at least approximately linearly related to each other. We could make bivariate scatter plots to check this assumption.

A basic prerequisite for a factor analysis application is that the variables not be independent. To check this requirement, we could test H 0 : P ρ = I by using the test in Section 7.4.3.

Some writers have suggested that R -1 should be a near-diagonal matrix in order to successfully fit a factor analysis model. To assess how close R -1 is to a diagonal matrix, Kaiser (1970) proposed a measure of sampling adequacy ,

<!-- formula-not-decoded -->

where r 2 i j is the square of an element from R and q 2 i j is the square of an element from Q = DR -1 D , with D = [ ( diag R -1 ) 1 / 2 ] -1 . As R -1 approaches a diagonal matrix, MSA approaches 1. Kaiser and Rice (1974) suggest that MSA should exceed .8 for satisfactory results to be expected. We show some results for MSA in Example 13.7(b).

In summary, there are many data sets to which factor analysis should not be applied. One indication that R is inappropriate for factoring is the failure of the methods in Section 13.4 to clearly and rather objectively choose a value for m . If the scree plot does not have a pronounced bend or the eigenvalues do not show a large gap around 1, then R is likely to be unsuitable for factoring. In addition, the communality estimates after factoring should be fairly large.

To balance the 'good' examples in this chapter, we now give an example involving a data set that cannot be successfully modeled by factor analysis. Likewise, the problems at the end of the chapter include both 'good' and 'bad' data sets.

Example 13.7(a). As an illustration of an application of factor analysis that is less successful than previous examples in this chapter, we consider the diabetes data of Table 3.6. The correlation matrix for the five variables is as follows:

<!-- formula-not-decoded -->

The correlations are all small and the variables do not appear to have much in common. The MSA value is .49. The eigenvalues are 1.40, 1.21, 1.04, .71, and .65. Three factors would be required to account for 73% of the variance and four factors to reach 87%. This is not a useful reduction in dimensionality. The eigenvalues are plotted in a scree graph in Figure 13.7. The lack of a clear value of m is apparent.

Figure 13.7. Scree graph for diabetes data.

<!-- image -->

It is evident from the small correlations in R that the communalities of the variables will not be large. The principal component method, which essentially estimates the initial communalities as 1, gave very different final communality estimates than did the iterated principal factor method:

|                                  |   Communalities |   Communalities |   Communalities |   Communalities |   Communalities |
|----------------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|
| Principal component method       |            0.71 |            0.91 |            0.71 |            0.67 |            0.64 |
| Iterated principal factor method |            0.31 |            0.16 |            0.35 |            0.37 |            0.33 |

The communalities obtained by the iterated approach reflect more accurately the small correlations among the variables.

The varimax rotated factor loadings for three factors extracted by the iterated principal factor method are given in Table 13.11. The first factor is associated with variables 3 and 4, the second factor with variables 1 and 5, and the third with variable

Table 13.11. Varimax Rotated Factor Loadings for Iterated Principal Factors from the Diabetes Data

|                        | Rotated Loadings   | Rotated Loadings   | Rotated Loadings   |               |
|------------------------|--------------------|--------------------|--------------------|---------------|
| Variable               | f 1                | f 2                | f 3                | Communalities |
| 1                      | - .08              | . 54               | .12                | .31           |
| 2                      | .01                | .01                | . 40               | .16           |
| 3                      | . 57               | - .15              | - .03              | .35           |
| 4                      | . 57               | .22                | .02                | .37           |
| 5                      | .19                | . 47               | - .27              | .33           |
| Variance accounted for | .69                | .59                | .24                | 1.52          |

2. This clustering of variables can be seen in R , where variables 1 and 5 have a correlation of .21, variables 3 and 4 have a correlation of .29, and variable 2 has very low correlations with all other variables. However, these correlations (.21 and .29) are small, and in this case the collapsing of five variables to three factors is not a useful reduction in dimensionality, especially since the first three eigenvalues account for only 73% of tr ( R ) . The 73% is not convincingly greater than 60%, which we would expect from three original variables picked at random. This conclusion is borne out by a test of H 0 : P ρ = I . Using (7.37) and (7.38), we obtain

<!-- formula-not-decoded -->

With 1 2 p ( p -1 ) = 10 degrees of freedom, the .05 critical value for this approximate χ 2 -test is 18.31, and we have no basis to question the independence of the five variables. Thus the three factors we obtained are very likely an artifact of the present sample and would not reappear in another sample from the same population.

Example 13.7(b). For data sets used in previous examples in this chapter, the values of MSA from (13.60) are calculated as follows:

Seishu data:

MSA = . 53 ,

Sons data:

MSA = . 82 ,

Voice data: MSA = . 73

,

MSA . 49 .

Diabetes data: =

The MSA value cannot be computed for the perception data, because R is singular.

These results do not suggest great confidence in the MSA index as a sole guide to the suitability of R for factoring. We see a wide disparity in the MSA values for the first three data sets. Yet all three yielded successful factor analyses. These three MSA values seem to be inversely related to the number of factors: In the sons data, there were indications that one factor would suffice; the voice data clearly had two factors; and for the Seishu data, there were four factors.

The MSA for the diabetes data is close to that of the Seishu data. Yet the diabetes data are totally unsuitable for factor analysis, whereas the factor analysis of the Seishu data is very convincing.

## 13.8 THE RELATIONSHIP OF FACTOR ANALYSIS TO PRINCIPAL COMPONENT ANALYSIS

Both factor analysis and principal component analysis have the goal of reducing dimensionality. Because the objectives are similar, many authors discuss principal

component analysis as another type of factor analysis. This can be confusing, and we wish to underscore the distinguishing characteristics of the two techniques.

Two of the differences between factor analysis and principal component analysis were mentioned in Section 13.1: (1) In factor analysis, the variables are expressed as linear combinations of the factors, whereas the principal components are linear functions of the variables, and (2) in principal component analysis, the emphasis is on explaining the total variance ∑ i sii , as contrasted with the attempt to explain the covariances in factor analysis.

Additional differences are that (3) principal component analysis requires essentially no assumptions, whereas factor analysis makes several key assumptions; (4) the principal components are unique (assuming distinct eigenvalues of S ), whereas the factors are subject to an arbitrary rotation; and (5) if we change the number of factors, the (estimated) factors change. This does not happen in principal components.

The ability to rotate to improve interpretability is one of the advantages of factor analysis over principal components. If finding and describing some underlying factors is the goal, factor analysis may prove more useful than principal components; we would prefer factor analysis if the factor model fits the data well and we like the interpretation of the rotated factors. On the other hand, if we wish to define a smaller number of variables for input into another analysis, we would ordinarily prefer principal components, although this can sometimes be accomplished with factor scores. Occasionally, principal components are interpretable, as in the size and shape components in Example 12.8.1.

## PROBLEMS

- 13.1 Show that the assumptions lead to (13.2), var ( yi ) = λ 2 i 1 + λ 2 i 2 +···+ λ 2 i m + ψ i .
- 13.3 Show that f ∗ = T ′ f in (13.18) satisfies the assumptions (13.6) and (13.7), E ( f ∗ ) = 0 and cov ( f ∗ ) = I .
- 13.2 Verify directly that cov ( y , f ) = 𝚲 as in (13.13).
- 13.4 Show that ∑ i j e 2 i j ≤ θ 2 m + 1 + θ 2 m + 2 +··· + θ 2 p as in (13.34), where the ei j 's are the elements of E = S -( ˆ 𝚲 ˆ 𝚲 ′ + ˆ 𝚿 ) and the θ i 's are eigenvalues of S .
- 13.5 Show that ∑ p i = 1 ∑ m j = 1 ˆ λ 2 i j is equal to the sum of the first m eigenvalues and also equal to the sum of all p communalities, as in (13.46).
- 13.6 In Example 13.3.2, the correlation matrix for the perception data was shown to have an eigenvalue equal to 0. Find the multicollinearity among the five variables that this implies.
- 13.7 Use the words data of Table 5.9.
- (a) Obtain principal component loadings for two factors.
- (b) Do a graphical rotation of the two factors.
- (c) Do a varimax rotation and compare the results with those in part (b).

- 13.8 Use the ramus bone data of Table 3.6.
- (a) Extract loadings by the principal component method and do a varimax rotation. Use two factors.
- (b) Do all variables have a complexity of 1? Carry out an oblique rotation to improve the loadings.
- (c) What is the angle between the oblique axes? Would a single factor ( m = 1 ) be more appropriate here?
- 13.9 Carry out a factor analysis of the rootstock data of Table 6.2. Combine the six groups into a single sample.
- (a) Estimate the loadings for two factors by the principal component method and do a varimax rotation.
- (b) Did the rotation improve the loadings?
- 13.10 Use the fish data of Table 6.17. Combine the three groups into a single sample.
- (a) Obtain loadings on two factors by the principal component method and do a varimax rotation.
- (b) Notice the similarity of loadings for y 1 and y 2. Is there any indication in the correlation matrix as to why this is so?
- (c) Compute factor scores.
- (d) Using the factor scores, carry out a MANOVA comparing the three groups.
- 13.11 Carry out a factor analysis of the flea data in Table 5.5. Combine the two groups into a single sample.
- (a) From an examination of the eigenvalues greater than 1, the scree plot, and the percentages, is there a clear choice of m ?
- (b) Extract two factors by the principal component method and carry out a varimax rotation.
- (c) Is the rotation an improvement? Try an oblique rotation.
- 13.12 Use the engineer data of Table 5.6. Combine the two groups into a single sample.
- (a) Using a scree plot, the number of eigenvalues greater than 1, and the percentages; is there a clear choice of m ?
- (b) Extract three factors by the principal component method and carry out a varimax rotation.
- (c) Extract three factors by the principal factor method and carry out a varimax rotation.
- (d) Compare the results of parts (b) and (c).

## 13.13 Use the probe word data of Table 3.5.

- (a) Obtain loadings for two factors by the principal component method and carry out a varimax rotation.
- (b) Notice the near duplication of loadings for y 2 and y 4. Is there any indication in the correlation matrix as to why this is so?
- (c) Is the rotation satisfactory? Try an oblique rotation.

## C H A P T E R 14

## Cluster Analysis

## 14.1 INTRODUCTION

In cluster analysis we search for patterns in a data set by grouping the (multivariate) observations into clusters. The goal is to find an optimal grouping for which the observations or objects within each cluster are similar, but the clusters are dissimilar to each other. We hope to find the natural groupings in the data, groupings that make sense to the researcher.

Cluster analysis differs fundamentally from classification analysis (Chapter 9). In classification analysis, we allocate the observations to a known number of predefined groups or populations. In cluster analysis, neither the number of groups nor the groups themselves are known in advance.

To group the observations into clusters, many techniques begin with similarities between all pairs of observations. In many cases the similarities are based on some measure of distance. Other cluster methods use a preliminary choice for cluster centers or a comparison of within- and between-cluster variability. It is also possible to cluster the variables, in which case the similarity could be a correlation; see Section 14.7.

We can search for clusters graphically by plotting the observations. If there are only two variables ( p = 2 ) , we can be do this in a scatter plot (see Section 3.3). For p &gt; 2, we can plot the data in two dimensions using principal components (see Section 12.4) or biplots (see Section 15.3). For an example of a principal component plot, see Figure 12.7 in Section 12.4, in which four clear groupings of points can be observed. Another approach to plotting is provided by projection pursuit , which seeks two-dimensional projections that reveal clusters [see Friedman and Tukey (1974); Huber (1985); Sibson (1984); Jones and Sibson (1987); Yenyukov (1988); Posse (1990); Nason (1995); Ripley (1996, pp. 296-303)].

Cluster analysis has also been referred to as classification, pattern recognition (specifically, unsupervised learning), and numerical taxonomy. The techniques of cluster analysis have been extensively applied to data in many fields, such as medicine, psychiatry, sociology, criminology, anthropology, archaeology, geology, geography, remote sensing, market research, economics, and engineering.

We shall concentrate largely on quantitative variables [for categorical variables, see Gordon (1999) or Everitt (1993)]. The data matrix [see (3.17)] can be written as

<!-- formula-not-decoded -->

Two common approaches to clustering the observation vectors are hierarchical clustering and partitioning. In hierarchical clustering we typically start with n clusters, one for each observation, and end with a single cluster containing all n observations. At each step, an observation or a cluster of observations is absorbed into another cluster. We can also reverse this process, that is, start with a single cluster containing all n observations and end with n clusters of a single item each (see Section 14.3.10). In partitioning , we simply divide the observations into g clusters. This can be done by starting with an initial partitioning or with cluster centers and then reallocating the observations according to some optimality criterion. Other clustering methods that we will discuss are based on fitting mixtures of multivariate normal distributions or searching for regions of high density sometimes called modes.

where y ′ i is a row (observation vector) and y ( j ) is a column (corresponding to a variable). We generally wish to group the n y ′ i 's (rows) into g clusters. We may also wish to cluster the columns y ( j ) , j = 1, 2 , . . . , p (see Section 14.7).

There is an abundant literature on cluster analysis. Useful monographs and reviews have been given by Gordon (1999), Everitt (1993), Khattree and Naik (2000, Chapter 6), Kaufman and Rousseuw (1990), Seber (1984, Chapter 7), Anderberg (1973), and Hartigan (1975a).

## 14.2 MEASURES OF SIMILARITY OR DISSIMILARITY

Since cluster analysis attempts to identify the observation vectors that are similar and group them into clusters, many techniques use an index of similarity or proximity between each pair of observations. A convenient measure of proximity is the distance between two observations. Since a distance increases as two units become further apart, distance is actually a measure of dissimilarity .

A common distance function is the Euclidean distance between two vectors x = ( x 1 , x 2 , . . . , x p ) ′ and y = ( y 1 , y 2 , . . . , yp ) ′ , defined as

<!-- formula-not-decoded -->

To adjust for differing variances and covariances among the p variables, we could use the statistical distance

<!-- formula-not-decoded -->

[see (3.79)], where S is the sample covariance matrix. After the clusters are formed, S could be computed as the pooled within-cluster covariance matrix, but we do not know beforehand what the clusters will be. If we compute S on the unpartitioned sample, there will be distortion of the variances and covariances because of the groups in the data (assuming there really are some natural clusters). We therefore usually use the Euclidean distance given by (14.2). In some clustering procedures, it is not necessary to take the square root in (14.2) or (14.3).

Other distance measures have been suggested, for example, the Minkowski metric

<!-- formula-not-decoded -->

For r = 2 , d ( x , y ) in (14.4) becomes the Euclidean distance given in (14.2). For p = 2 and r = 1, (14.4) measures the 'city block' distance between two observations. There are distance measures for categorical data; see Gordon (1999, Chapter 2).

For the n observation vectors y 1, y 2 , . . . , y n , we can compute an n × n matrix D = ( di j ) of distances (or dissimilarities), where di j = d ( y i , y j ) is usually given by (14.2), d ( y i , y j ) = √ ( y i -y j ) ′ ( y i -y j ) . We sometimes use D = ( d 2 i j ) , where d 2 i j = d 2 ( y i , y j ) = ( y i -y j ) ′ ( y i -y j ) is the square of (14.2). The matrix D typically is symmetric with diagonal elements equal to zero.

The scale of measurement of the variables is an important consideration when using the Euclidean distance measure in (14.2). Changing the scale can affect the relative distances among the items. For example, suppose three items have the following bivariate measurements ( y 1 , y 2 ) : ( 2 , 5 ) , ( 4 , 2 ) , ( 7 , 9 ) . Using di j as given by (14.2), the matrix D = ( di j ) for these items is

<!-- formula-not-decoded -->

However, if we multiply y 1 by 100 as, for example, in changing from meters to centimeters, the matrix becomes

<!-- formula-not-decoded -->

and the largest distance is now d 13 instead of d 23. The distance rankings have been altered by scaling.

To counter this problem, each variable could be standardized in the usual way by subtracting the mean and dividing by the standard deviation of the variable. However, such scaling would ordinarily be based on the entire data set, that is, on all n values in

each column of Y in (14.1). In this case, the variables that best separate clusters might no longer do so after division by standard deviations that include between-cluster variation. If we use standardized variables, the clusters could be less well separated. The question of scaling is, therefore, not an easy one. However, standardization of this type is recommended by many authors.

By (14.2), the squared Euclidean distance between two observations x = ( x 1 , x 2 , . . . , x p ) ′ and y = ( y 1 , y 2 , . . . , yp ) ′ is d 2 ( x , y ) = ∑ p j = 1 ( x j -y j ) 2 . This can be expressed as

<!-- formula-not-decoded -->

where v 2 x = ∑ p j = 1 ( x j -x ) 2 and x = ∑ p j = 1 x j / p , with similar expressions for v 2 y and y . The correlation rxy in (14.5) is given by

<!-- formula-not-decoded -->

In Figure 14.1, we illustrate the profile (see Sections 5.9 and 6.8) for each of two observation vectors x and y . The squared Eulcidean distance in (14.5) can be used to compare the profiles of x and y in terms of levels, variation, and shape, where x and y are the levels of the two profiles, v x and v y are the variations of the profiles, and the correlation rxy is a measure of the closeness of the shapes of the two profiles. The closer rxy is to 1, the greater is the similarity in shape of the two profiles. Note that x and v x are the mean and variation of the p variables within the observation vector x , not over the n observations in the data set. A similar comment can be made about y and v y . Likewise, the correlation rxy is between the two observation vectors x and y , not between two variables. The use of rxy has been questioned by Jardine and Sibson (1971) and Wishart (1971), but Strauss, Bartko, and Carpenter (1973)

Figure 14.1. Profiles for two observation vectors x and y .

<!-- image -->

found the correlation to be superior to the Euclidean distance for finding the clusters in a particular data set.

## 14.3 HIERARCHICAL CLUSTERING

## 14.3.1 Introduction

Hierarchical methods and other clustering algorithms represent an attempt to find 'good' clusters in the data using a computationally efficient technique. It is not generally feasible to examine all possible clustering possibilities for a data set, especially a large one. The number of ways of partitioning a set of n items into g clusters is given by

<!-- formula-not-decoded -->

[see Duran and Odell (1974, Chapter 4), Jensen (1969), and Seber (1984, p. 379)]. This can be approximated by g n / g ! , which is large even for moderate values of n and g . For example, N ( 25 , 10 ) ∼ = 2 . 8 × 10 18 . The total possible number of clusters for a set of n items is ∑ n g = 1 N ( n , g ) , which, for n = 25, is greater than 10 19 . Hence, hierarchical methods and other approaches permit us to search for a reasonable solution without having to look at all possible arrangements.

As noted in Section 14.1, hierarchical clustering algorithms involve a sequential process. In each step of the agglomerative hierarchical approach, an observation or a cluster of observations is merged into another cluster. In this process, the number of clusters shrinks and the clusters themselves grow larger. We start with n clusters (individual items) and end with one single cluster containing the entire data set. An alternative approach, called the divisive method, starts with a single cluster containing all n items and partitions a cluster into two clusters at each step (see Section 14.3.10). The end result of the divisive approach is n clusters of one item each. Agglomerative methods are more commonly used than divisive methods. In either type of hierarchical clustering, a decision must be made as to the optimal number of clusters (see Section 14.5).

At each step of an agglomerative hierarchical approach, the two closest clusters are merged into a single new cluster. The process is therefore irreversible in the sense that any two items that are once lumped together in a cluster cannot be separated later in the procedure; any early mistakes cannot be corrected. Similarly, in a divisive hierarchical method, items cannot be moved to other clusters. An optional approach is to carry out a hierarchical procedure followed by a partitioning procedure in which items can be moved from one cluster to another (see Section 14.4.1).

Since an agglomerative hierarchical procedure combines the two closest clusters at each step, we must consider the question of measuring the similarity or dissimilarity of two clusters. Different approaches to measuring distance between clusters

give rise to different hierarchical methods. Agglomerative techniques are discussed in Sections 14.3.2-14.3.9, and the divisive approach is considered in Section 14.3.10.

## 14.3.2 Single Linkage (Nearest Neighbor)

In the single linkage method, the distance between two clusters A and B is defined as the minimum distance between a point in A and a point in B :

<!-- formula-not-decoded -->

where d ( y i , y j ) is the Euclidean distance in (14.2) or some other distance between the vectors y i and y j . This approach is also called the nearest neighbor method.

At each step in the single linkage method, the distance (14.8) is found for every pair of clusters, and the two clusters with smallest distance are merged. The number of clusters is therefore reduced by 1. After two clusters are merged, the procedure is repeated for the next step: the distances between all pairs of clusters are calculated again, and the pair with minimum distance is merged into a single cluster.

The results of a hierarchical clustering procedure can be displayed graphically using a tree diagram , also known as a dendrogram , which shows all the steps in the hierarchical procedure, including the distances at which clusters are merged. Dendrograms are shown in Figures 14.2 and 14.3 in Examples 14.3.2(a) and 14.3.2(b).

Example 14.3.2(a). Hartigan (1975a, p. 28) compared the crime rates per 100,000 population for various cities. The data are in Table 14.1 (taken from the 1970 U.S.

Table 14.1. City Crime Rates per 100,000 Population

| City        |   Murder |   Rape |   Robbery |   Assault |   Burglary |   Larceny |   Auto Theft |
|-------------|----------|--------|-----------|-----------|------------|-----------|--------------|
| Atlanta     |     16.5 |   24.8 |       106 |       147 |       1112 |       905 |          494 |
| Boston      |      4.2 |   13.3 |       122 |        90 |        982 |       669 |          954 |
| Chicago     |     11.6 |   24.7 |       340 |       242 |        808 |       609 |          645 |
| Dallas      |     18.1 |   34.2 |       184 |       293 |       1668 |       901 |          602 |
| Denver      |      6.9 |   41.5 |       173 |       191 |       1534 |      1368 |          780 |
| Detroit     |     13   |   35.7 |       477 |       220 |       1566 |      1183 |          788 |
| Hartford    |      2.5 |    8.8 |        68 |       103 |       1017 |       724 |          468 |
| Honolulu    |      3.6 |   12.7 |        42 |        28 |       1457 |      1102 |          637 |
| Houston     |     16.8 |   26.6 |       289 |       186 |       1509 |       787 |          697 |
| Kansas City |     10.8 |   43.2 |       255 |       226 |       1494 |       955 |          765 |
| Los Angeles |      9.7 |   51.8 |       286 |       355 |       1902 |      1386 |          862 |
| New Orleans |     10.3 |   39.7 |       266 |       283 |       1056 |      1036 |          776 |
| New York    |      9.4 |   19.4 |       522 |       267 |       1674 |      1392 |          848 |
| Portland    |      5   |   23   |       157 |       144 |       1530 |      1281 |          488 |
| Tucson      |      5.1 |   22.9 |        85 |       148 |       1206 |       756 |          483 |
| Washington  |     12.5 |   27.6 |       524 |       217 |       1496 |      1003 |          793 |

Statistical Abstract). In order to illustrate the use of the distance matrix in single linkage clustering, we use the first six observations in Table 14.1 (Atlanta through Detroit).

The distance matrix D is given by

| City    |   Distance between Cities |   Distance between Cities |   Distance between Cities |   Distance between Cities |   Distance between Cities |   Distance between Cities |
|---------|---------------------------|---------------------------|---------------------------|---------------------------|---------------------------|---------------------------|
| Atlanta |                       0   |                     536.6 |                     516.4 |                     590.2 |                     693.6 |                     716.2 |
| Boston  |                     536.6 |                       0   |                     447.4 |                     833.1 |                     915   |                     881.1 |
| Chicago |                     516.4 |                     447.4 |                       0   |                     924   |                    1073.4 |                     971.5 |
| Dallas  |                     590.2 |                     833.1 |                     924   |                       0   |                     527.7 |                     464.5 |
| Denver  |                     693.6 |                     915   |                    1073.4 |                     527.7 |                       0   |                     358.7 |
| Detroit |                     716.2 |                     881.1 |                     971.5 |                     464.5 |                     358.7 |                       0   |

The smallest distance is 358.7 between Denver and Detroit, and therefore these two cities are joined at the first step to form C 1 ={ Denver, Detroit } . In the next step, the distance matrix is calculated for Atlanta, Boston, Chicago, Dallas, and C 1:

| Atlanta   |     0 |   536.6 |   516.4 |   590.2 |   693.6 |
|-----------|-------|---------|---------|---------|---------|
| Boston    | 536.6 |     0   |   447.4 |   833.1 |   881.1 |
| Chicago   | 516.4 |   447.4 |     0   |   924   |   971.5 |
| Dallas    | 590.2 |   833.1 |   924   |     0   |   464.5 |
| C 1       | 693.6 |   881.1 |   971.5 |   464.5 |     0   |

Note that all elements of this distance matrix are contained in the original distance matrix. This same pattern will hold in subsequent distance matrices and is also characteristic of the complete linkage method [see Example 14.3.3(a)]. The smallest distance is 447.4 between Boston and Chicago. Therefore C 2 = { Boston, Chicago } . At the next step, the distance matrix is calculated for Atlanta, Dallas, C 1, and C 2:

| Atlanta   |     0 |   516.4 |   590.2 |   693.6 |
|-----------|-------|---------|---------|---------|
| C 2       | 516.4 |     0   |   833.1 |   881.1 |
| Dallas    | 590.2 |   833.1 |     0   |   464.5 |
| C 1       | 693.6 |   881.1 |   464.5 |     0   |

The smallest distance is 464.5 between Dallas and C 1, so that C 3 ={ Dallas, C 1 } . The distance matrix for Atlanta, C 2, and C 3 is given by

| Atlanta   |     0 |   516.4 |   590.2 |
|-----------|-------|---------|---------|
| C 2       | 516.4 |     0   |   833.1 |
| C 3       | 590.2 |   833.1 |     0   |

The smallest distance is 516.4, which defines C 4 = { Atlanta, C 2 } . The distance matrix for C 3 and C 4 is

<!-- formula-not-decoded -->

Figure 14.2. Dendrogram for single linkage of the first six observations in the city crime data in Table 14.1 [See Example 14.3.2(a)].

<!-- image -->

The last cluster is given by C 5 = { C 3 , C 4 } . The dendrogram for the steps in this example is given in Figure 14.2. The order in which the clusters were formed and the relative distances at which they formed can all be seen. Note that the distance scale runs from right to left.

Example 14.3.2(b). To further illustrate the single linkage method of clustering, we use the complete city crime data from Table 14.1. The dendrogram in Figure 14.3 shows the cluster groupings attained by the single linkage method.

Figure 14.3. Dendrogram for single linkage of the complete city crime data from Table 14.1 [see Example 14.3.2(b)].

<!-- image -->

## 14.3.3 Complete Linkage (Farthest Neighbor)

In the complete linkage approach, also called the farthest neighbor method, the distance between two clusters A and B is defined as the maximum distance between a point in A and a point in B :

<!-- formula-not-decoded -->

At each step, the distance (14.9) is found for every pair of clusters, and the two clusters with the smallest distance are merged.

Example 14.3.3(a). As in Example 14.3.2(a) for single linkage clustering, we illustrate the use of the distance matrix in complete linkage clustering with the first six observations of the city crime data in Table 14.1. The initial distance matrix is exactly the same as in Example 14.3.2(a):

| City    |   Distance |   Distance |   Distance |   Distance |   Distance |   Distance |
|---------|------------|------------|------------|------------|------------|------------|
| Atlanta |        0   |      536.6 |      516.4 |      590.2 |      693.6 |      716.2 |
| Boston  |      536.6 |        0   |      447.4 |      833.1 |      915   |      881.1 |
| Chicago |      516.4 |      447.4 |        0   |      924   |     1073.4 |      971.5 |
| Dallas  |      590.2 |      833.1 |      924   |        0   |      527.7 |      464.5 |
| Denver  |      693.6 |      915   |     1073.4 |      527.7 |        0   |      358.7 |
| Detroit |      716.2 |      881.1 |      971.5 |      464.5 |      358.7 |        0   |

The smallest distance is 358.7 between Denver and Detroit, and these two therefore form the first cluster, C 1 = { Denver , Detroit } . Note that since the first cluster is based on the initial distance matrix, it will be the same regardless of which hierarchical clustering method is used.

In the next step, the distance matrix is calculated for Atlanta, Boston, Chicago, Dallas, and C 1:

| Atlanta   |     0 |   536.6 |   516.4 |   590.2 |   716.2 |
|-----------|-------|---------|---------|---------|---------|
| Boston    | 536.6 |     0   |   447.4 |   833.1 |   915   |
| Chicago   | 516.4 |   447.4 |     0   |   924   |  1073.4 |
| Dallas    | 590.2 |   833.1 |   924   |     0   |   527.7 |
| C         | 716.2 |   915   |  1073.4 |   527.7 |     0   |

1

Note that this distance matrix differs from its analog for the second step in Example 14.3.2(a) only in the distances between C 1 and the other cities. All elements of this matrix and subsequent distance matrices below are contained in the original distance matrix for the six cities. The smallest distance is 447.4 between Boston and Chicago. Therefore, C 2 = { Boston , Chicago } . At the next step, distances are calculated for Atlanta, Dallas, C 1, and C 2:

| Atlanta   |     0 |   536.6 |   590.2 |   716.2 |
|-----------|-------|---------|---------|---------|
| C 2       | 536.6 |     0   |   924   |   833.1 |
| Dallas    | 590.2 |   924   |     0   |   527.7 |
| C 1       | 693.6 |   881.1 |   527.7 |     0   |

The smallest distance, 527.7, defines C 3 = { Dallas , C 1 } . The distance matrix for Atlanta, C 2, and C 3 is given by

<!-- formula-not-decoded -->

The smallest distance is 536.6 between Atlanta and C 3, so that C 4 = { Atlanta, C 2 } . The distance matrix for C 3 and C 4 is

<!-- formula-not-decoded -->

The last cluster is given by C 5 = { C 3 , C 4 } . The dendrogram in Figure 14.4 shows the steps in this example.

Figure 14.4. Dendrogram for complete linkage of the first six observations in the city crime data in Table 14.1 [see Example 14.3.3(a)].

<!-- image -->

Example 14.3.3(b). To further illustrate the complete linkage method, we use the complete crime data in Table 14.1. The dendrogram in Figure 14.5 shows the clusters found for this data set by the complete linkage approach. There are some differences between these groupings and the groupings from single linkage in Figure 14.3.

Figure 14.5. Dendrogram for complete linkage of the complete city crime data of Table 14.1 [see Example 14.3.3(b)].

<!-- image -->

## 14.3.4 Average Linkage

In the average linkage approach, the distance between two clusters A and B is defined as the average of the nAnB distances between the nA points in A and the nB points in B :

<!-- formula-not-decoded -->

where the sum is over all y i in A and all y j in B . At each step, we join the two clusters with the smallest distance, as measured by (14.10).

Example 14.3.4. Figure 14.6 shows the dendrogram resulting from the average linkage method applied to the city crime data in Table 14.1. The solution is the same as the complete linkage solution for this data set given in Example 14.3.3(b) and Figure 14.5.

## 14.3.5 Centroid

In the centroid method, the distance between two clusters A and B is defined as the Euclidean distance between the mean vectors (often called centroids) of the two clusters:

<!-- formula-not-decoded -->

where y A and y B are the mean vectors for the observation vectors in A and the observation vectors in B , respectively, and d ( y A , y B ) is defined in (14.2). We define y A and y B in the usual way, that is, y A = ∑ n A i = 1 y i / nA . The two clusters with the smallest distance between centroids are merged at each step.

After two clusters A and B are joined, the centroid of the new cluster AB is given by the weighted average

<!-- formula-not-decoded -->

Example 14.3.5. Figure 14.7 shows the dendrogram resulting from using the centroid clustering method on the complete city crime data in Table 14.1.

Note the two crossovers in the dendrogram in Figure 14.7. Boston and Chicago join at a distance of 447.4. Then that cluster joins with { Atlanta , Tucson , Hartford } at a distance of 441.1. Finally, all five join with New Orleans at a distance of 393.8. Crossovers are discussed in Section 14.3.8a.

Figure 14.6. Dendrogram for average linkage clustering of the data in Table 14.1 (see Example 14.3.4).

<!-- image -->

Figure 14.7. Dendrogram for the centroid clustering of the complete city crime data in Table 14.1 (see Example 14.3.5).

<!-- image -->

## 14.3.6 Median

If two clusters A and B are combined using the centroid method, and if A contains a larger number of items than B , then the new centroid y AB = ( nA y A + nB y B )/( nA + nB ) may be much closer to y A than to y B . To avoid weighting the mean vectors according to cluster size, we can use the median (midpoint) of the line joining A and B as the point for computing new distances to other clusters:

<!-- formula-not-decoded -->

The two clusters with the smallest distance between medians are merged at each step. Note that the median in (14.13) is not the ordinary median in the statistical sense. The terminology arises from a median of a triangle, namely, the line from a vertex to the midpoint of the opposite side.

Example 14.3.6. Figure 14.8 shows the dendrogram resulting from using the median distance clustering method on the complete city crime data in Table 14.1. In Figure 14.8, we see the same two crossovers as in Figure 14.7.

## 14.3.7 Ward's Method

Ward's method , also called the incremental sum of squares method , uses the withincluster (squared) distances and the between-cluster (squared) distances (Ward 1963, Wishart 1969a). If AB is the cluster obtained by combining clusters A and B , then the sum of within-cluster distances (of the items from the cluster mean vectors) are

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

where y AB = ( nA y A + nB y B )/( nA + nB ) , as in (14.12), and nA , nB , and nAB = nA + nB are the numbers of points in A , B , and AB , respectively. Since these sums of distances are equivalent to within-cluster sums of squares, they are denoted by SSE A , SSE B , and SSE AB .

Ward's method joins the two clusters A and B that minimize the increase in SSE, defined as

<!-- formula-not-decoded -->

<!-- image -->

Median Distance

Figure 14.8. Dendrogram for the median clustering method applied to the complete city crime data in Table 14.1 (see Example 14.3.6).

It can be shown that the increase I AB in (14.17) has the following two equivalent forms:

<!-- formula-not-decoded -->

Thus by (14.19), minimizing the increase in SSE is equivalent to minimizing the between-cluster distances. If A consists only of y i and B consists only of y j , then SSE A and SSE B are zero, and (14.17) and (14.19) reduce to

<!-- formula-not-decoded -->

Ward's method is related to the centroid method in Section 14.3.5. If the distance d ( y A , y B ) in (14.11) is squared and compared to (14.19), the only difference is the coefficient nAnB /( nA + nB ) for Ward's method. Thus the cluster sizes have an impact on Ward's method but not on the centroid method. Writing nAnB /( nA + nB ) in (14.19) as

<!-- formula-not-decoded -->

we see that as nA and nB increase, nAnB /( nA + nB ) increases. Writing the coefficient as

<!-- formula-not-decoded -->

we see that as nB increases with nA fixed, nAnB /( nA + nB ) increases. Therefore, compared to the centroid method, Ward's method is more likely to join smaller clusters or clusters of equal size.

Example 14.3.7. Figure 14.9 shows the dendrogram resulting from using Ward's clustering method on the complete city crime data in Table 14.1. The vertical axis is I AB / ∑ n i = 1 ( y i -y ) ′ ( y i -y ) , where y is the overall mean vector for the data.

## 14.3.8 Flexible Beta Method

Suppose clusters A and B have just been merged to form cluster AB . A general formula for the distance between AB and any other cluster C was given by Lance and Williams (1967):

<!-- image -->

Increase in SSE

Figure 14.9. Dendrogram for Ward's method applied to the complete city crime data in Table 14.1 (see Example 14.3.7).

<!-- formula-not-decoded -->

The distances D ( C , A ) , D ( C , B ) , and D ( A , B ) are from the distance matrix before joining A and B . The distances from AB to other clusters as given by (14.20) would be used (along with distances between other pairs of clusters) to form the next distance matrix for choosing the pair of clusters with smallest distance. This pair would then be joined at the next step.

To simplify (14.20), Lance and Williams (1967) suggested the following constraints on the parameter values:

<!-- formula-not-decoded -->

With α A = α B and γ = 0, we have 2 α A = 1 -β or α A = α B = ( 1 -β)/ 2, and we need only choose a value of β . The resulting hierarchical clustering procedure is called the flexible beta method.

The choice of β determines the characteristics of the flexible beta clustering procedure. Lance and Williams (1967) suggested the use of a small negative value of β , such as β = -. 25. If there are (or might be) outliers in the data, the use of a smaller value of β , such as β = -. 5, may be more likely to isolate these outliers into simple clusters.

The distances defined for the agglomerative hierarchical methods in Sections 14.3.2-14.3.7 can all be expressed as special cases of (14.20). The requisite parameter values are given in Table 14.2. For the centroid, median, and Ward's methods, the

Table 14.2. Parameter Values for (14.20)

| Cluster Method   | α A                     | α B                     | β                         | γ     |
|------------------|-------------------------|-------------------------|---------------------------|-------|
| Single linkage   | 1 2                     | 1 2                     | 0                         | - 1 2 |
| Complete linkage | 1 2                     | 1 2                     | 0                         | 1 2   |
| Average linkage  | n A n n                 | n B n n                 | 0                         | 0     |
| Centroid         | A + B n A n A + n B     | A + B n B n A + n B     | - n A n B ( n A + n B ) 2 | 0     |
| Median           | 1 2                     | 1 2                     | - 1 4                     | 0     |
| Ward's method    | n A + n C               | n B + n C               | - n C                     | 0     |
| Flexible beta    | A + n B + n ( 1 - β)/ 2 | A + n B + n ( 1 - β)/ 2 | A + n B + n β(< 1 )       | 0     |

distances in (14.20) must be squared distances (assuming Euclidean distances). For the other methods in Table 14.2, the distances may be either squared or unsquared.

We illustrate the choice of parameter values in Table 14.2 for the single linkage method. Using α A = α B = 1 2 , β = 0, and γ = -1 2 as in the first row of Table 14.2, (14.20) becomes

<!-- formula-not-decoded -->

If D ( C , A ) &gt; D ( C , B ) , then | D ( C , A ) -D ( C , B ) | = D ( C , A ) -D ( C , B ) , and (14.21) reduces to

<!-- formula-not-decoded -->

On the other hand, if D ( C , A ) &lt; D ( C , B ) , then | D ( C , A ) -D ( C , B ) | = D ( C , B ) -D ( C , A ) , and (14.21) reduces to

<!-- formula-not-decoded -->

Thus, (14.21) can be written as

<!-- formula-not-decoded -->

which is equivalent to (14.8), the definition of distance for the single linkage method.

Example 14.3.8. Figures 14.10 and 14.11 show dendrograms produced when using the flexible beta clustering method on the complete city crime data in Table 14.1, with β = -. 25 and β = -. 75. The two results are similar.

## 14.3.9 Properties of Hierarchical Methods

## 14.3.9a Monotonicity

If an item or a cluster joins another cluster at a distance that is less than the distance for the previous merger of two clusters, we say that an inversion or a reversal has occurred. The reversal is represented by a crossover in the dendrogram. Examples of crossovers can be found in Figures 14.7 and 14.8.

A hierarchical method in which reversals cannot occur is said to be monotonic , because the distance at each step is greater than the distance at the previous step. A distance measure or clustering method that is monotonic is also called ultrametric .

We now show that the single linkage and complete linkage methods are monotonic. Let dk be the distance at which two clusters are joined at the k th step. We can describe steps k and k + 1 in terms of four clusters A , B , C , and D . Suppose D ( A , B ) is less than the distance between any other pair among these four clusters, so that A and B are joined at step k to form AB . Then

<!-- formula-not-decoded -->

Figure 14.10. Dendrogram for the flexible beta method with β = -. 25 applied to the complete city crime data in Table 14.1 (see Example 14.3.8).

<!-- image -->

Figure 14.11. Dendrogram for the flexible beta method with β = -. 75 applied to the complete city crime data in Table 14.1 (see Example 14.3.8).

<!-- image -->

[If D ( A , B ) is less than these three distances, it is less than the other two possible distances, D ( A , D ) and D ( B , D ) .] Suppose at step k + 1 we join AB and C or we join C and D . If we merge C and D , then by (14.25), dk = D ( A , B ) &lt; D ( C , D ) = dk + 1. If we join AB and C , then for single linkage (14.24) gives

<!-- formula-not-decoded -->

By (14.25), both of D ( A , C ) and D ( B , C ) exceed D ( A , B ) , and this also holds for complete linkage. Thus, the single linkage and complete linkage methods are monotonic.

For the methods in Table 14.2 other than single linkage and complete linkage, we have γ = 0; then by (14.20) and (14.25),

<!-- formula-not-decoded -->

Thus we need α A + α B + β ≥ 1 for monotonicity. Using this criterion, we see that all methods in Table 14.1 (beyond the first two) are monotonic except the centroid and median methods. (These two methods showed crossovers in the dendrograms in Figures 14.7 and 14.8.) Because of lack of monotonicity, some authors do not recommend the centroid and median methods.

## 14.3.9b Contraction or Dilation

We now consider the characteristics of the distances or proximities between the original points. As clusters form, the properties of this space of distances may be altered somewhat. A clustering method that does not alter the spatial properties is referred to by Lance and Williams (1967) as space-conserving . A method that is not spaceconserving may either contract or dilate the space.

A method is space-contracting if newly formed clusters appear to move closer to individual observations, so that an individual item tends to join an existing cluster rather than join with another individual item to form a new cluster. This tendency is also called chaining .

A method is space-dilating if newly formed clusters appear to move away from individual observations, so that individual items tend to form new clusters rather than join existing clusters. In this case, clusters appear to be more distinct than they are.

Dubien and Warde (1979) described the spatial properties as follows. Suppose that the distances among three clusters satisfy

<!-- formula-not-decoded -->

Then a cluster method is space-conserving if

<!-- formula-not-decoded -->

A method is space-contracting if the first inequality in (14.27) does not hold and space-dilating if the second inequality does not hold.

The single linkage method is very space-contracting, with marked chaining tendencies. For this reason, single linkage is not recommended by some authors. Complete linkage on the other hand, is very space-dilating, with a tendency to artificially impose cluster boundaries.

Other hierarchical methods fall in between the extremes represented by single linkage and complete linkage. The centroid and average linkage methods are largely space-conserving, whereas Ward's method is space-contracting. Whenever a method produces reversals for a particular data set, it can be considered to be space-contracting. Thus, for example, the centroid method is space-conserving unless it has reversals, whereupon it becomes space-contracting.

The flexible beta method is space-contracting for β &gt; 0, space-conserving for β = 0, and space-dilating for β &lt; 0. A small degree of dilation may help define cluster boundaries, but too much dilation may lead to too many clusters in the early stages. Thus the recommended value of β = -. 25 may represent a good compromise.

Example 14.3.9b. To illustrate chaining in the single linkage method, consider the data plotted in Figure 14.12 (similar to Everitt 1993, p. 68). There are two distinct clusters, A and C , with intervening points labeled B that do not belong to A or C .

In Figure 14.13, the two-cluster solution for single linkage clustering places C 1 and C 11 into one cluster and all other points into another cluster. The three-cluster solution has two clusters with C 's and a cluster with A 's and B 's.

Figure 14.12. Two distinct clusters with intervening individuals.

<!-- image -->

Figure 14.13. Single linkage clustering of the data in Figure 14.12.

<!-- image -->

Adendrogram for average linkage clustering of the data in Figure 14.12 is given in Figure 14.14. For this data set, the average linkage method is more robust to chaining. The two-cluster solution separates the C 's from the A 's and B 's. The three-cluster solution completely separates the three groups, A , B , and C .

Figure 14.14. Average linkage clustering of the data in Figure 14.12.

<!-- image -->

## 14.3.9c Other Properties

The single linkage method has been criticized by many authors because of its chaining tendencies and its sensitivity to errors in distances between observations. On the other hand, the single linkage approach is better than the other methods at identifying clusters that have curvy shapes instead of spherical or elliptical shapes, and it is somewhat robust to outliers in the data.

Ward's method and the average linkage method are also relatively insensitive to outliers. For example, in the average linkage method, outliers tend to remain isolated in the early stages and to join with other outliers rather than to join with large clusters or with less compact clusters. This is due to two properties of the average linkage method: (1) the average distance between two groups (squared Euclidean distance) increases as the points in the groups are more spread out, and (2) the average distance increases as the size of the groups increases.

These two properties of the average linkage method are illustrated in one dimension in Figure 14.15 (similar to Jobson 1992, pp. 524-525), where cluster A has one point at z 1 and cluster B has two points, b 1 and b 2, located at z 2 -h and z 2 + h . The average squared distance between A and B is

<!-- formula-not-decoded -->

Thus the average distance between A and B increases as the spread of b 1 and b 2 increases (that is, as h increases).

To illustrate the second property of the average linkage method, suppose cluster B in Figure 14.15 consists of a single point located at z 2. Then, the distance between A and B is ( z 1 -z 2 ) 2 , and A is closer to B than it is if B consists of two points.

Figure 14.15. Clusters in a single dimension.

<!-- image -->

The centroid method is fairly robust to outliers. Complete linkage is somewhat sensitive to outliers and tends to produce clusters of the same size and shape. Ward's method tends to yield spherical clusters of the same size.

Many studies conclude that the best overall performers are Ward's method and the average linkage method. However, there seems to be an interaction between methods and data sets; that is, some methods work better for certain data sets, and other methods work better for other data sets.

A good strategy is to try several methods. If the results agree to some extent, you may have found some natural clusters in the data.

## 14.3.10 Divisive Methods

In the agglomerative hierarchical methods covered in Sections 14.3.2-14.3.9, we begin with n items and end with a single cluster containing all n items. As noted in the second paragraph of Section 14.3.1, a divisive hierarchical method starts with a single cluster of n items and divides it into two groups. At each step thereafter, one of the groups is divided into two subgroups. The ultimate result of a divisive algorithm is n clusters of one item each. The results can be shown in a dendrogram.

Divisive methods suffer from the same potential drawback as the agglomerative methods-namely, once a partition is made, an item cannot be moved into another group it does not belong to at the time of the partitioning. However, if larger clusters are of interest, then the divisive approach may sometimes be preferred over the agglomerative approach, in which the larger clusters are reached only after a large number of joinings of smaller groups.

Divisive algorithms are generally of two classes: monothetic and polythetic. In a monothetic approach, the division of a group into two subgroups is based on a single variable, whereas, the polythetic approach uses all p variables to make the split.

If the variables are binary (quantitative variables can be converted to binary variables), the monothetic approach can easily be applied. Division into two groups is based on presence or absence of an attribute. The variable (attribute) is chosen that maximizes a chi-square statistic or an information statistic; see Everitt (1993, pp. 8788) or Gordon (1999, pp. 130-134).

For a monothetic approach using a quantitative variable y , we seek to maximize the between-group sum of squares,

<!-- formula-not-decoded -->

where n 1 and n 2 are the two group sizes (with n 1 + n 2 = n ) , y 1 and y 2 are the group means, and y is the overall mean based on all n observations. The sum of squares SSB would be calculated for all possible splits into two groups of sizes n 1 and n 2 and for each of the p variables. The final division would be based on the variable that maximizes SSB / n i = 1 ( yi -y ) 2 .

∑ For a polythetic approach, we consider a technique proposed by MacNaughtonSmith et al. (1964). To divide a group, we work with a splinter group and the remainder. We seek the item in the remainder whose average distance (dissimilarity) from

other items in the remainder, minus its average distance from items in the splinter group, is largest. If the largest difference is positive, the item is shifted to the splinter group. If the largest difference is negative, the procedure stops, and the division is complete. We can start the splinter group with the item that has the largest average distance from the other items in the group.

Example 14.3.10. In Table 14.3 we have the track records of eight countries (Dawkins 1989). Based on the distance matrix for these eight observations, the average distance from each observation to the other seven observations is given in Table 14.4. Since USA has the greatest average distance to the other countries, USA becomes the first observation in the splinter group. Now, the average distance between each observation in the remainder to the other six observations in the remainder is calculated. Then the (average) distance between USA and each item in the remainder is calculated. (This may be found using the distance matrix since there is only one observation in the splinter group.) Finally, the difference between the average distance to the remainder and the average distance to the splinter group is calculated. The results are in Table 14.5. Because Australia has a positive difference in Table 14.5, it is added to the splinter group with USA. This process is repeated for the six countries in the remainder; the results are given in Table 14.6. Since no difference in Table 14.6 is positive, the process stops, giving the following clusters:

Table 14.3. Athletic Records for Eight Countries

| Country   |     1 |     2 |     3 |    4 |    5 | 6     |     7 |      8 |
|-----------|-------|-------|-------|------|------|-------|-------|--------|
| Australia | 10.31 | 20.06 | 44.84 | 1.74 | 3.57 | 13,28 | 27.66 | 128.3  |
| Belgium   | 10.34 | 20.68 | 45.04 | 1.73 | 3.6  | 13.22 | 27.45 | 129.95 |
| Canada    | 10.17 | 20.22 | 45.68 | 1.76 | 3.63 | 13.55 | 28.09 | 130.15 |
| GDR       | 10.12 | 20.33 | 44.87 | 1.73 | 3.56 | 13.17 | 27.42 | 129.92 |
| GB        | 10.11 | 20.21 | 44.93 | 1.7  | 3.51 | 13.01 | 27.51 | 129.13 |
| Kenya     | 10.46 | 20.66 | 44.92 | 1.73 | 3.55 | 13.10 | 27.8  | 129.75 |
| USA       |  9.93 | 19.75 | 43.86 | 1.73 | 3.53 | 13.20 | 27.43 | 128.22 |
| USSR      | 10.07 | 20    | 44.6  | 1.75 | 3.59 | 13.20 | 27.53 | 130.55 |

Event: (1) 100 m (s), (2) 200 m (s), (3) 400 m (s), (4) 800 m (min), (5) 1500 m (min), (6) 5000 m (min), (7) 10000 m (min), (8) Marathon (min).

Table 14.4. Average Distance from Each Country to the Other Seven

| Country   |   Average Distance | Country   |   Average Distance |
|-----------|--------------------|-----------|--------------------|
| USA       |              2.068 | USSR      |              1.513 |
| Aust      |              1.643 | Canada    |              1.594 |
| GB        |              1.164 | Kenya     |              1.156 |
| GDR       |              1.083 | Belgium   |              1.16  |

Table 14.5. Average Distances to Remainder and Splinter Group for Seven Countries

| Country   |   Average Distance to Remainder (1) |   Average Distance to Splinter Group (2) | Difference (1) -(2)   |
|-----------|-------------------------------------|------------------------------------------|-----------------------|
| Australia |                               1.729 |                                    1.126 | .603                  |
| GB        |                               1.108 |                                    1.504 | - . 396               |
| GDR       |                               0.918 |                                    2.07  | - 1 . 151             |
| USSR      |                               1.355 |                                    2.464 | - 1 . 111             |
| Canada    |                               1.392 |                                    2.808 | - 1 . 416             |
| Kenya     |                               0.986 |                                    2.173 | - 1 . 186             |
| Belgium   |                               0.975 |                                    2.329 | - 1 . 353             |

Table 14.6. Average Distances to Remainder and Splinter Group for Six Countries

| Country   |   Average Distance to Remainder (1) |   Average Distance to Splinter Group (2) | Difference (1) -(2)   |
|-----------|-------------------------------------|------------------------------------------|-----------------------|
| GB        |                               1.144 |                                    1.216 | - . 072               |
| GDR       |                               0.767 |                                    1.872 | - 1 . 105             |
| USSR      |                               1.169 |                                    2.373 | - 1 . 203             |
| Canada    |                               1.249 |                                    2.457 | - 1 . 208             |
| Kenya     |                               0.865 |                                    1.884 | - 1 . 019             |
| Belgium   |                               0.813 |                                    2.058 | - 1 . 245             |

C 1 = { USA , Australia } , C 2 = { GB , GDR , USSR , Canada , Kenya , Belgium } . We could continue and divide C 2 into two groups in the same way.

## 14.4 NONHIERARCHICAL METHODS

In this section, we discuss three nonhierarchical techniques: partitioning, mixtures of distributions, and density estimation. Among these three methods, partitioning is the most commonly used.

## 14.4.1 Partitioning

In the partitioning approach, the observations are separated into g clusters without using a hierarchical approach based on a matrix of distances or similarities between all pairs of points. The methods described in this section are sometimes called optimization methods rather than partitioning .

An attractive strategy would be to examine all possible ways to partition n items into g clusters and find the optimal clustering according to some criterion. However, the number of possible partitions as given by (14.7) is prohibitively large for even moderate values of n and g . Thus we seek simpler techniques.

## 14.4.1a k-Means

We now consider an approach to partitioning that is usually called the k -means method . (We will continue to use the notation g rather than k for the number of clusters.) The method allows the items to be moved from one cluster to another, a reallocation that is not available in the hierarchical methods.

We first select g items to serve as seeds . These are later replaced by the centroids (mean vectors) of the clusters. There are various ways we can choose the seeds: select g items at random (perhaps separated by a specified minimum distance), choose the first g points in the data set (again subject to a minimum distance requirement), select the g points that are mutually farthest apart, find the g points of maximum density, or specify g regularly spaced points in a gridlike pattern (these would not be actual data points).

For these methods of selecting seeds, the number of clusters, g , must be specified. Alternatively, a minimum distance between seeds may be specified, and then all items that satisfy this criterion are chosen as seeds.

After the seeds are chosen, each remaining point in the data set is assigned to the cluster with the nearest seed (based on Euclidean distance). As soon as a cluster has more than one member, the cluster seed is replaced by the centroid.

After all items are assigned to clusters, each item is examined to see if it is closer to the centroid of another cluster than to the centroid of its own cluster. If so, the item is moved to the new cluster and the two cluster centroids are updated. This process is continued until no further improvement is possible.

The k -means procedure is somewhat sensitive to the initial choice of seeds. It might be advisable to try the procedure again with another choice of seeds. If different initial choices of seeds produce widely different final clusters, or if convergence is extremely slow, there may be no natural clusters in the data.

The k -means partitioning method can also be used as a possible improvement on hierarchical techniques. We first cluster the items using a hierarchical method and then use the centroids of these clusters as seeds for a k -means approach, which will allow points to be reallocated from one cluster to another.

Example 14.4.1a. Protein consumption in 25 European countries for nine food groups is given in Table 14.7 (Hand et al. 1994, p. 298). In order to illustrate the sensitivity of the k -means clustering method to the initial choice of seeds, we use the following four methods of choosing seeds:

1. Select at random g observations that are at least a distance r apart.
2. Select the first g observations that are at least a distance r apart.
3. Select the g observations that are mutually farthest apart.
4. Use the g centroids from the g -cluster solution from the average linkage (hierarchical) clustering method.

To help choose g , the number of clusters, we plot the first two principal components in Figure 14.16. It appears that there may be at least five clusters. For the

Table 14.7. Protein Data

| Country       |   Red Meat |   White Meat |   Eggs |   Milk |   Fish |   Cereals |   Starchy Foods |   Nuts |   Fruits/Veg. |
|---------------|------------|--------------|--------|--------|--------|-----------|-----------------|--------|---------------|
| Albania       |       10.1 |          1.4 |    0.5 |    8.9 |    0.2 |      42.3 |             0.6 |    5.5 |           1.7 |
| Austria       |        8.9 |         14   |    4.3 |   19.9 |    2.1 |      28   |             3.6 |    1.3 |           4.3 |
| Belgium       |       13.5 |          9.3 |    4.1 |   17.5 |    4.5 |      26.6 |             5.7 |    2.1 |           4   |
| Bulgaria      |        7.8 |          6   |    1.6 |    8.3 |    1.2 |      56.7 |             1.1 |    3.7 |           4.2 |
| Czech.        |        9.7 |         11.4 |    2.8 |   12.5 |    2   |      34.3 |             5   |    1.1 |           4   |
| Denmark       |       10.6 |         10.8 |    3.7 |   25   |    9.9 |      21.9 |             4.8 |    0.7 |           2.4 |
| E. Germany    |        8.4 |         11.6 |    3.7 |   11.1 |    5.4 |      24.6 |             6.5 |    0.8 |           3.6 |
| Finland       |        9.5 |          4.9 |    2.7 |   33.7 |    5.8 |      26.3 |             5.1 |    1   |           1.4 |
| France        |       18   |          9.9 |    3.3 |   19.5 |    5.7 |      28.1 |             4.8 |    2.4 |           6.5 |
| Greece        |       10.2 |          3   |    2.8 |   17.6 |    5.9 |      41.7 |             2.2 |    7.8 |           6.5 |
| Hungary       |        5.3 |         12.4 |    2.9 |    9.7 |    0.3 |      40.1 |             4   |    5.4 |           4.2 |
| Ireland       |       13.9 |         10   |    4.7 |   25.8 |    2.2 |      24   |             6.2 |    1.6 |           2.9 |
| Italy         |        9   |          5.1 |    2.9 |   13.7 |    3.4 |      36.8 |             2.1 |    4.3 |           6.7 |
| Netherlands   |        9.5 |         13.6 |    3.6 |   23.4 |    2.5 |      22.4 |             4.2 |    1.8 |           3.7 |
| Norway        |        9.4 |          4.7 |    2.7 |   23.3 |    9.7 |      23   |             4.6 |    1.6 |           2.7 |
| Poland        |        6.9 |         10.2 |    2.7 |   19.3 |    3   |      36.1 |             5.9 |    2   |           6.6 |
| Portugal      |        6.2 |          3.7 |    1.1 |    4.9 |   14.2 |      27   |             5.9 |    4.7 |           7.9 |
| Romania       |        6.2 |          6.3 |    1.5 |   11.1 |    1   |      49.6 |             3.1 |    5.3 |           2.8 |
| Spain         |        7.1 |          3.4 |    3.1 |    8.6 |    7   |      29.2 |             5.7 |    5.9 |           7.2 |
| Sweden        |        9.9 |          7.8 |    3.5 |   24.7 |    7.5 |      19.5 |             3.7 |    1.4 |           2   |
| Switzerland   |       13.1 |         10.1 |    3.1 |   23.8 |    2.3 |      25.6 |             2.8 |    2.4 |           4.9 |
| UK            |       17.4 |          5.7 |    4.7 |   20.6 |    4.3 |      24.3 |             4.7 |    3.4 |           3.3 |
| USSR          |        9.3 |          4.6 |    2.1 |   16.6 |    3   |      43.6 |             6.4 |    3.4 |           2.9 |
| W. Germany    |       11.4 |         12.5 |    4.1 |   18.8 |    3.4 |      18.6 |             5.2 |    1.5 |           3.8 |
| Yugosloslavia |        4.4 |          5   |    1.2 |    9.5 |    0.6 |      55.9 |             3   |    5.7 |           3.2 |

first method, we select five observations at random that are at least a distance r = 1 from each other. The five chosen seeds are Ireland, UK, Poland, Greece, and East Germany. Using these seeds, the k -means method produced the clusters identified in Table 14.8 along with the distance of each observation from its cluster centroid.

To view the clusters, we plot the first two discriminant functions (see Section 8.4.1) in Figure 14.17. The first two discriminant functions show good separation for clusters 2, 3, and 4 but poor separation for clusters 1 and 5.

We now select the first five observations as clusters seeds. With these seeds, the k -means clustering method produced the clusters in Table 14.9. The first two discriminant functions are plotted in Figure 14.18. Good separation of clusters is seen except for clusters 2 and 3.

We next choose as cluster seeds the five observations that are mutually farthest apart. These seeds gave rise to the clusters in Table 14.10. The first two discriminant functions are plotted in Figure 14.19. Clusters 1, 3, and 4 seem very well separated, but clusters 2 and 5 show considerable overlap.

Figure 14.16. First two principal components z 1 and z 2 for the protein data in Table 14.7.

<!-- image -->

Table 14.8. k -Means Cluster Solution for Seeds Chosen at Random

| Country     |   Cluster |   Distance from Centroid | Country    |   Cluster |   Distance from Centroid |
|-------------|-----------|--------------------------|------------|-----------|--------------------------|
| Portugal    |         1 |                    1.466 | Sweden     |         4 |                    1.594 |
| Spain       |         1 |                    1.466 | E. Germany |         4 |                    1.966 |
| Netherlands |         2 |                    1.123 | Norway     |         4 |                    2.031 |
| Austria     |         2 |                    1.217 | France     |         4 |                    2.621 |
| Czech.      |         2 |                    1.385 | Romania    |         5 |                    1.066 |
| Switzerland |         2 |                    1.657 | Yugoslavia |         5 |                    1.701 |
| Poland      |         2 |                    1.914 | Bulgaria   |         5 |                    1.741 |
| Ireland     |         3 |                    1.334 | Italy      |         5 |                    2.092 |
| UK          |         3 |                    1.821 | Hungary    |         5 |                    2.443 |
| Finland     |         3 |                    2.261 | USSR       |         5 |                    2.613 |
| Belgium     |         4 |                    1.201 | Albania    |         5 |                    2.725 |
| W. Germany  |         4 |                    1.405 | Greece     |         5 |                    2.741 |

Figure 14.17. First two discriminant functions z 1 and z 2 for the clusters in Table 14.8.

<!-- image -->

Table 14.9. k -Means Cluster Solution Using the First Five Observations as Seeds

| Country     |   Cluster |   Distance from Centroid | Country    | Cluster   | Distance from Centroid   |
|-------------|-----------|--------------------------|------------|-----------|--------------------------|
| Albania     |         1 |                    0     | Romania    | 4         | 1.415                    |
| Netherlands |         2 |                    0.648 | Bulgaria   | 4         | 1.587                    |
| Austria     |         2 |                    1     | Yugoslavia | 4         | 1.784                    |
| W. Germany  |         2 |                    1.087 | Italy      | 4         | 1.898                    |
| Switzerland |         2 |                    1.489 | Greece     | 4         | 2.450                    |
| Belgium     |         3 |                    1.368 | Poland     | 5         | 1.709                    |
| Sweden      |         3 |                    1.462 | Czech.     | 5         | 1.956                    |
| Denmark     |         3 |                    1.666 | USSR       | 5         | 2.218                    |
| Ireland     |         3 |                    1.832 | E. Germany | 5         | 2.285                    |
| Norway      |         3 |                    1.927 | Spain      | 5         | 2.344                    |
| UK          |         3 |                    2.076 | Hungary    | 5         | 2.558                    |
| Finland     |         3 |                    2.341 | Portugal   | 5         | 3.859                    |
| France      |         3 |                    2.629 |            |           |                          |

Figure 14.18. First two discriminant functions z 1 and z 2 for the clusters in Table 14.9.

<!-- image -->

Table 14.10. k -Means Cluster Solution Using as Seeds the Five Observations Furthest Apart

| Country     |   Cluster |   Distance from Centroid | Country   | Cluster   | Distance from Centroid   |
|-------------|-----------|--------------------------|-----------|-----------|--------------------------|
| Romania     |         1 |                    0.601 | France    | 2         | 2.358                    |
| Yugoslavia  |         1 |                    1.159 | Poland    | 2         | 2.405                    |
| Bulgaria    |         1 |                    1.435 | UK        | 2         | 2.537                    |
| Albania     |         1 |                    2.421 | Greece    | 3         | 1.075                    |
| Hungary     |         1 |                    2.54  | Italy     | 3         | 1.075                    |
| Belgium     |         2 |                    0.956 | Portugal  | 4         | 1.466                    |
| W. Germany  |         2 |                    1.012 | Spain     | 4         | 1.466                    |
| Netherlands |         2 |                    1.416 | Norway    | 5         | 1.054                    |
| Austria     |         2 |                    1.663 | Sweden    | 5         | 1.191                    |
| Czech.      |         2 |                    1.706 | Finland   | 5         | 1.545                    |
| Switzerland |         2 |                    1.713 | Denmark   | 5         | 1.708                    |
| Ireland     |         2 |                    1.839 | USSR      | 5         | 2.780                    |
| E. Germany  |         2 |                    2.042 |           |           |                          |

Figure 14.19. First two discriminant functions z 1 and z 2 for the clusters in Table 14.10.

<!-- image -->

Finally, we obtain a five-cluster solution from average linkage and use the centroids of these clusters as the new seeds. The clusters in Table 14.11 result. The first two discriminant functions are plotted in Figure 14.20. All five clusters are well separated in the first two discriminant functions. These clusters show some resemblance to those in the principal component plot given in Figure 14.16.

Table 14.11. k -Means Cluster Solution Using Seeds from Average Linkage

| Country     |   Cluster |   Distance from Centroid | Country   | Cluster   | Distance from Centroid   |
|-------------|-----------|--------------------------|-----------|-----------|--------------------------|
| Romania     |         1 |                    0.97  | Norway    | 2         | 2.287                    |
| Yugoslavia  |         1 |                    1.182 | UK        | 2         | 2.354                    |
| Bulgaria    |         1 |                    1.339 | France    | 2         | 2.600                    |
| Albania     |         1 |                    1.97  | Finland   | 2         | 2.683                    |
| Belgium     |         2 |                    1.152 | Greece    | 3         | 1.075                    |
| W. Germany  |         2 |                    1.245 | Italy     | 3         | 1.075                    |
| Netherlands |         2 |                    1.547 | Portugal  | 4         | 1.466                    |
| Sweden      |         2 |                    1.604 | Spain     | 4         | 1.466                    |
| Ireland     |         2 |                    1.744 | Czech.    | 5         | 1.337                    |
| Denmark     |         2 |                    1.766 | Poland    | 5         | 1.579                    |
| Switzerland |         2 |                    1.831 | USSR      | 5         | 1.964                    |
| Austria     |         2 |                    2.037 | Hungary   | 5         | 2.023                    |
| E. Germany  |         2 |                    2.251 |           |           |                          |

Figure 14.20. First two discriminant functions z 1 and z 2 for the clusters in Table 14.11.

<!-- image -->

## 14.4.1b Other Partitioning Criteria

We now consider three partitioning methods that are not based directly on the distance from a point to the centroid of a cluster. These methods are based on the between-cluster and within-cluster sum of squares and products matrices H and E defined in (6.9) and (6.10) for one-way MANOVA. For well defined clusters, we would like E to be 'small' and H to be 'large.'

The three criteria are as follows:

1. Minimize tr ( E ) .
2. Minimize | E | .
3. Maximize tr ( E -1 H ) .

Using criterion 1, for example, we would move an item with observation vector y to the cluster for which tr ( E ) is minimized after the move.

We can express the first criterion in two alternative forms. By (6.10), we have

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

where E i = ∑ n j = 1 ( y i j -y i . )( y i j -y i . ) ′ is the sum of squares and products matrix of deviations of observations from the mean vector for the i th cluster. In (14.28) we use the notation of Section 6.1.2 for a balanced design, in which n is the number of observations in each cluster.

We can write tr ( E i ) in (14.29) in the form

<!-- formula-not-decoded -->

Thus tr ( E i ) is the sum of the (squared) Euclidean distances from the individual points to the centroid of the i th cluster.

A second form of (14.28) was given by Seber (1984, p. 277) as

<!-- formula-not-decoded -->

Hence minimizing tr ( E ) is equivalent to minimizing the sum of squared Euclidean distances between all pairs of points in a cluster.

The second criterion, minimizing | E | , is related to /Lambda1 = | E | / | E + H | in (6.13). Minimizing | E | is equivalent to minimizing Wilks' /Lambda1 for the clusters.

Another way to look at minimizing | E | is to consider the effect of adding a point y to a cluster with centroid y . Let u = y -y . By (14.28), E is a sum of terms of the form uu ′ = ( y -y )( y -y ) ′ . Thus (ignoring the change in centroid with the added observation y ), the increase in | E | is

<!-- formula-not-decoded -->

Hence, the minimum increase in | E | is obtained by adding y to the cluster for which the standardized distance u ′ E -1 u of y from y is the smallest. By comparison, the tr ( E ) criterion would add y to the cluster for which u ′ u is minimum [see (14.30)].

The third criterion, maximizing tr ( E -1 H ) , is related to the Lawley-Hotelling statistic U ( s ) = tr ( E -1 H ) = ∑ s i = 1 λ i in (6.28), where λ 1, λ 2 , . . . , λ s are the eigenvalues of E -1 H and s = min ( p , g -1 ) . Associated with each λ i is the eigenvector a i and the discriminant function zi = a ′ i y (see Section 8.4). The largest eigenvalue, λ 1, and the accompanying first discriminant function, z 1 = a ′ 1 y , have the greatest

influence on tr ( E -1 H ) . Maximizing tr ( E -1 H ) has the inclination to produce elliptical clusters of the same size. These clusters would tend to follow a straight-line trend, especially if the first eigenvalue dominates the others. If the initial clusters or seeds are lined up in a different direction than the 'true clusters,' maximizing tr ( E -1 H ) may not correct the error in subsequent iterations.

Since tr ( E ) involves only the diagonal elements, the first criterion ignores the correlations and tends to yield spherical clusters. The second criterion, minimizing | E | , takes correlations into account and tends to produce elliptical clusters. These clusters have a tendency to be of the same shape because E /ν E is a pooled estimator of the covariance matrix. A modification that may be useful is ∏ g i = 1 | E i | , where E i is the error matrix for the i th cluster [see (14.29)].

Finally, we compare the three criteria in terms of invariance to nonsingular linear transformations v i j = Ay i j + b , where A is a constant nonsingular matrix and b is a vector of constants. The first criterion, minimizing tr ( E ) , is not invariant to such linear transformations, whereas the other two criteria are invariant to these transformations. Therefore, minimizing tr ( E ) will likely give different partitions for the raw data and standardized data.

## 14.4.2 Other Methods

We discuss mixtures of distributions in Section 14.4.2a and density estimation in Section 14.4.2b.

## 14.4.2a Mixtures of Distributions

In this method, we assume the existence of g distributions (usually multivariate normal), and we wish to assign each of the n items in the sample to the distribution it most likely belongs to. Such an approach is related to classification analysis in Chapter 9. Along with partitioning in Section 14.4.1, this method has the property that points can be transferred from one cluster to another, but it requires more assumptions than partitioning.

We define the density of a mixture of g distributions as the weighted average

<!-- formula-not-decoded -->

where 0 ≤ α i ≤ 1 , ∑ g i = 1 α i = 1, and f ( y , 𝛍 i , 𝚺 i ) is the multivariate normal distribution Np ( 𝛍 i , 𝚺 i ) given in (4.2).

Clusters could be formed in two ways. The first approach is to assign an item with observation vector y to the cluster Ci with largest value of the estimated posterior probability

<!-- formula-not-decoded -->

[see Rencher (1998, Sections 6.2.4 and 6.3.1)], where ˆ α i , ˆ 𝛍 i , and ˆ 𝚺 i are maximum likelihood estimates and h ( y ) is given by (14.32) with estimates inserted for parameters. The posterior probability (14.33) is an estimate of the probability that an item with observation vector y belongs to the i th cluster, Ci .

The second approach is to assign an item with observation vector y to the cluster with largest value of

<!-- formula-not-decoded -->

[see (9.14)]. For either of these approaches [based on (14.33) or (14.34)], we need the estimates ˆ α i , ˆ 𝛍 i , and ˆ 𝚺 i . These estimates are obtained by maximizing the likelihood function L = ∏ n j = 1 h ( y j ) , where h ( y j ) is given by (14.32). The results are n

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

(Everitt 1993, p. 111), where ˆ P ( Ci | y j ) is given by (14.33). These three equations must be solved iteratively. For a given value of g , we can begin with initial estimates or guesses for the parameters and adjust them by iteration (this approach is related to the EM algorithm mentioned in Section 3.11). If g is not known, we can begin with g = 1 and then successively try g = 2, g = 3, and so on, until the results are satisfactory.

The total number of parameters to be estimated is large. There are p parameters in each 𝛍 i , 1 2 p ( p + 1 ) unique parameters in each 𝚺 i , and g -1 values of α i (the remaining ˆ α i is found by ∑ g i = 1 ˆ α i = 1), for a total of

<!-- formula-not-decoded -->

parameters. If the sample size n is not sufficiently large to estimate all of these parameters, we could assume a common covariance matrix 𝚺 , which reduces the number of parameters by 1 2 ( g -1 ) p ( p + 1 ) .

The method of mixtures is invariant to full-rank linear transformations and is somewhat robust to the assumption of normality. The technique works better if the g densities are well separated or the sample sizes are large.

Example 14.4.2a. To illustrate the clustering method based on mixtures of distributions, we use the protein consumption data of Table 14.7. Because of the small number of countries in the data set, there are not enough degrees of freedom to estimate

a different covariance matrix for each cluster. Hence we assume equal covariance matrices and estimate a pooled covariance matrix ˆ 𝚺 . For illustration purposes, we choose g = 5, as in Example 14.4.1a.

We use the five clusters found by Ward's method to obtain initial estimates of α i , 𝛍 i , and 𝚺 . Then the maximum likelihood equations are solved iteratively to find the following estimates:

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Then assigning each country to the cluster for which it has the highest posterior probability of membership as in (14.33) yields the following clusters:

| Cluster 1                                             | Cluster 2                                                                   | Cluster 3                     | Cluster 4                        | Cluster 5                   |
|-------------------------------------------------------|-----------------------------------------------------------------------------|-------------------------------|----------------------------------|-----------------------------|
| Albania, Czech., Greece, Hungary, Italy, Poland, USSR | Austria, Belgium, France, Ireland, Netherlands, Switzerland, UK, W. Germany | Bulgaria, Romania, Yugoslavia | Denmark, Finland, Norway, Sweden | E. Germany, Portugal, Spain |

## 14.4.2b Density Estimation

In the method of density estimation , or density searching , we seek regions of high density sometimes called modes . No assumption is made about the form of the density, as was done in Section 14.4.2a. We could estimate the density using a kernel function as in Section 9.7.2. Alternatively, we simply attempt to separate regions with a high concentration of points from regions with a low density.

To find regions of high density, we first choose a radius r and a value of k , the number of points in a k -nearest neighbor scheme. For each of the n points in the data, the number of points within a sphere of radius r is found. A point is called a dense point if at least k other points are contained in its sphere.

If a dense point is more than a distance r from all other dense points, it becomes the nucleus of a new cluster. If a dense point is within a distance r from at least one dense point that belongs to a cluster, it is added to the cluster. If the dense point is within a distance r of two or more clusters, these clusters are combined. Two clusters are also combined if the smallest distance between their dense points is less than the average of the 2 k smallest distances between the original n points. The value of r can be gradually increased so that more points become dense. Another option is to begin with the specified value of r for each point and then gradually increase r until k observations are contained in its sphere.

Example 14.4.2b. To illustrate the density estimation method, we use the protein data. For each pair of values of k and r , the value of r was allowed to increase if needed, as described above. For the following values of k and r , the number of clusters obtained are given.

|   k / r |   1.6 |   1.7 |   1.8 |   1.9 |   2.0 |   2.1 |   2.2 |   2.3 |   2.4 |   2.5 |   2.6 |   2.7 |   2.8 |
|---------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|
|       2 |     5 |     5 |     5 |     4 |     4 |     4 |     4 |     4 |     3 |     3 |     3 |     3 |     3 |
|       3 |     3 |     3 |     3 |     3 |     3 |     3 |     3 |     3 |     2 |     2 |     2 |     2 |     2 |
|       4 |     3 |     3 |     3 |     3 |     3 |     3 |     3 |     3 |     2 |     2 |     2 |     2 |     2 |

The five-cluster solution found by setting r = 1 . 8 and k = 2 is

| Cluster 1                                                                  | Cluster 2                        | Cluster 3                                       | Cluster 4                       | Cluster 5                      |
|----------------------------------------------------------------------------|----------------------------------|-------------------------------------------------|---------------------------------|--------------------------------|
| Austria, Belgium France, Ireland, Netherlands, Switzerland, UK, W. Germany | Denmark, Finland, Norway, Sweden | Albania, Bulgaria, Hungary, Romania, Yugoslavia | Czech., E. Germany Poland, USSR | Greece, Italy, Portugal, Spain |

This partitioning into five clusters is perhaps more reasonable than that found in Example 14.4.2a. The first two discriminant functions for these five clusters are plotted in Figure 14.21.

Figure 14.21. First two discriminant functions for the clusters found in Example 14.4.2b.

<!-- image -->

## 14.5 CHOOSING THE NUMBER OF CLUSTERS

In hierarchical clustering, we can select g clusters from the dendrogram by cutting across the branches at a given level of the distance measure used by one of the axes. This is illustrated in Figure 14.22, which is the dendrogram for the average linkage method (Section 14.3.4) applied to the city crime data in Table 14.1 (see Figure 14.16). Cutting the dendrogram at a level of 700 yields two clusters. Cutting it at 535 gives three clusters.

We wish to determine the value of g that provides the best fit to the data. One approach is to look for large changes in distances at which clusters are formed. For example, in Figure 14.22, the largest change in levels occurs in going from two clusters to a single cluster. The change in distance between the two-cluster solution and the three-cluster solution is 82 units squared. The difference between the threecluster solution and the four-cluster solution is 73 units squared, and the change between the four- and five-cluster solutions is only 26 units squared. In this case we would choose two clusters.

A formalization of this procedure was proposed by Mojena (1977): choose the number of groups given by the first stage in the dendrogram at which

<!-- formula-not-decoded -->

where α 1, α 2 , . . . , α n are the distance values for stages with n , n -1 , . . . , 1 clusters, α and s α are the mean and standard deviation of the α 's, and k is a constant. Mojena

Figure 14.22. Cutting the dendrogram to choose the number of clusters.

<!-- image -->

(1977) suggested using a value of k in the range 2.75 to 3.5, but Milligan and Cooper (1985) recommended k 1 . 25, based on a simulation study.

An index that can be used with either hierarchical or partitioning methods is

<!-- formula-not-decoded -->

- =

The value of g that maximizes c is chosen. A related approach is to choose the value of g that minimizes

<!-- formula-not-decoded -->

To compare two cluster solutions with g 1 and g 2 clusters, where g 2 &gt; g 1, we can use the test statistic

<!-- formula-not-decoded -->

which has an approximate F -distribution with p ( g 2 -g 1 ) and p ( n -g 2 ) degrees of freedom [Beale (1969)]. The matrices E 1 and E 2 are within-cluster sums of squares and products matrices corresponding to g 1 and g 2. The hypothesis is that the cluster solutions with g 1 and g 2 clusters are equally valid, and rejection implies that the cluster solution with g 2 clusters is better than the solution with g 1 clusters ( g 2 &gt; g 1 ) . The F -approximation in (14.39) may not be sufficiently accurate to justify the use of p -values.

## 14.6 CLUSTER VALIDITY

To check the validity of a cluster solution, it may be possible to test the hypothesis that there are no clusters or groups in the population from which the sample at hand was taken. For example, the hypothesis could be that the population represents a single unimodal distribution such as the multivariate normal, or that the observations arose from a uniform distribution. Formal tests of hypotheses of this type concerning cluster validity are reviewed by Gordon (1999, Section 7.2).

Across-validation approach can also be used to check the validity or stability of a clustering result. The data are randomly divided into two subsets, say A and B , and the cluster analysis is carried out separately on each of A and B . The results should be similar if the clusters are valid. An alternative approach is the following (Gordon 1999, Section 7.1; Milligan 1996):

1. Use some clustering method to partition subset A into g clusters.
2. Partition subset B into g clusters in two ways:
3. (a) Assign each item in B to the cluster in A that it is closest to by using, for example, the distance to cluster centroids.
4. (b) Use the same clustering method on B that was used on A .
3. Compare the results of (a) and (b) in step 2.

## 14.7 CLUSTERING VARIABLES

In some cases, it may be of interest to cluster the p variables rather than the n observations. For a similarity measure between each pair of variables, we would usually use the correlation. Since most clustering methods use dissimilarities (such as distances), we need to convert the correlation matrix R = ( ri j ) to a dissimilarity matrix. This can conveniently be done by replacing each ri j by 1 -| ri j | or 1 -r 2 i j . Using the resulting dissimilarity matrix, we can apply a clustering method such as a hierarchical technique to cluster the variables.

Figure 14.23. Dendrogram for clustering the variables of Table 14.1 using average linkage (see Example 14.7).

<!-- image -->

Figure 14.24. Dendrogram for clustering the variables of Table 14.1 using Ward's method (see Example 14.7).

<!-- image -->

Clustering of variables can sometimes be done successfully with factor analysis, which groups the variables corresponding to each factor; see Sections 13.1 and 13.5.

Example 14.7. Weillustrate clustering of variables using the city crime data in Table 14.1. We first calculate the correlation matrix R = ( ri j ) and then transform R to a dissimilarity matrix D = ( 1 -r 2 i j ) . The variables are then clustered using both average linkage and Ward's clustering methods, and the dendrograms are given in Figures 14.23 and 14.24, respectively. Both clustering methods yield the same solution.

Table 14.12. Rotated Factor Loadings for City Crime Data

| Variables   | Factor 1   | Factor 2   | Factor 3   |
|-------------|------------|------------|------------|
| Murder      | - . 063    | . 734      | .142       |
| Rape        | .504       | . 659      | .160       |
| Robbery     | .133       | .355       | . 726      |
| Assault     | .298       | . 740      | .398       |
| Burglary    | . 764      | .221       | .181       |
| Larceny     | . 847      | - . 014    | .244       |
| Auto theft  | .240       | .097       | . 584      |

We next carry out a factor analysis of the data and compare the resulting groups of variables with the clusters obtained with the average linkage and Ward's methods. The factor loadings are estimated using the principal factor method (Section 13.3.2) with squared multiple correlations as initial communality estimates, and the loadings are then rotated with a varimax rotation (Section 13.5.2b). The rotated factor pattern is given in Table 14.12. The highest loading in each row is bolded. The first factor deals with crimes associated with the home. The second factor involves crimes that are violent in nature. The third factor consists of crimes of theft outside the home. Note that the three-cluster solutions found by both average linkage and Ward's methods are identical to the grouping of variables in the factor analysis solution, namely, (1) murder, rape, and assault, (2) robbery and auto theft, and (3) burglary and larceny. Since all three methods agree, we have some confidence in the validity of the solution.

## PROBLEMS

- 14.1 Show that d 2 ( x , y ) = ∑ p j = 1 ( x j -y j ) 2 from (14.2) is equal to (14.5), d 2 ( x , y ) = (v x -v y ) 2 + p ( x -y ) 2 + 2 v x v y ( 1 -rxy ) , where v 2 x = ∑ p j = 1 ( x j -x ) 2 , x = ∑ p j = 1 x j / p , and ryx is defined in (14.6).
- 14.2 (a) Show that I AB = nA ( y A -y AB ) ′ ( y A -y AB ) + nB ( y B -y AB ) ′ ( y B -y AB ) as in (14.18).
- (b) Show that (14.18) is equal to (14.19); that is,

<!-- formula-not-decoded -->

- 14.3 Using the hints provided in each case, show that the parameter values for (14.20) in Table 14.2 produce appropriate distances for the following cluster methods.

- (a) Complete linkage. Use an approach analogous to that in Section 14.3.8 for the single linkage method.
- (b) Average linkage. Write (14.20) in terms of parameter values for average linkage in Table 14.2. Then use (14.9).
- (c) Centroid method. Show that

<!-- formula-not-decoded -->

where y AB = ( nA y A + nB y B )/( nA + nB ) .

- (e) Ward's method. Show that
- (d) Median method. Use nA = nB in (14.12) and (14.40) [see part (c)].

<!-- formula-not-decoded -->

where I AB is defined in (14.17).

- 14.4 Showthat for all methods in Table 14.2 for which γ = 0, we have D ( C , AB ) &gt; (α A + α B + β) D ( A , B ) as in (14.26).
- 14.5 Verify the statement in the last paragraph of Section 14.4.1b, namely, that the first criterion in Section 14.4.1b is not invariant to nonsingular linear transformations v i j = Ay i j + b , where A is a p × p nonsingular matrix, and that the other two criteria are invariant to such transformations. Use the following approach:
- (a) Show that H v = AH y A ′ and E v = AE y A ′ .
- (b) Show that minimizing tr ( E ) is not invariant.
- (c) Show that minimizing | E | is invariant.
- (d) Show that maximizing tr ( E -1 H ) is invariant.
- 14.6 Verify the statement in Section 14.4.2a that in 𝛍 i , i = 1, 2 , . . . , g ; 𝚺 i , i = 1, 2 , . . . , g ; and α i , i = 1, 2 , . . . , g -1; the total number of parameters is given by 1 2 g ( p + 1 )( p + 2 ) -1 as in (14.35).
- 14.7 Use the ramus bone date of Table 3.6. Carry out the following cluster methods and compare to the principal component plot in Figure 12.5.
- (a) Find a two-cluster solution using the single linkage method.
- (b) Find a two-cluster solution using the average linkage method and compare to the result in (a). Which seems better?

- (c) Carry out a cluster analysis using the Ward's, complete linkage, centroid, and median methods.
- (d) Use the flexible beta method with β = -. 25 , β = -. 5, and β = -. 75.
- 14.8 Use the hematology data of Table 4.3.
- (a) Carry out a cluster analysis using the centroid method and find the distance between the centroids of the two-cluster solution.
- (b) Carry out a cluster analysis using the average linkage method. How many clusters are indicated in the dendrogram?
- (c) Using the two-cluster solution from part (b), label observations from one cluster as group 1 and the observations from the other cluster as group 2. Calculate and plot the discriminant function, as in Example 8.2. Do the two clusters overlap?
- 14.9 Use all the variables of the Seishu data of Table 7.1.
- (a) Find the three-cluster solution using the single linkage, complete linkage, average linkage, centroid, median, and Ward's methods. Which observation appears to be an outlier? Which cluster is the same in all six solutions?
- (b) Using the cluster found in part (a) to be common to all solutions as group 1 and the rest of the observations as group 2, calculate and plot the discriminant function, as in Problem 14.8(c). Do the two clusters overlap?
- 14.10 Use the first 20 observations of the temperature data of Table 7.2. Standardize the variables (columns) before doing the following:
- (a) Carry out a k -means cluster analysis using as initial seeds the five observations that are mutually farthest apart. Plot the first two discriminant functions using the five clusters as groups.
- (b) Repeat part (a) using the first five observations as initial seeds.
- (c) Repeat part (a) using as initial seeds the centroids of the five-cluster solution found using Ward's method. Plot the dendrogram resulting from Ward's method.
- (d) Repeat part (c) using average linkage instead of Ward's method. Compare the results with those in part (c).
- (e) Plot the first and second principal components and the second and third components. Which cluster solutions found in parts (a)-(d) seem to agree most with the principal component plots?
- (f) Repeat parts (a) and (b) using three initial seeds instead of five. How do the cluster solutions compare?
- (g) Repeat part (c) using three initial seeds instead of five. How does the cluster solution compare to your answer in part (f)?
- 14.11 Table 14.13 contains air pollution data from 41 U.S. cities (Sokal and Rohlf 1981, p. 619). The variables are as follows:

Table 14.13. Air Pollution Levels in U.S. Cities

| Cities               |   y 1 |   y 2 |   y 3 |   y 4 |   y 5 |   y 6 |   y 7 |
|----------------------|-------|-------|-------|-------|-------|-------|-------|
| Phoenix              |    10 |  70.3 |   213 |   582 |   6   |  7.05 |    36 |
| Little Rock          |    13 |  61   |    91 |   132 |   8.2 | 48.52 |   100 |
| San Francisco        |    12 |  56.7 |   453 |   716 |   8.7 | 20.66 |    67 |
| Denver               |    17 |  51.9 |   454 |   515 |   9   | 12.95 |    86 |
| Hartford             |    56 |  49.1 |   412 |   158 |   9   | 43.37 |   127 |
| Wilmington           |    36 |  54   |    80 |    80 |   9   | 40.25 |   114 |
| Washington           |    29 |  57.3 |   434 |   757 |   9.3 | 38.89 |   111 |
| Jacksonville         |    14 |  68.4 |   136 |   529 |   8.8 | 54.47 |   116 |
| Miami                |    10 |  75.5 |   207 |   335 |   9   | 59.8  |   128 |
| Atlanta              |    24 |  61.5 |   368 |   497 |   9.1 | 48.34 |   115 |
| Chicago              |   110 |  50.6 |  3344 |  3369 |  10.4 | 34.44 |   122 |
| Indianapolis         |    28 |  52.3 |   361 |   746 |   9.7 | 38.74 |   121 |
| Des Moines           |    17 |  49   |   104 |   201 |  11.2 | 30.85 |   103 |
| Wichita              |     8 |  56.6 |   125 |   277 |  12.7 | 30.58 |    82 |
| Louisville           |    30 |  55.6 |   291 |   593 |   8.3 | 43.11 |   123 |
| New Orleans          |     9 |  68.3 |   204 |   361 |   8.4 | 56.77 |   113 |
| Baltimore            |    47 |  55   |   625 |   905 |   9.6 | 41.31 |   111 |
| Detroit              |    35 |  49.9 |  1064 |  1513 |  10.1 | 30.96 |   129 |
| Minneapolis-St. Paul |    29 |  43.5 |   699 |   744 |  10.6 | 25.94 |   137 |
| Kansas City          |    14 |  54.5 |   381 |   507 |  10   | 37    |    99 |
| St. Louis            |    56 |  55.9 |   775 |   622 |   9.5 | 35.89 |   105 |
| Omaha                |    14 |  51.5 |   181 |   347 |  10.9 | 30.18 |    98 |
| Albuquerque          |    11 |  56.8 |    46 |   244 |   8.9 |  7.77 |    58 |
| Albany               |    46 |  47.6 |    44 |   116 |   8.8 | 33.36 |   135 |
| Buffalo              |    11 |  47.1 |   391 |   463 |  12.4 | 36.11 |   166 |
| Cincinnati           |    23 |  54   |   462 |   453 |   7.1 | 39.04 |   132 |
| Cleveland            |    65 |  49.7 |  1007 |   751 |  10.9 | 34.99 |   155 |
| Columbus             |    26 |  51.5 |   266 |   540 |   8.6 | 37.01 |   134 |
| Philadelphia         |    69 |  54.6 |  1692 |  1950 |   9.6 | 39.93 |   115 |
| Pittsburgh           |    61 |  50.4 |   347 |   520 |   9.4 | 36.22 |   147 |
| Providence           |    94 |  50   |   343 |   179 |  10.6 | 42.75 |   125 |
| Memphis              |    10 |  61.6 |   337 |   624 |   9.2 | 49.1  |   105 |
| Nashville            |    18 |  59.4 |   275 |   448 |   7.9 | 46    |   119 |
| Dallas               |     9 |  66.2 |   641 |   844 |  10.9 | 35.94 |    78 |
| Houston              |    10 |  68.9 |   721 |  1233 |  10.8 | 48.19 |   103 |
| Salt Lake City       |    28 |  51   |   137 |   176 |   8.7 | 15.17 |    89 |
| Norfolk              |    31 |  59.3 |    96 |   308 |  10.6 | 44.68 |   116 |
| Richmond             |    26 |  57.8 |   197 |   299 |   7.6 | 42.59 |   115 |
| Seattle              |    29 |  51.1 |   379 |   531 |   9.4 | 38.79 |   164 |
| Charleston           |    31 |  55.2 |    35 |    71 |   6.5 | 40.75 |   148 |
| Milwaukee            |    16 |  45.7 |   569 |   717 |  11.8 | 29.07 |   123 |

- y 1 = SO2 content of air in micrograms per cubic meter
- y 3 = Number of manufacturing enterprises employing 20 or more workers
- y 2 = Average annual temperature in ◦ F
- y 4 = Population size (1970 census) in thousands
- y 6 = Average annual precipitation in inches
- y 5 = Average annual wind speed in miles per hour
- y 7 = Average number of days with precipitation per year

Standardize each variable to mean 0 and standard deviation 1. Carry out a cluster analysis using the density estimation method with k equal to 2, 3, 4, 5 and values of r ranging from .2 to 2 by increments of .2 for each value of k . What is the maximum value of k that produces a two-cluster solution?

- 14.12 Table 14.14 gives the yields of winter wheat in each of the years 1970-1973 at 12 different sites in England (Hand et al. 1994, p. 31).
- (a) Carry out a cluster analysis using the density estimation method with k = 2, 3, 4 and r = . 2, . 4 , . . . , 2 . 0.
- (c) Plot the first two principal components and compare with the plot in part (b).
- (b) Plot the first two discriminant functions from the three-cluster solution obtained with k = 2 and r = 1.
- (d) Repeat part (b) using a two-cluster solution obtained with k = 3 and r = 1. Which two clusters of the three-cluster solution found in part (b) merged into one cluster?

Table 14.14. Yields of Winter Wheat (kg per unit area)

|                  |    Year |    Year |    Year |    Year |
|------------------|---------|---------|---------|---------|
| Site             | 1970    | 1971    | 1972    | 1973    |
| Cambridge        |   46.81 |   39.4  |   55.64 |   32.61 |
| Cockle Park      |   46.49 |   34.07 |   45.06 |   41.02 |
| Harpers Adams    |   44.03 |   42.03 |   40.32 |   50.23 |
| Headley Hall     |   52.24 |   36.19 |   47.03 |   34.56 |
| Morley           |   36.55 |   43.06 |   38.07 |   43.17 |
| Myerscough       |   34.88 |   49.72 |   40.86 |   50.08 |
| Rosemaund        |   56.14 |   47.67 |   43.48 |   38.99 |
| Seale-Hayne      |   45.67 |   27.3  |   45.48 |   50.32 |
| Sparsholt        |   42.97 |   46.87 |   38.78 |   47.49 |
| Sutton Bonington |   54.44 |   49.34 |   24.48 |   46.94 |
| Terrington       |   54.95 |   52.05 |   50.91 |   39.13 |
| Wye              |   48.94 |   48.63 |   31.69 |   59.72 |

## C H A P T E R 15

## Graphical Procedures

In Sections 15.1, 15.2, and 15.3, we consider three graphical techniques: multidimensional scaling, correspondence analysis, and biplots. These methods are designed to reduce dimensionality and portray relationships among observations or variables.

## 15.1 MULTIDIMENSIONAL SCALING

## 15.1.1 Introduction

In the dimension reduction technique called multidimensional scaling , we begin with the distances δ i j between each pair of items. We wish to represent the n items in a low-dimensional coordinate system, in which the distances di j between items closely match the original distances δ i j , that is,

<!-- formula-not-decoded -->

The final distances di j are usually Euclidean. The original distances δ i j may be actual measured distances between observations y i and y j in p dimensions, such as

<!-- formula-not-decoded -->

On the other hand, the distances δ i j may be only a proximity or similarity based on human judgment-for example, the perceived degree of similarity between all pairs of brands of a certain type of appliance (for a discussion of similarities and dissimilarities, see Section 14.2). The goal of multidimensional scaling is a plot that exhibits information about how the items relate to each other or provides some other meaningful interpretation of the data. For example, the aim may be seriation or ranking; if the points lie close to a curve in two dimensions, then the ordering of points along the curve is used to rank the points.

If the observation vectors y i , i = 1, 2 , . . . , n , are available and we calculate distances using (15.1) or a similar measure, or if the original y i 's are not available, but we have actual distances between items, then the process of reduction to a lower dimensional geometric representation is called metric multidimensional scaling . If

the original distances are only similarities based on judgment, the process is called nonmetric multidimensional scaling , and the final spatial representation preserves only the rank order among the similarities. We consider metric scaling in Section 15.1.2 and nonmetric scaling in Section 15.1.3. For useful discussions of various aspects of multidimensional scaling, see Davidson (1983); Gordon (1999, Sections 6.2 and 6.3); Kruskal and Wish (1978); Mardia, Kent, and Bibby (1979, Chapter 14); Seber (1984, Section 5.5); Young (1987); Jobson (1992, Section 10.3); Shepard, Romney, and Nerlove (1972); and Romney, Shepard, and Nerlove (1972).

## 15.1.2 Metric Multidimensional Scaling

In this section, we consider metric multidimensional scaling , which is also known as the classical solution and as principal coordinate analysis . We begin with an n × n distance matrix D = (δ i j ) . Our goal is to find n points in k dimensions such that the interpoint distances di j in the k dimensions are approximately equal to the values of δ i j in D . Typically, we use k = 2 for plotting purposes, but k = 1 or 3 may also be useful.

The points are found as follows:

1. Construct the n × n matrix A = ( ai j ) = ( -1 2 δ 2 i j ) , where δ i j is the i j th element of D .
2. Construct the n × n matrix B = ( bi j ) , with elements bi j = ai j -ai . -a . j + a .. , where ai . = ∑ n j = 1 ai j / n , a . j = ∑ n i = 1 ai j / n , a .. = ∑ i j ai j / n 2 . The matrix B can be written as

<!-- formula-not-decoded -->

It can be shown that there exists a q -dimensional configuration z 1, z 2 , . . . , z n with interpoint distances di j = ( z i -z j ) ′ ( z i -z j ) such that di j = δ i j if and only if B is positive semidefinite of rank q (Schoenberg 1935; Young and Householder 1938; Gower 1966; Seber 1984, p. 236).

3. Since B is a symmetric matrix, we can use the spectral decomposition in (2.109) to write B in the form

<!-- formula-not-decoded -->

where V is the matrix of eigenvectors of B and 𝚲 is the diagonal matrix of eigenvalues of B . If B is positive semidefinite of rank q , there are q positive eigenvalues, and the remaining n -q eigenvalues are zero. If 𝚲 1 = diag (λ 1 , λ 2 , . . . , λ q ) contains the positive eigenvalues and V 1 = ( v 1 , v 2 , . . . , v q ) contains the corresponding eigenvectors, then we can express (15.3)

in the form where

<!-- formula-not-decoded -->

4. The rows z ′ 1 , z ′ 2 , . . . , z ′ n of Z in (15.4) are the points whose interpoint distances di j = ( z i -z j ) ′ ( z i -z j ) match the δ i j 's in the original distance matrix D , as noted following (15.2).
5. Since q in (15.4) will typically be too large to be of practical interest and we would prefer a smaller dimension k for plotting, we can use the first k eigenvalues and corresponding eigenvectors in (15.4) to obtain n points whose interpoint distances di j are approximately equal to the corresponding δ i j 's.
6. If B is not positive semidefinite, but its first k eigenvalues are positive and relatively large, then these eigenvalues and associated eigenvectors may be used in (15.4) to construct points that give reasonably good approximations to the δ i j 's.

Note that the method used to obtain Z from B closely resembles principal component analysis. Note also that the solution Z in (15.4) is not unique, since a shift in origin or a rotation will not change the distances di j . For example, if C is a q × q orthogonal matrix producing a rotation [see (2.101)], then

<!-- formula-not-decoded -->

Thus the rotated points Cz i have the same interpoint distances di j .

Example 15.1.2(a). To illustrate the first four steps of the above algorithm for metric multidimensional scaling, consider the 5 × 5 distance matrix

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

The matrix A = ( -1 2 δ 2 i j ) in step 1 is given by

<!-- formula-not-decoded -->

For the means, we have ¯ a 1 . = ¯ a . 1 = -16 / 5, ¯ ai . = ¯ a . i = -36 / 5, i = 2 , . . . , 5, ¯ a .. = -32 / 5. With n = 5, the matrix B in step 2 is given by

<!-- formula-not-decoded -->

The rank of B is clearly 2. For step 3, the (nonzero) eigenvalues and corresponding eigenvectors of B are given by λ 1 = 16 , λ 2 = 16,

<!-- formula-not-decoded -->

Then for step 3 we have, by (15.4),

<!-- formula-not-decoded -->

It can be shown (step 4) that the distance matrix for these five points is D . The five points constitute a square with each side of length 4 and a center point at the origin. The five points (rows of Z ) are plotted in Figure 15.1.

Example 15.1.2(b). For another example of metric multidimensional scaling, we use airline distances between 10 U.S. cities, as given in Table 15.1 (Kruskal and Wish 1978, pp. 7-9). The points given by metric multidimensional scaling are plotted in Figure 15.2. Notice that north and south have been reversed; the eigenvectors v i in (15.4) are normalized but are subject to multiplication by -1.

Figure 15.1. Plot of the five points found in Example 15.1.2(a)

<!-- image -->

## 15.1.3 Nonmetric Multidimensional Scaling

Suppose the m = n ( n -1 )/ 2 dissimilarities δ i j cannot be measured as in (15.1) but can be ranked in order,

<!-- formula-not-decoded -->

Table 15.1. Airline Distances between Ten U.S. Cities

|   City |    1 |    2 |    3 |    4 |    5 |    6 |    7 |    8 |    9 |   10 |
|--------|------|------|------|------|------|------|------|------|------|------|
|      1 |    0 |  587 | 1212 |  701 | 1936 |  604 |  748 | 2139 | 2182 |  543 |
|      2 |  587 |    0 |  920 |  940 | 1745 | 1188 |  713 | 1858 | 1737 |  597 |
|      3 | 1212 |  920 |    0 |  879 |  831 | 1726 | 1631 |  949 | 1021 | 1494 |
|      4 |  701 |  940 |  879 |    0 | 1374 |  968 | 1420 | 1645 | 1891 | 1220 |
|      5 | 1936 | 1745 |  831 | 1374 |    0 | 2339 | 2451 |  347 |  959 | 2300 |
|      6 |  604 | 1188 | 1726 |  968 | 2339 |    0 | 1092 | 2594 | 2734 |  923 |
|      7 |  748 |  713 | 1631 | 1420 | 2451 | 1092 |    0 | 2571 | 2408 |  205 |
|      8 | 2139 | 1858 |  949 | 1645 |  347 | 2594 | 2571 |    0 |  678 | 2442 |
|      9 | 2182 | 1737 | 1021 | 1891 |  959 | 2734 | 2408 |  678 |    0 | 2329 |
|     10 |  543 |  597 | 1494 | 1220 | 2300 |  923 |  205 | 2442 | 2329 |    0 |

Cities: (1) Atlanta, (2) Chicago, (3) Denver, (4) Houston, (5) Los Angeles, (6) Miami, (7) New York, (8) San Francisco, (9) Seattle, (10) Washington, D.C.

Figure 15.2. Plot of the points found in Example 15.1.2(b).

<!-- image -->

where r 1 s 1 indicates the pair of items with the smallest dissimilarity and rmsm represents the pair with greatest dissimilarity. In nonmetric multidimensional scaling, we seek a low-dimensional representation of the points such that the rankings of the distances

<!-- formula-not-decoded -->

match exactly the ordering of dissimilarities in (15.5). Thus, although metric scaling uses the magnitudes of the δ i j 's, nonmetric scaling is based only on the rank order of the δ i j 's.

For a given set of points with distances di j , a plot of di j versus δ i j may not be monotonic; that is, the ordering in (15.6) may not match exactly the ordering in (15.5). A lack of monotonicity of this type is illustrated in Figure 15.3.

In Figure 15.3, the dashed line and open circles show some values of ˆ di j that are estimated in such a way that the plot becomes monotonic. Suitable ˆ di j 's can be estimated by monotonic regression , in which we seek values of ˆ di j to minimize the scaled sum of squared differences

<!-- formula-not-decoded -->

Figure 15.3. Plot of distance d versus dissimilarity δ illustrating lack of monotonicity. The dashed line represents best fit by monotonic regression.

<!-- image -->

subject to the constraint

<!-- formula-not-decoded -->

where r 1 s 1, r 2 s 2 , . . . , rmsm are defined as in (15.5) and (15.6) (Kruskal 1964a, 1964b). The minimum value of S 2 for a given dimension, k , is called the STRESS. Note that the ˆ di j 's are not distances. They are merely numbers used as a reference to assess the monotonicity of the di j 's. The ˆ di j 's are sometimes called disparities .

The minimum value of the STRESS over all possible configurations of points can be found using the following algorithm.

1. Rank the m = n ( n -1 )/ 2 distances or dissimilarities δ i j as in (15.5).
2. Choose a value of k and an initial configuration of points in k dimensions. The initial configuration could be n points chosen at random from a uniform or normal distribution, n evenly spaced points in k -dimensional space, or the metric solution obtained by treating the ordinal measurements as continuous and using the algorithm in Section 15.1.2.
3. For the initial configuration of points, find the interitem distances di j . Find the corresponding ˆ di j 's by monotonic regression as defined above using (15.7).
4. Choose a new configuration of points whose distances di j minimize S 2 in (15.7) with respect to the ˆ di j 's found in step 3. One approach is to use an iterative gradient technique such as the method of steepest descent or the NewtonRaphson method.
5. Using monotonic regression, find new ˆ di j 's for the di j 's found in step 4. This gives a new value of STRESS.
6. Repeat steps 4 and 5 until STRESS converges to a minimum over all possible k -dimensional configurations of points.

Figure 15.4. Ideal plot of minimum STRESS versus k .

<!-- image -->

7. Using the preceding six steps, calculate the minimum STRESS for values of k starting at k = 1 and plot these. As k increases, the curve will decrease, with occasional exceptions due to round off or numerical anomalies in the search procedure for minimum STRESS. We look for a discernible bend in the plot, following which the curve is low and relatively flat. An ideal plot is shown in Figure 15.4. The curve levels off after k = 2, which is convenient for plotting the resulting n points in 2 dimensions.

There is a possibility that the minimum value of STRESS found by the above seven steps for a given value of k may be a local minimum rather than the global minimum. Such an anomaly may show up in the plot of minimum STRESS versus k . The possibility of a local minimum can be checked by repeating the procedure, starting with a different initial configuration.

As was the case with metric scaling, the final configuration of points from a nonmetric scaling is invariant to a rotation of axes.

Example 15.1.3. The voting records for 15 congressmen from New Jersey on 19 environmental bills are given in Table 15.2 in the form of a dissimilarity matrix (Hand et al. 1994, p. 235). The congressmen are identified by party: R 1 for Republican 1, D 2 for Democrat 2, etc. Each entry shows how often the indicated congressman voted differently from each of the other 14.

Using an initial configuration of points from a multivariate normal distribution with mean vector 𝛍 = 0 and 𝚺 = I , we find an 'optimal' configuration of points for each of k = 1 , 2 , . . . , 5. A plot of the STRESS is given in Figure 15.5.

From the plot of STRESS vs. number of dimensions, we see that either two or three dimensions will be sufficient. For plotting purposes, we choose two dimensions, which has a STRESS value of .113. The plot of the first two dimensions is given in Figure 15.6. It is apparent that the plot separates the Republicans from the Democrats except for Republican 6, who voted much the same as the Democrats.

Table 15.2. Dissimilarity Matrix for Voting Records of 15 Congressmen

|     |   R 1 |   R 2 |   D 1 |   D 2 |   R 3 |   R 4 |   R 5 |   D 3 |   D 4 |   D 5 |   D 6 |   R 6 |   R 7 |   R 8 |   D 7 |
|-----|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|
| R 1 |     0 |     8 |    15 |    15 |    10 |     9 |     7 |    15 |    16 |    14 |    15 |    16 |     7 |    11 |    13 |
| R 2 |     8 |     0 |    17 |    12 |    13 |    13 |    12 |    16 |    17 |    15 |    16 |    17 |    13 |    12 |    16 |
| D 1 |    15 |    17 |     0 |     9 |    16 |    12 |    15 |     5 |     5 |     6 |     5 |     4 |    11 |    10 |     7 |
| D 2 |    15 |    12 |     9 |     0 |    14 |    12 |    13 |    10 |     8 |     8 |     8 |     6 |    15 |    10 |     7 |
| R 3 |    10 |    13 |    16 |    14 |     0 |     8 |     9 |    13 |    14 |    12 |    12 |    12 |    10 |    11 |    11 |
| R 4 |     9 |    13 |    12 |    12 |     8 |     0 |     7 |    12 |    11 |    10 |     9 |    10 |     6 |     6 |    10 |
| R 5 |     7 |    12 |    15 |    13 |     9 |     7 |     0 |    17 |    16 |    15 |    14 |    15 |    10 |    11 |    13 |
| D 3 |    15 |    16 |     5 |    10 |    13 |    12 |    17 |     0 |     4 |     5 |     5 |     3 |    12 |     7 |     6 |
| D 4 |    16 |    17 |     5 |     8 |    14 |    11 |    16 |     4 |     0 |     3 |     2 |     1 |    13 |     7 |     5 |
| D 5 |    14 |    15 |     6 |     8 |    12 |    10 |    15 |     5 |     3 |     0 |     1 |     2 |    11 |     4 |     6 |
| D 6 |    15 |    16 |     5 |     8 |    12 |     9 |    14 |     5 |     2 |     1 |     0 |     1 |    12 |     5 |     5 |
| R 6 |    16 |    17 |     4 |     6 |    12 |    10 |    15 |     3 |     1 |     2 |     1 |     0 |    12 |     6 |     4 |
| R 7 |     7 |    13 |    11 |    15 |    10 |     6 |    10 |    12 |    13 |    11 |    12 |    12 |     0 |     9 |    13 |
| R 8 |    11 |    12 |    10 |    10 |    11 |     6 |    11 |     7 |     7 |     4 |     5 |     6 |     9 |     0 |     9 |
| D 7 |    13 |    16 |     7 |     7 |    11 |    10 |    13 |     6 |     5 |     6 |     5 |     4 |    13 |     9 |     0 |

Figure 15.5. Plot of STRESS for each value of k for the voting data in Table 15.2.

<!-- image -->

<!-- image -->

Dimension 1

Figure 15.6. Plot of points found using initial points from a multivariate normal distribution.

<!-- image -->

Dimension 1

Figure 15.7. Plot of points found using initial points from a uniform distribution.

Figure 15.8. Plot of points found using initial points from a metric solution.

<!-- image -->

We now use a different initial configuration of points drawn from a uniform distribution. The resulting plot is given in Figure 15.7. The results are very similar, with the exception of R 1 and R 3.

We next use a third initial configuration of points resulting from the metric solution, as described in Section 15.1.2. The resulting plot is given in Figure 15.8. All three plots are very similar, indicating a good fit.

## 15.2 CORRESPONDENCE ANALYSIS

## 15.2.1 Introduction

Correspondence analysis is a graphical technique for representing the information in a two-way contingency table, which contains the counts (frequencies) of items for a cross-classification of two categorical variables. With correspondence analysis, we construct a plot that shows the interaction of the two categorical variables along with the relationship of the rows to each other and of the columns to each other. In Sections 15.2.2-15.2.4, we consider correspondence analysis for ordinary two-way contingency tables. In Section 15.2.5 we consider multiple correspondence analysis for three-way and higher-order contingency tables. Useful treatments of correspondence analysis have been given by Greenacre (1984), Jobson (1992, Section 9.4), Khattree and Naik (1999, Chapter 7), Gower and Hand (1996, Chapters 4 and 9), and Benz« ecri (1992).

To test for significance of association of the two categorical variables in a contingency table, we could use a chi-square test or a log-linear model, both of which represent an asymptotic approach. Since correspondence analysis is associated with the chi-square approach, we will review it in Section 15.2.3. If a contingency table has some cell frequencies that are small or zero, the chi-square approximation is not very satisfactory. In this case, some categories can be combined to increase the cell frequencies. Correspondence analysis may be useful in identifying the categories that are similar, which we may thereby wish to combine.

In correspondence analysis, we plot a point for each row and a point for each column of the contingency table. These points are, in effect, projections of the rows and columns of the contingency table onto a two-dimensional Euclidean space. The goal is to preserve as far as possible the relationship of the rows (or columns) to each other in a two-dimensional space. If two row points are close together, the profiles of the two rows (across the columns) are similar. Likewise, two column points that are close together represent columns with similar profiles across the rows (see Section 15.2.2 for a definition of profiles). If a row point is close to a column point, this combination of categories of the two variables occurs more frequently than would occur by chance if the two variables were independent. Another output of a correspondence analysis is the inertia , or amount of information in each of the two dimensions in the plot (see Section 15.2.4).

## 15.2.2 Row and Column Profiles

A contingency table with a rows and b columns is represented in Table 15.3. The entries nij are the counts or frequencies for every two-way combination of row and column (every cell). The marginal totals are shown using the familiar dot notation: ni . = ∑ b j = 1 nij and n . j = ∑ a i = 1 nij . The overall total frequency is denoted by n instead of n .. for simplicity: n = i j nij .

∑ The frequencies nij in a contingency table can be converted to relative frequencies pi j by dividing by n : pi j = nij / n . The matrix of relative frequencies is called the correspondence matrix and is denoted by P :

Table 15.3. Contingency Table with a Rows and b Columns

|              | Columns   | Columns   | Columns   | Columns   |             |
|--------------|-----------|-----------|-----------|-----------|-------------|
|              | 1         | 2         | · · ·     | b         | Row Total   |
| 1 2 Rows     | n 11 n 21 | n 12 n 22 | · · ·     | n 1 b n   | n 1 . n 2 . |
|              | . .       | . . .     | · · ·     | 2 b       |             |
| .            | .         |           |           | .         | .           |
| . .          |           |           |           | . .       | . .         |
| a            | n a 1     | n a 2     | · · ·     | n ab      | n a .       |
| Column Total | n . 1     | n . 2     | · · ·     | n . b     | n           |

Table 15.4. Correspondence Matrix of Relative Frequencies

|              | Columns   | Columns   | Columns     | Columns     |             |
|--------------|-----------|-----------|-------------|-------------|-------------|
|              | 1         | 2         | · · ·       | b           | Row Total   |
| 1 2 Rows     | p 11 p 21 | p 12 p 22 | · · · · · · | p 1 b p 2 b | p 1 . p 2 . |
|              | . .       | . .       |             | . .         | . .         |
| . .          | .         | .         |             | .           |             |
| .            |           |           |             |             | .           |
| a            | p a 1     | p a 2     | · · ·       | p ab        | p a .       |
| Column Total | p . 1     | p . 2     | · · ·       | p . b       | 1           |

<!-- formula-not-decoded -->

In Table 15.4 we show the contingency table in Table 15.3 converted to a correspondence matrix.

The last column of Table 15.4 contains the row sums pi . = ∑ b j = 1 pi j . This column vector is denoted by r and can be obtained as

<!-- formula-not-decoded -->

where j is an a × 1 vector of 1's. Similarly, the last row of Table 15.4 contains the column sums p . j = ∑ a i = 1 pi j . This row vector is denoted by c ′ and can be obtained as

<!-- formula-not-decoded -->

where j ′ is a 1 × b vector of 1's. The elements of the vectors r and c are sometimes referred to as row and column masses . The correspondence matrix and marginal totals in Table 15.4 can be expressed as

<!-- formula-not-decoded -->

We now convert each row and column of P to a profile. The i th row profile r ′ i , i = 1, 2 , . . . , a , is defined by dividing the i th row of either Table 15.3 or 15.4 by its marginal total:

<!-- formula-not-decoded -->

The elements in each r ′ i are relative frequencies, and therefore they sum to 1:

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

and using (2.55), the matrix R of row profiles can be expressed as

<!-- formula-not-decoded -->

Similarly, the j th column profile c j , j = 1, 2 , . . . , b , is defined by dividing the j th column of either Table 15.3 or Table 15.4 by its marginal total:

<!-- formula-not-decoded -->

The elements in each c j are relative frequencies, and therefore they sum to 1:

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

and using (2.56), the matrix C of column profiles can be expressed as

By defining

By defining

<!-- formula-not-decoded -->

The vector r is defined in (15.9) as the column vector of row sums of P . It can also be expressed as a weighted average of the column profiles:

<!-- formula-not-decoded -->

Similarly, c ′ in (15.10) is the row vector of column sums of P and can be expressed as a weighted average of the row profiles:

<!-- formula-not-decoded -->

Note that ∑ b j = 1 p . j = ∑ a i = 1 pi . = 1, or

<!-- formula-not-decoded -->

where the first j is a × 1 and the second is b × 1. Therefore, the p . j 's and pi . 's serve as appropriate weights in the weighted averages (15.20) and (15.21).

Example 15.2.2. In Table 15.5 (Hand et al. 1994, p. 12) we have the number of piston ring failures in each of three legs in four compressors found in the same building (all four compressors are oriented in the same direction). We obtain the correspondence matrix in Table 15.6 by dividing each element of Table 15.5 by n = ∑ i j nij = 166.

Table 15.5. Piston Ring Failures

|              | Leg   | Leg    | Leg   |           |
|--------------|-------|--------|-------|-----------|
| Compressor   | North | Center | South | Row Total |
| 1            | 17    | 17     | 12    | 46        |
| 2            | 11    | 9      | 13    | 33        |
| 3            | 11    | 8      | 19    | 38        |
| 4            | 14    | 7      | 28    | 49        |
| Column Total | 53    | 41     | 72    | 166       |

Table 15.6. Correspondence Matrix Obtained from Table 15.5

|              | Leg   | Leg    | Leg   |           |
|--------------|-------|--------|-------|-----------|
| Compressor   | North | Center | South | Row Total |
| 1            | .102  | .102   | .072  | .277      |
| 2            | .066  | .054   | .078  | .199      |
| 3            | .066  | .048   | .114  | .229      |
| 4            | .082  | .042   | .169  | .295      |
| Column Total | .319  | .247   | .434  | 1.000     |

The vectors of row and column sums (marginal totals) in Table 15.6 are given by (15.9) and (15.10) as

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

The matrix of row profiles is given by (15.15) as

<!-- formula-not-decoded -->

The matrix of column profiles is given by (15.19) as

<!-- formula-not-decoded -->

## 15.2.3 Testing Independence

In Section 15.2.1, we noted that the data in a contingency table can be used to check for association of two categorical variables. If the two variables are denoted by x and y , then the assumption of independence can be expressed in terms of probabilities as

<!-- formula-not-decoded -->

where xi and y j correspond to the i th row and j th column of the contingency table. Using the notation in Table 15.4, we can estimate (15.23) as

<!-- formula-not-decoded -->

The usual chi-square statistic for testing independence of x and y (comparing pi j with pi . p . j for all i , j ) is given by

<!-- formula-not-decoded -->

which is approximately (asymptotically) distributed as a chi-square random variable with ( a -1 )( b -1 ) degrees of freedom. The statistic in (15.25) can also be written in terms of the frequencies nij rather than the relative frequencies pi j :

<!-- formula-not-decoded -->

Two other alternative forms of (15.25) are

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

In vector and matrix form, (15.27) and (15.28) can be written as

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

where r , c , r i , c j , D r , and D c are defined in (15.9), (15.10), (15.12), (15.16), (15.14), and (15.18), respectively. Thus, in (15.29) we compare r i to c for each i , and in (15.30) we compare c j to r for each j . Either of these is equivalent to testing independence by comparing pi j to pi . p . j for all i , j , since all the definitions of χ 2 in (15.25)-(15.30) are equal. Thus, the following three statements of independence are equivalent (for simplicity, we express the three statements in terms of sample quantities rather than their theoretical counterparts):

1. pi j = pi . p . j for all i , j (or P = rc ′ ).
2. All rows r ′ i of R in (15.15) are equal (and equal to their weighted average, c ′ ).
3. All columns c j of C in (15.19) are equal (and equal to their weighted average, r ).

Thus, if the variables x and y were independent, we would expect the rows of the contingency table to have similar profiles, or equivalently, the columns to have similar profiles. We can compare the row profiles to each other by comparing each row profile r ′ i to the weighted average c ′ of the row profiles defined in (15.21). This comparison is made in the χ 2 statistic (15.29). Similarly, we compare column profiles in (15.30).

The chi-square statistic in (15.25) can be expressed in vector and matrix terms as

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

where λ 2 1 , λ 2 2 , . . . , λ 2 k are the nonzero eigenvalues of D -1 r ( P -rc ′ ) D -1 c ( P -rc ′ ) ′ and

<!-- formula-not-decoded -->

The rank of P -rc ′ is ordinarily k = min [ ( a -1 ), ( b -1 ) ] . It is clear that the rank is less than min ( a , b ) since

<!-- formula-not-decoded -->

[see (15.9) and (15.22)].

Example 15.2.3. In order to test independence of the rows (compressors) and columns (legs) of Table 15.5 in Example 15.2.2, we perform a chi-square test. Using (15.25) or (15.26), we obtain χ 2 = 11 . 722, with 6 degrees of freedom, for which the p -value is .0685. There is some evidence of lack of independence between leg and compressor.

## 15.2.4 Coordinates for Plotting Row and Column Profiles

We now obtain coordinates of the row points and column points for the best twodimensional representation of the data in a contingency table. As we will see, the metric for the row points and column points is the same, and the two sets of points can therefore be superimposed on the same plot.

In multidimensional scaling in Section 15.1, we transformed the distance matrix and then factored it by a spectral decomposition to obtain coordinates for plotting. In correspondence analysis, the matrix P -rc ′ is not symmetric, and we therefore use a singular value decomposition to obtain coordinates.

We first scale P -rc ′ to obtain

<!-- formula-not-decoded -->

whose elements are

<!-- formula-not-decoded -->

as in (15.25). The a × b matrix Z has rank k = min ( a -1 , b -1 ) , the assumed rank of P -rc ′ . We factor Z using the singular value decomposition (2.117):

<!-- formula-not-decoded -->

The columns of the a × k matrix U are (normalized) eigenvectors of ZZ ′ ; the columns of the b × k matrix V are (normalized) eigenvectors of Z ′ Z ; and 𝚲 = diag (λ 1 , λ 2 , . . . , λ k ) , where λ 2 1 , λ 2 2 , . . . , λ 2 k are the nonzero eigenvalues of Z ′ Z and of ZZ ′ . The eigenvectors in U and V correspond to the eigenvalues λ 2 1 , λ 2 2 , . . . , λ 2 k . Since the columns of U and V are orthonormal, U ′ U = V ′ V = I . The values λ 1, λ 2 , . . . , λ k in 𝚲 are called the singular values of Z . Note that, by (15.35),

<!-- formula-not-decoded -->

The (nonzero) eigenvalues of ZZ ′ in (15.38) are the same as those of

<!-- formula-not-decoded -->

(see Section 2.11.5). The matrix expression in (15.39) is the same as that in (15.31). We have therefore denoted the eigenvalues as λ 2 1 , λ 2 2 , . . . , λ 2 k as in (15.32).

We can obtain a decomposition of P -rc ′ by equating the right-hand sides of (15.35) and (15.37) and solving for P -rc ′ :

<!-- formula-not-decoded -->

where A = D 1 / 2 r U , B = D 1 / 2 c V , a i and b i are columns of A and B , and 𝚲 = diag (λ 1 , λ 2 , . . . , λ k ) .

Since U ′ U = V ′ V = I , A and B in (15.40) are scaled so that A ′ D -1 r A = B ′ D -1 c B = I . With this scaling, the decomposition in (15.40) is often called the generalized singular value decomposition .

In (15.40) the rows of P -rc ′ are expressed as linear combinations of the rows of B ′ , which are the columns of B = ( b 1 , b 2 , . . . , b k ) . The coordinates (coefficients) for the i th row of P -rc ′ are found in the i th row of A 𝚲 . In like manner, the coordinates for the columns of P -rc ′ are given by the columns of 𝚲 B ′ , since the columns of 𝚲 B ′ provide coefficients for the columns of A = ( a 1 , a 2 , . . . , a k ) in (15.40).

To find coordinates for the row deviations r ′ i -c ′ in R -jc ′ and the column deviations c j -r in C -rj ′ , we express the two matrices as functions of P -rc ′ (see Problem 15.8):

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Thus the coordinates for the row deviations in R -jc ′ with respect to the axes provided by b 1, b 2 , . . . , b k are given by the columns of

<!-- formula-not-decoded -->

Similarly, the coordinates for the column deviations in C -rj ′ with respect to the axes a 1, a 2 , . . . , a k are given by the columns of

<!-- formula-not-decoded -->

Therefore, to plot the coordinates for the row profile deviations r ′ i -c ′ , i = 1, 2 , . . . , a , in two dimensions, we plot the rows of the first two columns of X :

<!-- formula-not-decoded -->

Similarly, to plot the coordinates for the column profile deviations c j -r , j = 1, 2 , . . . , b , in two dimensions, we plot the rows of the first two columns of Y :

<!-- formula-not-decoded -->

Both plots can be superimposed on the same graph because A and B in (15.40) share the same singular values λ 1, λ 2 , . . . , λ k in 𝚲 . Distances between row points and distances between column points are meaningful. For example, the distance between two row points is related to the chi-square metric implicit in (15.29). The

chi-square distance between two row profiles r i and r j is given by

<!-- formula-not-decoded -->

If two row points (or two column points) are close, the two rows (or two columns) could be combined into a single category if necessary to improve the chi-square approximation.

The distance between a row point and a column point is not meaningful, but the proximity of a row point and a column point has meaning as noted in Section 15.2.1, namely, that these two categories of the two variables occur more frequently than would be expected to happen by chance if the two variables were independent.

The weighted average (weighted by pi ) of the chi-square distances ( r i -c ) ′ D -1 c ( r i -c ) between the row profiles r i and their mean c [see (15.21)] is called the total inertia . By (15.29) this can be expressed as χ 2 / n :

<!-- formula-not-decoded -->

As noted following (15.21), ∑ i pi . = 1, and therefore the pi . 's serve as appropriate weights.

By (15.32), we can write (15.45) as

<!-- formula-not-decoded -->

Therefore, the contribution of each of the first two dimensions (axes) of our plot to the total inertia in (15.45) is λ 2 1 / ∑ i λ 2 i and λ 2 2 / ∑ i λ 2 i . The combined contribution of the two dimensions is

<!-- formula-not-decoded -->

If (15.47) is large, then the points in the plane of the first two dimensions account for nearly all the variation in the data, including the associations. The total inertia in (15.45) and (15.46) can also be described in terms of the columns by using (15.30):

<!-- formula-not-decoded -->

Since the inertia associated with the axes for columns is the same as that for rows, the row and column points can be plotted on the same axes.

Some computer programs use a singular value decomposition of P rather than of P -rc ′ . The results are the same if the first singular value (which is 1) is discarded along with the first column of A (which is r ) and the first column of B (which is c ).

Figure 15.9. Row points (1, 2, 3, 4) and column points (center, north, south).

<!-- image -->

Example 15.2.4. We continue the analysis of the piston ring data of Table 15.5. A correspondence analysis is performed and a plot of the row and column points is given in Figure 15.9. Row points do not lie near other row points and columns points do not lie near column points. However, compressor 1 seems to be closely associated with the center leg, compressor 2 with the north leg, and compressor 4 with the south leg. These associations illustrate the lack of independence between compressor and leg position.

Singular values and inertias are given in Table 15.7. Most of the variation is due to the first dimension, and the first two dimensions explain all the variation because rank ( Z ) = min ( a -1 , b -1 ) = min ( 4 -1 , 3 -1 ) = 2, where Z is defined in (15.35).

Table 15.7. Singular Values ( λ i ), Inertia ( λ 2 i ), Chi-Square ( n λ 2 i ), and Percent ( λ 2 i / ∑ j λ 2 j ) for the Data in Table 15.5

| Singular Value   |   Inertia |   Chi-Square |   Percent |
|------------------|-----------|--------------|-----------|
| .26528           |   0.07037 |      11.6819 |     99.66 |
| .01560           |   0.00024 |       0.0404 |      0.34 |
| Total            |   0.07062 |      11.7223 |    100    |

## 15.2.5 Multiple Correspondence Analysis

Correspondence analysis of a two-way contingency table can be extended to a threeway or higher-order multiway table. By the method of multiple correspondence analysis , we obtain a two-dimensional graphical display of the information in the multiway contingency table. The method involves a correspondence analysis of an indicator matrix G , in which there is a row for each item. Thus the number of rows of G is the total number of items in the sample. The number of columns of G is the total number of categories in all variables. The elements of G are 1's and 0's. In each row, an element is 1 if the item belongs in the corresponding category of the variable; otherwise, the element is 0. Thus the number of 1's in a row of G is the number of variables; for a four-way contingency table, for example, there would be four 1's in each row of G .

We illustrate a four-way classification with the (contrived) data in Table 15.8. There are n = 12 items (people) and p = 4 categorical variables. The four variables and their categories are listed in Table 15.9. The indicator matrix G for the data in Table 15.8 is given in Table 15.10.

A correspondence analysis on G is equivalent to a correspondence analysis on G ′ G , which is called the Burt matrix . This equivalence can be justified as follows. In the singular value decomposition G = U 𝚲 V ′ , the matrix V contains eigenvectors

Table 15.8. A List of 12 People and Their Categories on Four Variables

|   Person | Gender   | Age    | Marital Status   | Hair Color   |
|----------|----------|--------|------------------|--------------|
|        1 | Male     | Young  | Single           | Brown        |
|        2 | Male     | Old    | Single           | Red          |
|        3 | Female   | Middle | Married          | Blond        |
|        4 | Male     | Old    | Single           | Black        |
|        5 | Female   | Middle | Married          | Black        |
|        6 | Female   | Middle | Single           | Brown        |
|        7 | Male     | Young  | Married          | Red          |
|        8 | Male     | Old    | Married          | Blond        |
|        9 | Male     | Middle | Single           | Brown        |
|       10 | Female   | Young  | Married          | Black        |
|       11 | Female   | Old    | Single           | Brown        |
|       12 | Male     | Young  | Married          | Blond        |

Table 15.9. The Categories for the Four Variables in Table 15.8

| Variable                             | Levels                                                                        |
|--------------------------------------|-------------------------------------------------------------------------------|
| Gender Age Marital status Hair color | Male, female Young, middle-aged, old Single, married Blond, brown, black, red |

Table 15.10. Indicator Matrix G for the Data in Table 15.8

|   Person | Gender   | Age   | Marital Status   | Hair Color   |
|----------|----------|-------|------------------|--------------|
|        1 | 1 0      | 1 0 0 | 1 0              | 0 1 0 0      |
|        2 | 1 0      | 0 0 1 | 1 0              | 0 0 0 1      |
|        3 | 0 1      | 0 1 0 | 0 1              | 1 0 0 0      |
|        4 | 1 0      | 0 0 1 | 1 0              | 0 0 1 0      |
|        5 | 0 1      | 0 1 0 | 0 1              | 0 0 1 0      |
|        6 | 0 1      | 0 1 0 | 1 0              | 0 1 0 0      |
|        7 | 1 0      | 1 0 0 | 0 1              | 0 0 0 1      |
|        8 | 1 0      | 0 0 1 | 0 1              | 1 0 0 0      |
|        9 | 1 0      | 0 1 0 | 1 0              | 1 0 0 0      |
|       10 | 0 1      | 1 0 0 | 0 1              | 0 0 1 0      |
|       11 | 0 1      | 0 0 1 | 1 0              | 0 1 0 0      |
|       12 | 1 0      | 1 0 0 | 0 1              | 1 0 0 0      |

of G ′ G . The same matrix V would be used in the spectral decomposition of G ′ G . Thus the columns of V are used in plotting coordinates for the columns of G or the columns of G ′ G . If G is n × p with p &lt; n , then G ′ G would be smaller in size than G .

A correspondence analysis of G ′ G yields only column coordinates. A point is plotted for each column of G (or of G ′ G ). Thus each point represents a category (attribute) of one of the variables.

The Burt matrix G ′ G has a square block on the diagonal for each variable and a rectangular block off-diagonal for each pair of variables. Each diagonal block is a diagonal matrix showing the frequencies for the categories in the corresponding variable. Each off-diagonal block is a two-way contingency table for the corresponding pair of variables. In Table 15.11, we show the G ′ G matrix for the G matrix in Table 15.10.

Table 15.11. Burt Matrix G ′ Gfor the Matrix G in Table 15.10

|   Gender |   Gender |   Age |   Age |   Age |   Marital Status |   Marital Status |   Hair Color |   Hair Color |   Hair Color |   Hair Color |
|----------|----------|-------|-------|-------|------------------|------------------|--------------|--------------|--------------|--------------|
|        7 |        0 |     3 |     1 |     3 |                4 |                3 |            3 |            1 |            1 |            2 |
|        0 |        5 |     1 |     3 |     1 |                2 |                3 |            1 |            2 |            2 |            0 |
|        3 |        1 |     4 |     0 |     0 |                1 |                3 |            1 |            1 |            1 |            1 |
|        1 |        3 |     0 |     4 |     0 |                2 |                2 |            2 |            1 |            1 |            0 |
|        3 |        1 |     0 |     0 |     4 |                3 |                1 |            1 |            1 |            1 |            1 |
|        4 |        2 |     1 |     2 |     3 |                6 |                0 |            1 |            3 |            1 |            1 |
|        3 |        3 |     3 |     2 |     1 |                0 |                6 |            3 |            0 |            2 |            1 |
|        3 |        1 |     1 |     2 |     1 |                1 |                3 |            4 |            0 |            0 |            0 |
|        1 |        2 |     1 |     1 |     1 |                3 |                0 |            0 |            3 |            0 |            0 |
|        1 |        2 |     1 |     1 |     1 |                1 |                2 |            0 |            0 |            3 |            0 |
|        2 |        0 |     1 |     0 |     1 |                1 |                1 |            0 |            0 |            0 |            2 |

Table 15.12. Singular Values ( λ i ), Inertia ( λ 2 i ), and Chi-square ( n λ 2 i ) for the Burt Matrix G ′ Gin Table 15.11

| Singular Value   |   Inertia |   Chi-Square |   Percent |
|------------------|-----------|--------------|-----------|
| .68803           |   0.47338 |       31.551 |     27.05 |
| .67451           |   0.45497 |       30.324 |     26    |
| .51492           |   0.26515 |       17.672 |     15.15 |
| .50000           |   0.25    |       16.663 |     14.29 |
| .41941           |   0.1759  |       11.724 |     10.05 |
| .33278           |   0.11074 |        7.381 |      6.33 |
| .14091           |   0.01986 |        1.323 |      1.13 |
| Total            |   1.75    |      116.638 |    100    |

Distances between points are not as meaningful as in correspondence analysis, but points in the same quadrant or approximate vicinity indicate an association. If two close points represent attributes of the same variable, the two attributes may be combined into a single attribute.

Since the Burt matrix G ′ G has only two-way contingency tables, three-way and higher-order interactions are not represented in the plot. The various two-way tables are analyzed simultaneously, however.

Figure 15.10. Plot of points representing the 11 columns of Table 15.10 or 15.11.

<!-- image -->

Example 15.2.5(a). We continue the illustration in this section. A correspondence analysis of the Burt matrix G ′ G in Table 15.11 yields the singular values, inertia, and chi-squared values in Table 15.12. The first two singular values account for only 53.05% of the total variation. A plot of the first two dimensions for the 11 columns in Table 15.10 or 15.11 is given in Figure 15.10. It appears that married and blond hair have a greater association that would be expected by chance alone. Another association is that between female and middle age.

Example 15.2.5(b). Table 15.13 (Edwards and Kreiner 1983) is a five-way contingency table of employed men between the ages of 18 and 67 who were asked whether they themselves carried out repair work on their home, as opposed to hiring a craftsperson to do the job. The five categorical variables are as follows:

Work of respondent: skilled, unskilled, office,

Tenure: rent, own,

Age: under 30, 31-45, over 45,

Accommodation type: apartment, house,

Response to repair question: yes, no.

A multiple correspondence analysis produced the intertia and singular values in Table 15.14. The plot of the first two dimensions is given in Figure 15.11.

Table 15.13. Do-It-Yourself Data

|           |        |          | Accommodation Type   | Accommodation Type   | Accommodation Type   | Accommodation Type   | Accommodation Type   | Accommodation Type   |
|-----------|--------|----------|----------------------|----------------------|----------------------|----------------------|----------------------|----------------------|
|           |        |          | Apartment            | Apartment            | Apartment            | House                | House                | House                |
|           |        |          | Age                  | Age                  | Age                  | Age                  | Age                  | Age                  |
| Work      | Tenure | Response | ≤ 30                 | 31-45                | ≥ 46                 | ≤ 30                 | 31-45                | ≥ 46                 |
| Skilled   | Rent   | Yes      | 18                   | 15                   | 6                    | 34                   | 10                   | 2                    |
|           |        | No       | 15                   | 13                   | 9                    | 28                   | 4                    | 6                    |
|           | Own    | Yes      | 5                    | 3                    | 1                    | 56                   | 56                   | 35                   |
|           |        | No       | 1                    | 1                    | 1                    | 12                   | 21                   | 8                    |
|           | Rent   | Yes      | 17                   | 10                   | 15                   | 29                   | 3                    | 7                    |
|           |        | No       | 34                   | 17                   | 19                   | 44                   | 13                   | 16                   |
| Unskilled | Own    | Yes      | 2                    | 0                    | 3                    | 23                   | 52                   | 49                   |
|           |        | No       | 3                    | 2                    | 0                    | 9                    | 31                   | 51                   |
|           | Rent   | Yes      | 30                   | 23                   | 21                   | 22                   | 13                   | 21                   |
|           |        | No       | 25                   | 19                   | 40                   | 25                   | 16                   | 12                   |
| Office    | Own    | Yes      | 8                    | 5                    | 1                    | 54                   | 191                  | 102                  |
| Office    |        | No       | 4                    | 2                    | 2                    | 19                   | 76                   | 61                   |

Table 15.14. Singular Values ( λ i ), Inertia ( λ 2 i ), and Chi-Square ( n λ 2 i ) for the Do-ItYourself Data in Table 15.13

| Singular Value   |   Inertia | Chi-Square   |   Percent |
|------------------|-----------|--------------|-----------|
| .60707           |   0.36853 | 3,446.5      |     26.32 |
| .49477           |   0.2448  | 2,289.4      |     17.49 |
| .45591           |   0.20785 | 1,943.9      |     14.85 |
| .42704           |   0.18237 | 1,705.5      |     13.03 |
| .40516           |   0.16415 | 1,535.2      |     11.73 |
| .39392           |   0.15517 | 1,451.2      |     11.08 |
| .27771           |   0.07713 | 721.3        |      5.51 |
| Total            |   1.4     | 13,092.9     |    100    |

Figure 15.11. Plot of points representing the 12 categories in Table 15.13.

<!-- image -->

Unskilled employment has a high association with not doing one's own repairs. Doing one's own repairs is associated with owning a house, age between 31 and 45, and doing office work. Living in an apartment is associated with renting.

## 15.3 BIPLOTS

## 15.3.1 Introduction

Abiplot is a two-dimensional representation of a data matrix Y [see (3.17)] showing a point for each of the n observation vectors (rows of Y ) along with a point for each of the p variables (columns of Y ). The prefix bi refers to the two kinds of points; not to the dimensionality of the plot. The method presented here could, in fact, be generalized to a three-dimensional (or higher-order) biplot. Biplots were introduced by Gabriel (1971) and have been discussed at length by Gower and Hand (1996); see also Khattree and Naik (2000), Jacoby (1998, Chapter 7), and Seber (1984, pp. 204212).

If p = 2, a simple scatter plot, as in Section 3.3, has both kinds of information, namely, a point for each observation and the two axes representing the variables. We can see at a glance the placement of the points relative to each other and relative to the variables.

When p &gt; 2, we can obtain a two-dimensional plot of the observations by plotting the first two principal components of S as in Section 12.4. We can then add a representation of the p variables to the plot of principal components to obtain a biplot. The principal component approach is discussed in Section 15.3.2. A method based on the singular value decomposition is presented in Section 15.3.3, and other methods are reviewed in Section 15.3.5.

## 15.3.2 Principal Component Plots

Aprincipal component is given by z = a ′ y , where a is an eigenvector of S , the sample covariance matrix, and y is a p × 1 observation vector (see Section 12.2). There are p eigenvectors a 1, a 2 , . . . , a p , and thus there are p principal components z 1, z 2 , . . . , z p for each observation vector y i , i = 1, 2 , . . . , n . Hence (using the centered form) the observation vectors are transformed to zi j = a ′ j ( y i -y ) = ( y i -y ) ′ a j , i = 1, 2 , . . . , n ; j = 1, 2 , . . . , p . Each p × 1 observation vector y i is transformed to a p × 1 vector of principal components,

<!-- formula-not-decoded -->

where A = ( a 1 , a 2 , . . . , a p ) is the p × p matrix whose columns are (normalized) eigenvectors of S . [Note that the matrix A in (15.49) is the transpose of A in (12.3)]. With Z and Y c defined as

<!-- formula-not-decoded -->

[see (10.13)], we can express the principal components in (15.49) as

<!-- formula-not-decoded -->

Since the eigenvectors a j of the symmetric matrix S are mutually orthogonal (see Section 2.11.6), A = ( a 1 , a 2 , . . . , a p ) is an orthogonal matrix and AA ′ = I . Multiplying (15.51) on the right by A ′ , we obtain

<!-- formula-not-decoded -->

The best two-dimensional representation of Y c is given by taking the first two columns of Z and the first two columns of A . If the resulting matrices are denoted by Z 2 and A 2, we have

<!-- formula-not-decoded -->

The fit in (15.53) is best in a least squares sense. If the left side of (15.53) is represented by Y c = B = ( bi j ) and the right side by Z 2 A ′ 2 = C = ( ci j ) , then n i = 1 p j = 1 ( bi j -ci j ) 2 is minimized (Seber 1984, p. 206).

∑ ∑ The coordinates for the n observations are the rows of Z 2, and the coordinates for the p variables are the rows of A 2 (columns of A ′ 2 ). The coordinates are discussed further in Section 15.3.4.

The adequacy of the fit in (15.53) can be evaluated by examining the first two eigenvalues λ 1 and λ 2 of S . Thus a large value (close to 1) of

<!-- formula-not-decoded -->

would indicate that Y c is represented well visually in the plot.

## 15.3.3 Singular Value Decomposition Plots

We can also obtain Y c = ZA ′ in (15.52) by means of the singular value decomposition of Y c . By (2.117), we have

<!-- formula-not-decoded -->

where 𝚲 = diag (λ 1 , λ 2 , . . . , λ p ) is a diagonal matrix containing square roots of the (nonzero) eigenvalues λ 2 1 , λ 2 2 , . . . , λ 2 p of Y ′ c Y c (and of Y c Y ′ c ) , the columns of U are the corresponding eigenvectors of Y c Y ′ c , and the columns of V are the corresponding eigenvectors of Y ′ c Y c .

The product U 𝚲 in (15.54) is equal to Z , the matrix of principal component scores in (15.51). To see this we multiply (15.54) by V , which is orthogonal because it contains the (normalized) eigenvectors of the symmetric matrix Y ′ c Y c (see Section 2.11.6). This gives

<!-- formula-not-decoded -->

By (10.17), Y ′ c Y c is equal to ( n -1 ) S . By (2.106), eigenvectors of ( n -1 ) S are also eigenvectors of S . Thus V is the same as A in (15.51), which contains eigenvectors of S . Hence, Y c V in (15.55) becomes

<!-- formula-not-decoded -->

We can therefore write (15.54) as

<!-- formula-not-decoded -->

Thus the singular value decomposition of Y c gives the same factoring as the expression in (15.52) based on principal components.

## 15.3.4 Coordinates

In this section, we consider the coordinates for the methods of Sections 15.3.2 and 15.3.3. Let us return to (15.53), the two-dimensional representation of Y c based on principal components (which is the same representation as that based on the singular value decomposition):

<!-- formula-not-decoded -->

The elements of (15.57) are of the form

<!-- formula-not-decoded -->

Thus each observation is represented as a linear combination, the coordinates (coefficients) being the elements of the vector ( zi 1 , zi 2 ) and the axes being the elements of the vector ( aj 1 , aj 2 ) . We therefore plot the points ( zi 1 , zi 2 ) , i = 1, 2 , . . . , n , and the points ( aj 1 , aj 2 ) , j = 1, 2 , . . . , p . To distinguish them and to show relationship of the points to the axes, the points ( aj 1 , aj 2 ) are connected to the origin with a straight line forming an arrow. If necessary, the scale of the points ( aj 1 , aj 2 ) could be adjusted to be compatible with that of the principal components ( zi 1 , zi 2 ) .

The Euclidean distance between two points ( zi 1 , zi 2 ) and ( zk 1 , zk 2 ) is approximately equal to the distance between the corresponding points (rows) y ′ i and y ′ k in the data matrix Y . If all of the principal components were used, as in (15.51) and

(15.52), the distance would be the same, but with only two principal components, the distance is an approximation.

The cosine of the angle between the arrows (lines) drawn to each pair of axis points ( aj 1 , aj 2 ) and ( ak 1 , ak 2 ) shows the correlation between the two corresponding variables [see (3.14) and (3.15)]. Thus a small angle between two vectors indicates that the two variables are highly correlated, two variables whose vectors form a 90 ◦ angle are uncorrelated, and an angle greater than 90 ◦ indicates that the variables are negatively correlated.

The values of the p variables in the i th observation vector y i (corrected for means) are related to the perpendicular projection of the point ( z 1 i , z 2 i ) on the p vectors from the origin to the points ( aj 1 , aj 2 ) representing variables. The further from the origin a projection falls on an arrow, the larger the value of the observation on that variable. Hence the vectors will be oriented toward the observations that have larger values on the corresponding variables.

Example 15.3.4. Using the city crime data of Table 14.1, we illustrate the principal component approach. The first two eigenvectors of the sample covariance matrix S are given by

<!-- formula-not-decoded -->

The matrix of the first two principal components is given by

<!-- formula-not-decoded -->

<!-- image -->

Dimension 1

Figure 15.12. Principal components biplot for city crime data in Table 14.1.

The coordinates for the 16 cities are found in Z 2, and the coordinates for the 7 variables are found in A 2. The plot of the city and variable points is given in Figure 15.12. The observation points are spread out, whereas the variable points are clustered tightly around the origin. Suitable scaling of the eigenvectors in A 2 would enable the arrows representing the variables to pass through the points (see Example 15.3.5).

## 15.3.5 Other Methods

The singular value decomposition of Y c is given in (15.54) as

<!-- formula-not-decoded -->

In Section 15.3.3, it was shown that U 𝚲 = Z and V = A [see (15.56)], so that (15.58) can be written as

<!-- formula-not-decoded -->

which is equivalent to the principal component solution Y c = ZA ′ in (15.52). Alternative factorings may be of interest. Two that have been considered are

then

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

If we denote the submatrices consisting of the first two columns of U and V as U 2 and V 2, respectively, and define 𝚲 2 = diag (λ 1 , λ 2 ) , then the two-dimensional representations of (15.59) and (15.60) are

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

For the biplot corresponding to (15.61), we plot the set of points ( ui 1 , ui 2 ) , i = 1, 2 , . . . , n , and the set of points (λ 1 v j 1 , λ 2 v j 2 ) , j = 1, 2 , . . . , p , with the latter points connected to the origin by an arrow to show the axes. For the biplot arising from (15.62), we plot the set of points ( √ λ 1 ui 1 , √ λ 2 ui 2 ) , i = 1, 2 , . . . , n , and the set of points ( √ λ 1 v j 1 , √ λ 2 v j 2 ) , j = 1, 2 , . . . , p , with the latter points connected to the origin with an arrow.

The presence of λ 1 and λ 2 in (15.61) and (15.62) provides scaling that is absent in (15.57). For many data sets the scaling in (15.62) will be adequate with no further adjustment.

If we write (15.59) in the form

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

(see Problem 15.10). With suitable scaling of the eigenvectors in U and V , we could eliminate the coefficients involving n -1 from (15.64) and (15.65).

By (15.64) (with scaling to eliminate n -1), the (Euclidean) distance ( u i -u k ) ′ ( u i -u k ) between two points u i and u k is equal to the Mahalanobis distance ( y i -y k ) ′ S -1 ( y i -y k ) between the corresponding points y i and y k in the data matrix Y :

<!-- formula-not-decoded -->

(see Problem 15.11). By (15.65), the covariance s jk between the j th and k th variables (columns of Y ) is given by

<!-- formula-not-decoded -->

where h ′ j and h ′ k are rows of H . By (3.14) and (3.15), this can be converted to the correlation

<!-- formula-not-decoded -->

so that the angle between the two vectors h j and h k is related to the correlation between the j th and k th variables.

The two-dimensional representation of u i and h j in (15.61) has the approximate Mahalanobis distance and correlation properties discussed earlier.

Example 15.3.5. Using the city crime data of Table 14.1, we illustrate the singular value decomposition method with the factorings in (15.61) and (15.62). The matrices U 2, 𝚲 2, and V 2 are

<!-- formula-not-decoded -->

𝚲 2 = diag ( 1503 . 604 , 678 . 615 ).

By (15.61), the two-dimensional representation is given by plotting the rows of U 2 and the rows of V 2 𝚲 2 (or the columns of 𝚲 2 V ′ 2 ) . For V 2 𝚲 2 we have

<!-- formula-not-decoded -->

The plot of the observation points and variable points is given in Figure 15.13. For (15.62), we obtain

<!-- formula-not-decoded -->

The plot of these coordinates is given in Figure 15.14. For this data set, the factoring given by (15.62) in Figure 15.14 is preferred because it plots both observation and variable points on the same scale. The factorings shown in Figures 15.12 and 15.13 would need an adjustment in scaling.

## PROBLEMS

- 15.1 In step 2 of the algorithm for metric scaling in Section 15.1.2, the matrix B = ( bi j ) is defined in terms of A = ( ai j ) as bi j = ai j -¯ ai . -¯ a . j +¯ a .. . Show that bi j in B = ( I -1 n J ) A ( I -1 n J ) in (15.2) is equivalent to bi j = ai j - ¯ ai . - ¯ a . j + ¯ a .. .
- 15.2 Verify the result stated in step 2 of the algorithm in Section 15.1.2, namely, that there exists a q -dimensional configuration z 1, z 2 , . . . , z n such that di j = δ i j if and only if B is positive semidefinite of rank q . Use the following approach.

<!-- image -->

Dimension 1

Figure 15.13. Plot of U 2 and V 2 𝚲 2 for the city crime data in Table 14.1.

<!-- image -->

Figure 15.14. Plot of U 2 𝚲 1 / 2 2 and V 2 𝚲 1 / 2 2 for the city crime data in Table 14.1.

- (a) Assuming the existence of z 1, z 2 , . . . , z n such that δ 2 i j = d 2 i j = ( z i -z j ) ′ ( z i -z j ) , show that B is positive semidefinite.
- 15.3 (a) Show that r = ∑ b j = 1 p . j c j in (15.20) is the same as r = ( p 1 . , p 2 . , . . . , pa .) ′ in (15.9).
- (b) Assuming B is positive semidefinite, show that there exist z 1, z 2 , . . . , z n such that d 2 i j = ( z i -z j ) ′ ( z i -z j ) = δ 2 i j .
- (b) Show that c ′ = ∑ a i = 1 pi . r ′ i in (15.21) is equivalent to c ′ = ( p . 1 , p . 2 , . . . , p . b ) in (15.10).
- 15.4 Show that j ′ r = c ′ j = 1 as in (15.22).
- 15.5 Show that the chi-square statistic in (15.26) is equal to that in (15.25).
- 15.6 (a) Show that the chi-square statistic in (15.27) is equal to that in (15.25).
- (b) Show that the chi-square statistic in (15.28) is equal to that in (15.25).
- 15.7 (a) Show the chi-square statistic in (15.29) is equal to that in (15.27).
- (b) Show the chi-square statistic in (15.30) is equal to that in (15.28).
- 15.8 (a) Show that R -jc ′ = D -1 r ( P -rc ′ ) as in (15.41).
- (b) Show that C -rj ′ = D -1 c ( P -rc ′ ) as in (15.42).
- 15.9 Show that if all the principal components were used, the distance between z i and z k would be the same as between y i and y k , as noted in Section 15.3.4.
- 15.10 (a) Show that UU ′ = Y c S -1 Y ′ c /( n -1 ) as in (15.64).
- 15.11 Show that ( u i -u k ) ′ ( u i -u k ) = ( y i -y k ) ′ S -1 ( y i -y k ) as in (15.66).
- (b) Show that HH ′ = ( n -1 ) S as in (15.65).
- 15.12 In Table 15.15, we have road distances between major UK towns (Hand et al. 1994, p. 346). The towns are as follows:

A = Aberdeen, B = Birmingham, C = Brighton, D = Bristol, E = Cardiff, F = Carlisle, G = Dover, H = Edinburgh, I = Fort William, J = Glasgow, K = Holyhead, L = Hull, M = Inverness, N = Leeds, O = Liverpool, P = London, Q = Manchester, R = Newcastle, S = Norwich, T = Nottingham, U = Penzance, V = Plymouth, W = Sheffield.

- (a) Find the matrix B as in (15.2).
- (b) Using the spectral decomposition, find the first two columns of the matrix Z as in (15.4).
- (c) Create a metric multidimensional scaling plot of the first two dimensions. What do you notice about the positions of the cities?
- 15.13 Zhang, Helander, and Drury (1996) analyzed a 43 × 43 similarity matrix for 43 descriptors of comfort, such as calm, tingling, restful, etc. For the similarity matrix, see the Wiley ftp site (Appendix C).
- (a) Carry out a metric multidimensional scaling analysis and plot the first two dimensions. What pattern is seen in the plot?

Table 15.15. Road Distances between Major UK Towns

<!-- image -->

| V   | 299                                                                                                                                 |
|-----|-------------------------------------------------------------------------------------------------------------------------------------|
| U   | 78 370                                                                                                                              |
| T   | 331 260 44                                                                                                                          |
| S   | 120 425 354 148                                                                                                                     |
| R   | 254 160 486 414 134                                                                                                                 |
| Q   | 144 185 70 359 287 37                                                                                                               |
| P   | 204 285 115 131 312 241 169                                                                                                         |
| O   | 216 35 175 242 109 370 300 79                                                                                                       |
| N   | 75 199 44 93 173 73 406 335 35                                                                                                      |
| M   | 382 383 571 378 268 543 449 726 655 422                                                                                             |
| L   | 431 61 130 190 99 142 151 93 419 348 67                                                                                             |
| K   | 223 488 168 105 268 125 268 308 177 423 352 159                                                                                     |
| J   | 327 272 172 222 224 412 218 152 384 289 567 496 263                                                                                 |
| I   | 102 438 382 65 333 335 523 329 241 494 400 678 607 374                                                                              |
| H   | 133 46 328 231 158 206 225 413 219 108 368 273 568 496 247                                                                          |
| G   | 466 608 497 353 264 657 271 302 79 289 352 171 217 364 292 255                                                                      |
| F   | 398 99 209 98 228 173 258 123 125 313 119 57 285 190 468 397 164                                                                    |
| E   | 302 245 402 511 401 205 253 560 240 205 155 193 320 268 166 234 162 204                                                             |
| D   | 47 282 210 381 491 381 237 233 540 220 185 120 172 300 233 146 196 125 184                                                          |
| C   | 170 205 378 78 478 587 477 333 283 636 264 282 60 269 350 169 196 287 216 234                                                       |
| B   | 185 88 108 198 206 298 407 297 153 135 456 115 102 120 89 202 176 55 274 203 86                                                     |
| A   | B 431 C 611 D 515 E 535 F 232 G 595 H 126 I 159 J 146 K 461 L 360 M 106 N 335 O 358 P 546 Q 352 R 237 S 497 T 402 U 701 V 630 W 376 |

Table 15.16. Dissimilarity Matrix for World War II Politicians

| Person      | Hitler   | Mussolini   | Churchill   | Eisenhower   |
|-------------|----------|-------------|-------------|--------------|
| Hitler      | 0        | 5           | 11          | 15           |
| Mussolini   | 5        | 0           | 14          | 16           |
| Churchill   | 11       | 14          | 0           | 7            |
| Eisenhower  | 15       | 16          | 7           | 0            |
| Stalin      | 8        | 13          | 11          | 16           |
| Attlee      | 17       | 18          | 11          | 16           |
| Franco      | 5        | 3           | 12          | 14           |
| De Gaulle   | 10       | 11          | 5           | 8            |
| Mao Tse     | 16       | 18          | 16          | 17           |
| Truman      | 17       | 18          | 8           | 6            |
| Chamberlain | 12       | 14          | 10          | 7            |
| Tito        | 16       | 17          | 8           | 12           |
|             | Stalin   | Attlee      | Franco      | De Gaulle    |
| Hitler      | 8        | 17          | 5           | 10           |
| Mussolini   | 13       | 18          | 3           | 11           |
| Churchill   | 11       | 11          | 12          | 5            |
| Eisenhower  | 16       | 16          | 14          | 8            |
| Stalin      | 0        | 15          | 13          | 11           |
| Attlee      | 15       | 0           | 16          | 12           |
| Franco      | 13       | 16          | 0           | 9            |
| De Gaulle   | 11       | 12          | 9           | 0            |
| Mao Tse     | 12       | 16          | 17          | 13           |
| Truman      | 14       | 12          | 16          | 9            |
| Chamberlain | 16       | 9           | 10          | 11           |
| Tito        | 12       | 13          | 12          | 7            |
|             | Mao Tse  | Truman      | Chamberlain | Tito         |
| Hitler      | 16       | 17          | 12          | 16           |
| Mussolini   | 18       | 18          | 14          | 17           |
| Churchill   | 16       | 8           | 10          | 8            |
| Eisenhower  | 17       | 6           | 7           | 12           |
| Stalin      | 12       | 14          | 16          | 12           |
| Attlee      | 16       | 12          | 9           | 13           |
| Franco      | 17       | 16          | 10          | 12           |
| De Gaulle   | 13       | 9           | 11          | 7            |
| Mao Tse     | 0        | 12          | 17          | 10           |
| Truman      | 12       | 0           | 9           | 11           |
| Chamberlain | 17       | 9           | 0           | 15           |
| Tito        | 10       | 11          | 15          | 0            |

- (b) For an alternative approach, carry out a cluster analysis of the configuration of points found in part (a), using Ward's method. Create a dendrogram of the cluster solution. How many clusters are indicated?
- 15.14 Use the politics data of Table 15.16 (Everitt 1987, Table 6.7). Two subjects assessed the degree of dissimilarity between World War II politicians. The data matrix represents the sum of the dissimilarities between the two subjects.
- (a) For k = 6, create an initial configuration of points by choosing 12 random observations taken from a multivariate normal distribution with mean vector 0 and covariance matrix I 6.
- (b) Carry out a nonmetric multidimensional scaling analysis using the seeds found in part (a). Find the value of the STRESS statistic.
- (c) Repeat parts (a) and (b) for k = 1 , . . . , 5. Plot the STRESS values against the values of k . How many dimensions should be kept? Plot the final configuration of points with two dimensions.
- (d) Repeat parts (a)-(c) using an initial configuration of points from a multivariate normal with different mean vector and covariance matrix from those in part (a). How many dimensions should be kept? Plot the final configuration of points with two dimensions. How does this solution compare to that in part (c)?
- (e) Repeat parts (a)-(c) using an initial configuration of points from a uniform distribution over ( 0 , 1 ) . How many dimensions should be kept? Plot the final configuration of points with two dimensions.
- (f) Repeat part (e) using as initial configuration of points the metric multidimensional scaling solution found by treating the ordinal measurements as continuous. How many dimensions should be kept? Plot the final configuration of points with two dimensions.

Table 15.17. Birth and Death Months of 1281 People

## Publisher's Note:

Permission to reproduce this image online was not granted by the copyright holder. Readers are kindly asked to refer to the printed version of this chapter.

- 15.15 In Table 15.17 we have the months of birth and death for 1281 people (Andrews and Herzberg 1985, Table 71.2).
- (a) Find the correspondence matrix P as in (15.8).
- (b) Find the matrices R and C , as in (15.15) and (15.19).
- (c) Perform a chi-square test for independence between birth and death months.
- (d) Plot the row and column deviations as in Example 15.2.5(a).
- 15.16 In Table 15.18, we have a cross-classification of crimes in Norway in 1984 categorized by type and site (Clausen 1998, p. 9).
- (a) Find the correspondence matrix P as in (15.8).
- (b) Find the matrices R and C as in (15.15) and (15.19).
- (c) Perform a chi-square test for independence between type of crime and site.
- (d) Plot the row and column deviations as in Example 15.2.4.
- 15.17 In Table 15.19, we have a six-way contingency table (Andrews and Herzberg 1985, Table 34.1). Carry out a multiple correspondence analysis.
- (a) Set up an indicator matrix G and find the Burt matrix G ′ G .
- (b) Perform a correspondence analysis on the Burt matrix found in part (a) and plot the coordinates.
- (c) What associations are present?
- 15.18 Use the protein consumption data of Table 14.7.
- (a) Create a biplot using the principal component approach in (15.53) or (15.57).
- (b) Create a biplot using the singular value decomposition approach with the factoring as in (15.61).
- (c) Create a biplot using the singular value decomposition approach with the factoring as in (15.62).
- (d) Which of the three biplots best represents the data?
- 15.19 Use the perception data of Table 13.1.

Table 15.18. Crimes by Type and Site

| Part of Country   |   Burglary |   Fraud |   Vandalism |   Total |
|-------------------|------------|---------|-------------|---------|
| Oslo area         |        395 |    2456 |        1758 |    4609 |
| Mid Norway        |        147 |     153 |         916 |    1216 |
| North Norway      |        694 |     327 |        1347 |    2368 |
| Total             |       1236 |    2936 |        4021 |    8193 |

Table 15.19. Byssinosis Data

<!-- image -->

## Table 15.19. (Continued)

<!-- image -->

- (a) Create a biplot using the principal component approach in (15.53) or (15.57).
- (b) Create a biplot using the singular value decomposition approach with the factoring as in (15.61).
- (c) Create a biplot using the singular value decomposition approach with the factoring as in (15.62).
- (d) Which of the three biplots best represents the data?

## 15.20 Use the cork data of Table 6.21.

- (a) Create a biplot using the principal component approach in (15.53) or (15.57).
- (b) Create a biplot using the singular value decomposition approach with the factoring as in (15.61).
- (c) Create a biplot using the singular value decomposition approach with the factoring as in (15.62).
- (d) Which of the three biplots best represents the data?

## A P P E N D I X A

## Tables

Table A.1. Upper Percentiles for √ b 1

<!-- formula-not-decoded -->

The sampling distribution of √ b 1 is symmetric about zero, and lower percentage points corresponding to negative skewness are given by the negative of the table values. Reject the hypothesis of normality if √ b 1 is greater than the table value or less than the negative of the table value.

|    |   Upper Percentiles |   Upper Percentiles | Upper Percentiles   |   Upper Percentiles |   Upper Percentiles |
|----|---------------------|---------------------|---------------------|---------------------|---------------------|
| n  |              10     |               5     | 2.5 1               |               0.5   |               0.1   |
| 4  |               0.831 |               0.987 | 1.070 1.120         |               1.137 |               1.151 |
| 5  |               0.821 |               1.049 | 1.207 1.337         |               1.396 |               1.464 |
| 6  |               0.795 |               1.042 | 1.239 1.429         |               1.531 |               1.671 |
| 7  |               0.782 |               1.018 | 1.230 1.457         |               1.589 |               1.797 |
| 8  |               0.765 |               0.998 | 1.208 1.452         |               1.605 |               1.866 |
| 9  |               0.746 |               0.977 | 1.184 1.433         |               1.598 |               1.898 |
| 10 |               0.728 |               0.954 | 1.159 1.407         |               1.578 |               1.906 |
| 11 |               0.71  |               0.931 | 1.134 1.381         |               1.553 |               1.899 |
| 12 |               0.693 |               0.91  | 1.109 1.353         |               1.526 |               1.882 |
| 13 |               0.677 |               0.89  | 1.085 1.325         |               1.497 |               1.859 |
| 14 |               0.662 |               0.87  | 1.061 1.298         |               1.468 |               1.832 |
| 15 |               0.648 |               0.851 | 1.039 1.272         |               1.44  |               1.803 |
| 16 |               0.635 |               0.834 | 1.018 1.247         |               1.412 |               1.773 |
| 17 |               0.622 |               0.817 | .997 1.222          |               1.385 |               1.744 |
| 18 |               0.61  |               0.801 | .978 1.199          |               1.359 |               1.714 |
| 19 |               0.599 |               0.786 | .960 1.176          |               1.334 |               1.685 |
| 20 |               0.588 |               0.772 | .942 1.155          |               1.31  |               1.657 |
| 21 |               0.578 |               0.758 | .925 1.134          |               1.287 |               1.628 |
| 22 |               0.568 |               0.746 | .909 1.114          |               1.265 |               1.602 |
| 23 |               0.559 |               0.733 | .894 1.096          |               1.243 |               1.575 |
| 24 |               0.55  |               0.722 | .880 1.078          |               1.223 |               1.55  |
| 25 |               0.542 |               0.71  | .866 1.060          |               1.203 |               1.526 |

ISBN: 0-471-41889-7

Table A.2. Coefficients for Transforming √ b 1 to a Standard Normal

| n     | δ           | 1 /λ          | n       | δ           | 1 /λ          |
|-------|-------------|---------------|---------|-------------|---------------|
|       |             |               | 62      | 3.389       | 1.0400        |
|       |             |               | 64      | 3.420       | 1.0449        |
| 8     | 5.563       | .3030         | 66      | 3.450       | 1.0495        |
| 9     | 4.260       | .4080         | 68      | 3.480       | 1.0540        |
| 10    | 3.734       | .4794         | 70      | 3.510       | 1.0581        |
| 11    | 3.447       | .5339         | 72      | 3.540       | 1.0621        |
| 12    | 3.270       | .5781         | 74      | 3.569       | 1.0659        |
| 13    | 3.151       | .6153         | 76      | 3.599       | 1.0695        |
| 14    | 3.069       | .6473         | 78      | 3.628       | 1.0730        |
| 15    | 3.010       | .6753         | 80      | 3.657       | 1.0763        |
| 16    | 2.968       | .7001         | 82      | 3.686       | 1.0795        |
| 17    | 2.937       | .7224         | 84      | 3.715       | 1.0825        |
| 18    | 2.915       | .7426         | 86      | 3.744       | 1.0854        |
| 19    | 2.900       | .7610         | 88      | 3.772       | 1.0882        |
| 20    | 2.890       | .7779         | 90      | 3.801       | 1.0909        |
| 21    | 2.884       | .7934         | 92      | 3.829       | 1.0934        |
| 22    | 2.882       | .8078         | 94      | 3.857       | 1.0959        |
| 23    | 2.882       | .8211         | 86      | 3.885       | 1.0983        |
| 24    | 2.884       | .8336         | 98      | 3.913       | 1.1006        |
| 25    | 2.889       | .8452         | 100     | 3.940       | 1.1028        |
| 26    | 2.895       | .8561         | 105     | 4.009       | 1.1080        |
| 27    | 2.902       | .8664         | 110     | 4.076       | 1.1128        |
| 28    | 2.910       | .8760         | 115     | 4.142       | 1.1172        |
| 29    | 2.920       | .8851         | 120     | 4.207       | 1.1212        |
| 30    | 2.930       | .8938         | 125     | 4.272       | 1.1250        |
| 31    | 2.941       | .9020         | 130     | 4.336       | 1.1285        |
| 32    | 2.952       | .9097         | 135     | 4.398       | 1.1318        |
| 33    | 2.964       | .9171         | 140     | 4.460       | 1.1348        |
| 34    | 2.977       | .9241         | 145     | 4.521       | 1.1377        |
| 35    | 2.990       | .9308         | 150     | 4.582       | 1.1403        |
| 36    | 3.003       | .9372         | 155     | 4.641       | 1.1428        |
| 37    | 3.016       | .9433         | 160     | 4.700       | 1.1452        |
| 38    | 3.030       | .9492         | 165     | 4.758       | 1.1474        |
| 39    | 3.044       | .9548         | 170     | 4.816       | 1.1496        |
| 40    | 3.058       | .9601         | 175     | 4.873       | 1.1516        |
| 41    | 3.073       | .9653         | 180     | 4.929       | 1.1535        |
| 42    | 3.087       | .9702         | 185     | 1.985       | 1.1553        |
| 43    | 3.102       | .9750         | 190     | 5.040       | 1.1570        |
| 44    | 3.117       | .9795         | 195     | 5.094       | 1.1586        |
| 45    | 3.131       | .9840         | 200     | 5.148       | 1.1602        |
| 46    | 3.146       | .9882         | 205     | 5.202       | 1.1616        |
| 47    | 3.161       | .9923         | 210     | 5.255       | 1.1631        |
| 48    | 3.176       | .9963         | 215     | 5.307       | 1.1644        |
| 49 50 | 3.192 3.207 | 1.0001 1.0038 | 220 225 | 5.359 5.410 | 1.1657 1.1669 |

## Table A.2. ( Continued )

|   52 |   3.237 |   1.0108 |   230 |   5.461 |   1.1681 |
|------|---------|----------|-------|---------|----------|
|   54 |   3.268 |   1.0174 |   235 |   5.511 |   1.1693 |
|   56 |   3.298 |   1.0235 |   240 |   5.561 |   1.1704 |
|   58 |   3.329 |   1.0293 |   245 |   5.611 |   1.1714 |
|   60 |   3.359 |   1.0348 |   250 |   5.66  |   1.1724 |

Values of δ and 1 /λ are such that g ( √ b 1 ) = δ sinh -1 ( √ b 1 /λ) is approximately N (0, 1).

## Table A.3. Percentiles for b 2

Upper and lower percentiles for the sample coefficient of kurtosis. Reject the hypothesis of normality if b 2 is greater than an upper percentile or less than a lower percentile.

<!-- formula-not-decoded -->

| Sample size   |   Percentiles |   Percentiles |   Percentiles |   Percentiles |   Percentiles |   Percentiles |   Percentiles |   Percentiles |   Percentiles |   Percentiles |   Percentiles |   Percentiles |
|---------------|---------------|---------------|---------------|---------------|---------------|---------------|---------------|---------------|---------------|---------------|---------------|---------------|
| Sample size   |          1    |          2    |          2.5  |          5    |         10    |         20    |         80    |         90    |         95    |         97.5  |         98    |         99    |
| 7             |          1.25 |          1.3  |          1.34 |          1.41 |          1.53 |          1.7  |          2.78 |          3.2  |          3.55 |          3.85 |          3.93 |          4.23 |
| 8             |          1.31 |          1.37 |          1.4  |          1.46 |          1.58 |          1.75 |          2.84 |          3.31 |          3.7  |          4.09 |          4.2  |          4.53 |
| 9             |          1.35 |          1.42 |          1.45 |          1.53 |          1.63 |          1.8  |          2.98 |          3.43 |          3.86 |          4.28 |          4.41 |          4.82 |
| 10            |          1.39 |          1.45 |          1.49 |          1.56 |          1.68 |          1.85 |          3.01 |          3.53 |          3.95 |          4.4  |          4.55 |          5    |
| 12            |          1.46 |          1.52 |          1.56 |          1.64 |          1.76 |          1.93 |          3.06 |          3.55 |          4.05 |          4.56 |          4.73 |          5.2  |
| 15            |          1.55 |          1.61 |          1.64 |          1.72 |          1.84 |          2.01 |          3.13 |          3.62 |          4.13 |          4.66 |          4.85 |          5.3  |
| 20            |          1.65 |          1.71 |          1.74 |          1.82 |          1.95 |          2.13 |          3.21 |          3.68 |          4.17 |          4.68 |          4.87 |          5.36 |
| 25            |          1.72 |          1.79 |          1.83 |          1.91 |          2.03 |          2.2  |          3.23 |          3.68 |          4.16 |          4.65 |          4.82 |          5.3  |
| 30            |          1.79 |          1.86 |          1.9  |          1.98 |          2.1  |          2.26 |          3.25 |          3.68 |          4.11 |          4.59 |          4.75 |          5.21 |
| 35            |          1.84 |          1.91 |          1.95 |          2.03 |          2.14 |          2.31 |          3.27 |          3.68 |          4.1  |          4.53 |          4.68 |          5.13 |
| 40            |          1.89 |          1.96 |          1.98 |          2.07 |          2.19 |          2.34 |          3.28 |          3.67 |          4.06 |          4.46 |          4.61 |          5.04 |
| 45            |          1.93 |          2    |          2.03 |          2.11 |          2.22 |          2.37 |          3.28 |          3.65 |          4    |          4.39 |          4.52 |          4.94 |
| 50            |          1.95 |          2.03 |          2.06 |          2.15 |          2.25 |          2.41 |          3.28 |          3.62 |          3.99 |          4.33 |          4.45 |          4.88 |

Table A.4. Percentiles for D'Agostino's Test for Normality

Upper and lower percentiles for the statistic

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

where and the observations y 1 , y 2 , . . . , yn are ordered as y ( 1 ) ≤ y ( 2 ) ≤ · · · ≤ y ( n ) . Reject the hypothesis of normality if Y is greater than an upper percentile or less than a lower percentile.

|     | Percentiles of Y   | Percentiles of Y   | Percentiles of Y   | Percentiles of Y   | Percentiles of Y   |   Percentiles of Y |   Percentiles of Y |   Percentiles of Y |   Percentiles of Y |   Percentiles of Y |
|-----|--------------------|--------------------|--------------------|--------------------|--------------------|--------------------|--------------------|--------------------|--------------------|--------------------|
| n   | .5                 | 1.0                | 2.5                | 5                  | 10                 |             90     |             95     |             97.5   |             99     |             99.5   |
| 10  | - 4 . 66           | - 4 . 06           | - 3 . 25           | - 2 . 62           | - 1 . 99           |              0.149 |              0.235 |              0.299 |              0.356 |              0.385 |
| 12  | - 4 . 63           | - 4 . 02           | - 3 . 20           | - 2 . 58           | - 1 . 94           |              0.237 |              0.329 |              0.381 |              0.44  |              0.479 |
| 14  | - 4 . 57           | - 3 . 97           | - 3 . 16           | - 2 . 53           | - 1 . 90           |              0.308 |              0.399 |              0.46  |              0.515 |              0.555 |
| 16  | - 4 . 52           | - 3 . 92           | - 3 . 12           | - 2 . 50           | - 1 . 87           |              0.367 |              0.459 |              0.526 |              0.587 |              0.613 |
| 18  | - 4 . 47           | - 3 . 87           | - 3 . 08           | - 2 . 47           | - 1 . 85           |              0.417 |              0.515 |              0.574 |              0.636 |              0.667 |
| 20  | - 4 . 41           | - 3 . 83           | - 3 . 04           | - 2 . 44           | - 1 . 82           |              0.46  |              0.565 |              0.628 |              0.69  |              0.72  |
| 22  | - 4 . 36           | - 3 . 78           | - 3 . 01           | - 2 . 41           | - 1 . 81           |              0.497 |              0.609 |              0.677 |              0.744 |              0.775 |
| 24  | - 4 . 32           | - 3 . 75           | - 2 . 98           | - 2 . 39           | - 1 . 79           |              0.53  |              0.648 |              0.72  |              0.783 |              0.822 |
| 26  | - 4 . 27           | - 3 . 71           | - 2 . 96           | - 2 . 37           | - 1 . 77           |              0.559 |              0.682 |              0.76  |              0.827 |              0.867 |
| 28  | - 4 . 23           | - 3 . 68           | - 2 . 93           | - 2 . 35           | - 1 . 76           |              0.586 |              0.714 |              0.797 |              0.868 |              0.91  |
| 30  | - 4 . 19           | - 3 . 64           | - 2 . 91           | - 2 . 33           | - 1 . 75           |              0.61  |              0.743 |              0.83  |              0.906 |              0.941 |
| 32  | - 4 . 16           | - 3 . 61           | - 2 . 88           | - 2 . 32           | - 1 . 73           |              0.631 |              0.77  |              0.862 |              0.942 |              0.983 |
| 34  | - 4 . 12           | - 3 . 59           | - 2 . 86           | - 2 . 30           | - 1 . 72           |              0.651 |              0.794 |              0.891 |              0.975 |              1.02  |
| 36  | - 4 . 09           | - 3 . 56           | - 2 . 85           | - 2 . 29           | - 1 . 71           |              0.669 |              0.816 |              0.917 |              1     |              1.05  |
| 38  | - 4 . 06           | - 3 . 54           | - 2 . 83           | - 2 . 28           | - 1 . 70           |              0.686 |              0.837 |              0.941 |              1.03  |              1.08  |
| 40  | - 4 . 03           | - 3 . 51           | - 2 . 81           | - 2 . 26           | - 1 . 70           |              0.702 |              0.857 |              0.964 |              1.06  |              1.11  |
| 42  | - 4 . 00           | - 3 . 49           | - 2 . 80           | - 2 . 25           | - 1 . 69           |              0.716 |              0.875 |              0.986 |              1.09  |              1.14  |
| 44  | - 3 . 98           | - 3 . 47           | - 2 . 78           | - 2 . 24           | - 1 . 68           |              0.73  |              0.892 |              1.01  |              1.11  |              1.17  |
| 46  | - 3 . 95           | - 3 . 45           | - 2 . 77           | - 2 . 23           | - 1 . 67           |              0.742 |              0.908 |              1.02  |              1.13  |              1.19  |
| 48  | - 3 . 93           | - 3 . 43           | - 2 . 75           | - 2 . 22           | - 1 . 67           |              0.754 |              0.923 |              1.04  |              1.15  |              1.22  |
| 50  | - 3 . 91           | - 3 . 41           | - 2 . 74           | - 2 . 21           | - 1 . 66           |              0.765 |              0.937 |              1.06  |              1.18  |              1.24  |
| 60  | - 3 . 81           | - 3 . 34           | - 2 . 68           | - 2 . 17           | - 1 . 64           |              0.812 |              0.997 |              1.13  |              1.26  |              1.34  |
| 70  | - 3 . 73           | - 3 . 27           | - 2 . 64           | - 2 . 14           | - 1 . 61           |              0.849 |              1.05  |              1.19  |              1.33  |              1.42  |
| 80  | - 3 . 67           | - 3 . 22           | - 2 . 60           | - 2 . 11           | - 1 . 59           |              0.878 |              1.08  |              1.24  |              1.39  |              1.48  |
| 90  | - 3 . 61           | - 3 . 17           | - 2 . 57           | - 2 . 09           | - 1 . 58           |              0.902 |              1.12  |              1.28  |              1.44  |              1.54  |
| 100 | - 3 . 57           | - 3 . 14           | - 2 . 54           | - 2 . 07           | - 1 . 57           |              0.923 |              1.14  |              1.31  |              1.48  |              1.59  |
| 150 | - 3 . 409          | - 3 . 009          | - 2 . 452          | - 2 . 004          | - 1 . 520          |              0.99  |              1.233 |              1.423 |              1.623 |              1.746 |
| 200 | - 3 . 302          | - 2 . 922          | - 2 . 391          | - 1 . 960          | - 1 . 491          |              1.032 |              1.29  |              1.496 |              1.715 |              1.853 |
| 250 | - 3 . 227          | - 2 . 861          | - 2 . 348          | - 1 . 926          | - 1 . 471          |              1.06  |              1.328 |              1.545 |              1.779 |              1.927 |

Table A.5. Upper Percentiles for b 1 , p and Upper and Lower Percentiles for b 2 , p .

Reject the hypothesis of multivariate normality if b 1 , p is greater than table value. Reject if b 2 , p is greater than upper percentile or if b 2 , p is less than lower percentile. The statistics b 1 , p and b 2 , p are defined in Section 4.4.2.

(continued)

| p = 2 Upper and Lower Percentiles for b 2 , p Percentiles   | 99                                                                                                                           | 10.378 10.881 11.159 11.387 11.478 11.609 11.628 11.594 11.453 11.181 10.994 10.753 10.537 10.325 10.188 10.253 9.506 9.219 9.061 8.874   |
|-------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------|
|                                                             | 9.781                                                                                                                        | 97.5                                                                                                                                      |
|                                                             | 10.150 10.375 10.562 10.628 10.691 10.584 10.556 10.563 10.372 10.250 10.106 9.981 9.885 9.806 9.475 9.269 9.031 8.917 8.749 |                                                                                                                                           |
|                                                             | 95                                                                                                                           | 9.203 9.593 9.769 9.941 10.005 10.114 10.159 10.156 10.109 9.987 9.889 9.781 9.694 9.688 9.556 9.300 9.141 8.916 8.787 8.647              |
|                                                             |                                                                                                                              | 90                                                                                                                                        |
|                                                             | 8.606 8.947 9.162 9.331 9.403 9.469 9.503 9.516 9.497 9.453 9.401 9.356 9.309 9.256 9.210 9.027 8.919 8.776 8.664 8.547      | 5.057                                                                                                                                     |
|                                                             | 10                                                                                                                           | 5.232 5.358 5.482 5.555 5.717 5.871 6.038 6.229 6.403 6.505 6.602 6.683 6.749 6.793 6.972 7.083 7.245 7.342 7.464                         |
|                                                             |                                                                                                                              | 5                                                                                                                                         |
|                                                             | 4.887 5.053 5.179 5.318 5.382 5.533 5.689 5.855 6.139 6.239 6.335 6.437 6.539 6.622 6.665 6.858 6.979 7.142 7.252 7.369      |                                                                                                                                           |
|                                                             | 2.5                                                                                                                          | 4.722 4.899 5.015 5.149 5.219 5.262 5.525 5.692 5.871 6.083 6.189 6.290 6.372 6.475 6.521 6.749 6.889 7.052 7.171 7.295                   |
|                                                             |                                                                                                                              | 1                                                                                                                                         |
|                                                             | 4.580 4.732 4.842 4.977 5.045 5.175 5.351 5.518 5.703 5.909 6.015 6.139 6.223 6.332 6.389 6.615 6.761 6.949 7.079 7.232      |                                                                                                                                           |
|                                                             | n                                                                                                                            | 10 12 14 16 18 20 25 30 40 50 60 70 80 90 100 150 200 300 400 600                                                                         |
|                                                             |                                                                                                                              | 99.9                                                                                                                                      |
|                                                             | 6.994 6.744 6.419 6.062 5.737 5.425 4.719 4.238 3.369 2.706 2.200 1.863 1.587 1.400 1.231 .794 .569 .369 .275 .183           |                                                                                                                                           |
|                                                             | 99                                                                                                                           | 5.194 4.938 4.581 4.231 3.962 3.669 3.106 2.681 2.087 1.744 1.444 1.244 1.056 .919 .831 .531 .394 .256 .197 .131                          |

Table A.5. ( Continued )

554

| p = 2 Upper and Lower Percentiles for b 2 ,   | 99             | 8.747 8.656 8.532 8.461 8.412 8.376 8.326 8.291   |                                     | 99   | 15.6 16.4 17.1 17.5 17.8 18.0 18.2 18.3 18.2 18.0   |
|-----------------------------------------------|----------------|---------------------------------------------------|-------------------------------------|------|-----------------------------------------------------|
| p = 2 Upper and Lower Percentiles for b 2 ,   | 97.5           | 8.641 8.569 8.463 8.401 8.359 8.327 8.284 8.254   |                                     | 97.5 | 15.0 15.9 16.5 16.8 17.1 17.2 17.4 17.5 17.4 17.3   |
| p = 2 Upper and Lower Percentiles for b 2 ,   | 95             | 8.562 8.497 8.405 8.351 8.314 8.286 8.248 8.222   |                                     | 95   | 14.4 15.2 15.8 16.1 16.4 16.5 16.7 16.7 16.7 16.7   |
| p = 2 Upper and Lower Percentiles for b 2 ,   | 90             | 8.472 8.419 8.339 8.293 8.262 8.240 8.207 8.186   |                                     | 90   | 14.0 14.7 15.1 15.4 15.5 15.7 15.9 16.0 16.1 16.1   |
| p = 2 Upper and Lower Percentiles for b 2 ,   | Percentiles 10 | 7.536 7.585 7.661 7.707 7.738 7.760 7.793 7.714   |                                     | 10   | 10.7 11.0 11.3 11.5 11.6 11.8 12.1 12.3 12.7 12.9   |
| p = 2 Upper and Lower Percentiles for b 2 ,   | 5              | 7.451 7.504 7.595 7.649 7.686 7.714 7.752 7.778   |                                     | 5    | 10.4 10.7 10.9 11.1 11.3 11.4 11.8 12.0 12.4 12.6   |
| p = 2 Upper and Lower Percentiles for b 2 ,   | 2.5            | 7.372 7.433 7.537 7.599 7.641 7.673 7.716 7.746   |                                     | 2.5  | 10.2 10.4 10.6 10.8 11.0 11.1 11.4 11.6 12.0 12.3   |
| p = 2 Upper and Lower Percentiles for b 2 ,   | 1              | 7.304 7.367 7.460 7.535 7.588 7.624 7.674 7.709   |                                     | 1    | 10.0 10.2 10.4 10.5 10.7 10.8 11.1 11.3 11.7 11.9   |
| p = 2 Upper and Lower Percentiles for b 2 ,   | n              | 800 1000 1500 2000 2500 3000 4000 5000            |                                     | n    | 10 12 14 16 18 20 25 30 40 50                       |
|                                               | 99.9           | .137 .110 .074 .044 .037 .028 .022                | = Upper Percentiles for Percentiles | 99.9 | 11.5 10.5 9.7 8.9 8.3 7.7 6.5 5.6 4.2 3.4           |
|                                               | 99             | .099 .079 .053 .032 .027 .020 .016                | = Upper Percentiles for Percentiles | 99   | 8.8 8.1 7.4 6.8 6.4 6.0 5.2 4.4 3.5 2.8             |
|                                               | 97.5           | .083 .066 .044 .027 .022 .017 .013                | = Upper Percentiles for Percentiles | 97.5 | 7.7 7.1 6.5 6.1 5.6 5.3 4.5 3.9 3.0 2.4             |
|                                               | Percentiles 95 | .071 .057 .038 .023 .019 .014 .011                | = Upper Percentiles for Percentiles | 95   | 6.9 6.4 5.9 5.4 5.1 4.7 3.9 3.3 2.7 2.2             |
|                                               | 92.5           | .064 .051 .034 .021 .017 .013 .010                | = Upper Percentiles for Percentiles | 92.5 | 6.5 5.9 5.4 4.9 4.6 4.2 3.5 3.0 2.4 1.9             |
|                                               | 90             | .058 .046 .031 .019 .016 .012 .009                | = Upper Percentiles for Percentiles | 90   | 6.0 5.5 5.0 4.6 4.2 3.9 3.3 2.8 2.2 1.7             |
|                                               | n              | 800 1000 1500 2500 3000 4000 5000                 | = Upper Percentiles for Percentiles | n    | 10 12 14 16 18 20 25 30 40 50                       |

Table A.5. ( Continued )

| p = 3 Upper and Lower Percentiles for b 2 , p   | 17.9 17.7 17.6 17.5 17.4 17.0 16.8 16.5 16.3 15.97 15.85 15.77 15.63 15.55 15.45 15.39 15.35   |               |             | 99 24.0   | 25.4 (continued)   |
|-------------------------------------------------|------------------------------------------------------------------------------------------------|---------------|-------------|-----------|--------------------|
| p = 3 Upper and Lower Percentiles for b 2 , p   | 17.2 17.1 17.0 16.9 16.8 16.5 16.3 16.1 16.0 15.81 15.71 15.64 15.53 15.46 15.38 15.33 15.30   |               |             | 97.5      | 23.0 24.2          |
| p = 3 Upper and Lower Percentiles for b 2 , p   | 16.6 16.6 16.5 16.5 16.4 16.2 16.1 15.9 15.8 15.67 15.59 15.53 15.44 15.39 15.32 15.28 15.25   | for b 2 , p   |             | 95        | 22.4 23.3          |
| p = 3 Upper and Lower Percentiles for b 2 , p   | 16.1 16.1 16.1 16.0 16.0 15.9 15.8 15.7 15.6 15.51 15.45 15.41 15.34 15.30 15.25 15.21 15.19   | Percentiles   |             | 90        | 21.5 22.3          |
| p = 3 Upper and Lower Percentiles for b 2 , p   | 13.1 13.2 13.3 13.5 13.5 13.8 14.0 14.2 14.3 14.4 14.5 14.53 14.62 14.67 14.73 14.77 14.80     | p = 4 Lower   | Percentiles | 10        | 17.8 18.3          |
| p = 3 Upper and Lower Percentiles for b 2 , p   | 12.8 13.0 13.1 13.2 13.3 13.6 13.8 14.0 14.1 14.3 14.3 14.41 14.52 14.58 14.66 14.71 14.74     | and           |             | 5         | 17.6 18.0          |
| p = 3 Upper and Lower Percentiles for b 2 , p   | 12.5 12.6 12.8 12.9 13.0 13.3 13.5 13.8 13.9 14.1 14.2 14.30 14.43 14.51 14.60 14.65 14.69     | Upper         |             | 2.5       | 17.3 17.7          |
| p = 3 Upper and Lower Percentiles for b 2 , p   | 12.1 12.3 12.4 12.5 12.6 13.0 13.2 13.6 13.7 13.9 14.1 14.17 14.33 14.42 14.53 14.59 14.63     |               |             | 1         | 17.0 17.4          |
| p = 3 Upper and Lower Percentiles for b 2 , p   | 60 70 80 90 100 150 200 300 400 600 800 1000 1500 2000 3000 4000 5000                          |               |             | n         | 10 12              |
|                                                 | 2.9 2.5 2.2 1.9 1.7 1.15 .87 .58 .44 .294 .221 .177 .118 .089 .059 .044 .035                   | 4 Percentiles | Percentiles | 99.9      | 17.9 16.2          |
|                                                 | 2.4 2.0 1.7 1.5 1.3 .90 .68 .46 .34 .230 .173 .139 .093 .069 .046 .035 .028                    | b 1 , p       |             | 99        | 15.3 13.9          |
|                                                 | 2.0 1.7 1.5 1.3 1.18 .80 .60 .40 .30 .203 .153 .122 .082 .061 .041 .031 .025                   | for           |             | 97.5      | 13.3 12.2          |
|                                                 | 1.8 1.5 1.3 1.16 1.05 .71 .54 .36 .272 .182 .137 .109 .073 .055 .037 .027 .022                 | p =           |             | 95        | 12.2 11.2          |
|                                                 | 1.6 1.4 1.2 1.08 .97 .66 .50 .33 .252 .168 .127 .010 .068 .051 .034 .025 .020                  | Upper         |             | 92.5      | 11.6 10.6          |
|                                                 | 1.5 1.3 1.13 1.01 .92 .62 .47 .32 .237 .159 .119 .095 .064 .048 .032 .024 .019                 |               | 90          | 11.1      | 10.1               |
|                                                 | 60 70 80 90 100 150 200 300 400 600 800 1000 1500 2000 3000 4000 5000                          |               | n           | 10        | 12                 |

Table A.5. ( Continued )

<!-- image -->

| p = 4 Upper and Lower Percentiles for b 2 , p   | 99 26.1 26.6 26.9 27.1 27.3   | 27.4 27.4 27.3 27.2 27.0 26.9 26.8 26.7 26.3 26.0 25.7 25.46 25.21 25.06 24.96 24.79 24.69 24.57 24.50 24.45                                  |
|-------------------------------------------------|-------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------|
| p = 4 Upper and Lower Percentiles for b 2 , p   | 97.5                          | 25.0 25.4 25.8 26.1 26.4 26.6 26.7 26.6 26.6 26.5 26.4 26.3 26.2 25.9 25.6 25.3 25.20 25.01 24.89 24.80 24.66 24.58 24.48 24.42 24.37         |
| p = 4 Upper and Lower Percentiles for b 2 , p   | 95                            | 24.0 24.4 24.7 25.0 25.4 25.5 25.7 25.7 25.7 25.7 25.6 25.6 25.6 25.42 25.29 25.11 24.99 24.83 24.74 24.67 24.55 24.48 24.40 24.35 24.31      |
| p = 4 Upper and Lower Percentiles for b 2 , p   | 90                            | 23.0 23.4 23.8 24.0 24.5 24.7 25.0 25.1 25.14 25.15 25.15 25.14 25.12 25.03 24.95 24.83 24.75 24.63 24.56 24.51 24.42 24.37 24.31 24.27 24.24 |
| p = 4 Upper and Lower Percentiles for b 2 , p   | 10                            | 18.6 18.9 19.2 19.4 19.8 20.2 21.0 21.0 21.3 21.5 21.7 21.8 21.9 22.33 22.57 22.85 23.02 23.21 23.32 23.40 23.51 23.58 23.66 23.71 23.74      |
| p = 4 Upper and Lower Percentiles for b 2 , p   | 5                             | 18.3 18.6 18.8 19.0 19.5 19.8 20.3 20.6 20.9 21.0 21.2 21.4 21.5 22.0 22.2 22.57 22.77 23.01 23.15 23.24 23.38 23.47 23.57 23.63 23.67        |
| p = 4 Upper and Lower Percentiles for b 2 , p   | 2.5                           | 18.0 18.2 18.4 18.6 19.1 19.4 19.9 20.3 20.5 20.7 21.0 21.1 21.2 21.7 22.0 22.33 22.56 22.83 22.99 23.10 23.27 23.37 23.49 23.56 23.61        |
| p = 4 Upper and Lower Percentiles for b 2 , p   | 1                             | 17.7 18.0 18.2 18.4 18.8 19.1 19.6 20.0 20.2 20.4 20.6 20.8 20.9 21.4 21.7 22.1 22.3 22.63 22.82 22.94 23.14 23.26 23.40 23.48 23.54          |
| p = 4 Upper and Lower Percentiles for b 2 , p   | n                             | 14 16 18 20 25 30 40 50 60 70 80 90 100 150 200 300 400 600 800 1000 1500 2000 3000 4000 5000                                                 |
|                                                 | 99.9                          | 14.8 13.6 12.6 11.6 9.7 8.1 6.2 5.0 4.2 3.7 3.2 2.9 2.6 1.76 1.33 .89 .67 .45 .34 .271 .181 .136 .091 .068 .054                               |
|                                                 | 99                            | 12.7 11.6 10.7 9.9 8.1 6.8 5.2 4.2 3.5 3.0 2.7 2.4 2.2 1.46 1.10 .74 .56 .37 .280 .224 .150 .112 .075 .056 .045                               |
|                                                 | 97.5                          | 11.2 10.3 9.5 8.8 7.1 6.0 4.6 3.8 3.2 2.8 2.4 2.2 1.97 1.33 1.00 .67 .51 .34 .255 .204 .136 .102 .068 .051 .041                               |
|                                                 | 95                            | 10.2 9.4 8.7 8.0 6.6 5.6 4.3 3.5 2.9 2.5 2.2 2.0 1.81 1.22 .92 .62 .47 .31 .234 .188 .125 .094 .063 .047 .038                                 |
|                                                 | 92.5                          | 9.7 8.8 8.0 7.4 6.2 5.3 4.1 3.3 2.8 2.4 2.1 1.89 1.71 1.16 .87 .59 .44 .295 .222 .177 .118 .089 .059 .045 .039                                |
|                                                 | 90                            | 9.2 8.4 7.7 7.0 5.9 5.0 3.9 3.1 2.7 2.3 2.0 1.81 1.64 1.11 .84 .56 .42 .282 .212 .170 .113 .085 .057 .043 .034                                |
|                                                 | n                             | 14 16 18 20 25 30 40 50 60 70 80 90 100 150 200 300 400 600 800 1000 1500 2000 3000 4000 5000                                                 |

## Table A.6. Upper Percentiles for Test of Single Multivariate Normal Outlier

Upper percentage points for the test statistic

<!-- formula-not-decoded -->

This tests for a single outlier in a sample of size n from a multivariate normal distribution. Reject and conclude that the outlier is significant if D 2 ( n ) exceeds the table value.

|     | p = 2    | p = 2    | p = 3    | p = 3    | p = 4    | p = 4    | p = 5    | p = 5    |
|-----|----------|----------|----------|----------|----------|----------|----------|----------|
| n   | α = . 05 | α = . 01 | α = . 05 | α = . 01 | α = . 05 | α = . 01 | α = . 05 | α = . 01 |
| 5   | 3.17     | 3.19     |          |          |          |          |          |          |
| 6   | 4.00     | 4.11     | 4.14     | 4.16     |          |          |          |          |
| 7   | 4.71     | 4.95     | 5.01     | 5.10     | 5.12     | 5.14     |          |          |
| 8   | 5.32     | 5.70     | 5.77     | 5.97     | 6.01     | 6.09     | 6.11     | 6.12     |
| 9   | 5.85     | 6.37     | 6.43     | 6.76     | 6.80     | 6.97     | 7.01     | 7.08     |
| 10  | 6.32     | 6.97     | 7.01     | 7.47     | 7.50     | 7.79     | 7.82     | 7.98     |
| 12  | 7.10     | 8.00     | 7.99     | 8.70     | 8.67     | 9.20     | 9.19     | 9.57     |
| 14  | 7.74     | 8.84     | 8.78     | 9.71     | 9.61     | 10.37    | 10.29    | 10.90    |
| 16  | 8.27     | 9.54     | 9.44     | 10.56    | 10.39    | 11.36    | 11.20    | 12.02    |
| 18  | 8.73     | 10.15    | 10.00    | 11.28    | 11.06    | 12.20    | 11.96    | 12.98    |
| 20  | 9.13     | 10.67    | 10.49    | 11.91    | 11.63    | 12.93    | 12.62    | 13.81    |
| 25  | 9.94     | 11.73    | 11.48    | 13.18    | 12.78    | 14.40    | 13.94    | 15.47    |
| 30  | 10.58    | 12.54    | 12.24    | 14.14    | 13.67    | 15.51    | 14.95    | 16.73    |
| 35  | 11.10    | 13.20    | 12.85    | 14.92    | 14.37    | 16.40    | 15.75    | 17.73    |
| 40  | 11.53    | 13.74    | 13.36    | 15.56    | 14.96    | 17.13    | 16.41    | 18.55    |
| 45  | 11.90    | 14.20    | 13.80    | 16.10    | 15.46    | 17.74    | 16.97    | 19.24    |
| 50  | 12.23    | 14.60    | 14.18    | 16.56    | 15.89    | 18.27    | 17.45    | 19.83    |
| 100 | 14.22    | 16.95    | 16.45    | 19.26    | 18.43    | 21.30    | 20.26    | 23.17    |
| 200 | 15.99    | 18.94    | 18.42    | 21.47    | 20.59    | 23.72    | 22.59    | 25.82    |
| 500 | 18.12    | 21.22    | 20.75    | 23.95    | 23.06    | 26.37    | 25.21    | 28.62    |

Table A.7. Upper Percentage Points of Hotelling's T 2 Distribution

| p = 10                | 1066.774 351.421 193.842 132.582 101.499 83.121 71.127 62.746 56.587 51.884 48.184 45.202 42.750 40.699 38.961 37.469                                                                                                                                                                 |
|-----------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| p = 9                 | 872.317 290.806 161.967 111.676 86.079 70.907 60.986 54.041 48.930 45.023 41.946 39.463 37.419 35.709 34.258 33.013 31.932                                                                                                                                                            |
| p = 8                 | 697.356 235.873 132.903 92.512 71.878 59.612 51.572 45.932 41.775 38.592 36.082 34.054 32.384 30.985 29.798 28.777 27.891 27.114                                                                                                                                                      |
| p = 6 p = 7           | 405.920 143.050 541.890 83.202 186.622 59.403 106.649 47.123 75.088 39.764 58.893 34.911 49.232 31.488 42.881 28.955 38.415 27.008 35.117 25.467 32.588 24.219 30.590 23.189 28.975 22.324 27.642 21.588 26.525 20.954 25.576 20.403 24.759 19.920 24.049 19.492 23.427 19.112 22.878 |
| p = 5                 | α = . 05 289.446 105.157 62.561 45.453 36.561 31.205 27.656 25.145 23.281 21.845 20.706 19.782 19.017 18.375 17.828 17.356 16.945 16.585 16.265 15.981 15.726                                                                                                                         |
| p = 4                 | 192.468 72.937 44.718 33.230 27.202 23.545 21.108 19.376 18.086 17.089 16.296 15.651 15.117 14.667 14.283 13.952 13.663 13.409 13.184 12.983 12.803 12.641                                                                                                                            |
| p = 3                 | 114.986 46.383 29.661 22.720 19.028 16.766 15.248 14.163 13.350 12.719 12.216 11.806 11.465 11.177 10.931 10.719 10.533 10.370 10.225 10.095 9.979 9.874 9.779                                                                                                                        |
| p = 2                 | 57.000 25.472 17.361 13.887 12.001 10.828 10.033 9.459 9.026 8.689 8.418 8.197 8.012 7.856 7.722 7.606 7.504 7.415 7.335 7.264 7.200 7.142 7.089 7.041                                                                                                                                |
| p = 1                 | 18.513 10.128 7.709 6.608 5.987 5.591 5.318 5.117 4.965 4.844 4.747 4.667 4.600 4.543 4.494 4.451 4.414 4.381 4.351 4.325 4.301 4.279 4.260 4.242 4.225                                                                                                                               |
| Degrees of Freedom, ν | 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26                                                                                                                                                                                                                    |

05

.

=

α

| 36.176   | 35.043 27.783 26.326 25.256 24.437 23.790 22.834                                                         | 34.044 33.156 29.881 22.162 21.663 21.279 20.973 20.725 20.196 19.692 18.976 18.570 18.307   |
|----------|----------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|
| 30.985   | 30.149 29.407 28.742 26.252 24.624 23.477 22.627 21.972 21.451 20.676                                    | 20.127 19.718 19.401 19.149 18.943 18.504 18.083 17.484 17.141 16.919                        |
| 26.428   | 25.818 25.272 24.781 22.913 21.668 20.781 20.117 19.600 19.188 18.571 16.814 16.469                      | 18.130 17.801 17.544 17.340 17.172 15.975 15.692 15.507                                      |
| 22.388   | 21.950 21.555 21.198 19.823 18.890 18.217 17.709 17.311 16.992 16.510 15.121 14.845                      | 16.165 15.905 15.702 15.540 15.407 14.447 14.217 14.067                                      |
| 18.770   | 18.463 18.184 17.931 16.944 16.264 15.767 15.388 15.090 14.850 14.485 14.022 13.867 13.741 13.639        | 14.222 13.417 13.202 12.890 12.710 12.592                                                    |
| 15.496   | 15.287 15.097 14.924 14.240 13.762 13.409 13.138 12.923 12.748 12.482 12.289 12.142 12.027 11.934        | 11.858 11.693 11.531 11.297 11.160 11.070                                                    |
| 12.493   | 12.359 12.236 12.123 11.674 11.356 11.118 10.934 10.787 10.668 10.484 10.350 10.248 10.167 10.102 10.048 | 9.931 9.817 9.650 9.552 9.488                                                                |
| 9.692    | 9.612 9.539 9.471 9.200 9.005 8.859 8.744 8.652 8.577 8.460 8.181 8.105                                  | 8.375 8.309 8.257 8.215 8.031 7.922 7.857 7.815                                              |
| 6.997    | 6.957 6.885 6.744 6.642 6.564 6.503 6.454 6.413 6.350                                                    | 6.303 6.267 6.239 6.216 6.196 6.155 6.113 6.052 6.015 5.991                                  |
| 4.210    | 6.919                                                                                                    | 3.851                                                                                        |
|          | 4.196 4.183 4.171 4.121 4.085 4.057 4.034 4.016 4.001 3.978 3.947 3.936 3.927                            | 3.960 3.920 3.904 3.888 3.865 3.841                                                          |
| 27       | 28 29 30 35 40 45 50 55 60 70 150 200 400                                                                | 80 90 100 110 120 1000 ∞                                                                     |

(continued)

Table A.7. ( Continued )

560

Degrees of

10

=

p

9

=

p

8

=

p

7

=

p

6

=

p

5

=

p

4

=

p

3

=

p

2

=

p

1

=

p

ν

Freedom,

01

.

α

= 2 98.503 3 34.116 297.000 4 21.198 82.177 594.997 5 16.258 45.000 147.283 992.494 6 13.745 31.857 75.125 229.679 1489.489 7 12.246 25.491 50.652 111.839 329.433 2085.984 8 11.259 21.821 39.118 72.908 155.219 446.571 2781.978 9 10.561 19.460 32.598 54.890 98.703 205.293 581.106 3577.472 10 10.044 17.826 28.466 44.838 72.882 128.067 262.076 733.045 4472.464 11 9.646 16.631 25.637 38.533 58.618 93.127 161.015 325.576 902.392 5466.956 12 9.330 15.722 23.588 34.251 49.739 73.969 115.640 197.555 395.797 1089.149 13 9.074 15.008 22.041 31.171 43.745 62.114 90.907 140.429 237.692 472.742 14 8.862 14.433 20.834 28.857 39.454 54.150 75.676 109.441 167.499 281.428 15 8.683 13.960 19.867 27.060 36.246 48.472 65.483 90.433 129.576 196.853 16 8.531 13.566 19.076 25.626 33.672 44.240 58.241 77.755 106.391 151.316 17 8.400 13.231 18.418 24.458 31.788 40.975 52.858 68.771 90.969 123.554 18 8.285 12.943 17.861 23.487 30.182 38.385 48.715 62.109 80.067 105.131 19 8.185 12.694 17.385 22.670 28.852 36.283 45.435 56.992 71.999 92.134 20 8.096 12.476 16.973 21.972 27.734 34.546 42.779 52.948 65.813 82.532 21 8.017 12.283 16.613 21.369 26.781 33.088 40.587 49.679 60.932 75.181 22 7.945 12.111 16.296 20.843 25.959 31.847 38.750 46.986 56.991 69.389 23 7.881 11.958 16.015 20.381 25.244 30.779 37.188 44.730 53.748 64.719 24 7.823 11.820 15.763 19.972 24.616 29.850 35.846 42.816 51.036 60.879 25 7.770 11.695 15.538 19.606 24.060 29.036 34.680 41.171 48.736 57.671 26 7.721 11.581 15.334 19.279 23.565 28.316 33.659 39.745 46.762 54.953 27 7.677 11.478 15.149 18.983 23.121 27.675 32.756 38.496 45.051 52.622

01

.

=

α

∞

| 50.604 48.839 47.283   | 41.651        |        |   38.135 35.737 |   33.998 |   32.682 |        | 31.650        | 30.139        |   29.085 28.310 |   27.714 | 27.243        |   26.862 | 26.054        |   25.287 |   24.209 23.600 | 23.209   |
|------------------------|---------------|--------|-----------------|----------|----------|--------|---------------|---------------|-----------------|----------|---------------|----------|---------------|----------|-----------------|----------|
| 43.554 42.234          | 41.062 36.743 | 33.984 |          32.073 |   30.673 |   29.603 | 28.760 |               | 27.515 26.642 |          25.995 |   25.496 | 25.101        |   24.779 | 24.096 23.446 |   22.525 |          22.003 | 21.666   |
| 37.393 36.414          | 35.538 32.259 | 30.12  |          28.617 |   27.504 |   26.647 |        | 25.967        | 24.957 24.242 |          23.71  |   23.299 | 22.972        |   22.705 | 22.137 21.592 |   20.818 |          20.376 | 20.090   |
| 31.954 31.236          | 30.589 28.135 | 26.502 |          25.34  |   24.47  |   23.795 |        | 23.257 22.451 | 21.877        |          21.448 |   21.115 | 20.849 20.632 |   20.167 | 19.720        |   19.08  |          18.743 | 18.475   |
| 27.101 26.584          | 26.116 24.314 | 23.094 |          22.214 |   21.55  |   21.03  |        | 20.613        | 19.986 19.536 |          19.197 |   18.934 | 18.722 18.549 |   18.178 | 17.819        |   17.303 |          17.006 | 16.812   |
| 22.721 22.359          | 22.029 20.743 | 19.858 |          19.211 |   18.718 |   18.331 |        | 18.018        | 17.543 17.201 |          16.942 |   16.74  | 16.577 16.444 |   16.156 | 15.877        |   15.473 |          15.239 | 15.086   |
| 18.715 18.471          | 18.247 17.366 | 16.75  |          16.295 |   15.945 |   15.667 | 15.442 | 15.098        | 14.849        |          14.66  |   14.511 | 14.391 14.292 |   14.079 | 13.871        |   13.569 |          13.392 | 13.277   |
| 14.980 14.825          | 14.683 14.117 | 13.715 |          13.414 |   13.181 |   12.995 | 12.843 | 12.611        | 12.440        |          12.31  |   12.208 | 12.125 12.057 |   11.909 | 11.764        |   11.551 |          11.426 | 11.345   |
| 11.383 11.295          | 11.215 10.890 | 10.655 |          10.478 |   10.34  |   10.228 | 10.137 | 9.996         | 9.892         |           9.813 |    9.75  | 9.699 9.657   |    9.565 | 9.474         |    9.341 |           9.262 | 9.210    |
| 7.636 7.598            | 7.562 7.419   |  7.314 |           7.234 |    7.171 |    7.119 | 7.077  | 7.011         | 6.963         |           6.925 |    6.895 | 6.871 6.851   |    6.807 | 6.763         |    6.699 |           6.66  | 6.635    |
| 28 29                  | 30 35         | 40     |          45     |   50     |   55     |        | 60 70         | 80            |          90     |  100     | 110 120       |  150     | 200           |  400     |        1000     |          |

number of variables.

=

p

Note:

Table A.8. Bonferonni t -Values, t α/ 2 k ,ν , α = . 05

|    | k          | k          | k             | k             | k                    | k           | k          | k          | k          |
|----|------------|------------|---------------|---------------|----------------------|-------------|------------|------------|------------|
|    | 1          | 2          | 3             | 4 5           | 6                    |             | 8          | 9          | 10         |
|    | 7 100 α/ k | 7 100 α/ k | 7 100 α/ k    | 7 100 α/ k    | 7 100 α/ k           | 7 100 α/ k  | 7 100 α/ k | 7 100 α/ k | 7 100 α/ k |
| ν  | 5.0000     | 2.5000     | 1.6667        | 1.2500        | 1.0000               | .8333 .7143 | .6250      | .5556      | .5000      |
| 2  | 4.3027     | 6.2053     | 7.6488 8.8602 | 9.9248        | 10.8859              | 11.7687     | 12.5897    | 13.3604    | 14.0890    |
| 3  | 3.1824     | 4.1765     | 4.8567        | 5.3919        | 5.8409               | 6.5797      | 6.8952     | 7.1849     | 7.4533     |
| 4  | 2.7764     | 3.4954     | 3.9608        | 4.3147 4.6041 | 6.2315 4.8510        | 5.0675      | 5.2611     | 5.4366     | 5.5976     |
| 5  | 2.5706     | 3.1634     | 3.5341        | 3.8100        | 4.2193               | 4.3818      | 4.5257     | 4.6553     | 4.7733     |
| 6  | 2.4469     | 2.9687     | 3.2875        | 3.5212        | 4.0321 3.7074 2.8630 | 3.9971      | 4.1152     | 4.2209     | 4.3168     |
| 7  | 2.3646     | 2.8412     | 3.1276        | 3.3353        | 3.4995 3.6358        | 3.7527      | 3.8552     | 3.9467     | 4.0293     |
| 8  | 2.3060     | 2.7515     | 3.0158        | 3.2060 3.3554 | 3.4789               | 3.5844      | 3.6766     | 3.7586     | 3.8325     |
| 9  | 2.2622     | 2.6850     | 2.9333        | 3.1109        | 3.2498 3.3642        | 3.4616      | 3.5465     | 3.6219     | 3.6897     |
| 10 | 2.2281     | 2.6338     | 2.8701        | 3.0382 3.1693 | 3.2768               | 3.3682      | 3.4477     | 3.5182     | 3.5814     |
| 11 | 2.2010     | 2.5931     | 2.8200        | 2.9809        | 3.2081               | 3.2949      | 3.3702     | 3.4368     | 3.4966     |
| 12 | 2.1788     | 2.5600     | 2.7795        | 3.1058 2.9345 | 3.0545 3.1527        | 3.2357      | 3.3078     | 3.3714     | 3.4284     |
| 13 | 2.1604     | 2.5326     | 2.7459        | 2.8961 3.0123 | 3.1070               | 3.1871      | 3.2565     | 3.3177     | 3.3725     |
| 14 | 2.1448     | 2.5096     | 2.7178        | 2.8640 2.9768 | 3.0688               | 3.1464      | 3.2135     | 3.2727     | 3.3257     |
| 15 | 2.1314     | 2.4899     | 2.6937        | 2.8366 2.9467 | 3.0363               | 3.1118      | 3.1771     | 3.2346     | 3.2860     |
| 16 | 2.1199     | 2.4729     | 2.6730        | 2.8131 2.9208 | 3.0083               | 3.0821      | 3.1458     | 3.2019     | 3.2520     |
| 17 | 2.1098     | 2.4581     | 2.6550        | 2.7925 2.8982 | 2.9840               | 3.0563      | 3.1186     | 3.1735     | 3.2224     |
| 18 | 2.1009     | 2.4450     | 2.6391        | 2.7745 2.8784 | 2.9627               | 3.0336      | 3.0948     | 3.1486     | 3.1966     |
| 19 | 2.0930     | 2.4334     | 2.6251        | 2.7586 2.8609 | 2.9439               | 3.0136      | 3.0738     | 3.1266     | 3.1737     |
| 20 | 2.0860     | 2.4231     | 2.6126        | 2.7444 2.8453 | 2.9271               | 2.9958      | 3.0550     | 3.1070     | 3.1534     |
| 21 | 2.0796     | 2.4138     | 2.6013        | 2.7316 2.8314 | 2.9121               | 2.9799      | 3.0382     | 3.0895     | 3.1352     |
| 22 | 2.0739     | 2.4055     | 2.5912        | 2.7201 2.8188 | 2.8985               | 2.9655      | 3.0231     | 3.0737     | 3.1188     |
| 23 | 2.0687     | 2.3979     | 2.5820        | 2.7097 2.8073 | 2.8863               | 2.9525      | 3.0095     | 3.0595     | 3.1040     |
| 24 | 2.0639     | 2.3909     | 2.5736        | 2.7002 2.7969 | 2.8751               | 2.9406      | 2.9970     | 3.0465     | 3.0905     |
| 25 | 2.0595     | 2.3846     | 2.5660        | 2.6916 2.7874 | 2.8649               | 2.9298      | 2.9856     | 3.0346     | 3.0782     |
| 26 | 2.0555     | 2.3788     | 2.5589        | 2.6836        | 2.7787 2.8555        | 2.9199      | 2.9752     | 3.0237     | 3.0669     |

TABLES Table A.8. ( Continued )

|      | k        | k        | k        | k        | k        | k        | k        | k        | k        | k        |
|------|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------|
|      | 1        | 2        | 3        | 4        | 5        | 6        | 7        | 8        | 9        | 10       |
|      | 100 α/ k | 100 α/ k | 100 α/ k | 100 α/ k | 100 α/ k | 100 α/ k | 100 α/ k | 100 α/ k | 100 α/ k | 100 α/ k |
| ν    | 5.0000   | 2.5000   | 1.6667   | 1.2500   | 1.0000   | .8333    | .7143    | .6250    | .5556    | .5000    |
| 27   | 2.0518   | 2.3734   | 2.5525   | 2.6763   | 2.7707   | 2.8469   | 2.9107   | 2.9656   | 3.0137   | 3.0565   |
| 28   | 2.0484   | 2.3685   | 2.5465   | 2.6695   | 2.7633   | 2.8389   | 2.9023   | 2.9567   | 3.0045   | 3.0469   |
| 29   | 2.0452   | 2.3638   | 2.5409   | 2.6632   | 2.7564   | 2.8316   | 2.8945   | 2.9485   | 2.9959   | 3.0380   |
| 30   | 2.0423   | 2.3596   | 2.5357   | 2.6574   | 2.7500   | 2.8247   | 2.8872   | 2.9409   | 2.9880   | 3.0298   |
| 35   | 2.0301   | 2.3420   | 2.5145   | 2.6334   | 2.7238   | 2.7966   | 2.8575   | 2.9097   | 2.9554   | 2.9960   |
| 40   | 2.0211   | 2.3289   | 2.4989   | 2.6157   | 2.7045   | 2.7759   | 2.8355   | 2.8867   | 2.9314   | 2.9712   |
| 45   | 2.0141   | 2.3189   | 2.4868   | 2.6021   | 2.6896   | 2.7599   | 2.8187   | 2.8690   | 2.9130   | 2.9521   |
| 50   | 2.0086   | 2.3109   | 2.4772   | 2.5913   | 2.6778   | 2.7473   | 2.8053   | 2.8550   | 2.8984   | 2.9370   |
| 55   | 2.0040   | 2.3044   | 2.4694   | 2.5825   | 2.6682   | 2.7370   | 2.7944   | 2.8436   | 2.8866   | 2.9247   |
| 60   | 2.0003   | 2.2990   | 2.4630   | 2.5752   | 2.6603   | 2.7286   | 2.7855   | 2.8342   | 2.8768   | 2.9146   |
| 70   | 1.9944   | 2.2906   | 2.4529   | 2.5639   | 2.6479   | 2.7153   | 2.7715   | 2.8195   | 2.8615   | 2.8987   |
| 80   | 1.9901   | 2.2844   | 2.4454   | 2.5554   | 2.6387   | 2.7054   | 2.7610   | 2.8086   | 2.8502   | 2.8870   |
| 90   | 1.9867   | 2.2795   | 2.4395   | 2.5489   | 2.6316   | 2.6978   | 2.7530   | 2.8002   | 2.8414   | 2.8779   |
| 100  | 1.9840   | 2.2757   | 2.4349   | 2.5437   | 2.6259   | 2.6918   | 2.7466   | 2.7935   | 2.8344   | 2.8707   |
| 110  | 1.9818   | 2.2725   | 2.4311   | 2.5394   | 2.6213   | 2.6868   | 2.7414   | 2.7880   | 2.8287   | 2.8648   |
| 120  | 1.9799   | 2.2699   | 2.4280   | 2.5359   | 2.6174   | 2.6827   | 2.7370   | 2.7835   | 2.8240   | 2.8599   |
| 250  | 1.9695   | 2.2550   | 2.4102   | 2.5159   | 2.5956   | 2.6594   | 2.7124   | 2.7577   | 2.7972   | 2.8322   |
| 500  | 1.9647   | 2.2482   | 2.4021   | 2.5068   | 2.5857   | 2.6488   | 2.7012   | 2.7460   | 2.7850   | 2.8195   |
| 1000 | 1.9623   | 2.2448   | 2.3980   | 2.5022   | 2.5808   | 2.6435   | 2.6957   | 2.7402   | 2.7790   | 2.8133   |
| ∞    | 1.9600   | 2.2414   | 2.3940   | 2.4977   | 2.5758   | 2.6383   | 2.6901   | 2.7344   | 2.7729   | 2.8070   |

(continued)

564 TABLES Table A.8. ( Continued )

|    |      11 |      12 |      13 |      14 |   15 100 α/ k |      16 |      17 |      18 |      19 |
|----|---------|---------|---------|---------|---------------|---------|---------|---------|---------|
| ν  |  0.4545 |  0.4167 |  0.3846 |  0.3571 |        0.3333 |  0.3125 |  0.2941 |  0.2778 |  0.2632 |
| 2  | 14.7818 | 15.4435 | 16.078  | 16.6883 |       17.2772 | 17.8466 | 18.3984 | 18.9341 | 19.4551 |
| 3  |  7.7041 |  7.9398 |  8.1625 |  8.3738 |        8.5752 |  8.7676 |  8.9521 |  9.1294 |  9.3001 |
| 4  |  5.7465 |  5.8853 |  6.0154 |  6.138  |        6.2541 |  6.3643 |  6.4693 |  6.5697 |  6.6659 |
| 5  |  4.8819 |  4.9825 |  5.0764 |  5.1644 |        5.2474 |  5.3259 |  5.4005 |  5.4715 |  5.5393 |
| 6  |  4.4047 |  4.4858 |  4.5612 |  4.6317 |        4.6979 |  4.7604 |  4.8196 |  4.8759 |  4.9295 |
| 7  |  4.1048 |  4.1743 |  4.2388 |  4.2989 |        4.3553 |  4.4084 |  4.4586 |  4.5062 |  4.5514 |
| 8  |  3.8999 |  3.9618 |  4.0191 |  4.0724 |        4.1224 |  4.1693 |  4.2137 |  4.2556 |  4.2955 |
| 9  |  3.7513 |  3.8079 |  3.8602 |  3.9088 |        3.9542 |  3.9969 |  4.0371 |  4.0752 |  4.1114 |
| 10 |  3.6388 |  3.6915 |  3.7401 |  3.7852 |        3.8273 |  3.8669 |  3.9041 |  3.9394 |  3.9728 |
| 11 |  3.5508 |  3.6004 |  3.6462 |  3.6887 |        3.7283 |  3.7654 |  3.8004 |  3.8335 |  3.8648 |
| 12 |  3.4801 |  3.5274 |  3.5709 |  3.6112 |        3.6489 |  3.6842 |  3.7173 |  3.7487 |  3.7783 |
| 13 |  3.4221 |  3.4674 |  3.5091 |  3.5478 |        3.5838 |  3.6176 |  3.6493 |  3.6793 |  3.7076 |
| 14 |  3.3736 |  3.4173 |  3.4576 |  3.4949 |        3.5296 |  3.5621 |  3.5926 |  3.6214 |  3.6487 |
| 15 |  3.3325 |  3.3749 |  3.4139 |  3.4501 |        3.4837 |  3.5151 |  3.5447 |  3.5725 |  3.5989 |
| 16 |  3.2973 |  3.3386 |  3.3765 |  3.4116 |        3.4443 |  3.4749 |  3.5036 |  3.5306 |  3.5562 |
| 17 |  3.2667 |  3.307  |  3.344  |  3.3783 |        3.4102 |  3.44   |  3.468  |  3.4944 |  3.5193 |
| 18 |  3.2399 |  3.2794 |  3.3156 |  3.3492 |        3.3804 |  3.4095 |  3.4369 |  3.4626 |  3.487  |
| 19 |  3.2163 |  3.255  |  3.2906 |  3.3235 |        3.354  |  3.3826 |  3.4094 |  3.4347 |  3.4585 |
| 20 |  3.1952 |  3.2333 |  3.2683 |  3.3006 |        3.3306 |  3.3587 |  3.385  |  3.4098 |  3.4332 |
| 21 |  3.1764 |  3.2139 |  3.2483 |  3.2802 |        3.3097 |  3.3373 |  3.3632 |  3.3876 |  3.4106 |
| 22 |  3.1595 |  3.1965 |  3.2304 |  3.2618 |        3.2909 |  3.3181 |  3.3436 |  3.3676 |  3.3903 |
| 23 |  3.1441 |  3.1807 |  3.2142 |  3.2451 |        3.2739 |  3.3007 |  3.3259 |  3.3495 |  3.3719 |
| 24 |  3.1302 |  3.1663 |  3.1994 |  3.23   |        3.2584 |  3.2849 |  3.3097 |  3.3331 |  3.3552 |
| 25 |  3.1175 |  3.1532 |  3.1859 |  3.2162 |        3.2443 |  3.2705 |  3.295  |  3.3181 |  3.34   |
| 26 |  3.1058 |  3.1412 |  3.1736 |  3.2035 |        3.2313 |  3.2572 |  3.2815 |  3.3044 |  3.326  |

TABLES Table A.8. ( Continued )

|      | k        | k        | k        | k        | k        | k        | k        | k        | k        |
|------|----------|----------|----------|----------|----------|----------|----------|----------|----------|
|      | 11       | 12       | 13       | 14       | 15       | 16       | 17       | 18       | 19       |
|      | 100 α/ k | 100 α/ k | 100 α/ k | 100 α/ k | 100 α/ k | 100 α/ k | 100 α/ k | 100 α/ k | 100 α/ k |
| ν    | .4545    | .4167    | .3846    | .3571    | .3333    | .3125    | .2941    | .2778    | .2632    |
| 27   | 3.0951   | 3.1301   | 3.1622   | 3.1919   | 3.2194   | 3.2451   | 3.2691   | 3.2918   | 3.3132   |
| 28   | 3.0852   | 3.1199   | 3.1517   | 3.1811   | 3.2084   | 3.2339   | 3.2577   | 3.2801   | 3.3013   |
| 29   | 3.0760   | 3.1105   | 3.1420   | 3.1712   | 3.1982   | 3.2235   | 3.2471   | 3.2694   | 3.2904   |
| 30   | 3.0675   | 3.1017   | 3.1330   | 3.1620   | 3.1888   | 3.2138   | 3.2373   | 3.2594   | 3.2802   |
| 35   | 3.0326   | 3.0658   | 3.0962   | 3.1242   | 3.1502   | 3.1744   | 3.1971   | 3.2185   | 3.2386   |
| 40   | 3.0069   | 3.0393   | 3.0690   | 3.0964   | 3.1218   | 3.1455   | 3.1676   | 3.1884   | 3.2081   |
| 45   | 2.9872   | 3.0191   | 3.0482   | 3.0751   | 3.1000   | 3.1232   | 3.1450   | 3.1654   | 3.1846   |
| 50   | 2.9716   | 3.0030   | 3.0318   | 3.0582   | 3.0828   | 3.1057   | 3.1271   | 3.1472   | 3.1661   |
| 55   | 2.9589   | 2.9900   | 3.0184   | 3.0446   | 3.0688   | 3.0914   | 3.1125   | 3.1324   | 3.1511   |
| 60   | 2.9485   | 2.9792   | 3.0074   | 3.0333   | 3.0573   | 3.0796   | 3.1005   | 3.1202   | 3.1387   |
| 70   | 2.9321   | 2.9624   | 2.9901   | 3.0156   | 3.0393   | 3.0613   | 3.0818   | 3.1012   | 3.1194   |
| 80   | 2.9200   | 2.9500   | 2.9773   | 3.0026   | 3.0259   | 3.0476   | 3.0679   | 3.0870   | 3.1050   |
| 90   | 2.9106   | 2.9403   | 2.9675   | 2.9924   | 3.0156   | 3.0371   | 3.0572   | 3.0761   | 3.0939   |
| 100  | 2.9032   | 2.9327   | 2.9596   | 2.9844   | 3.0073   | 3.0287   | 3.0487   | 3.0674   | 3.0851   |
| 110  | 2.8971   | 2.9264   | 2.9532   | 2.9778   | 3.0007   | 3.0219   | 3.0417   | 3.0604   | 3.0779   |
| 120  | 2.8921   | 2.9212   | 2.9479   | 2.9724   | 2.9951   | 3.0162   | 3.0360   | 3.0545   | 3.0720   |
| 250  | 2.8635   | 2.8919   | 2.9178   | 2.9416   | 2.9637   | 2.9842   | 3.0034   | 3.0213   | 3.0383   |
| 500  | 2.8505   | 2.8785   | 2.9041   | 2.9276   | 2.9494   | 2.9696   | 2.9885   | 3.0063   | 3.0230   |
| 1000 | 2.8440   | 2.8719   | 2.8973   | 2.9207   | 2.9423   | 2.9624   | 2.9812   | 2.9988   | 3.0154   |
| ∞    | 2.8376   | 2.8653   | 2.8905   | 2.9137   | 2.9352   | 2.9552   | 2.9738   | 2.9913   | 3.0078   |

Table A.9. Lower Critical Values of Wilks Λ , α = . 05

<!-- formula-not-decoded -->

where λ 1 , λ 2 , . . . , λ s are eigenvalues of E -1 H . Reject H 0 if /Lambda1 ≤ table value. a Multiply entry by 10 -3 .

| ν E      | 1         | 2         | 3         | 4         | 5         | 6         | 7         | 8         | 9         | 10        | 11        | 12        |
|----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
| p = 1    | p = 1     | p = 1     | p = 1     | p = 1     | p = 1     | p = 1     | p = 1     | p = 1     | p = 1     | p = 1     | p = 1     | p = 1     |
| 1        | 6.16 a    | 2.50 a    | 1.54 a    | 1.11 a    | .868 a    | .712 a    | .603 a    | .523 a    | .462 a    | .413 a    | .374 a    | .341 a    |
| 2        | .098      | .050      | .034      | .025      | .020      | .017      | .015      | .013      | .011      | .010      | 9 . 28 a  | 8 . 51 a  |
| 3        | .229      | .136      | .097      | .076      | .062      | .053      | .046      | .041      | .036      | .033      | .030      | .028      |
| 4        | .342      | .224      | .168      | .135      | .113      | .098      | .086      | .076      | .069      | .063      | .058      | .053      |
| 5        | .431      | .302      | .236      | .194      | .165      | .144      | .128      | .115      | .104      | .096      | .088      | .082      |
| 6        | .501      | .368      | .296      | .249      | .215      | .189      | .169      | .153      | .140      | .129      | .119      | .111      |
| 7        | .556      | .425      | .349      | .298      | .261      | .232      | .209      | .190      | .175      | .161      | .150      | .140      |
| 8        | .601      | .473      | .396      | .343      | .303      | .271      | .246      | .225      | .208      | .193      | .180      | .169      |
| 9        | .638      | .514      | .437      | .382      | .341      | .308      | .281      | .258      | .239      | .223      | .209      | .196      |
| 10       | .668      | .549      | .473      | .418      | .376      | .341      | .313      | .289      | .269      | .251      | .236      | .222      |
| 11       | .694      | .580      | .505      | .450      | .407      | .372      | .343      | .318      | .297      | .278      | .262      | .247      |
| 12       | .717      | .607      | .534      | .479      | .436      | .400      | .370      | .345      | .323      | .304      | .286      | .271      |
| 13       | .736      | .631      | .560      | .506      | .462      | .426      | .396      | .370      | .347      | .327      | .310      | .294      |
| 14       | .753      | .652      | .583      | .529      | .486      | .450      | .420      | .393      | .370      | .350      | .332      | .315      |
| 15       | .768      | .671      | .603      | .551      | .508      | .473      | .442      | .415      | .392      | .371      | .352      | .336      |
| 16       | .781      | .688      | .622      | .571      | .529      | .493      | .462      | .436      | .412      | .391      | .372      | .355      |
| 17       | .792      | .703      | .639      | .589      | .548      | .512      | .482      | .455      | .431      | .410      | .390      | .373      |
| 18       | .803      | .717      | .655      | .606      | .565      | .530      | .499      | .473      | .449      | .427      | .408      | .390      |
| 19       | .813      | .730      | .669      | .621      | .581      | .546      | .516      | .490      | .466      | .444      | .425      | .407      |
| 20       | .821      | .741      | .683      | .636      | .596      | .562      | .532      | .505      | .482      | .460      | .440      | .423      |
| 21       | .829      | .752      | .695      | .649      | .610      | .576      | .547      | .520      | .497      | .475      | .455      | .437      |
| 22       | .836      | .762      | .706      | .661      | .623      | .590      | .561      | .534      | .511      | .489      | .470      | .452      |
| 23       | .843      | .771      | .717      | .673      | .635      | .603      | .574      | .548      | .524      | .503      | .483      | .465      |
| 24       | .849      | .779      | .727      | .684      | .647      | .615      | .586      | .560      | .537      | .516      | .496      | .478      |
| 25       | .855      | .787      | .736      | .694      | .658      | .626      | .598      | .572      | .549      | .528      | .508      | .490      |
| 26       | .860      | .794      | .744      | .703      | .668      | .637      | .609      | .583 .594 | .560      | .539      | .520      | .502      |
| 27       | .865      | .801      | .752      | .712      | .677      | .647      | .619      |           | .571      | .551      | .531      | .513      |
| 28       | .870      | .807      | .760      | .721      | .686      | .656      | .629      | .604      | .582      | .561      | .542      | .524      |
| 29       | .874      | .813      | .767      | .729      | .695      | .665      | .638      | .614      | .592      | .571      | .552      | .535      |
| 30       | .878      | .819      | .774      | .736      | .703      | .674      | .647      | .623      | .601      | .581      | .562      | .544      |
| 40       | .907      | .861      | .824      | .793      | .766      | .741      | .718      | .696      | .677      | .658      | .641      | .625      |
| 60       | .938      | .905      | .879      | .856      | .835      | .816      | .798      | .781      | .766      | .751      | .736      | .723      |
| 80       | .953      | .928      | .907      | .889      | .873      | .858      | .843      | .829      | .816      | .804      | .792      | .780      |
| 100      | .962      | .942      | .925      | .910      | .897      | .884      | .872      | .860      | .849      | .838      | .828      | .818      |
| 120      | .968      | .951      | .937      | .925      | .913      | .902      | .891      | .882      | .872      | .863      | .854      | .845      |
| 140      | .973      | .958      | .946      | .935      | .925      | .915      | .906      | .897      | .889      | .881      | .873      | .865      |
| 170      | .978      | .965      | .955      | .946      | .937      | .929      | .922      | .914      | .907      | .900      | .893      | .887      |
| 200      | .981 .984 | .970      | .962      | .954 .961 | .947 .955 | .940 .949 | .933      | .926      | .920      | .914 .928 | .908 .923 | .902 .918 |
| 240 320  | .988      | .975 .981 | .968 .976 | .971      | .966      | .962      | .944 .957 | .938 .953 | .933 .949 | .945      | .941      | .937      |
| 440      | .991      | .986      | .982      | .979      | .975      | .972      | .969      | .966      | .963      | .960      | .957      | .954      |
|          | .994      | .990      | .987      | .984      | .982      | .979      |           |           | .972      | .970      | .968      | .966      |
| 600      | .995      | .993      | .990      | .988      | .986      | .984      | .977      | .975      | .979      | .977      | .976      | .974      |
| 800 1000 | .996      | .994      | .992      | .991      | .989      | .988      | .983 .986 | .981 .985 | .983      | .982      | .981      | .979      |

TABLES Table A.9. ( Continued )

| ν E      | 1         | 2         | 3         | 4         | 5         | 6         | 7         | 8         | 9         | 10        | 11        | 12        |
|----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
| p 2      | p 2       | p 2       | p 2       | p 2       | p 2       | p 2       | p 2       | p 2       | p 2       | p 2       | p 2       | p 2       |
| 1        | .000      | .000      | .000      | .000      | .000      | = .000    | .000      | .000      | .000      | .000      | .000      | .000      |
| 2        | 2.50 a    | .641 a    | .287 a    | .162 a    | .104 a    | .072 a    | .053 a    | .041 a    | .032 a    | .026 a    | .022 a    | .018 a    |
| 3        | .050      | .018      | 9 . 53 a  | 5 . 84 a  | 3 . 95 a  | 2 . 85 a  | 2 . 15 a  | 1 . 68 a  | 1 . 35 a  | 1 . 11 a  | . 928 a   | . 787 a   |
| 4        | .136      | .062      | .036      | .023      | .017      | .012      | 9 . 56 a  | 7 . 62 a  | 6 . 21 a  | 5 . 17 a  | 4 . 36 a  | 3 . 73 a  |
| 5        | .224      | .117      | .074      | .051      | .037      | .028      | .023      | .018      | .015      | .013      | .011      | .009      |
| 6        | .302      | .175      | .116      | .084      | .063      | .049      | .040      | .033      | .027      | .023      | .020      | .017      |
| 7        | .368      | .230      | .160      | .119      | .092      | .074      | .060      | .050      | .042      | .036      | .032      | .028      |
| 8        | .4256     | .280      | .203      | .155      | .122      | .099      | .082      | .069      | .059      | .051      | .045      | .040      |
| 9        | .473      | .326      | .243      | .190      | .153      | .126      | .106      | .090      | .078      | .068      | .060      | .053      |
| 10       | .514      | .367      | .281      | .223      | .183      | .152      | .129      | .111      | .097      | .085      | .075      | .067      |
| 11       | .549      | .404      | .316      | .255      | .212      | .179      | .153      | .133      | .116      | .102      | .091      | .082      |
| 12       | .580      | .437      | .348      | .286      | .240      | .204      | .176      | .154      | .136      | .120      | .108      | .097      |
| 13       | .607      | .467      | .378      | .314      | .266      | .229      | .199      | .175      | .155      | .138      | .124      | .112      |
| 14       | .631      | .495      | .405      | .340      | .291      | .252      | .221      | .195      | .174      | .156      | .141      | .128      |
| 15       | .652      | .519      | .431      | .365      | .315      | .275      | .242      | .215      | .193      | .174      | .157      | .143      |
| 16       | .671      | .542      | .454      | .389      | .337      | .296      | .263      | .235      | .211      | .191      | .174      | .159      |
| 17       | .688      | .562      | .476      | .410      | .359      | .317      | .282      | .254      | .229      | .208      | .190      | .174      |
| 18       | .703      | .581      | .496      | .431      | .379      | .337      | .301      | .272      | .246      | .225      | .206      | .189      |
| 19       | .717      | .598      | .515      | .450      | .398      | .355      | .320      | .289      | .263      | .241      | .221      | .204      |
| 20       | .730      | .614      | .532      | .468      | .416      | .373      | .337      | .306      | .279      | .256      | .236      | .218      |
| 21       | .741      | .629      | .548      | .485      | .433      | .390      | .354      | .322      | .295      | .271      | .251      | .232      |
| 22       | .752      | .643      | .564      | .501      | .449      | .406      | .370      | .338      | .310      | .286      | .265      | .246      |
| 23       | .762      | .656      | .578      | .516      | .465      | .422      | .385      | .353      | .325      | .300      | .279      | .259      |
| 24       | .771      | .668      | .591      | .530      | .479      | .436      | .399      | .367      | .339      | .314      | .292      | .272      |
| 25       | .779      | .679      | .604      | .544      | .493      | .450      | .413      | .381      | .353      | .328      | .305      | .285      |
| 26       | .787      | .689      | .616      | .556      | .506      | .464      | .427      | .395      | .366      | .341      | .318      | .297      |
| 27       | .794      | .699      | .627      | .568      | .519      | .477      | .440      | .407      | .379      | .353      | .330      | .309      |
| 28       | .801      | .708      | .638      | .580      | .531      | .489      | .452      | .420      | .391      | .365      | .342      | .321      |
| 29       | .807      | .717      | .648      | .591      | .542      | .501      | .464      | .432      | .403      | .377      | .354      | .332      |
| 30       | .813      | .725      | .657      | .601      | .553      | .512      | .475      | .443      | .414      | .388      | .365      | .344      |
| 40       | .858      | .786 .853 | .730      | .682      | .640      | .602      | .568      | .537      | .509      | .484      | .460      | .439      |
| 60       | .903      |           | .811      | .774      | .741      | .710      | .682      | .656      | .632      | .609      | .588      | .568      |
| 80       | .927      | .888      | .854      | .825      | .798      | .772      | .749      | .727      | .706      | .686      | .667      | .649      |
| 100      | .941      | .909      | .882      | .857      | .834      | .813      | .793      | .774      | .755      | .738      | .721      | .705      |
| 120      | .951      | .924      | .900      | .879      | .860      | .841      | .823      | .807      | .791      | .775      | .760      | .746      |
| 140      | .958      | .934      | .914      | .895      | .878      | .862      | .846      | .831      | .817      | .803      | .790      | .777      |
| 170 200  | .965 .970 | .946 .954 | .929 .939 | .913 .926 | .898 .913 | .885 .901 | .871 .889 | .859 .878 | .846 .867 | .834 .857 | .823 .847 | .812 .837 |
| 320      | .975      | .961      | .949      | .938      | .927      | .937      | .929      | .922      | .888      | .879      | .870      | .862      |
| 240      | .981      | .971      | .962      | .953      | .945      | .917      | .907      | .897      | .914      | .907      | .901      | .894      |
| 440      | .986      | .979      | .972      | .965      | .959      | .953      | .948      | .942      | .937      | .932      | .926      | .921      |
|          |           |           | .979      | .975      | .970      | .966      | .961      | .957      | .953      | .949      | .945      | .942      |
| 600      | .990      | .984      | .984      | .981      | .977      |           | .971      | .968      | .965      | .962      | .959      | .956      |
| 800 1000 | .993 .994 | .988 .991 | .987      | .985      | .982      | .974 .979 | .977      | .974      | .972      | .969      | .967      | .964      |

a Multiply entry by 10 -3 .

(continued)

568 Table A.9. ( Continued

)

|         | ν H       | ν H    | ν H       | ν H       | ν H       | ν H       | ν H       | ν H                 | ν H                | ν H       | ν H       |
|---------|-----------|--------|-----------|-----------|-----------|-----------|-----------|---------------------|--------------------|-----------|-----------|
| ν E     | 1         | 2      | 3         | 4         | 5         | 6         | 7         | 8                   | 9 10               | 11        | 12        |
|         | p = 3     | p = 3  | p = 3     | p = 3     | p = 3     | p = 3     | p = 3     | p = 3               | p = 3              | p = 3     | p = 3     |
| 1       | .000      | .000   | .000      | .000      | .000      | .000      | .000      | .000 .000           | .000               | .000      | .000      |
| 2       | .000      | .000   | .000      | .000      | .000      | .001 a    | .002 a    | .004 a              | a .008 a           | .010 a    | .013 a    |
| 3       | 1 . 70 a  | .354 a | .179 a    | .127 a    | .105 a    | .095 a    | .091 a    | .090 a              | .005 .091 a .092 a | .095 a    | .098 a    |
| 4       | .034      | .010   | .004      | .002      | .001      | .001      | .809 a    | .659 a              | .562 a .496 a      | .449 a    | .416 a    |
| 5       | .097      | .036   | .018      | .010      | 6.36 a    | 4 . 37 a  | 3 . 20 a  | 2 . 46 a 1 . 97     | a 1 . 64 a         | 1 . 40 a  | 1 . 22 a  |
| 6       | .168      | .074   | .040      | .024      | .016      | .011      | .008      | .006 .004           | 3 . 94 a           | 3 . 28 a  | 2 . 79 a  |
| 7       | .236      | .116   | .068      | .043      | .029      | .021      | .016      | .012 9 . 49         | a 7 . 67 a         | 6 . 35 a  | 5 . 35 a  |
| 8       | .296      | .160   | .099      | .066      | .046      | .034      | .026      | .020                | .013               | .011      | 9 . 00 a  |
| 9       | .349      | .203   | .131      | .091      | .066      | .049      | .038      | .016 .030 .024      | .020               | .016      | .014      |
| 10      | .396      | .243   | .164      | .117      | .086      | .066      | .052      | .041                | .034 .028          | .023      | .020      |
| 11      | .437      | .281   | .196      | .143      | .108      | .084      | .067      | .054 .044           | .037               | .031      | .026      |
| 12      | .473      | .316   | .226      | .169      | .130      | .103      | .083      | .067 .056           | .047               | .040      | .034      |
| 13      | .505      | .348   | .255      | .194      | .152      | .122      | .099      | .082                | .058               | .049      | .042      |
| 14      | .534      | .378   | .283      | .219      | .174      | .141      | .116      | .096                | .068 .081 .069     | .059      | .051      |
| 15      | .560      | .405   | .309      | .243      | .195      | .160      | .133      | .111 .095           | .081               | .070      | .061      |
| 16      | .583      | .431   | .334      | .266      | .216      | .179      | .149      | .127 .108           | .093               | .081      | .071      |
| 17      | .603      | .454   | .357      | .288      | .236      | .197      | .166      | .142                | .122 .106          | .092      | .081      |
| 18      | .622      | .476   | .379      | .309      | .256      | .215      | .183      | .157                | .136 .118          | .104      | .092      |
| 19      | .639      | .496   | .399      | .329      | .275      | .233      | .199      | .172 .149           | .131               | .115      | .102      |
| 20      | .655      | .515   | .419      | .348      | .293      | .250      | .215      | .187 .163           | .144               | .127      | .113      |
| 21      | .669      | .532   | .437      | .366      | .310      | .266      | .230      | .201                | .177 .156          | .139      | .124      |
| 22      | .683      | .548   | .454      | .383      | .327      | .282      | .246      | .215 .190           | .169               | .150      | .135      |
| 23      | .695      | .564   | .470      | .399      | .343      | .298      | .260      | .229 .203           | .181               | .162      | .146      |
| 24      | .706      | .578   | .486      | .415      | .359      | .313      | .275      | .243 .216           | .193               | .173      | .156      |
| 25      | .717      | .591   | .500      | .430      | .374      | .327      | .289      | .256 .229           | .205               | .185      | .167      |
| 26      | .727      | .604   | .514      | .444      | .388      | .341      | .302      | .269 .241           | .217               | .196      | .178      |
| 27      | .736      | .616   | .527      | .458      | .401      | .355      | .315      | .282 .253           | .229               | .207      | .188      |
| 28      | .744      | .627   | .540      | .471      | .415      | .368      | .328      | .294 .265           | .240               | .218      | .199      |
| 29      | .752      | .638   | .552      | .483      | .427      | .380      | .340      | .306 .277           | .251               | .229      | .209      |
| 30      | .760      | .648   | .563      | .495      | .439      | .392      | .352      | .318 .288           | .262               | .239      | .219      |
| 40      | .816      | .724   | .651      | .591      | .539      | .494      | .454      | .419 .387 .555      | .359               | .334      | .311      |
| 60      | .875      | .808   | .752      | .704      | .661      | .623      | .587      | .526                | .498               | .473 .566 | .449      |
| 80      | .905      | .853   | .808      | .769      | .733      | .700      | .670      | .641 .615           | .590               |           | .544      |
| 100     | .924      | .881   | .844      | .810      | .780      | .751      | .725      | .700 .676           | .654               | .632      | .612      |
| 120 140 | .936      | .900   | .868      | .839      | .813      | .788      | .764      | .742 .721           | .700               | .681      | .663      |
|         | .945      | .913   | .886      |           |           |           | .794      | .774 .755           | .736               |           |           |
| 200     | .961      | .939   | .919      | .861 .900 | .837 .883 | .815 .866 | .850      | .792 .835 .820      | .806               | .719 .792 | .702 .779 |
| 170     | .955      | .928   | .905      | .884      | .864      | .845      | .827      | .809                | .776               | .761      | .746      |
|         |           | .961   |           |           |           |           |           |                     |                    | .864      | .854      |
| 240     | .968      | .949   | .932      | .916      | .901      | .887      | .873      | .860                | .848 .835          | .823      | .811      |
| 320     | .976 .982 | .972   | .948 .962 | .936 .953 | .925 .945 | .914 .937 | .903 .929 | .893 .883 .921 .913 | .873 .906          | .899      | .891      |
| 440 600 | .987      | .979   | .972      | .966      | .959      | .953      | .947      | .941 .936           | .930               | .924      | .919      |
| 800     | .990      | .984   | .979      | .974      | .969      | .965      | .960      | .956                | .951 .947          | .943      | .939      |
| 1000    | .992      | .987   | .983      | .979      | .975      | .972      | .968      | .964                | .961 .957          | .954      | .950      |

a Multiply entry by 10 -3 .

TABLES Table A.9. ( Continued )

| ν H               | ν H       | ν H       | ν H       | ν H       | ν H       | ν H       | ν H      | ν H       | ν H       | ν H       | ν H       |
|-------------------|-----------|-----------|-----------|-----------|-----------|-----------|----------|-----------|-----------|-----------|-----------|
| ν E               | 1         | 2         | 3         | 4         | 5 6       | 7         | 8        | 9         | 10        | 11        | 12        |
|                   |           |           |           |           | p =       | 4         |          |           |           |           |           |
| 1 .000            |           | .000      | .000      | .000      | .000      | .000      | .000     | .000      | .000      | .000      | .000      |
| 2 .000            | .000 .000 | .000      | .000      | .000      | .000      | .000      | .000     | .000      | .000      | .000      | .000      |
| 3 .000            | .000      | .000      | .000      | .000      | .001 a    | .001 a    | .001 a   | .002 a    | .002 a    | .002 a    | .003 a    |
| 4 1 . 38 a        | .292 a    | .127 a    | .075 a    | .052 a    | .040 a    | .033 a    | .029 a   | .026 a    | .025 a    | .023 a    | .022 a    |
| 5 .026            | 6 . 09 a  | 2 . 31 a  | 1 . 13 a  | .647 a    | .416 a    | .292 a    | .218 a   | .172 a    | .141 a    | .120 a    | .105 a    |
| 6 .076            | .024      | .010      | 5 . 07 a  | 2 . 90 a  | 1 . 82 a  | 1 . 22 a  | .872 a   | .652 a    | .508 a    | .409 a    | .338 a    |
| 7 .135            | .051      | .024      | .013      | 7 . 74 a  | 4 . 94 a  | 3 . 34 a  | 2 . 36 a | 1 . 74 a  | 1 . 33 a  | 1 . 05 a  | .848 a    |
| 8 .194            | .084      | .043      | .025      | .015      | .010      | 6 . 98 a  | 4 . 99 a | 3 . 70 a  | 2 . 82 a  | 2 . 21 a  | 1 . 77 a  |
| 9 .249            | .119      | .066      | .040      | .026      | .017      | .012      | 8 . 91 a | 6 . 66 a  | 5 . 11 a  | 4 . 01 a  | 3 . 21 a  |
| 10 .298           | .155      | .091      | .057      | .038      | .027      | .019      | .014     | .011      | 8 . 29 a  | 6 . 54 a  | 5 . 25 a  |
| 11 .343           | .190      | .117      | .077      | .053      | .037      | .027      | .021     | .016      | .012      | 9 . 84 a  | 7 . 95 a  |
| 12 .382           | .223      | .143      | .097      | .068      | .049      | .037      | .028     | .022      | .017      | .014      | .011      |
| 13 .418           | .255      | .169      | .117      | .085      | .063      | .047      | .037     | .029      | .023      | .019      | .015      |
| 14 .450           | .286      | .194      | .138      | .102      | .077      | .059      | .046     | .037      | .030      | .024      | .020      |
| 15 .479           | .314      | .219      | .159      | .119      | .091      | .071      | .056     | .045      | .037      | .030      | .025      |
| 16 .506           | .340      | .243      | .180      | .136      | .106      | .083      | .067     | .054      | .044      | .037      | .031      |
| 17 .529           | .365      | .266      | .200      | .154      | .121      | .096      | .078     | .064      | .053      | .044      | .037      |
| 18 .551           | .389      | .288      | .219      | .171      | .136      | .109      | .089     | .074      | .061      | .051      | .044      |
| 19 .571           | .410      | .309      | .239      | .188      | .151      | .123      | .101     | .084      | .070      | .059      | .051      |
| 20 .589           | .431      | .329      | .257      | .205      | .166      | .136      | .113     | .094      | .079      | .068      | .058      |
| 21 .606           | .450      | .348      | .275      | .221      | .181      | .149      | .124     | .105      | .089      | .076      | .065      |
| 22 .621           | .468      | .366      | .292      | .237      | .195      | .162      | .136     | .115      | .098      | .085      | .073      |
| 23 .636           | .485      | .383      | .309      | .253      | .210      | .175      | .148     | .126      | .108      | .093      | .081      |
| 24 .649           | .501      | .399      | .325      | .268      | .224      | .188      | .160     | .137      | .118      | .102      | .089      |
| 25 .661           | .516      | .415      | .340      | .283      | .237      | .201      | .172     | .148      | .128      | .111      | .097      |
| 26 .673           | .530      | .430      | .355      | .297      | .251      | .214      | .183     | .158      | .138      | .120      | .106      |
| 27 .684           | .544      | .444      | .369      | .311      | .264      | .226      | .195     | .169      | .147      | .129      | .114      |
| 28 .694           | .556      | .458      | .383      | .324      | .277      | .238      | .206     | .180      | .157      | .138      | .122      |
| 29 .703           | .568      | .471      | .396      | .337      | .289      | .250      | .217     | .190      | .167      | .147      | .131      |
| 30 .712           | .580      | .483      | .409      | .349      | .301      | .261      | .228     | .200      | .177      | .157      | .139      |
| 40 .779           | .668      | .583      | .513      | .455      | .406      | .364      | .327     | .295      | .267      | .243      | .221      |
| 60 .849           | .767      | .700      | .643      | .592      | .547      | .507      | .471     | .438      | .409      | .382      | .357      |
| 80 .885           | .821      | .766      | .718      | .675      | .636      | .600      | .567     | .536      | .508      | .482      | .457      |
| 100 .908          | .854      | .809      | .768      | .730      | .696      | .664      | .634     | .606      | .580      | .555      | .532      |
| 120 .923          | .877      | .838      | .802      | .770      | .739      | .711      | .684     | .658      | .634      | .611      | .590      |
| 140 .934          | .894      | .860      | .828      | .799      | .772      | .746      | .721     | .698      | .676      | .655      | .635      |
| 170 .945          | .912      | .883      | .856      | .831      | .808      | .785      | .764     | .743      | .724      | .705      | .687      |
| 200 .953          | .925      | .900      | .876      | .855      | .834      | .814      | .795     | .777      | .759      | .742      | .726      |
| 240 .961          | .937      | .916      | .896      | .877      |           | .842      | .826     | .810      | .795      | .780      | .765      |
| 320 .971          | .952      | .936      | .921      | .907      | .859 .893 | .879      | .866     | .854      | .841      | .829      | .818      |
| 440 .979          | .965      | .953      | .942      | .931      | .921      | .911      | .901     | .891      | .882      | .872      | .863      |
|                   |           |           |           |           |           |           | .926     |           |           |           |           |
| 600 .984 800 .988 | .974 .981 | .966 .974 | .957 .968 | .949 .961 | .941 .956 | .934 .950 | .944     | .919 .938 | .912 .933 | .905 .927 | .898 .922 |
| 1000 .991         | .985      | .979      |           |           |           | .960      | .955     | .950      | .946      | .941      | .937      |
|                   |           |           | .974      | .969      | .964      |           |          |           |           |           |           |

a Multiply entry by 10 -3 .

(continued)

570

Table A.9. ( Continued )

| ν E     | 1         | 2         | 3        | 4         | 5         | 6          | 7         | 8         | 9         | 10        | 11        | 12        |
|---------|-----------|-----------|----------|-----------|-----------|------------|-----------|-----------|-----------|-----------|-----------|-----------|
| 1       | .000      | .000      | .000     | .000      | .000      | p = 5 .000 | .000      | .000      | .000      | .000      | .000      | .000      |
| 2       | .000      | .000      | .000     | .000      | .000      | .000       | .000      | .000      | .000      | .000      | .000      | .000      |
| 3       | .000      | .000      | .000     | .000      | .000      | .000       | .000      | .000      | .000      | .000      | .000      | .000      |
| 4       | .000      | .000      | .000     | .000      | .001 a    | .001 a     | .001 a    | .001 a    | .001 a    | .001 a    | .001 a    | .001 a    |
| 5       | 1 . 60 a  | .291 a    | .105 a   | .052 a    | .031 a    | .021 a     | .015 a    | .012 a    | .010 a    | .008 a    | .007 a    | .007 a    |
| 6       | .021      | 4 . 39 a  | 1 . 48 a | .647 a    | .335 a    | .197 a     | .126 a    | .087 a    | .064 a    | .049 a    | .039 a    | . 032 a   |
| 7       | .063      | .017      | 6 . 36 a | 2 . 90 a  | 1 . 51 a  | .872 a     | .544 a    | .361 a    | .253 a    | .185 a    | .141 a    | .110 a    |
| 8       | .114      | .037      | .016     | 7 . 74 a  | 4 . 21 a  | 2 . 48 a   | 1 . 56 a  | 1 . 03 a  | .716 a    | .516 a    | .385 a    | .296 a    |
| 9       | .165      | .063      | .029     | .015      | 8 . 79 a  | 5 . 35 a   | 3 . 43 a  | 2 . 30 a  | 1 . 61 a  | 1 . 16 a  | .861 a    | .657 a    |
| 10      | .215      | .092      | .046     | .026      | .015      | 9 . 64 a   | 6 . 34 a  | 4 . 34 a  | 3 . 06 a  | 2 . 22 a  | 1 . 66 a  | 1 . 27 a  |
| 11      | .261      | .122      | .066     | .038      | .024      | .015       | .010      | 7 . 22 a  | 5 . 17 a  | 3 . 80 a  | 2 . 86 a  | 2 . 19 a  |
| 12      | .303      | .153      | .086     | .053      | .034      | .022       | .015      | .011      | 7 . 99 a  | 5 . 95 a  | 4 . 51 a  | 3 . 49 a  |
| 13      | .341      | .183      | .108     | .068      | .045      | .031       | .022      | .016      | .012      | 8 . 68 a  | 6 . 66 a  | 5 . 19 a  |
| 14      | .376      | .212      | .130     | .085      | .057      | .040       | .029      | .021      | .016      | .012      | 9 . 31 a  | 7 . 32 a  |
| 15      | .407      | .239      | .152     | .102      | .070      | .050       | .037      | .027      | .021      | .016      | .012      | 9 . 88 a  |
| 16      | .436      | .266      | .174     | .119      | .084      | .061       | .045      | .034      | .026      | .020      | .016      | .013      |
| 17      | .462      | .291      | .195     | .136      | .098      | .072       | .054      | .042      | .032      | .025      | .020      | .016      |
| 18      | .486      | .315      | .216     | .154      | .113      | .084       | .064      | .050      | .039      | .031      | .025      | .020      |
| 19      | .508      | .337      | .236     | .171      | .127      | .096       | .074      | .058      | .046      | .037      | .030      | .024      |
| 20      | .529      | .359      | .256     | .188      | .142      | .109       | .085      | .067      | .053      | .043      | .035      | .029      |
| 21      | .548      | .379      | .275     | .205      | .156      | .121       | .095      | .076      | .061      | .050      | .041      | .034      |
| 22      | .565      | .398      | .293     | .221      | .171      | .134       | .106      | .085      | .069      | .057      | .047      | .039      |
| 23      | .581      | .416      | .310     | .237      | .185      | .146       | .117      | .095      | .077      | .064      | .053      | .044      |
| 24      | .596      | .433      | .327     | .253      | .199      | .159       | .128      | .104      | .086      | .071      | .060      | .050      |
| 25      | .610      | .449      | .343     | .268      | .213      | .171       | .139      | .114      | .094      | .079      | .066      | .056      |
| 26      | .623      | .465      | .359     | .283      | .226      | .183       | .150      | .124      | .103      | .087      | .073      | .062      |
| 27      | .635      | .479      | .374     | .297      | .239      | .195       | .161      | .134      | .112      | .094      | .080      | .068      |
| 28      | .647      | .493      | .388     | .311      | .252      | .207       | .172      | .143      | .121      | .102      | .087      | .075      |
| 29      | .658      | .506      | .401     | .324      | .265      | .219       | .182      | .153      | .130      | .110      | .094      | .081      |
| 30      | .668      | .519      | .415     | .337      | .277      | .230       | .193      | .163      | .138      | .118      | .102      | .088      |
|         | .744      | .617      | .522     | .446      | .384      | .333       | .291      |           | .224      | .198      | .176      | .156      |
| 40 60   | .825      | .729      | .652     | .587      | .531      | .482       | .438      | .255 .400 | .366      | .336      | .308      | .284      |
| 80      | .867      | .791      | .727     | .672      | .623      | .578       | .538      | .502      | .469      | .438      | .410      | .385      |
| 100     | .893      | .830      | .776     | .728      | .685      | .645       | .609      | .576      | .544      | .516      | .489      | .464      |
| 120     | .910      | .856      | .810     | .768      | .730      | .694       | .661      | .631      | .602      | .575      | .549      | .525      |
| 140     | .923      | .876      | .835     | .798      | .763      | .731       | .701      | .673      | .647      | .621 .675 | .598 .654 | .575 .633 |
| 170 200 | .936 .945 | .897 .912 | .862     | .830 .854 | .801      | .773 .803  | .747 .780 | .722 .758 | .698 .736 | .716      | .696      | .677      |
| 240     | .954      |           | .882     | .877      | .828      |            |           |           | .775      | .757      |           |           |
| 300     | .966      | .944      | .925     | .906      | .855 .889 | .833 .872  | .813 .856 | .841      | .825      | .811      | .797      | .783      |
|         |           | .926      | .900     |           |           |            |           | .793      |           |           | .739      | .722      |
| 440     | .975      | .959      | .945     | .931      | .918      | .905       | .893      | .881      | .870      | .858      | .847      | .836 .877 |
| 600     | .982      | .970      | .959     | .949      | .939      | .930       | .920      | .911      | .903      | .894      | .885      |           |
| 800     | .986      | .977      | .969     | .961      | .954      | .947       | .940      | .933      | .926      | .919      | .913      | .906      |
| 1000    | .989      | .982      | .975     | .969      | .963      | .957       | .951      | .946      | .940      | .935      | .929      | .924      |

a Multiply entry by 10 -3 .

TABLES Table A.9. ( Continued )

|   ν E | 1         | 2         | 3         | 4         | 5        | 6          | 7         | 8         | 9         | 10        | 11        | 12       |
|-------|-----------|-----------|-----------|-----------|----------|------------|-----------|-----------|-----------|-----------|-----------|----------|
|     1 | .000      | .000      | .000      | .000      | .000     | p = 6 .000 | .000      | .000      | .000      | .000      | .000      | .000     |
|     2 | .000      | .000      | .000      | .000      | .000     | .000       | .000      | .000      | .000      | .000      | .000      | .000     |
|     3 | .000      | .000      | .000      | .000      | .000     | .000       | .000      | .000      | .000      | .000      | .000      | .000     |
|     4 | .000      | .000      | .000      | .000      | .000     | .000       | .000      | .000      | .000      | .000      | .000      | .000     |
|     5 | .007 a    | .002 a    | .001 a    | .001 a    | .001 a   | .000       | .000      | .000      | .000      | .000      | .000      | .000     |
|     6 | 2 . 04 a  | .315 a    | .095 a    | .040 a    | .021 a   | .012 a     | .008 a    | .006 a    | .004 a    | .003 a    | .003 a    | .002 a   |
|     7 | .019      | 3 . 48 a  | 1 . 05 a  | .416 a    | .197 a   | .106 a     | .063 a    | .040 a    | .027 a    | .020 a    | .015 a    | .011 a   |
|     8 | .054      | .013      | 4 . 37 a  | 1 . 82 a  | .872 a   | .465 a     | .270 a    | .168 a    | .111 a    | .076 a    | .055 a    | .041 a   |
|     9 | .098      | .029      | .011      | 4 . 94 a  | 2 . 48 a | 1 . 36 a   | .798 a    | .497 a    | .325 a    | .222 a    | .157 a    | .115 a   |
|    10 | .144      | .050      | .021      | .010      | 5 . 35 a | 3 . 04 a   | 1 . 83 a  | 1 . 16 a  | .762 a    | .521 a    | .369 a    | .269 a   |
|    11 | .189      | .074      | .034      | .017      | 9 . 64 a | 5 . 67 a   | 3 . 51 a  | 2 . 26 a  | 1 . 51 a  | 1 . 05 a  | .744 a    | .543 a   |
|    12 | .232      | .099      | .049      | .027      | .015     | 9 . 35 a   | 5 . 94 a  | 3 . 92 a  | 2 . 66 a  | 1 . 86 a  | 1 . 34 a  | .983 a   |
|    13 | .271      | .126      | .066      | .037      | .022     | .014       | 9 . 17 a  | 6 . 17 a  | 4 . 27 a  | 3 . 03 a  | 2 . 20 a  | 1 . 63 a |
|    14 | .308      | .152      | .084      | .049      | .031     | .020       | .013      | 9 . 07 a  | 6 . 38 a  | 4 . 59 a  | 3 . 37 a  | 2 . 52 a |
|    15 | .341      | .179      | .103      | .063      | .040     | .026       | .018      | .013      | 9 . 00 a  | 6 . 57 a  | 4 . 88 a  | 3 . 68 a |
|    16 | .372      | .204      | .122      | .077      | .050     | .034       | .024      | .017      | .012      | 8 . 97 a  | 6 . 74 a  | 5 . 14 a |
|    17 | .400      | .229      | .141      | .091      | .061     | .042       | .030      | .021      | .016      | .012      | 8 . 97 a  | 6 . 90 a |
|    18 | .426      | .252      | .160      | .106      | .072     | .051       | .037      | .027      | .020      | .015      | .012      | 8 . 97 a |
|    19 | .450      | .275      | .179      | .121      | .084     | .060       | .044      | .033      | .025      | .019      | .015      | .011     |
|    20 | .473      | .296      | .197      | .136      | .096     | .070       | .052      | .039      | .030      | .023      | .018      | .014     |
|    21 | .493      | .317      | .215      | .151      | .109     | .080       | .060      | .045      | .035      | .027      | .021      | .017     |
|    22 | .512      | .337      | .233      | .166      | .121     | .090       | .068      | .052      | .041      | .032      | .025      | .020     |
|    23 | .530      | .355      | .250      | .181      | .134     | .101       | .077      | .060      | .047      | .037      | .030      | .024     |
|    24 | .546      | .373      | .266      | .195      | .146     | .111       | .086      | .067      | .053      | .042      | .034      | .028     |
|    25 | .562      | .390      | .282      | .210      | .159     | .122       | .095      | .075      | .060      | .048      | .039      | .032     |
|    26 | .576      | .406      | .298      | .224      | .171     | .133       | .104      | .083      | .066      | .054      | .044      | .036     |
|    27 | .590      | .422      | .313      | .237      | .183     | .143       | .113      | .091      | .073      | .060      | .049      | .040     |
|    28 | .603      | .436      | .327      | .251      | .195     | .154       | .123      | .099      | .080      | .066      | .054      | .045     |
|    29 | .615      | .450      | .341      | .264      | .207     | .165       | .132      | .107      | .088      | .072      | .060      | .050     |
|    30 | .626      | .464      | .355      | .277      | .219     | .175       | .142      | .116      | .095      | .079      | .066      | .055     |
|    60 | .711      | .570      | .467      | .387      | .324     | .273       | .232      | .198      | .170      | .147      | .127      | .110     |
|    40 | .802      | .693      | .608      | .536      | .476     | .424       | .379      | .340      | .305      | .275      | .249      | .225     |
|    80 | .849      | .762      | .690      | .629      | .574     | .526       | .483      | .445      | .410      | .378      | .350      | .324     |
|   100 | .878      | .806      | .745      | .691      | .642     | .599       | .559      | .523      | .489      | .458      | .430      | .404     |
|   120 | .898      | .836      | .783      | .735      | .692     | .652       | .616      | .582      | .551      | .521      | .494      | .468     |
|   140 | .912      | .858      | .811      | .769      | .730     | .694       | .660      | .629      | .599      | .572      | .546      | .521     |
|   170 | .927 .938 | .882 .899 | .842 .864 | .806 .832 | .772     | .740       | .710 .748 | .682 .722 | .656 .698 | .630 .675 | .607 .653 | .584     |
|   200 | .948      | .915      | .886      | .858      | .803     | .774       |           |           | .741      | .721      | .701      | .632     |
|   240 |           |           |           |           | .833     | .808       | .785      | .763      |           |           |           | .682     |
|   320 | .961      | .936      | .913      | .892      | .872     | .852       | .834      | .816      | .799      | .782      | .766      | .750     |
|   440 | .972      | .953      | .936      | .920      | .905     | .890       | .876      | .862      | .849      | .836      | .823      | .811     |
|   600 | .979      | .965      | .953      | .941      | .930     | .918       | .908      | .897      | .887      | .877      | .867      | .857     |
|   800 | .984      | .974      | .964      | .955      | .947     | .938       | .930      | .922      | .914      | .906      | .898      | .891     |
|  1000 | .987      | .979      | .971      | .964      | .957     | .950       | .944      | .937      | .930      | .924      | .918      | .912     |

a Multiply entry by 10 -3 .

(continued)

572

Table A.9. ( Continued )

|   ν E | 1        | 2        | 3        | 4        | 5        | 6          | 7        | 8         | 9         | 10       | 11        | 12        |
|-------|----------|----------|----------|----------|----------|------------|----------|-----------|-----------|----------|-----------|-----------|
|     1 | .000     | .000     | .000     | .000     | .000     | p = 7 .000 | .000     | .000      | .000      | .000     | .000      | .000      |
|     2 | .000     | .000     | .000     | .000     | .000     | .000       | .000     | .000      | .000      | .000     | .000      | .000      |
|     3 | .000     | .000     | .000     | .000     | .000     | .000       | .000     | .000      | .000      | .000     | .000      | .000      |
|     4 | .000     | .000     | .000     | .000     | .000     | .000       | .000     | .000      | .000      | .000     | .000      | .000      |
|     5 | .000     | .000     | .000     | .000     | .000     | .000       | .000     | .000      | .000      | .000     | .000      | .000      |
|     6 | .043 a   | .006 a   | .002 a   | .001 a   | .001 a   | .000       | .000     | .000      | .000      | .000     | .000      | .000      |
|     7 | 2 . 62 a | .350 a   | .091 a   | .033 a   | .015 a   | .008 a     | .005 a   | .003 a    | .002 a    | .002 a   | .001 a    | .001 a    |
|     8 | .018     | 2 . 95 a | .809 a   | .292 a   | .126 a   | .063 a     | .034 a   | .020 a    | .013 a    | .009 a   | .006 a    | . 005 a   |
|     9 | .048     | .010     | 3 . 20 a | 1 . 22 a | .543 a   | .270 a     | .147 a   | .086 a    | .053 a    | .035 a   | .024 a    | .017 a    |
|    10 | .087     | .023     | 8 . 07 a | 3 . 34 a | 1 . 56 a | .798 a     | .440 a   | .259 a    | .160 a    | .104 a   | .070 a    | .049 a    |
|    11 | .128     | .040     | .016     | 6 . 97 a | 3 . 43 a | 1 . 83 a   | 1 . 04 a | .619 a    | .387 a    | .252 a   | .170 a    | .119 a    |
|    12 | .170     | .060     | .026     | .012     | 6 . 34 a | 3 . 51 a   | 2 . 05 a | 1 . 25 a  | .796 a    | .525 a   | .357 a    | .249 a    |
|    13 | .209     | .083     | .038     | .019     | .010     | 5 . 94 a   | 3 . 57 a | 2 . 23 a  | 1 . 45 a  | .967 a   | .665 a    | .468 a    |
|    14 | .246     | .106     | .052     | .027     | .015     | 9 . 17 a   | 5 . 67 a | 3 . 63 a  | 2 . 40 a  | 1 . 62 a | 1 . 13 a  | .804 a    |
|    15 | .281     | .129     | .067     | .037     | .022     | .013       | 8 . 37 a | 5 . 48 a  | 3 . 68 a  | 2 . 54 a | 1 . 79 a  | 1 . 28 a  |
|    16 | .313     | .153     | .083     | .047     | .029     | .018       | .012     | 7 . 80 a  | 5 . 34 a  | 3 . 73 a | 2 . 66 a  | 1 . 94 a  |
|    17 | .343     | .176     | .099     | .059     | .037     | .024       | .016     | .011      | 7 . 38 a  | 5 . 24 a | 3 . 78 a  | 2 . 78 a  |
|    18 | .370     | .199     | .116     | .071     | .045     | .030       | .020     | .014      | 9 . 81 a  | 7 . 06 a | 5 . 16 a  | 3 . 83 a  |
|    19 | .396     | .221     | .133     | .083     | .054     | .037       | .025     | .018      | .013      | 9 . 20 a | 6 . 80 a  | 5 . 10 a  |
|    20 | .420     | .242     | .149     | .096     | .064     | .044       | .031     | .022      | .016      | .012     | 8 . 72 a  | 6 . 60 a  |
|    21 | .442     | .263     | .166     | .109     | .074     | .052       | .037     | .026      | .019      | .014     | .011      | 8 . 34 a  |
|    22 | .462     | .283     | .183     | .123     | .085     | .060       | .043     | .031      | .023      | .018     | .013      | .010      |
|    23 | .482     | .301     | .199     | .136     | .095     | .068       | .050     | .037      | .028      | .021     | .016      | .013      |
|    24 | .499     | .320     | .215     | .149     | .106     | .077       | .057     | .042      | .032      | .025     | .019      | .015      |
|    25 | .516     | .337     | .230     | .162     | .117     | .086       | .064     | .048      | .037      | .029     | .022      | .018      |
|    26 | .532     | .354     | .246     | .175     | .128     | .095       | .071     | .055      | .042      | .033     | .026      | .020      |
|    27 | .547     | .370     | .260     | .188     | .139     | .104       | .079     | .061      | .047      | .037     | .029      | .024      |
|    28 | .561     | .385     | .275     | .201     | .150     | .113       | .087     | .068      | .053      | .042     | .033      | .027      |
|    29 | .574     | .399     | .289     | .214     | .161     | .123       | .095     | .074      | .059      | .047     | .037      | .030      |
|    30 | .586     | .413     | .302     | .226     | .172     | .132       | .103     | .081      | .064      | .052     | .042      | .034      |
|    40 | .679     | .526     | .417     | .335     | .273     | .224       | .185     | .154 .288 | .128      | .108     | .091      | .077      |
|    60 | .779     | .660     | .566     | .490     | .426     | .373       | .327     |           | .254      | .225     | .200      | .178      |
|    80 | .832     | .735     | .656     | .588     | .530     | .479       | .434     | .394      | .358      | .326     | .298      | .272      |
|   100 | .864     | .783     | .715     | .656     | .603     | .556       | .513     | .475      | .439      | .408     | .378      | .352      |
|   120 | .886     | .817     | .757     | .704     | .657     | .613       | .574     | .537      | .504      | .473     | .444      | .418      |
|   140 | .902     | .841     | .788     | .741     | .698     | .658       | .621     | .587      | .556      | .526     | .498      | .472      |
|   170 | .919     | .868     | .823     | .782     | .744     | .709       | .676     | .645      | .616 .662 | .589     | .563 .613 | .539 .590 |
|   200 | .931     | .887     | .848     | .812     | .778     | .747       | .717     | .689      |           | .637     |           |           |
|   240 | .942     | .905     | .871     | .841     | .812     | .784       | .758     | .733      | .709      | .687     | .665      | .644      |
|   320 | .957     | .928     | .902     | .878     | .855     | .833       | .812     | .792      | .773      | .754     | .736      | .719      |
|   440 | .968     | .947     | .928     | .910     | .893     | .876       | .860     | .844      | .829      | .814     | .800      | .786      |
|   600 | .977     | .961     | .947     | .933     | .920     | .908       | .895     | .883      | .872      | .860     | .849      | .838      |
|   800 | .982     | .971     | .960     | .950     | .940     | .930       | .920     | .911      | .902      | .893     | .884      | .876      |
|  1000 | .986     | .977     | .968     | .959     | .951     | .943       | .936     | .928      | .921      | .914     | .906      | .899      |

a Multiply entry by 10 -3 .

TABLES Table A.9. ( Continued )

| ν E   | 1         | 2         | 3         | 4         | 5         | 6          | 7        | 8              | 9 10                     | 11        | 12        |
|-------|-----------|-----------|-----------|-----------|-----------|------------|----------|----------------|--------------------------|-----------|-----------|
| 1     | .000      | .000      | .000      | .000      | .000      | p = 8 .000 | .000     | .000           | .000 .000                | .000      | .000      |
| 2     | .000      | .000      | .000      | .000      | .000      | .000       | .000     | .000           | .000 .000                | .000      | .000      |
| 3     | .000      | .000      | .000      | .000      | .000      | .000       | .000     | .000           | .000 .000                | .000      | .000      |
| 4     | .000      | .000      | .000      | .000      | .000      | .000       | .000     | .000           | .000 .000                | .000      | .000      |
| 5     | .000      | .000      | .000      | .000      | .000      | .000       | .000     | .000           | .000 .000                | .000      | .000      |
| 6     | .000      | .000      | .000      | .000      | .000      | .000       | .000     | .000           | .000 .000                | .000      | .000      |
| 7     | .138 a    | .015 a    | .004 a    | .001 a    | .001 a    | .000       | .000     | .000           | .000 .000                | .000      | .000      |
| 8     | 3 . 30 a  | .393 a    | .090 a    | .029 a    | .012 a    | .006 a     | .003 a   | .002 a         | .001 a .001 a            | .001 a    | .000      |
| 9     | .017      | 2 . 63 a  | .659 a    | .218 a    | .087 a    | .040 a     | .020 a   | .011 a         | .007 a .004 a            | .003 a    | .002 a    |
| 10    | .044      | 8 . 63 a  | 2 . 46 a  | .872 a    | .361 a    | .168 a     | .086 a   | .047 a         | .028 a .017 a            | .011 a    | .008 a    |
| 11    | .078      | .019      | 6 . 15 a  | 2 . 36 a  | 1 . 03 a  | .497 a     | .259 a   | .144 a         | .085 a .052 a            | .034 a    | .023 a    |
| 12    | .116      | .033      | .012      | 4 . 99 a  | 2 . 30 a  | 1 . 16 a   | .619 a   | .351 a         | .209 a .130 a            | .084 a    | .056 a    |
| 13    | .154      | .051      | .020      | 8 . 91 a  | 4 . 34 a  | 2 . 26 a   | 1 . 25 a | .727 a         | .441 a .278 a            | .181 a    | .122 a    |
| 14    | .190      | .070      | .030      | .014      | 7 . 22 a  | 3 . 92 a   | 2 . 23 a | 1 . 33 a       | .824 a .527 a            | .347 a    | .235 a    |
| 15    | .225      | .090      | .041      | .021      | .011      | 6 . 17 a   | 3 . 63 a | 2 . 22 a       | a .910 a                 | .608 a    | .416 a    |
| 16    | .258      | .111      | .054      | .028      | .016      | 9 . 06 a   | 5 . 48 a | 3 . 42 a       | 1 . 40 2 . 20 a 1 . 46 a | .987 a    | .683 a    |
| 17    | .289      | .133      | .067      | .037      | .021      | .013       | 7 . 80 a | 4 . 98 a 3 .   | 27 a 2 . 20 a            | 1 . 51 a  | 1 . 06 a  |
| 18    | .318      | .154      | .082      | .046      | .027      | .017       | .011     | 6 . 92 a 4 .   | 62 a 3 . 15 a            | 2 . 19 a  | 1 . 56 a  |
| 19    | .345      | .175      | .096      | .056      | .034      | .021       | .014     | 9 . 23 a 6 .   | 26 a 4 . 34 a            | 3 . 06 a  | 2 . 19 a  |
| 20    | .370      | .195      | .111      | .067      | .042      | .027       | .018     | .012 8 .       | 22 a 5 . 77 a            | 4 . 12 a  | 2 . 99 a  |
| 21    | .393      | .215      | .127      | .078      | .050      | .033       | .022     | .015           | .010 7 . 46 a            | 5 . 39 a  | 3 . 95 a  |
| 22    | .415      | .235      | .142      | .089      | .058      | .039       | .026     | .018           | .013 9 . 40 a            | 6 . 86 a  | 5 . 08 a  |
| 23    | .436      | .254      | .157      | .101      | .067      | .045       | .031     | .022           | .016 .012                | 8 . 56 a  | 6 . 39 a  |
| 24    | .455      | .272      | .172      | .113      | .076      | .052       | .037     | .026           | .019 .014                | .010      | 7 . 88 a  |
| 25    | .473      | .289      | .187      | .124      | .085      | .060       | .042     | .031           | .023 .017                | .013      | 9 . 56 a  |
| 26    | .490      | .306      | .201      | .136      | .095      | .067       | .048     | .035           | .026 .020                | .015      | .011      |
| 27    | .505      | .322      | .215      | .148      | .104      | .075       | .055     | .040           | .030 .023                | .017      | .013      |
| 28    | .520      | .338      | .229      | .160      | .114      | .083       | .061     | .045           | .034 .026                | .020      | .016      |
| 29    | .534      | .353      | .243      | .172      | .124      | .091       | .068     | .039           | .030                     | .023      | .018      |
| 30    | .548      | .367      | .256      | .183      | .134      | .099       | .074     | .051 .056      | .043 .034                | .026      | .021      |
| 40    | .649      | .485      | .372      | .290      | .229      | .182       | .146     | .118           | .096 .079                | .065      | .054      |
| 60    | .758      | .627      | .527      | .447      | .381      | .327       | .282     | .244           | .212 .184                | .161      | .141      |
| 80    | .815      | .709      | .623      | .551      | .489      | .435       | .389     | .348           | .313 .281                | .253      | .229      |
| 100   | .851      | .761      | .687      | .622      | .566      | .516       | .471     | .431           | .362                     | .333      |           |
| 120   | .875      | .798      | .732      | .675      | .623      | .577       | .535     | .395 .496 .461 | .429                     | .399      | .306      |
| 140   | .892      |           |           |           |           |            |          | .549           | .484                     |           | .372      |
|       | .911      | .825      | .767      | .715      | .667      | .625       | .585     | .610           | .515 .550                | .455      | .428      |
| 170   |           | .854      | .804      | .759      | .717      | .679       | .644     |                | .579 .602                | .523 .576 | .497      |
| 200   | .924      | .875      | .831      | .791      | .755      | .720       | .688     | .657           | .629 .655                |           | .551      |
| 240   | .936      | .895      | .858      | .823      | .791      | .761       | .732     | .705           | .679 .748                | .631      | .609 .689 |
| 320   | .952 .965 | .920 .942 | .891 .920 | .865 .900 | .839 .880 | .815 .862  | .792     | .770 .827      | .728 .810 .794           | .708 .778 | .762      |
| 600   |           |           | .941      | .926      |           |            | .844     | .870           |                          |           | .819      |
| 440   | .974      | .957      |           |           | .911      | .897       | .883     |                | .857 .844                | .831      |           |
| 800   | .981      | .968      | .955      | .944      | .933      | .922       | .911     | .901           | .890 .880                | .871      | .861      |
| 1000  | .985      | .974      | .964      | .955      | .946      | .937       | .928     | .920           | .911 .903                | .895      | .887      |

a Multiply entry by 10 -3 .

Table A.10. Upper Critical Values for Roy's Test, α = . 05

Roy's test statistic is given by

<!-- formula-not-decoded -->

where λ 1 is the largest eigenvalue of E -1 H . The parameters are

<!-- formula-not-decoded -->

Reject H 0 if θ &gt; table value.

|       | m     | m     | m     | m     | m     | m     | m     | m     | m     |
|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|
| N     | 0     | 1     | 2     | 3     | 4     | 5     | 7     | 10    | 15    |
| s = 2 | s = 2 | s = 2 | s = 2 | s = 2 | s = 2 | s = 2 | s = 2 | s = 2 | s = 2 |
| 5     | .565  | .651  | .706  | .746  | .776  | .799  | .834  | .868  | .901  |
| 10    | .374  | .455  | .514  | .561  | .598  | .629  | .679  | .732  | .789  |
| 15    | .278  | .348  | .402  | .446  | .483  | .515  | .567  | .627  | .696  |
| 20    | .221  | .281  | .329  | .369  | .404  | .434  | .486  | .546  | .620  |
| 25    | .184  | .236  | .278  | .314  | .346  | .375  | .424  | .484  | .558  |
| 30    | .157  | .203  | .241  | .274  | .303  | .330  | .376  | .433  | .507  |
| 40    | .122  | .159  | .190  | .218  | .243  | .266  | .306  | .359  | .428  |
| 50    | .099  | .130  | .157  | .180  | .202  | .222  | .259  | .306  | .370  |
| 60    | .084  | .110  | .133  | .154  | .173  | .191  | .223  | .266  | .326  |
| 80    | .064  | .085  | .103  | .119  | .135  | .149  | .176  | .211  | .263  |
| 120   | .043  | .058  | .070  | .082  | .093  | .104  | .123  | .150  | .190  |
| 240   | .022  | .030  | .036  | .042  | .048  | .054  | .065  | .080  | .103  |
| s = 3 | s = 3 | s = 3 | s = 3 | s = 3 | s = 3 | s = 3 | s = 3 | s = 3 | s = 3 |
| 5     | .669  | .729  | .770  | .800  | .822  | .840  | .867  | .894  | .920  |
| 10    | .472  | .537  | .586  | .625  | .656  | .683  | .725  | .770  | .819  |
| 15    | .362  | .422  | .469  | .508  | .541  | .569  | .616  | .669  | .730  |
| 20    | .293  | .346  | .390  | .427  | .458  | .486  | .533  | .589  | .656  |
| 25    | .246  | .294  | .333  | .367  | .397  | .424  | .470  | .525  | .594  |
| 30    | .212  | .255  | .291  | .322  | .350  | .375  | .419  | .473  | .543  |
| 40    | .166  | .201  | .232  | .259  | .283  | .305  | .345  | .395  | .462  |
| 50    | .136  | .167  | .192  | .216  | .237  | .257  | .292  | .339  | .402  |
| 60    | .116  | .142  | .164  | .185  | .204  | .221  | .254  | .296  | .355  |
| 80    | .089  | .109  | .127  | .144  | .160  | .174  | .201  | .237  | .288  |
| 120   | .061  | .075  | .088  | .100  | .111  | .122  | .142  | .169  | .209  |
| 240   | .031  | .039  | .046  | .052  | .058  | .064  | .075  | .090  | .114  |

TABLES Table A.10. ( Continued )

| N     | 0     | 1     | 2     | 3     | 4     | 5     | 7     | 10    | 15    |
|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|
| s = 4 | s = 4 | s = 4 | s = 4 | s = 4 | s = 4 | s = 4 | s = 4 | s = 4 | s = 4 |
| 5     | .739  | .782  | .813  | .836  | .854  | .868  | .889  | .911  | .933  |
| 10    | .547  | .601  | .641  | .674  | .700  | .723  | .759  | .798  | .840  |
| 15    | .431  | .482  | .523  | .558  | .587  | .612  | .654  | .701  | .756  |
| 20    | .354  | .402  | .441  | .474  | .503  | .529  | .572  | .623  | .684  |
| 25    | .301  | .344  | .380  | .412  | .440  | .464  | .507  | .559  | .624  |
| 30    | .261  | .301  | .334  | .364  | .390  | .414  | .455  | .507  | .572  |
| 40    | .207  | .240  | .269  | .294  | .318  | .339  | .377  | .426  | .490  |
| 50    | .171  | .199  | .224  | .247  | .268  | .287  | .322  | .367  | .428  |
| 60    | .145  | .170  | .193  | .213  | .232  | .249  | .280  | .322  | .380  |
| 80    | .112  | .132  | .150  | .167  | .182  | .196  | .223  | .259  | .309  |
| 120   | .077  | .091  | .104  | .116  | .127  | .138  | .158  | .185  | .226  |
| 240   | .040  | .047  | .054  | .061  | .067  | .073  | .084  | .100  | .124  |
| s = 5 | s = 5 | s = 5 | s = 5 | s = 5 | s = 5 | s = 5 | s = 5 | s = 5 | s = 5 |
| 5     | .788  | .821  | .845  | .863  | .877  | .888  | .906  | .924  | .942  |
| 10    | .607  | .651  | .685  | .713  | .735  | .755  | .786  | .820  | .857  |
| 15    | .488  | .533  | .569  | .599  | .625  | .648  | .685  | .728  | .777  |
| 20    | .407  | .449  | .485  | .515  | .542  | .565  | .604  | .651  | .708  |
| 25    | .349  | .388  | .422  | .451  | .477  | .500  | .540  | .588  | .648  |
| 30    | .305  | .341  | .373  | .400  | .425  | .448  | .487  | .535  | .597  |
| 40    | .243  | .275  | .302  | .327  | .349  | .370  | .406  | .453  | .514  |
| 50    | .202  | .230  | .254  | .276  | .296  | .315  | .348  | .392  | .451  |
| 60    | .173  | .197  | .219  | .238  | .257  | .274  | .304  | .345  | .401  |
| 80    | .134  | .154  | .171  | .188  | .203  | .217  | .243  | .278  | .329  |
| 120   | .093  | .107  | .120  | .132  | .143  | .154  | .174  | .201  | .241  |
| 240   | .048  | .056  | .063  | .069  | .076  | .082  | .093  | .109  | .134  |

( continued )

576 TABLES Table A.10. ( Continued )

| N     | 0     | 1     | 2     | 3     | 4     | 5     | 7     | 10    | 15    |
|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|
| s = 6 | s = 6 | s = 6 | s = 6 | s = 6 | s = 6 | s = 6 | s = 6 | s = 6 | s = 6 |
| 5     | .825  | .850  | .869  | .883  | .895  | .904  | .918  | .934  | .949  |
| 10    | .655  | .692  | .721  | .744  | .764  | .781  | .808  | .838  | .871  |
| 15    | .537  | .576  | .608  | .635  | .658  | .678  | .711  | .750  | .795  |
| 20    | .454  | .491  | .523  | .551  | .575  | .596  | .632  | .676  | .728  |
| 25    | .392  | .428  | .458  | .485  | .509  | .531  | .568  | .613  | .669  |
| 30    | .345  | .378  | .407  | .433  | .457  | .478  | .514  | .560  | .618  |
| 40    | .278  | .307  | .333  | .356  | .378  | .397  | .432  | .477  | .536  |
| 50    | .232  | .258  | .281  | .302  | .322  | .340  | .372  | .414  | .472  |
| 60    | .200  | .223  | .243  | .262  | .280  | .297  | .327  | .366  | .421  |
| 80    | .156  | .174  | .192  | .208  | .222  | .236  | .262  | .297  | .346  |
| 120   | .108  | .122  | .134  | .146  | .157  | .168  | .188  | .215  | .255  |
| 240   | .056  | .064  | .071  | .078  | .084  | .090  | .101  | .118  | .142  |
| s = 7 | s = 7 | s = 7 | s = 7 | s = 7 | s = 7 | s = 7 | s = 7 | s = 7 | s = 7 |
| 5     | .852  | .872  | .887  | .899  | .908  | .917  | .929  | .941  | .955  |
| 10    | .695  | .726  | .750  | .771  | .788  | .802  | .826  | .853  | .882  |
| 15    | .579  | .613  | .641  | .665  | .686  | .704  | .734  | .769  | .810  |
| 20    | .494  | .528  | .557  | .582  | .604  | .624  | .657  | .697  | .745  |
| 25    | .431  | .463  | .491  | .516  | .538  | .558  | .593  | .635  | .688  |
| 30    | .381  | .412  | .439  | .463  | .485  | .505  | .540  | .583  | .638  |
| 40    | .309  | .337  | .362  | .384  | .404  | .423  | .456  | .499  | .555  |
| 60    | .224  | .246  | .266  | .285  | .302  | .318  | .347  | .386  | .439  |
| 80    | .176  | .194  | .211  | .226  | .241  | .255  | .280  | .314  | .363  |
| 100   | .145  | .160  | .175  | .188  | .200  | .212  | .235  | .265  | .310  |
| 200   | .077  | .085  | .093  | .101  | .109  | .116  | .129  | .148  | .175  |
| 300   | .052  | .058  | .064  | .069  | .074  | .079  | .089  | .103  | .125  |
| 500   | .032  | .036  | .039  | .042  | .046  | .049  | .055  | .064  | .078  |
| 1000  | .016  | .018  | .020  | .022  | .023  | .025  | .028  | .033  | .041  |

TABLES Table A.10. ( Continued )

| N    | 0    | 1    | 2         | 3        | 4              | 5 7   | 10   | 15   |
|------|------|------|-----------|----------|----------------|-------|------|------|
|      |      |      |           |          | 8              |       |      |      |
| 5    | .874 | .890 | .902      | s = .912 | .920 .927      | .937  | .948 | .959 |
| 10   | .728 | .754 | .775      | .793     | .821           | .842  | .865 | .892 |
| 15   | .615 | .645 | .670      | .692     | .808 .710 .727 | .754  | .786 | .824 |
| 20   | .531 | .561 | .587      | .610     | .630 .648      | .679  | .716 | .761 |
| 25   | .466 | .495 | .521      | .544     | .565 .583      | .616  | .655 | .705 |
| 30   | .414 | .443 | .468      | .491     | .511           | .563  | .603 | .655 |
| 40   | .339 | .365 | .388      | .409     | .530 .428 .446 | .478  | .519 | .573 |
| 60   | .248 | .269 | .288      | .306     | .323 .338      | .367  | .404 | .456 |
| 80   | .195 | .213 | .229      | .244     | .259 .272      | .297  | .330 | .378 |
| 100  | .161 | .176 | .190      | .203     | .216 .228      | .250  | .279 | .323 |
| 200  | .086 | .094 | .103      | .110     | .118 .125      | .138  | .157 | .185 |
| 300  | .058 | .065 | .070      | .076     | .081 .086      | .096  | .109 | .130 |
| 500  | .036 | .040 | .043      | .047     | .050 .053      | .059  | .068 | .081 |
| 1000 | .018 | .020 | .022      | .024     | .025 .027      | .030  | .035 | .042 |
| 5    | .891 | .904 | .914      | s = .922 | 9 .929 .935    | .944  | .953 | .963 |
| 10   | .756 | .778 | .797      | .812     | .825 .837      | .855  | .876 | .901 |
| 15   | .647 | .674 | .696      | .715     | .732 .747      | .771  | .801 | .835 |
| 20   | .563 | .591 | .614      | .635     | .654 .670      | .698  | .733 | .775 |
| 25   | .497 | .525 | .549      | .570     | .589 .606      | .636  | .673 | .720 |
| 30   | .445 | .471 | .495      | .516     | .535           | .583  | .622 | .671 |
| 40   | .366 | .391 | .413      | .433     | .552 .451 .468 | .499  | .538 | .590 |
| 60   | .270 | .291 | .309      | .326     | .343 .358      | .385  | .421 | .472 |
| 80   | .214 | .231 | .247      | .262     | .276 .289      | .313  | .346 | .392 |
| 100  | .177 | .192 | .206      | .219     | .231 .242      | .264  | .293 | .336 |
| 200  | .095 | .104 | .112      | .119     | .127 .134      | .147  | .166 | .194 |
| 300  | .065 | .071 | .077      | .082     | .087 .092      | .102  | .115 | .136 |
| 500  | .040 | .043 | .047      | .051     | .054           | .063  | .072 | .086 |
| 1000 | .020 | .022 | .024      | .026     | .057 .028 .029 | .032  | .037 | .044 |
| 10   |      |      |           | s =      |                |       |      |      |
| 5    | .905 | .916 | .924      | .931     | .937 .941      | .949  | .958 | .967 |
| 10   | .780 | .799 | .815      | .829     | .840 .851      | .867  | .886 | .908 |
| 15   | .675 | .699 | .719      | .736     | .751 .764      | .787  | .814 | .846 |
| 20   | .592 | .617 | .639      | .658     | .675 .690      | .716  | .748 | .787 |
| 25   | .526 | .551 | .573      | .593     | .611 .627      | .655  | .690 | .734 |
| 30   | .473 | .497 | .519      | .539     | .557 .573      | .603  | .639 | .686 |
| 40   | .392 | .415 | .436      | .455     | .473 .489      | .518  | .555 | .605 |
| 60   | .292 | .311 | .329      | .346     | .361 .376      | .402  | .438 | .487 |
| 80   | .232 | .249 | .264      | .278     | .292 .305      | .329  | .361 | .406 |
| 100  | .193 | .207 | .220      | .233     | .245 .256      | .278  | .306 | .348 |
| 200  | .104 | .112 | .120 .083 | .128     | .135 .142      | .156  | .174 | .202 |
| 300  | .071 | .077 |           | .088     | .093 .098      | .108  | .122 | .143 |
| 500  | .044 | .047 | .051      | .054     | .058 .061      | .067  | .076 | .090 |
| 1000 | .022 | .024 | .026      | .028     | .030 .031      | .034  | .039 | .047 |

Table A.11. Upper Critical Values of Pillai's Statistic V ( s ) , α = . 05

V ( s ) = s ∑ i = 1 λ i 1 + λ i

where λ 1 , λ 2 , . . . , λ s are eigenvalues of E -1 H . Reject H 0 if V ( s ) exceeds table value. The parameters s , m , and N are defined in Table A.10.

| 25   | .218 .304 .379 .445 .506 .561 .612 .658 .702 .743 .781                                 |
|------|----------------------------------------------------------------------------------------|
| 20   | .263 .364 .451 .526 1.594 .655 .710 .761 .808 .851 .891                                |
| 15   | .333 .455 .556 .643 .719 .786 .846 .901 .950 .995 1.036                                |
| 10   | .451 .604 .725 .825 .910 .983 1.046 1.102 1.151 1.195 1.235 1.386 1.487 1.559          |
| 9    | .485 .646 .772 .875 .961 1.034 1.098 1.153 1.202 1.245 1.284 1.430 1.527 1.595         |
| 8    | .526 .694 .825 .930 1.018 1.091 1.154 1.209 1.257 1.299 1.337 1.477 1.568 1.632        |
| 7    | .573 .751 .886 .993 1.081 1.155 1.217 1.270 1.317 1.358 1.394 1.527 1.612 1.671        |
| 6    | s = 2 .629 .817 .956 1.065 1.153 1.226 1.286 1.338 1.383 1.422 1.456 1.580 1.658 1.711 |
| 5    | .698 .896 1.039 1.149 1.235 1.305 1.364 1.413 1.455 1.491 1.523 1.636 1.706 1.753      |
| 4    | .782 .991 1.137 1.245 1.329 1.395 1.450 1.495 1.534 1.567 1.595 1.695 1.756 1.796      |
| 3    | .890 1.109 1.254 1.358 1.436 1.497 1.546 1.586 1.620 1.649 1.673 1.758 1.807 1.840     |
| 2    | 1.031 1.258 1.397 1.492 1.560 1.613 1.654 1.687 1.714 1.737 1.757 1.822 1.860 1.885    |
| 1    | 1.232 1.452 1.573 1.649 1.703 1.742 1.772 1.796 1.815 1.831 1.844 1.888 1.913 1.929    |
| 0    | 1.536 1.706 1.784 1.829 1.859 1.880 1.895 1.907 1.917 1.924 1.931 1.951 1.963 1.969    |
| m    | 0 1 2 3 4 5 6 7 8 9 10 15 20 25                                                        |

s = 3 0 2.037 1.710 1.473 1.294 1.153 1.040 .947 .869 .803 .746 .697 .524 .420 .350 1 2.297 1.988 1.751 1.564 1.412 1.287 1.183 1.094 1.017 .950 .892 .682 .552 .453 2 2.447 2.168 1.943 1.759 1.606 1.477 1.367 1.273 1.190 1.117 1.053 .818 .668 .565 3 2.544 2.294 2.084 1.907 1.757 1.628 1.517 1.420 1.334 1.258 1.190 .937 .772 .656 4 2.612 2.386 2.191 2.023 1.878 1.752 1.641 1.543 1.456 1.378 1.308 1.042 .866 .740 5 2.662 2.457 2.276 2.117 1.978 1.854 1.745 1.648 1.561 1.482 1.411 1.137 .952 .818 6 2.701 2.514 2.345 2.194 2.061 1.941 1.835 1.739 1.652 1.573 1.502 1.222 1.030 .890 7 2.732 2.559 2.402 2.259 2.131 2.016 1.912 1.818 1.732 1.654 1.582 1.300 1.103 .957 8 2.757 2.597 2.449 2.314 2.192 2.081 1.979 1.887 1.803 1.726 1.655 1.371 1.170 1.020 9 2.777 2.629 2.490 2.362 2.244 2.137 2.039 1.949 1.866 1.790 1.720 1.436 1.23 10 2.795 2.656 2.525 2.403 2.291 2.187 2.092 2.004 1.923 1.848 1.779 1.496 1.3 15 2.853 2.748 2.646 2.549 2.457 2.370 2.288 2.211 21.39 2.071 2.007 20 2.885 2.802 2.718 2.637 2.560 2.485 2.414 2.347 2.283 2.222 2.163 25 2.906 2.836 2.766 2.697 2.630 2.565 2.503 2.443 2.385

( continued )

Table A.11. ( Continued )

| 25   |                                                                                       |
|------|---------------------------------------------------------------------------------------|
| 20   | .602 .761 .903 1.032 1.149                                                            |
| 15   | .744 .932 1.097 1.243 1.375 1.494 1.602 1.70 1.8                                      |
| 10   | .974 1.203 1.396 1.564 1.710 1.840 1.955 2.058 2.151 2.236 2.313 2.615                |
| 9    | 1.038 1.277 1.477 1.649 1.798 1.929 2.044 2.148 2.241 2.325 2.401 2.696               |
| 8    | 1.112 1.360 1.567 1.743 1.895 2.027 2.143 2.246 2.338 2.421 2.496 2.783               |
| 7    | 1.196 1.456 1.670 1.849 2.002 2.135 2.251 2.353 2.444 2.525 2.598 2.876               |
| N    | 6 s = 4 1.294 1.566 1.786 1.969 2.123 2.255 2.370 2.470 2.559 2.638 2.708 2.974 3.149 |
| 5    | 1.410 1.693 1.919 2.104 2.259 2.390 2.502 2.600 2.685 2.761 2.829 3.079 3.241         |
| 4    | 1.548 1.844 2.074 2.260 2.413 2.541 2.649 2.743 2.824 2.896 2.959 3.191 3.338         |
| 3    | 1.717 2.023 2.256 2.440 2.589 2.711 2.814 2.902 2.977 3.043 3.101 3.310 3.440         |
| 2    | 1.926 2.241 2.472 2.650 2.791 2.905 2.999 3.079 3.146 3.205 3.256 3.436 3.546         |
| 1    | 2.194 2.510 2.733 2.898 3.025 3.126 3.208 3.276 3.333 3.382 3.424 3.570 3.657         |
| 0    | 2.549 2.852 3.052 3.193 3.298 3.378 3.442 3.494 3.537 3.574 3.605 3.710 3.771         |
| m    | 0 1 2 3 4 5 6 7 8 9 10 15 20                                                          |

Table A.11. ( Continued )

| 25   |                                                             |                                                                         |
|------|-------------------------------------------------------------|-------------------------------------------------------------------------|
| 20   |                                                             |                                                                         |
| 15   |                                                             |                                                                         |
| 10   | 1.277 1.533 1.754 1.948 2.119 2.271 2.408                   | 1.601 1.881 2.125 2.341 2.534 2.706                                     |
| 9    | 1.356 1.622 1.850 2.048 2.222 2.377 2.514                   | 1.694 1.984 2.235 2.456 2.652 2.827                                     |
| 8    | 1.445 1.722 1.957 2.160 2.337 2.492 2.630                   | 1.799 2.099 2.358 2.583 2.782 2.959                                     |
| N 7  | 1.547 1.835 2.077 2.284 2.463 2.619 2.758 2.880             | 1.918 2.229 2.494 2.724 2.925 3.103                                     |
| 6    | s = 5 1.664 1.964 2.213 2.423 2.604 2.760 2.897 3.018 3.126 | s = 6 2.053 2.375 2.647 2.881 3.084 3.262                               |
| 5    | 1.801 2.122 2.367 2.580 2.761 2.916 3.052 3.170 3.275 3.369 | 3.45 2.209 2.542 2.821 3.057 3.260 3.438                                |
| 4    | 1.962 2.285 2.545 2.759 2.938 3.091 3.223 3.337 3.438 3.527 | 3.607 2.390 2.734 3.018 3.256 3.458 3.633 3.785 3.919                   |
| 3    | 2.155 2.488 2.751 2.964 3.140 3.288 3.414 3.522 3.617 3.700 | 3.774 2.604 2.958 3.245 3.482 3.681 3.851 3.998 4.126 4.239             |
| 2    | 2.389 2.731 2.993 3.201 3.370 3.510 3.627 3.728 3.815 3.890 | 3.957 2.859 3.221 3.508 3.741 3.934 4.097 4.236 4.356 4.461 4.553 4.635 |
| 1    | 2.681 3.025 3.281 3.478 3.635 3.762 3.868 3.957 4.033 4.099 | 4.156 3.171 3.535 3.817 4.041 4.223 4.375 4.502 4.611 4.706 4.788 4.860 |
| 0    | 3.055 3.390 3.628 3.805 3.941 4.050 4.138 4.212 4.274 4.327 | 4.372 3.559 3.917 4.185 4.391 4.556 4.690 4.802 4.896 4.976 5.045 5.106 |
| m    | 0 1 2 3 4 5 6 7 8 9                                         | 10 0 1 2 3 4 5 6 7 8 9 10                                               |

Table A.12. Upper Critical Values for the Lawley-Hotelling Test Statistic, α = . 05

The test statistic is ν EU ( s ) /ν H , where U ( s ) is the Lawley-Hotelling statistic. Reject H 0 if ν EU ( s ) /ν H &gt; table value.

∞

<!-- image -->

| 60     | 12.461 59.855 21.582 12.909 9.4610 7.6773 6.6048 5.3910 4.7274 4.3105 4.0243 3.8158 3.6569 3.3868 3.2168 3.1000 3.0140 2.8965 2.8196 2.7652 2.7247 2.6683 2.5559 2.4428     |
|--------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 40     | 12.409 59.830 21.636 12.972 9.5251 7.7417 6.6694 5.4564 4.7938 4.3780 4.0930 3.8855 3.7278 3.4605 3.2926 3.1774 3.0933 2.9787 2.9041 2.8516 2.8126 2.7586 2.6520 2.5470     |
| 25     | 12.316 59.785 21.733 13.083 9.6381 7.8549 6.7826 5.5701 4.9085 4.4939 4.2102 4.0042 3.8477 3.5832 3.4181 3.3051 3.2230 3.1115 3.0392 2.9886 2.9512 2.8994 2.7984 2.7002     |
| 20     | 12.254 59.755 21.797 13.156 9.7118 7.9285 6.8560 5.6433 4.9820 4.5678 4.2846 4.0791 3.9231 3.6598 3.4957 3.3836 3.3022 3.1919 3.1206 3.0706 3.0338 2.9829 2.8838 2.7879     |
| 15     | 12.153 59.705 21.901 13.275 9.8320 8.0480 6.9748 5.7612 5.0997 4.6856 4.4028 4.1976 4.0420 3.7798 3.6166 3.5054 3.4247 3.3154 3.2450 3.1957 3.1594 3.1093 3.0120 2.9182     |
| 12     | 12.052 59.655 22.003 13.391 9.9489 8.16399 7.0896 5.8745 5.2122 4.7977 4.5147 4.3094 4.1539 3.8919 3.7291 3.6181 3.5377 3.4289 3.3588 3.3099 3.2737 3.2240 3.1275 3.0346    |
| ν H 10 | = 2 11.804 59.606 22.104 13.504 10.063 8.2765 7.2008 5.9837 5.3200 4.9048 4.6213 4.4157 4.2600 3.9977 3.8347 3.7237 3.6433 3.5346 3.4646 3.4157 3.3796 3.3300 3.2336 3.1410 |
| 8      | 11.952 59.531 22.250 13.670 10.228 8.4396 7.3614 6.1405 5.4744 5.0574 4.7727 4.5663 4.4099 4.1465 3.9829 3.8715 3.7908 3.6817 3.6114 3.5624 3.5262 3.4764 3.3798 3.2870     |
| 6      | 11.562 59.407 22.484 13.934 10.491 8.6975 7.6145 6.3860 5.7147 5.2941 5.0067 4.7982 4.6402 4.3740 4.2086 4.0959 4.0143 3.9039 3.8328 3.7831 3.7465 3.6961 3.5983 3.5044     |
| 5      | 11.373 59.308 22.663 14.135 10.691 8.8927 7.8054 6.5702 5.8942 5.4703 5.1804 4.9700 4.8105 2.5415 4.3743 4.2604 4.1778 4.0661 3.9941 3.9439 3.9068 3.8557 3.7567 3.6614     |
| 4      | 11.098 59.161 22.918 14.422 10.975 9.1694 8.0752 6.8294 6.1461 4.7168 5.4230 5.2095 5.0475 4.7741 4.6040 4.8880 4.4039 4.2900 4.2166 4.1653 4.1275 4.0754 3.9742 3.8769     |
| 3      | 10.659 58.915 23.312 14.864 11.411 9.5937 8.4881 7.2243 6.5284 6.0902 5.7895 5.5708 5.4046 5.1237 4.9487 4.8291 4.7424 4.6249 4.5490 4.4960 4.4569 4.4030 4.2982 4.1973     |
| 2      | 9.8591 58.428 23.999 15.639 12.175 10.334 9.2069 7.9095 7.1902 6.7350 6.4217 6.1932 6.0192 5.7244 5.5401 5.4140 5.3224 5.1981 5.1178 5.0616 5.0200 4.9628 4.8514 4.7442     |

| 30.31   | 1.2068 39.366 22.190 15.653 12.375 9.1955 7.6777 6.7991 6.2287 5.8292 5.5341 5.0503 4.7575 4.5608 4.4195 4.2297 4.1078 4.0227 3.9600 3.8734 3.7042 3.5384   |
|---------|-------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 30.19   | 1.2063 39.462 22.294 15.755 12.473 9.2897 7.7700 6.8908 6.3204 5.9212 5.6266 5.1446 4.8535 4.6586 4.5189 4.3319 4.2123 4.1292 4.0680 3.9840 3.8212 3.6642   |
| 29.959  | 1.2054 39.635 22.479 15.934 12.646 9.4541 7.9301 7.0488 6.4774 6.0780 5.7834 5.3025 5.0129 4.8195 4.6813 4.4968 4.3793 4.2979 4.2381 4.1563 3.9988 3.8487   |
| 29.809  | 1.2048 39.750 22.600 16.051 12.758 9.5603 8.0330 7.1497 6.5772 6.1771 5.8822 5.4010 5.1116 4.9185 4.7806 4.5967 4.4798 4.3988 4.3395 4.2583 4.1023 3.9541   |
| 29.561  | 1.2038 39.937 22.799 16.241 12.941 9.7320 8.1982 7.3110 6.7360 6.3343 6.0383 5.5555 5.2654 5.0720 4.9340 4.7502 4.6334 4.5526 4.4935 4.4126 4.2574 4.1104   |
| 29.316  | 1.2028 40.120 22.992 16.427 13.118 9.8974 8.3566 7.4649 6.8868 6.4830 6.1853 5.7001 5.4085 5.2143 5.0757 4.8911 4.7739 4.6929 4.6336 4.5525 4.3970 4.2499   |
| 29.073  | 1.2018 40.300 23.182 16.608 13.290 10.057 8.5088 7.6122 7.0307 6.6244 6.3249 5.8365 5.5431 5.3476 5.2081 5.0224 4.9044 4.8229 4.7632 4.6817 4.5252 4.3773   |
| 28.712  | 1.2003 40.562 23.458 16.870 13.540 10.287 8.7271 7.8225 7.2355 6.8251 6.5224 6.0287 5.7319 5.5341 5.3929 5.2050 5.0856 5.0031 4.9426 4.8601 4.7017 4.5519   |
| 28.125  | 1.1978 40.983 23.899 17.288 13.934 10.649 9.0680 8.1495 7.5526 7.1347 6.8263 6.3227 6.0196 5.8175 5.6732 5.4809 5.3587 5.2742 5.2122 5.1276 4.9653 4.8116   |
| 27.665  | 1.1959 1.305 24.235 17.605 14.233 10.921 9.3234 8.3935 7.7884 7.3644 7.0513 6.5394 6.2311 6.0253 5.8783 5.6823 5.5577 5.4715 5.4084 5.3220 5.1562 4.9992    |
| 26.996  | 1.1929 41.764 24.715 18.056 14.657 11.306 9.6825 8.7356 8.1183 7.6851 7.3649 6.8407 6.5245 6.3132 6.1621 5.9606 5.8324 5.7436 5.6786 5.5896 5.4186 5.2565   |
| 25.930  | 1.1880 42.474 25.456 18.752 15.308 11.893 10.229 9.2550 8.6180 8.1701 7.8384 7.2943 6.9654 6.7453 6.5877 6.3773 6.2433 6.1504 6.0823 5.9891 5.8099 5.6397   |
| 3 a a   | 4 5 6 7 8 10 12 14 16 18 20 25 30 35 40 50 60 70 80 100 200                                                                                                 |

)

continued

(

)

Continued

(

Table A.12.

∞

<!-- image -->

| 60   | - - - - 23.072 14.891 11.624 9.9103 8.8644 8.1626 7.6601 6.8659 6.4026 6.0989 5.8844 5.6011 5.4222 5.2987 5.2084 5.0849 4.8471 4.6190                         |
|------|---------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 40   | - 2.019 62.13 33.75 23.214 15.021 11.747 10.029 8.9808 8.2778 7.7748 6.9805 6.5181 6.2156 6.0023 5.7214 5.5446 5.4230 5.3343 5.2133 4.9819 4.7629             |
| 25   | 55.46 2.0171 62.396 34.019 23.471 15.250 11.961 10.234 9.1810 8.4748 7.9696 7.1730 6.7101 6.4079 6.1952 5.9157 5.7403 5.6200 5.5323 5.4131 5.1863 4.9737      |
| 20   | 55.17 2.0171 62.573 34.200 23.639 15.399 12.099 10.366 9.3085 8.5996 8.0926 7.2933 6.8291 6.5262 6.3131 6.0334 5.8581 5.7378 5.6503 5.5313 5.3053 5.0940      |
| 15   | 54.71 2.0145 62.866 34.495 23.912 15.640 12.321 10.577 9.5119 8.7978 8.2871 7.4821 7.0147 6.7099 6.4955 6.2143 6.0381 5.9173 5.8294 5.7101 5.4836 5.2721      |
| 12   | = 4 54.258 2.0128 63.151 34.782 24.178 15.872 12.535 10.778 9.7054 8.9857 8.4708 7.6590 7.1876 6.8801 6.6640 6.3804 6.2027 6.0809 5.9924 5.8721 5.6439 5.4309 |
| 10   | 53.808 2.0112 63.432 35.064 24.437 16.098 12.741 10.972 9.8904 9.1647 8.6453 7.8261 7.3502 7.0397 6.8214 6.5350 6.3555 6.2325 6.1430 6.0215 5.7910 5.5758     |
| 8    | 53.142 2.0087 63.841 35.474 24.814 16.424 13.037 11.248 10.154 9.4190 8.8926 8.0616 7.5784 7.2631 7.0413 6.7501 6.5676 6.4426 6.3515 6.2279 5.9933 5.7743     |
| 6    | 52.054 2.0046 64.497 36.129 25.413 16.938 13.500 11.680 10.563 9.8121 9.2736 8.4223 7.9265 7.6026 7.3746 7.0751 6.8872 6.7584 6.6646 6.5372 6.2952 6.0692     |
| 5    | 51.204 2.0013 64.999 36.629 25.868 17.326 13.848 12.002 10.868 10.104 9.5560 8.6884 8.1825 7.8517 7.6188 7.3125 7.1202 6.9884 6.8924 6.7619 6.5139 6.2821     |
| 4    | 49.964 1.9964 65.715 37.343 26.516 17.875 14.338 12.455 11.295 10.512 9.9500 9.0585 8.5377 8.1968 7.9566 7.6404 7.4417 7.3054 7.2061 7.0711 6.8143 6.5741     |
| ν E  | 4 a 5 a 6 7 8 10 12 14 16 18 20 25 30 35 40 50 60 70 80 100 200                                                                                               |

=

| -             | - -    | - -           |   17.20 |        |   13.95 |   12.105 10.928 | 10.113   | 8.8745        | 8.1790        |   7.7339 | 7.4247        | 7.0229        | 6.7730        |   6.6024 |   6.4785 |   6.3103 5.9908 | 5.6899   |
|---------------|--------|---------------|---------|--------|---------|-----------------|----------|---------------|---------------|----------|---------------|---------------|---------------|----------|----------|-----------------|----------|
| - -           | -      | 47.35 24.422  |  17.365 | 14.1   |  12.25  |          11.068 |          | 10.252        | 9.0102 8.3141 |   7.8693 | 7.5607        | 7.1605        | 6.9124 6.7434 |   6.6208 |   6.455  |          6.1416 | 5.8499   |
| - -           | 90.04  | 47.723 24.740 |  17.647 | 14.361 |  12.499 |          11.31  |          | 10.488        | 9.2386 8.5389 |   8.0926 | 7.7833        | 7.3829 7.1351 | 6.9667        |   6.8448 |   6.6801 |          6.3702 | 6.0838   |
| - 3.032       | 90.29  | 47.973 24.947 |  17.83  | 14.53  |  12.659 |          11.463 |          | 10.637        | 9.3814 8.6785 |   8.2301 | 7.9195 7.5177 | 7.2692        | 7.1004        |   6.9782 |   6.8133 |          6.5032 | 6.2171   |
| - 3.0291      | 90.705 | 48.382 25.284 |  18.124 | 14.8   |  12.914 |          11.708 | 10.874   | 9.6061        | 8.8964        |   8.4437 | 8.1303        | 7.7248 7.4741 | 7.3039        |   7.1807 |   7.0145 |          6.7023 | 6.4144   |
| 86.88 3.0266  | 91.113 | 48.780 25.610 |  18.409 | 15.059 |  13.157 |          11.939 |          | 11.097 9.8168 | 9.0995        |   8.6419 | 8.3250        | 7.9150 7.6615 | 7.4894        |   7.3648 |   7.1968 |          6.8811 | 6.5902   |
| 86.160 3.0241 | 91.515 | 49.170 25.927 |  18.683 | 15.309 |  13.389 |          12.161 |          | 11.310 10.016 | 9.2907        |   8.8277 | 8.5070        | 8.0921 7.8355 | 7.6612        |   7.5351 |   7.3649 |          7.0451 | 6.7505   |
| 85.093 3.0204 | 92.102 | 49.739 26.387 |  19.079 | 15.666 |  13.722 |          12.476 | 11.612   | 10.297        | 9.5592        |   9.0879 | 8.7613 8.3385 | 8.0769        | 7.8991        |   7.7705 |   7.5969 |          7.2706 | 6.9698   |
| 83.352 3.0142 | 93.042 | 50.646 27.115 |  19.701 | 16.224 |  14.239 |          12.963 | 12.078   | 10.728        | 9.9689        |   9.4836 | 9.1469        | 8.7107 8.4406 | 8.2570        |   8.1241 |   7.9446 |          7.607  | 7.2955   |
| 81.991 3.0093 | 93.762 | 51.339 27.667 |  20.169 | 16.643 |  14.624 |          13.326 | 12.424   | 11.046        | 10.270        |   9.7739 | 9.4292 8.9825 | 8.7057        | 8.5174        |   8.3811 |   8.1969 |          7.8505 | 7.5305   |
| 5 a 6 a       | 7      | 8 10          |  12     | 14     |  16     |          18     | 20       | 25            | 30            |  35      | 40            | 50 60         | 70            |  80      | 100      |        200      | ∞        |

)

continued

(

Table A.12. ( Continued )

Multiply each entry in this row by 100.

<!-- image -->

| 35     | 41.993 25.925 19.677 16.455 14.513 13.223 11.343 10.331 9.7003 9.2699 8.7207 8.3851 8.1589 7.9959 7.7771 7.3685 6.9945       |
|--------|------------------------------------------------------------------------------------------------------------------------------|
| 30     | 42.136 26.044 19.783 16.553 14.607 13.313 11.428 10.414 9.7820 9.3508 8.8009 8.4651 8.2388 8.0759 7.8572 7.4494 7.0768       |
| 25     | 42.334 26.209 19.929 16.688 14.735 13.436 11.544 10.526 9.8921 9.4596 8.9082 8.5717 8.3450 8.1819 7.9629 7.5552 7.1832       |
| 20     | 42.626 26.451 20.144 16.886 14.921 13.615 11.711 10.687 10.049 9.6142 9.0598 8.7215 8.4938 8.3300 8.1102 7.7011 7.3284       |
| ν H 15 | p = 6 43.103 26.843 20.489 17.202 15.218 13.899 11.975 10.939 10.293 9.8535 9.2927 8.9507 8.7204 8.5548 8.3326 7.9193 7.5430 |
| 12     | 43.567 27.223 20.821 17.505 15.501 14.168 12.222 11.173 10.520 10.075 9.5067 9.1602 8.9269 8.7591 8.5340 8.1153 7.7340       |
| 10     | 44.019 27.590 21.141 17.795 15.772 14.424 12.456 11.395 10.733 10.282 9.7060 9.3547 9.1182 8.9480 8.7197 8.2950 7.9082       |
| 8      | 44.677 28.121 21.600 18.210 16.157 14.788 12.786 11.705 11.031 10.571 9.9832 9.6246 9.3830 9.2093 8.9760 8.5419 8.1463       |
| 6      | 45.722 28.959 22.321 18.858 16.755 15.351 13.293 12.180 11.484 11.009 10.402 10.031 9.7813 9.6014 9.3598 8.9099 8.4997       |
| ν E    | 10 12 14 16 18 20 25 30 35 40 50 60 70 80 100 200 ∞                                                                          |

TABLES

Table A.13. Orthogonal Polynomial Contrasts

|    |              | Variable   | Variable   | Variable   | Variable   | Variable   | Variable   | Variable   | Variable   | Variable   | Variable   |              |
|----|--------------|------------|------------|------------|------------|------------|------------|------------|------------|------------|------------|--------------|
| p  | Polynomial   | 1          | 2          | 3          | 4          | 5          | 6          | 7          | 8          | 9          | 10         | c ′ i c i    |
| 3  | Linear       | - 1        | 0          | 1          |            |            |            |            |            |            |            | 2            |
|    | Quadratic    | 1          | - 2        | 1          |            |            |            |            |            |            |            | 6            |
| 4  | Linear       | - 3        | - 1        | 1          | 3          |            |            |            |            |            |            | 20           |
|    | Quadratic    | 1          | - 1        | - 1        | 1          |            |            |            |            |            |            | 4            |
|    | Cubic        | - 1        | 3          | - 3        | 1          |            |            |            |            |            |            | 20           |
| 5  | Linear       | - 2        | - 1        | 0          | 1          | 2          |            |            |            |            |            | 10           |
|    | Quadratic    | 2          | - 1        | - 2        | - 1        | 2          |            |            |            |            |            | 14           |
|    | Cubic        | - 1        | 2          | 0          | - 2        | 1          |            |            |            |            |            | 10           |
|    | Quartic      | 1          | 4          | 6          | - 4        | 1          |            |            |            |            |            | 70           |
| 6  | Linear       | - 5        | - - 3      | - 1        | 1          | 3          | 5          |            |            |            |            | 70           |
|    | Quadratic    | 5          | - 1        | - 4        | - 4        | - 1        | 5          |            |            |            |            | 84           |
|    | Cubic        | - 5        | 7          | 4          | - 4        | - 7        | 5          |            |            |            |            | 180          |
|    | Quartic      | 1          | - 3        | 2          | 2          | - 3        | 1          |            |            |            |            | 28           |
|    | Quintic      | - 1        | 5          | - 10       | 10         | - 5        | 1          |            |            |            |            | 252          |
| 7  | Linear       | - 3        | - 2        | - 1        | 0          | 1          | 2          | 3          |            |            |            | 28           |
|    | Quadratic    | 5          | 0          | - 3        | - 4        | - 3        | 0          | 5          |            |            |            | 84           |
|    | Cubic        | - 1        | 1          | 1          | 0          | - 1        | - 1        | 1          |            |            |            | 6            |
|    | Quartic      | 3          | - 7        | 1          | 6          | 1          | - 7        | 3          |            |            |            | 154          |
|    | Quintic      | - 1        | 4          | 5          | 0          | 5          | - 4        | 1          |            |            |            | 84           |
|    | Sextic       | 1          | - 6        | - 15       | - 20       | 15         | - 6        | 1          |            |            |            | 924          |
| 8  | Linear       | - 7        | - 5        | - 3        | - 1        | 1          | 3          | 5          | 7          |            |            | 168          |
|    | Quadratic    | 7          | 1          | - 3        | - 5        | - 5        | - 3        | 1          | 7          |            |            | 168          |
|    | Cubic        | - 7        | 5          | 7          | 3          | - 3        | - 7        | - 5        | 7          |            |            | 264          |
|    | Quartic      | 7          | 13         | - 3        | 9          | 9          | - 3        | - 13       | 7          |            |            | 616          |
|    | Quintic      | - 7        | - 23       | - 17       | - 15       | 15         | 17         | - 23       | 7          |            |            | 2,184        |
|    | Sextic       | 1          | - 5        | 9          | 5          | - 5        | 9          | - 5        | 1          |            |            | 264          |
|    | Septic       | - 1        | 7          | - 21       | - 35       | - 35       | 21         | - 7        | 1          |            |            | 3,432        |
| 9  | Linear       | - 4        | - 3        | - 2        | - 1        | 0          | 1          | 2          | 3          | 4          |            | 60           |
|    | Quadratic    | 28         | 7          | - 8        | - 17       | - 20       | - 17       | - 8        | 7          | 28         |            | 2,772        |
|    | Cubic        | - 14       | 7          | 13         | 9          | 0          | - 9        | - 13       | - 7        | 14         |            | 990          |
|    | Quartic      | 14         | - 21       | - 11       | 9          | 18         | 9          | - 11       | - 21       | 14         |            | 2,002        |
|    | Quintic      | - 4        | 11         | - 4        | - 9        | 0          | 9          | 4          | 11         | 4          |            | 468          |
|    | Sextic       | 4          | - 17       | 22         | 1          | - 20       | 1          | 22         | - - 17     | 4          |            | 1,980        |
|    | Septic       | - 1        | 6          | - 14       | 14         | 0          | - 14       | 14         | - 6        | 1          |            | 858          |
|    | Octic        | 1          | - 8        | 28         | - 56       | 70         | - 56       | 28         | - 8        | 1          |            | 12,870       |
| 10 | Linear       | - 9        | - 7        | - 5        | - 3        | - 1        | 1          | 3          | 5          | 7          | 9          | 330          |
|    | Quadratic    | 6          | 2          | - 1        | - 3        | - 4        | 4          | 3          | 1          | 2          | 6          | 132          |
|    | Cubic        | - 42       | 14         | 35         | 31         | 12         | - - 12     | - - 31     | - - 35     | - 14       | 42         | 8,580        |
|    | Quartic      | 18         | 22         | - 17       | 3          | 18         | 18         | 3          | - 17       | - 22       | 18         | 2,860        |
|    | Quintic      | - 6        | - 14       | - 1        | - 11       | - 6        | 6          | 11         | 1          | - 14       | 6          | 780          |
|    | Sextic       | 3          | - 11       | 10         | 6          | - 8        | - 8        | 6          | 10         | 11         | 3          | 660          |
|    | Septic Octic | - 9 1      | 47 7       | - 86 20    | 92 28      | 56 14      | - 56 14    | - 42 28    | 86 20      | - 47 7     | 9 1        | 29,172 2,860 |
|    |              | 1          | - 9        | 36         | 84         |            | 126        | - 84       | 36         | - - 9      |            | 48,620       |
|    | Novic        | -          |            | -          | -          | - 126      |            | -          |            |            | 1          |              |

Note : Entries are rows c ′ i of the ( p -1 ) × p matrix C illustrated in (6.91) in Section 6.10.1.

588 Table A.14. Test for Equal Covariance Matrices, α = . 05

| ν     | k = 2   | k = 3   | k = 4   | k = 5   | k = 6       | k = 7       | k = 8       | k = 9       | k = 10   |
|-------|---------|---------|---------|---------|-------------|-------------|-------------|-------------|----------|
| p = 2 | p = 2   | p = 2   | p = 2   | p = 2   | p = 2       | p = 2       | p = 2       | p = 2       | p = 2    |
| 3     | 12.18   | 18.70   | 24.55   | 30.09   | 35.45       | 40.68       | 45.81       | 50.87       | 55.86    |
| 4     | 10.70   | 16.65   | 22.00   | 27.07   | 31.97       | 36.75       | 41.45       | 46.07       | 50.64    |
| 5     | 9.97    | 15.63   | 20.73   | 25.57   | 30.23       | 34.79       | 39.26       | 43.67       | 48.02    |
| 6     | 9.53    | 15.02   | 19.97   | 24.66   | 29.19       | 33.61       | 37.95       | 42.22       | 46.45    |
| 7     | 9.24    | 14.62   | 19.46   | 24.05   | 28.49       | 32.83       | 37.08       | 41.26       | 45.40    |
| 8     | 9.04    | 14.33   | 19.10   | 23.62   | 27.99       | 32.26       | 36.44       | 40.57       | 44.64    |
| 9     | 8.88    | 14.11   | 18.83   | 23.30   | 27.62       | 31.84       | 35.98       | 40.05       | 44.08    |
| 10    | 8.76    | 13.94   | 18.61   | 23.05   | 27.33       | 31.51       | 35.61       | 39.65       | 43.64    |
| 11    | 8.67    | 13.81   | 18.44   | 22.85   | 27.10       | 31.25       | 35.32       | 39.33       | 43.29    |
| 12    | 8.59    | 13.70   | 18.30   | 22.68   | 26.90       | 31.03       | 35.08       | 39.07       | 43.00    |
| 13    | 8.52    | 13.60   | 18.19   | 22.54   | 26.75       | 30.85       | 34.87       | 38.84       | 42.76    |
| 14    | 8.47    | 13.53   | 18.10   | 22.42   | 26.61       | 30.70       | 34.71       | 38.66       | 42.56    |
| 15    | 8.42    | 13.46   | 18.01   | 22.33   | 26.50       | 30.57       | 34.57       | 38.50       | 42.38    |
| 16    | 8.38    | 13.40   | 17.94   | 22.24   | 26.40       | 30.45       | 34.43       | 38.36       | 42.23    |
| 17    | 8.35    | 13.35   | 17.87   | 22.17   | 26.31       | 30.35       | 34.32       | 38.24       | 42.10    |
| 18    | 8.32    | 13.30   | 17.82   | 22.10   | 26.23       | 30.27       | 34.23       | 38.13       | 41.99    |
| 19    | 8.28    | 13.26   | 17.77   | 22.04   | 26.16       | 30.19       | 34.14       | 38.04       | 41.88    |
| 20    | 8.26    | 13.23   | 17.72   | 21.98   | 26.10       | 30.12       | 34.07       | 37.95       | 41.79    |
| 25    | 8.17    | 13.10   | 17.55   | 21.79   | 25.87       | 29.86       | 33.78       | 37.63       | 41.44    |
| 30    | 8.11    | 13.01   | 17.44   | 21.65   | 25.72       | 29.69       | 33.59       | 37.42       | 41.21    |
| p = 3 | p = 3   | p = 3   | p = 3   | p = 3   | p = 3       | p = 3       | p = 3       | p = 3       | p = 3    |
| 4     | 22.41   | 35.00   | 46.58   | 57.68   | 68.50       | 79.11       | 89.60       | 99.94       | 110.21   |
| 5     | 19.19   | 30.52   | 40.95   | 50.95   | 60.69       | 70.26       | 79.69       | 89.03       | 98.27    |
| 6     | 17.57   | 28.24   | 38.06   | 47.49   | 56.67       | 65.69       | 74.58       | 83.39       | 92.09    |
| 7     | 16.59   | 26.84   | 36.29   | 45.37   | 54.20       | 62.89       | 71.44       | 79.90       | 88.30    |
| 8     | 15.93   | 25.90   | 35.10   | 43.93   | 52.54       | 60.99       | 69.32       | 77.57       | 85.73    |
| 9     | 15.46   | 25.22   | 34.24   | 42.90   | 51.33       | 59.62       | 67.78       | 75.86       | 83.87    |
| 10    | 15.11   | 24.71   | 33.59   | 42.11   | 50.42       | 58.57       | 66.62       | 74.58       | 82.46    |
| 11    | 14.83   | 24.31   | 33.08   | 41.50   | 49.71       | 57.76       | 65.71       | 73.57       | 81.36    |
| 12    | 14.61   | 23.99   | 32.67   | 41.00   | 49.13       | 57.11       | 64.97       | 72.75       | 80.45    |
| 13    | 14.43   | 23.73   | 32.33   | 40.60   | 48.65       | 56.56       | 64.36       | 72.09       | 79.72    |
| 14    | 14.28   | 23.50   | 32.05   | 40.26   | 48.26       | 56.11       | 63.86       | 71.53       | 79.11    |
| 15    | 14.15   | 23.32   | 31.81   | 39.97   | 47.92       | 55.73       | 63.43       | 71.05       | 78.60    |
| 16    | 14.04   | 23.16   | 31.60   | 39.72   | 47.63       | 55.40       | 63.06       | 70.64       | 78.14    |
| 17    | 13.94   | 23.02   | 31.43   | 39.50   | 47.38       | 55.11       | 62.73       | 70.27       | 77.76    |
| 18    | 13.86   | 22.89   | 31.26   | 39.31   | 47.16       | 54.86       | 62.45       | 69.97       | 77.41    |
| 19    | 13.79   | 22.78   | 31.13   | 39.15   | 46.96       | 54.64       | 62.21       | 69.69       | 77.11    |
| 20    | 13.72   | 22.69   | 31.01   | 39.00   | 46.79       |             |             |             | 76.84    |
| 25    | 13.48   | 22.33   | 30.55   | 38.44   |             | 54.44       | 61.98       | 69.45       | 75.84    |
| 30    | 13.32   | 22.10   | 30.25   | 38.09   | 46.15 45.73 | 53.70 53.22 | 61.16 60.62 | 68.54 67.94 | 75.18    |

TABLES

Table A.14. ( Continued )

| ν     | k = 2   | k = 3   | k = 4   | k = 5   | k = 6   | k = 7   | k = 8   | k = 9   | k = 10   |
|-------|---------|---------|---------|---------|---------|---------|---------|---------|----------|
| p = 4 | p = 4   | p = 4   | p = 4   | p = 4   | p = 4   | p = 4   | p = 4   | p = 4   | p = 4    |
| 5     | 35.39   | 56.10   | 75.36   | 93.97   | 112.17  | 130.11  | 147.81  | 165.39  | 182.80   |
| 6     | 30.06   | 48.62   | 65.90   | 82.60   | 98.93   | 115.03  | 130.94  | 146.69  | 162.34   |
| 7     | 27.31   | 44.69   | 60.89   | 76.56   | 91.88   | 106.98  | 121.90  | 136.71  | 151.39   |
| 8     | 25.61   | 42.24   | 57.77   | 72.77   | 87.46   | 101.94  | 116.23  | 130.43  | 144.50   |
| 9     | 24.45   | 40.57   | 55.62   | 70.17   | 84.42   | 98.46   | 112.32  | 126.08  | 139.74   |
| 10    | 23.62   | 39.34   | 54.04   | 68.26   | 82.19   | 95.90   | 109.46  | 122.91  | 136.24   |
| 11    | 22.98   | 38.41   | 52.84   | 66.81   | 80.48   | 93.95   | 107.27  | 120.46  | 133.57   |
| 12    | 22.48   | 37.67   | 51.90   | 65.66   | 79.14   | 92.41   | 105.54  | 118.55  | 131.45   |
| 13    | 22.08   | 37.08   | 51.13   | 64.73   | 78.04   | 91.15   | 104.12  | 116.98  | 129.74   |
| 14    | 21.75   | 36.59   | 50.50   | 63.95   | 77.13   | 90.12   | 102.97  | 115.69  | 128.32   |
| 15    | 21.47   | 36.17   | 49.97   | 63.30   | 76.37   | 89.26   | 101.99  | 114.59  | 127.14   |
| 16    | 21.24   | 35.82   | 49.51   | 62.76   | 75.73   | 88.51   | 101.14  | 113.67  | 126.10   |
| 17    | 21.03   | 35.52   | 49.12   | 62.28   | 75.16   | 87.87   | 100.42  | 112.87  | 125.22   |
| 18    | 20.86   | 35.26   | 48.78   | 61.86   | 74.68   | 87.31   | 99.80   | 112.17  | 124.46   |
| 19    | 20.70   | 35.02   | 48.47   | 61.50   | 74.25   | 86.82   | 99.25   | 111.56  | 123.79   |
| 20    | 20.56   | 34.82   | 48.21   | 61.17   | 73.87   | 86.38   | 98.75   | 111.02  | 123.18   |
| 25    | 20.06   | 34.06   | 47.23   | 59.98   | 72.47   | 84.78   | 96.95   | 109.01  | 120.99   |
| 30    | 19.74   | 33.59   | 46.61   | 59.21   | 71.58   | 83.74   | 95.79   | 107.71  | 119.57   |
| p = 5 | p = 5   | p = 5   | p = 5   | p = 5   | p = 5   | p = 5   | p = 5   | p = 5   | p = 5    |
| 6     | 51.11   | 81.99   | 110.92  | 138.98  | 166.54  | 193.71  | 220.66  | 247.37  | 273.88   |
| 7     | 43.40   | 71.06   | 97.03   | 122.22  | 146.95  | 171.34  | 195.49  | 219.47  | 243.30   |
| 8     | 39.29   | 65.15   | 89.45   | 113.03  | 136.18  | 159.04  | 181.65  | 204.14  | 226.48   |
| 9     | 36.71   | 61.39   | 84.62   | 107.17  | 129.30  | 151.17  | 172.80  | 194.27  | 215.64   |
| 10    | 34.93   | 58.78   | 81.25   | 103.06  | 124.48  | 145.64  | 166.56  | 187.37  | 208.02   |
| 11    | 33.62   | 56.85   | 78.75   | 100.02  | 120.92  | 141.54  | 161.98  | 182.24  | 202.37   |
| 12    | 32.62   | 55.37   | 76.83   | 97.68   | 118.15  | 138.38  | 158.38  | 178.23  | 198.03   |
| 13    | 31.83   | 54.19   | 75.30   | 95.82   | 115.96  | 135.86  | 155.54  | 175.10  | 194.51   |
| 14    | 31.19   | 53.23   | 74.05   | 94.29   | 114.16  | 133.80  | 153.21  | 172.49  | 191.68   |
| 15    | 30.66   | 52.44   | 73.01   | 93.02   | 112.66  | 132.07  | 151.29  | 170.36  | 189.38   |
| 16    | 30.22   | 51.76   | 72.14   | 91.94   | 111.41  | 130.61  | 149.66  | 166.53  | 187.32   |
| 17    | 29.83   | 51.19   | 71.39   | 91.03   | 110.34  | 129.38  | 148.25  | 166.99  | 185.61   |
| 18    | 29.51   | 50.69   | 70.74   | 90.23   | 109.39  | 128.29  | 147.03  | 165.65  | 184.10   |
| 19    | 29.22   | 50.26   | 70.17   | 89.54   | 108.57  | 127.36  | 145.97  | 164.45  | 182.81   |
| 20    | 28.97   | 49.88   | 69.67   | 88.93   | 107.85  | 126.52  | 145.02  | 163.38  | 181.65   |
| 25    | 28.05   | 48.48   | 67.86   |         | 105.21  | 123.51  | 141.62  | 159.60  |          |
|       | 27.48   |         | 66.71   | 86.70   |         |         |         |         | 177.49   |
| 30    |         | 47.61   |         | 85.29   | 103.56  | 121.60  | 139.47  | 157.22  | 174.87   |

Note : Table contains upper percentage points for

<!-- formula-not-decoded -->

for k samples, each with ν degrees of freedom. Reject H 0 : 𝚺 1 = 𝚺 2 = · · · = 𝚺 k if -2ln M &gt; table value.

Table A.15. Test for Independence of p Variables

Upper percentage points for

<!-- formula-not-decoded -->

where ν is the degrees of freedom of S or R . Reject hypothesis of independence if u ′ is greater than table value. The χ 2 α values are shown for comparison, since u ′ is approximately χ 2 distributed with f = 1 2 p ( p -1 ) degrees of freedom.

| n        | p = 3    | p = 4    | p = 5    | p = 6 α . 05   | p = 7    | p = 8    | p = 9    | p = 10   |
|----------|----------|----------|----------|----------------|----------|----------|----------|----------|
| 4        | 8.020    |          |          | =              |          |          |          |          |
| 5        | 7.834    | 15.22    |          |                |          |          |          |          |
| 6        | 7.814    | 13.47    | 24.01    |                |          |          |          |          |
| 7        | 7.811    | 13.03    | 20.44    | 34.30          |          |          |          |          |
| 8        | 7.811    | 12.85    | 19.45    | 28.75          | 46.05    |          |          |          |
| 9        | 7.811    | 12.76    | 19.02    | 27.11          | 38.41    | 59.25    |          |          |
| 10       | 7.812    | 12.71    | 18.80    | 26.37          | 36.03    | 49.42    | 73.79    |          |
| 11       | 7.812    | 12.68    | 18.67    | 25.96          | 34.91    | 46.22    | 61.76    | 89.92    |
| 12       | 7.813    | 12.66    | 18.58    | 25.71          | 34.28    | 44.67    | 57.68    | 75.45    |
| 13       | 7.813    | 12.65    | 18.52    | 25.55          | 33.89    | 43.78    | 55.65    | 70.43    |
| 14       | 7.813    | 12.64    | 18.48    | 25.44          | 33.63    | 43.21    | 54.46    | 67.87    |
| 15       | 7.813    | 12.63    | 18.45    | 25.36          | 33.44    | 42.82    | 53.69    | 66.34    |
| 16       | 7.814    | 12.62    | 18.43    | 25.30          | 33.31    | 42.55    | 53.15    | 65.33    |
| 17       | 7.814    | 12.62    | 18.41    | 25.25          | 33.20    | 42.34    | 52.77    | 64.63    |
| 18       | 7.814    | 12.62    | 18.40    | 25.21          | 33.12    | 42.19    | 52.48    | 64.12    |
| 19       | 7.814    | 12.61    | 18.38    | 25.19          | 33.06    | 42.06    | 52.26    | 63.73    |
| 20       | 7.814    | 12.61    | 18.37    | 25.16          | 33.01    | 41.97    | 52.08    | 63.43    |
|          | 7.815    | 12.59    | 18.31    | 25.00          | 32.67    | 41.34    | 51.00    | 61.66    |
| α = . 01 | α = . 01 | α = . 01 | α = . 01 | α = . 01       | α = . 01 | α = . 01 | α = . 01 | α = . 01 |
| 4        | 11.79    |          |          |                |          |          |          |          |
| 5        | 11.41    | 21.18    |          |                |          |          |          |          |
| 6        | 11.36    | 18.27    | 32.16    |                |          |          |          |          |
| 7        | 11.34    | 17.54    | 26.50    | 44.65          |          |          |          |          |
| 8        | 11.34    | 17.24    | 24.95    | 36.09          | 58.61    |          |          |          |
| 9        | 11.34    | 17.10    | 24.29    | 33.63          | 47.05    | 74.01    |          |          |
| 10       | 11.34    | 17.01    | 23.95    | 32.54          | 43.59    | 59.36    | 90.87    |          |
| 11       | 11.34    | 16.96    | 23.75    | 31.95          | 42.00    | 54.83    | 73.03    | 109.53   |
| 12       | 11.34    | 16.93    | 23.62    | 31.60          | 41.13    | 52.70    | 67.37    | 88.05    |
| 13       | 11.34    | 16.90    | 23.53    | 31.36          | 40.59    | 51.49    | 64.64    | 81.20    |
| 14       | 11.34    | 16.89    | 23.47    | 31.20          | 40.23    | 50.73    | 63.06    | 77.83    |
| 15       | 11.34    | 16.87    | 23.42    | 31.09          | 39.97    | 50.22    | 62.05    | 75.84    |
| 16       | 11.34    | 16.86    | 23.39    | 31.00          | 39.79    | 49.85    | 61.36    | 74.56    |
| 17       | 11.34    | 16.86    | 23.36    | 30.94          | 39.65    | 49.59    | 60.86    | 73.66    |
| 18       | 11.34    | 16.85    | 23.34    | 30.88          | 39.54    | 49.38    | 60.49    | 73.01    |
| 19       | 11.34    | 16.85    | 23.32    | 30.84          | 39.46    | 49.22    | 60.21    | 72.52    |
| 20       | 11.34    | 16.84    | 23.31    | 30.81          | 39.39    | 49.09    | 59.99    | 72.15    |
|          | 11.34    | 16.81    | 23.21    | 30.58          | 38.93    | 48.28    | 58.57    | 69.92    |

## A P P E N D I X B

## Answers and Hints to Problems

## CHAPTER 2

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

- 2.19 (a) Eigenvalues: 2, 1,

<!-- formula-not-decoded -->

A

2

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

- 2.21 Eigenvalues: 1, 3, C = ( -. 7071 -. 7071 -. 7071 . 7071 ) ,

<!-- formula-not-decoded -->

- 2.22 (a) The spectral decomposition of A is given by A = CDC ′ , where

<!-- formula-not-decoded -->

- (b) The spectral decomposition of A 2 is given by A 2 = CDC ′ , where C is the same as in part (a) and D = diag ( 183 . 378 , 15 . 486 , 6 . 135 ) . Note that the diagonal elements of D are the squares of the diagonal elements of D in part (a).

<!-- formula-not-decoded -->

- (c) The spectral decomposition of A -1 is given by A -1 = CDC ′ , where

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

- -1

The diagonal elements of D are the reciprocals of those of D in part (a). The first two columns of C have been interchanged to match the interchange of the corresponding elements of D ; that is, D = ( 1 /λ 2, 1 /λ 1, 1 /λ 3 ) .

- 2.23 A = UDV ′ , where D = diag ( 13 . 161 , 7 , 000 , 3 . 433 ),

2.24 (a) j ′ a = ( 1 ) a 1 + ( 1 ) a 2 +··· + ( 1 ) an = i ai = a ′ j

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

- 2.26 By (2.27), ( A ′ A ) ′ = A ′ ( A ′ ) ′ . By (2.6), ( A ′ ) ′ = A . Thus, ( A ′ A ) ′ = A ′ A .

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

- 2.35 In (2.93) and (2.94), let A 11 = B , A 12 = c , A 21 = -c ′ , and A 22 = 1. Then equate the right-hand sides of (2.93) and (2.94) to obtain (2.95).

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

- 2.37 Show that C 0 by taking the determinant of both sides of C C I . Thus C is nonsingular and C -1 exists. Multiply C ′ C = I on the right by C -1 and on the left by C .
- = ∑ = ∑ = i j | | /negationslash= ′ =
- 2.38 Multiply ABx = λ x on the left by B . Then λ is an eigenvalue of BA , and Bx is an eigenvector.

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

- (c) Since A is positive definite, we have, from part (b), A 1 / 2 A 1 / 2 .

## CHAPTER 3

- 3.1 z = ∑ n i = 1 zi / n = ∑ i ayi / n = ( ay 1 +··· + ayn )/ n . Now factor a out of the sum.
- 3.2 The numerator of s 2 z is ∑ n i = 1 ( zi -z ) 2 = ∑ i ( ayi -ay ) 2 = ∑ i [ a ( yi -y ) ] 2 . 3.3 x = 4 , y = 4:
- | | = | |

| x y   | x - x   | y - y   | ( x - x )( y - y )   |
|-------|---------|---------|----------------------|
| 2 2   | - 2     | - 2     | 4                    |
| 2 4   | - 2     | 0       | 0                    |
| 2 6   | - 2     | 2       | - 4                  |
| 4 2   | 0       | - 2     | 0                    |
| 4 4   | 0       | 0       | 0                    |
| 4 6   | 0       | 2       | 0                    |
| 6 2   | 2       | - 2     | - 4                  |
| 6 4   | 2       | 0       | 0                    |
| 6 6   | 2       | 2       | 4                    |

Sum = 0

<!-- formula-not-decoded -->

- 3.7 The numerator of s 2 z is ∑ n i = 1 ( zi -z ) 2 = ∑ i ( a ′ y i -a ′ y ) 2 = ∑ i ( a ′ y i -a ′ y )( a ′ y i -a ′ y ) . The scalar a ′ y i is equal to its transpose, as in (2.39). Thus a ′ y i = ( a ′ y i ) ′ = y ′ i a , and ∑ i ( a ′ y i -a ′ y )( a ′ y i -a ′ y ) = ∑ i ( a ′ y i -a ′ y )( y ′ i a -y ′ a ) . By (2.22) and (2.24), this becomes ∑ i a ′ ( y i -y )( y i -y ) ′ a . Now factor out a ′ on the left and a on the right. See also (2.44).
- 3.6 z = ∑ n i = 1 zi / n = ∑ i a ′ y i / n = ( a ′ y 1 +··· + a ′ y n )/ n . Now factor out a ′ on the left. See also (2.42).
- 3.8 By (3.63) and (3.64),

<!-- formula-not-decoded -->

from which the result follows immediately.

<!-- formula-not-decoded -->

3.10 Answers are given in Examples 3.6 and 3.7.

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

3.15 rz w = -. 6106

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

## CHAPTER 4

- 4.1 | 𝚺 1 | = 1 , tr ( 𝚺 1 ) = 20 , | 𝚺 2 | = 4 , tr ( 𝚺 2 ) = 15. Thus tr ( 𝚺 1 ) &gt; tr ( 𝚺 2 ) , but | 𝚺 1 | &lt; | 𝚺 2 | . When converted to correlations, we have

<!-- formula-not-decoded -->

As noted at the end of Section 4.1.3, a decrease in intercorrelations or an increase in the variances will lead to a larger | 𝚺 | . In this case, the decrease in correlations from 𝚺 1 to 𝚺 2 outweighed the increase in the variances (the increase in trace).

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

- 4.3 By the last expression in Section 2.3.1,

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

The sum in the exponent of (4.13) follows from the basic algebra of exponents.

- 4.4 Since ( y -𝛍 ) ′ 𝚺 -1 ( y -𝛍 ) is a scalar, we have E [ ( y -𝛍 ) ′ 𝚺 -1 ( y -𝛍 ) ] = E { tr [ ( y -𝛍 ) ′ 𝚺 -1 ( y -𝛍 ) ]} = E { tr [ 𝚺 -1 ( y -𝛍 )( y -𝛍 ) ′ ]} = tr [ 𝚺 -1 E ( y -𝛍 )( y -𝛍 ) ′ ] = tr ( 𝚺 -1 𝚺 ) = tr ( I p ) = p .
- 4.6 We replace yi in √ b 1 by zi = ayi + b . By an extension of (3.3), z = ay + b . Then (4.18) becomes
- 4.5 The other two terms are of the form 1 2 ∑ n i = 1 ( y -𝛍 ) ′ 𝚺 -1 ( y i -y ) , which is equal to 1 2 [ ( y -𝛍 ) ′ 𝚺 -1 ] ∑ n i = 1 ( y i -y ) . This vanishes because ∑ n i = 1 ( y i -y ) = n y -n y = 0 .

<!-- formula-not-decoded -->

Similarly, if (4.19) is expressed in terms of zi = ayi + b , it reduces to b 2 in terms of yi .

- 4.7 β 2 , p = E [ ( y -𝛍 ) 𝚺 1 ( y -𝛍 ) ] 2 by (4.33). But when y is Np ( 𝛍 , 𝚺 ) , v = ( y -𝛍 ) ′ 𝚺 -1 ( y -𝛍 ) is distributed as χ 2 ( p ) by property 3 in Section 4.2. Then E (v 2 ) var (v) E (v) 2 .
- 4.8 To show that b 1 , p and b 2 , p are invariant under the transformation z = Ay i + b , where A is nonsingular, it is sufficient to show that gi j ( z ) = ( y i -y ) ′ ˆ 𝚺 -1 ( y j -y ) . By (3.67) and (3.68), z = Ay + b and ˆ 𝚺 z = A ˆ 𝚺 A ′ . Then gi j for z becomes
- ′ -= +[ ]

<!-- formula-not-decoded -->

- 4.9 Let i = ( n ) in (4.44); then solve for D 2 ( n ) in (4.43) and substitute into (4.44) to obtain F ( n ) in terms of w , as in (4.45).
- 4.10 (a) a ′ = ( 2 , -1 , 3 ), z = a ′ y is N ( 17 , 21 )
- (c) By property 4b in Section 4.2, y 2 is N ( 1 , 13 ) .

<!-- formula-not-decoded -->

- (d) By property 4a in Section 4.2, ( y 1 y 3 ) is N 2 [( 3 4 ) , ( 6 -2 -2 4 )] .

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

- 4.12 (a) a ′ = ( 4 , -2 , 1 , -3 ), z = a ′ y is N ( -30 , 153 )
- (c) By (4.6), ( y 𝛍 ) ′ 𝚺 -1 ( y 𝛍 ) is distributed as χ 2
- --3 .

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

- (e) By property 4a in Section 4.2, ( y 2 y 4 ) is N 2 [( 3 5 ) , ( 9 -6 -6 9 )] .
- (d) By property 4b in Section 4.2, y 3 is N ( -1 , 2 ) .

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

- 4.14 The variables in (b), (c), and (d) are independent.

4.15 The variables in (a), (c), (d), (f), (i), (j), and (n) are independent.

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

- 4.18 (a) By the central limit theorem in Section 4.3.2, √ n ( y -𝛍 ) is approximately Np ( 0 , 𝚺 ) .
- (b) y is approximately Np ( 𝛍 , 𝚺 / n ) .
- 4.19 (a) The plots show almost no deviation from normality.

<!-- formula-not-decoded -->

The values of √ b 1 show a small amount of positive skewness, but none exceeds the upper 2.5% critical value for √ b 1 given in Table A.1 as .942. The values of b 2 show negative kurtosis. For y 4, the kurtosis is significant, since b 2 &lt; 1 . 74, the lower 2.5 percentile in Table A.3.

<!-- formula-not-decoded -->

From Table A.4, the lower 2.5 percentile for Y is 3 . 04 and the upper 97.5 percentile is .628. We reject the hypothesis of normality only for y

- (d) z defined in (4.24) is approximately N ( 0 , 3 / n ) . To obtain a N (0,1) statistic, we calculate z z 3 n .

-3. ∗ = / √ /

<!-- formula-not-decoded -->

- (b) The .05 critical value from Table A.6 is 7.01. D 2 ( 10 ) = 7 . 54 &gt; 7 . 01.

<!-- formula-not-decoded -->

The plot of (v i , u ( i )) shows some evidence of nonlinearity and an outlier.

- (d) b 1 , p = 7 . 255, b 2 , p = 14 . 406. Both (barely) exceed upper .05 critical values in Table A.5.

<!-- formula-not-decoded -->

None of the values of √ b 1 exceeds 1.134 (from Table A.1) or is less than -1 . 134. None of the values of b 2 is less than 1.53 (from Table A.3). Thus there is no significant departure from normality.

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

The plot shows a sharp break from the fourth to the fifth points.

<!-- formula-not-decoded -->

- 4.23 (a) The Q -Q plots for y 1 and y 5 show little departure from normality. The Q -Q plots for y 2 and y 3 show some evidence of heavier tails than the normal. The Q -Q plots for y 4 and y 6 show some evidence of positive skewness.

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

- (b) D 2 ( 51 ) = 25 . 628. By extrapolation in Table A.6, the .05 critical value for p = 6 is approximately 19. Thus we reject the hypothesis of multivariate normality.
- 4.24 (a) D 2 i = 7 . 816, 3.640, 5 . 730 , . . . , 6 . 433
- (c) (v i , u ( i )) = (. 021 , . 024 ), (. 029 , . 028 ), . . . , (. 306 , . 523 ) . The plot shows nonlinearity for the last 4 points.
- (d) b 1 , p = 16 . 287 , b 2 , p = 58 . 337. By extrapolation to p = 6 in Table A.5, both appear to exceed their critical values.

## CHAPTER 5

- 5.1 By (5.6), we have

<!-- formula-not-decoded -->

- 5.2 From (5.9), we have

<!-- formula-not-decoded -->

- 5.3 By (5.13) and (5.14),

<!-- formula-not-decoded -->

- 5.4 It is assumed that y and x have a bivariate normal distribution. Let y i = ( yi xi ) . Then di can be expressed as di = yi -xi = a ′ y i , where a ′ = ( 1 , -1 ) . By property 1a in Section 4.2, di is N ( a ′ 𝛍 , a ′ 𝚺 a ) . Show that a ′ y = y -x , a ′ Sa = s 2 y -2 syx + s 2 x = s 2 d , and that T 2 = n ( a ′ y ) ′ ( a ′ Sa ) -1 ( a ′ y ) is the square of t = d /( sd / √ n ) .

∑ When this is expanded, we obtain s 2 d = s 2 y + s 2 x -2 syx .

<!-- formula-not-decoded -->

- 5.6 The solution is similar to that for Problem 5.1.

<!-- formula-not-decoded -->

- 5.9 Under H 03, we have C 𝛍 1 = 0 and C 𝛍 2 = 0 . Then

<!-- formula-not-decoded -->

Since y 1 and y 2 are independent,

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

- 5.10 CS pl C ′ /( n 1 + n 2 ) is the sample covariance matrix of Cy . Hence the equation immediately above (5.39) exhibits the characteristic form of the T 2 -distribution.
- 5.11 T 2 = . 061

<!-- formula-not-decoded -->

- 5.12 (a) T 2 = 85 . 3327
- 5.13 T 2 = 30 . 2860
- (b) t 1 = 1 . 1643, t 2 = 1 . 1006, t 3 = . 9692, t 4 = . 7299. None of these is significant. In fact, ordinarily they would not have been examined because the T 2 -test in part (a) did not reject H 0.
- 5.14 (a) T 2 = 1 . 8198
- 5.15 T 2 = 79 . 5510

<!-- formula-not-decoded -->

- 5.16 (a) T 2 = 133 . 4873
- (c) a ′ = (. 345 , -. 130 , -. 106 , -. 143 )
- (e) R 2 = . 782975, T 2 = 133 . 4873
- (d) T 2 = 133 . 4873
- (f) By (5.32), t 2 ( y 1 | y 2 , y 3 , y 4 ) = 35 . 9336, t 2 ( y 2 | y 1 , y 3 , y 4 ) = 5 . 7994, t 2 ( y 3 | y 1 , y 2 , y 4 ) = 1 . 7749, t 2 ( y 4 | y 1 , y 2 , y 3 ) = 8 . 2592
- 5.17 By (5.34), the test for parallelism gives T 2 = 132 . 6863. The discriminant function coefficient vector is given by (5.35) as a ′ = ( -. 362 , -. 223 , -. 137 ) .
- (g) By (5.29), T 2 ( y 3 , y 4 | y 1 , y 2 ) = 12 . 5206, F ( y 3 , y 4 | y 1 , y 2 ) = 6 . 0814
- 5.18 (a) T 2 = 66 . 6604
- (c) By (5.32),

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

- (d) By (5.29), T 2 ( y 4 , y 5 , y 6 | y 1 , y 2 , y 3 ) = 27 . 547.

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

## CHAPTER 6

6.1 (a) Using y i . = yi ./ n , we have

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

- 6.4 We need to show that ( 2 N + s + 1 )/( 2 m + s + 1 ) = (ν E -p + s )/ d . Using the definitions N = 1 2 (ν E -p -1 ) , m = 1 2 ( | ν H -p | -1 ), d = max ( p , ν H ) ,

and s = min ( p , ν H ) , we have 2 N + s + 1 = 2 ( 1 2 )(ν E -p -1 ) + s + 1 = ν E -p -1 + s + 1 = ν E -p + s . For the denominator, we have 2 m + s + 1 = 2 ( 1 2 )( | ν H -p | -1 ) + s + 1 = | ν H -p | + s . Suppose ν H &gt; p . Then | ν H -p | + s = ν H -p + p = ν H = d . On the other hand, if ν H &lt; p , then | ν H -p | + s = p -ν H + ν H = p = d .

- 6.5 If p ≤ ν H , we have s = p and | ν H -p | = ν H -p . Then (6.30) becomes

<!-- formula-not-decoded -->

which is the same as (6.31) because p = s .

- 6.6 When s = 1, we have V ( 1 ) = λ 1 /( 1 + λ 1 ) , U ( 1 ) = λ 1, /Lambda1 = 1 /( 1 + λ 1 ) , and θ = λ 1 /( 1 + λ 1 ) . Solving the last of these for λ 1 gives λ 1 = θ/( 1 -θ) , and the results in (6.34), (6.35), and (6.36) follow immediately.
- 6.7 With T 2 = ( n 1 + n 2 -2 ) U ( 1 ) and U ( 1 ) = θ/( 1 -θ) , we obtain (5.19). We obtain (5.18) from (5.19) by V ( 1 ) θ . A similar argument leads to (5.16).
- 6.8 (a) With y i . = y i ./ ni and y .. = y ../ N , we obtain
- =

<!-- formula-not-decoded -->

## 6.9 y 1 . -y .. becomes

<!-- formula-not-decoded -->

The first term in the sum is

<!-- formula-not-decoded -->

The second term in the sum is

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

6.11 From r 2 i = λ i /( 1 + λ i ) , obtain λ i = r 2 i /( 1 -r 2 i ) . Substitute this into 1 /( 1 + λ i ) to obtain the result.

6.12 Substitute AP = V ( s ) / s into (6.50) to obtain (6.26).

<!-- formula-not-decoded -->

- 6.14 Substitute A LH = U ( s ) /( s + U ( s ) ) from (6.51) into (6.52) to obtain F 3 in (6.31).

6.13 When s = 1, (6.51) becomes

<!-- formula-not-decoded -->

6.15 To show cov ( ci y i . ) = c 2 i 𝚺 / n , use (3.74), cov ( Ay ) = A 𝚺 A ′ , with A = ci I . 6.16 By (6.9),

<!-- formula-not-decoded -->

6.17 C is not square.

<!-- formula-not-decoded -->

cov ( Cy ..) = C 𝚺 C ′ / kn if there are no differences in the group means, C 𝛍 1, C 𝛍 2 , . . . , C 𝛍 k . This condition is assured by H 01 in (6.78).

- 6.19 For our purposes, it will suffice to show that T 2 has the characteristic form of the T 2 -distribution in (5.6).
- 6.20 If 𝚺 = σ 2 I , (6.89) becomes

<!-- formula-not-decoded -->

Show that ( I -J / p ) 2 = I -J / p . Then

<!-- formula-not-decoded -->

- 6.21 The (univariate) expected mean square corresponding to µ . in a one-way ANOVA is σ 2 + N µ 2 . Thus the mean square for µ . is tested with MSE. The corresponding multivariate test therefore uses H ∗ and E .
- 6.22 From (6.105) we have

<!-- formula-not-decoded -->

Substitute H ∗ = kn y .. y ′ .. to obtain

<!-- formula-not-decoded -->

Now use (2.95) with B = AEA ′ and c = √ kn Ay .. to obtain

<!-- formula-not-decoded -->

Multiply and divide by ν E and use (6.101) to obtain (6.106).

- 6.23 Solve for T 2 in (6.106).
- 6.24 In C 1 B ′ the rows of C 1 are multiplied by the rows of B . Show that C 1 B ′ = O .
- 6.26 Expand n ( y -A ˆ 𝛃 ) ′ S -1 ( y -A ˆ 𝛃 ) to four terms and substitute
- 6.25 As noted, the function ( y -A ˆ 𝛃 ) ′ S -1 ( y -A ˆ 𝛃 ) is similar to SSE = ( y -X ˆ 𝛃 ) ′ ( y -X ˆ 𝛃 ) in (10.4) and (10.6). By an argument similar to that used in Section 10.2.2 to obtain ˆ 𝛃 = ( X ′ X ) -1 X ′ y , it follows that ˆ 𝛃 = ( A ′ S -1 A ) -1 A ′ S -1 y . An alternative approach (for those familiar with differentiation with respect to a vector) is to expand ( y -A ˆ 𝛃 ) ′ S -1 ( y -A ˆ 𝛃 ) to four terms, differentiate with respect to ˆ 𝛃 , and set the result equal to 0 .

<!-- formula-not-decoded -->

into the last one.

<!-- formula-not-decoded -->

/Lambda1 = . 224 , V ( s ) = . 860 , U ( s ) = 3 . 08, and θ = . 747. All four are significant.

<!-- formula-not-decoded -->

- (c) The eigenvalues of E H are 2.9515 and .1273. The essential dimensionality of the space of the mean vectors is 1.
- -1
- (d) For 1, 2 vs. 3 we have /Lambda1 = . 270 , V ( s ) = . 730 , U ( s ) = 2 . 702, and θ = . 730. All four are significant. For 1 vs. 2 we obtain /Lambda1 = . 726 , V ( s ) = . 274 , U ( s ) = . 377, and θ = . 274. All four are significant.

<!-- formula-not-decoded -->

The F 's for y 2 and y 3 are significant. For the discriminant function z = a ′ y , where a is the first eigenvector of E -1 H , we have a ′ = ( -. 032 , -. 820 , . 533 , . 208 ) . Again y 2 and y 3 contribute most to separation of groups.

- (f) By(6.127), /Lambda1( y 3 , y 4 | y 1 , y 2 ) = /Lambda1( y 1 , y 2 , y 3 , y 4 )//Lambda1( y 1 , y 2 ) = . 224 /. 568 = . 395 &lt; /Lambda1 . 05 = . 725.
- (g) By (6.128),

<!-- formula-not-decoded -->

- 6.28 (a) S effect: /Lambda1 = . 00065 , V ( s ) = 2 . 357 , U ( s ) = 142 . 304 , θ = . 993. All are significant.

V effect: /Lambda1 = . 065 , V ( s ) = 1 . 107 , U ( s ) = 11 . 675 , θ = . 920. All are significant.

SV interaction: /Lambda1 = . 138 , V ( s ) = 1 . 321 , U ( s ) = 3 . 450 , θ = . 726. All are significant.

- (b) Contrast on V comparing 2 vs. 1, 3: /Lambda1 = . 0804 , V ( s ) = . 920 , U ( s ) = 11 . 445 , θ = . 920. All are significant.
- (c) Linear contrast for S : /Lambda1 = . 0073, V ( S ) = . 993, U ( s ) = 135 . 273, θ = . 993. All are significant.

Quadratic contrast for S : /Lambda1 = . 168, V ( s ) = . 832, U ( s ) = 4 . 956, θ = . 832. All are significant.

Cubic contrast for S : /Lambda1 = . 325, V ( s ) = . 675, U ( s ) = 2 . 076, θ = . 675. All are significant.

- (d) The ANOVA F 's for each variable are as follows:

| Source   |    y 1 |    y 2 |    y 3 |   y 4 |
|----------|--------|--------|--------|-------|
| S        | 980.21 | 214.24 | 876.13 | 73.91 |
| V        | 251.22 |   9.47 |  14.77 | 27.12 |
| SV       |  20.37 |   2.84 |   3.44 |  2.08 |

All F 's are significant except the last one, 2.08.

- (e) Test of significance of y 3 and y 4 adjusted for y 1 and y 2:

<!-- formula-not-decoded -->

- (f) Test of significance of each variable adjusted for the other three:

|                                   |      S |      V |     SV |
|-----------------------------------|--------|--------|--------|
| /Lambda1( y 1 | y 2 , y 3 , y 4 ) | 0.1158 | 0.2099 | 0.3082 |
| /Lambda1( y 2 | y 1 , y 3 , y 4 ) | 0.5586 | 0.8134 | 0.7967 |
| /Lambda1( y 3 | y 1 , y 2 , y 4 ) | 0.2271 | 0.9627 | 0.7604 |
| /Lambda1( y 4 y 1 , y 2 , y 3 )   | 0.6692 | 0.9795 | 0.8683 |

|

## 6.29 V = velocity (fixed), L = lubricant (random).

V effect (using H V L for error matrix): /Lambda1 = . 0492, V ( s ) = . 951, U ( s ) = 19 . 315, θ = . 951. With p = 2, ν H = 1, and ν E = 3, /Lambda1 . 05 = . 050, V ( s ) . 05 = . 950, U ( s ) . 05 = T 2 . 05 /ν E = 19 . 00, θ. 05 = . 950. Thus all four test statistics are significant.

L effect (using E for error matrix): /Lambda1 = . 692, V ( s ) = . 314, U ( s ) = . 438, θ = . 295. None is significant.

| 6.30   | Source                     |   /Lambda1 |   V ( s ) |   U ( s ) |     θ | Significant?   |
|--------|----------------------------|------------|-----------|-----------|-------|----------------|
|        | (a) Reagent                |    0.0993  |     1.126 |     6.911 | 0.868 | Yes            |
|        | (b) Contrast 1 vs. 2, 3, 4 |    0.146   |     0.854 |     5.871 | 0.854 | Yes            |
|        | Subjects                   |    8.2e-07 |     2.847 |  1091.13  | 0.999 | Yes            |

V L interaction (using E for error matrix): /Lambda1 = . 932, V ( s ) = . 069, U ( s ) = . 073, θ = . 061. None is significant.

6.31 P = proportion of filler, T = surface treatment, F = filler:

| Source   |   /Lambda1 |   V ( s ) |   U ( s ) |     θ | Significant?   |
|----------|------------|-----------|-----------|-------|----------------|
| P        |      0.138 |     0.977 |     5.441 | 0.841 | Yes            |
| T        |      0.08  |     0.92  |    11.503 | 0.92  | Yes            |
| PT       |      0.712 |     0.295 |     0.396 | 0.271 | No             |
| F        |      0.019 |     0.98  |    51.18  | 0.981 | Yes            |
| PF       |      0.179 |     0.958 |     3.835 | 0.784 | Yes            |
| T F      |      0.355 |     0.645 |     1.815 | 0.645 | Yes            |
| PT F     |      0.752 |     0.264 |     0.309 | 0.172 | No             |

- 6.32 A = period; P , T , and F are defined in Problem 6.31:

| Source   |   /Lambda1 |   V ( s ) |   U ( s ) |     θ | Significant?   |
|----------|------------|-----------|-----------|-------|----------------|
| A        |      0.021 |     0.979 |    47.099 | 0.979 | Yes            |
| AP       |      0.475 |     0.545 |     1.063 | 0.505 | No             |
| AT       |      0.142 |     0.858 |     6.049 | 0.858 | Yes            |
| APT      |      0.777 |     0.228 |     0.282 | 0.208 | No             |
| AF       |      0.095 |     0.905 |     9.486 | 0.905 | Yes            |
| APF      |      0.622 |     0.387 |     0.594 | 0.363 | No             |
| ATF      |      0.387 |     0.613 |     1.586 | 0.613 | Yes            |
| APTF     |      0.781 |     0.229 |     0.267 | 0.169 | No             |

For the between-subject factors and interactions, we have

| Source   |   df | F      | p -Value   |
|----------|------|--------|------------|
| P        |    2 | 21.79  | < . 0001   |
| T        |    1 | 78.34  | < . 0001   |
| PT       |    2 | 1.28   | .3143      |
| F        |    1 | 345.04 | < . 0001   |
| PF       |    2 | 15.79  | .0004      |
| TF       |    1 | 5.36   | .0392      |
| PTF      |    2 | .48    | .6294      |
| Error    |   12 |        |            |

- 6.33 For parallelism, we use (6.79) to obtain /Lambda1 = . 2397. For levels, we use (6.81) and (6.82) to obtain /Lambda1 = . 9651 and F = . 597. For flatness we use (6.84) to obtain T 2 = 110 . 521.
- (b) For each row c ′ i of C , we use T 2 i = n ( c ′ i y ) ′ ( c ′ i Sc i ) -1 c ′ i y , as in Example 6.9.2: T 2 1 = 17 . 0648, T 2 2 = . 3238, T 2 3 = . 2714. This can also be done by Wilks' /Lambda1 using /Lambda1 i = c ′ i Ec i / c ′ i ( E + H ∗ ) c i : /Lambda1 1 = . 6127, /Lambda1 2 = . 9882, /Lambda1 3 = . 9900.
- 6.34 (a) By (6.90), T 2 = 20 . 7420. By (6.105) or (6.106), /Lambda1 = . 5655.
- 6.35 The six variables represent two within-subjects factors: y 1 is A 1 B 1, y 2 is A 1 B 2, y 3 is A 1 B 3, x 1 is A 2 B 1, x 2 is A 2 B 2, and x 3 is A 2 B 2. Using linear and quadratic effects (other orthogonal contrasts could be used), the matrices A , B , and G in

(6.98), (6.99), and (6.100) become

<!-- formula-not-decoded -->

Using these in T 2 as given by (6.101), (6.102), and (6.103), we obtain T 2 A = 193 . 0901 , T 2 B = 2 . 8000, and T 2 AB = 6 . 8676. Using MANOVA tests for the same within-subjects factors, we obtain

| Source   |   /Lambda1 |   V ( s ) |   U ( s ) |     θ | Significant?   |
|----------|------------|-----------|-----------|-------|----------------|
| A        |      0.202 |     0.798 |     3.941 | 0.798 | Yes            |
| B        |      0.946 |     0.054 |     0.057 | 0.054 | No             |
| AB       |      0.877 |     0.123 |     0.14  | 0.123 | Yes            |

- 6.36 MANOVAtests for the within-subjects effect T (time), and interactions of time with the between-subjects effects C (cancer) and G (gender):

| Source   |   /Lambda1 |   V ( s ) |   U ( s ) |     θ |
|----------|------------|-----------|-----------|-------|
| T        |      0.258 |     0.742 |     2.874 | 0.742 |
| TC       |      0.363 |     0.809 |     1.299 | 0.444 |
| TG       |      0.929 |     0.071 |     0.077 | 0.071 |
| TCG      |      0.809 |     0.201 |     0.225 | 0.13  |

ANOVA F -tests for between-subjects factors and interactions:

| Source   |   df |    F |   p -Value |
|----------|------|------|------------|
| C        |    5 | 4.16 |      0.003 |
| G        |    1 | 2.69 |      0.107 |
| CG       |    5 | 0.37 |      0.869 |

- 6.37 (a) T 2 = 79 . 551
- 6.38 (a) T 2 = 1712 . 2201
- (b) Using t i = c ′ i y / √ c ′ i Sc i / n , where c ′ i is the i th row of C , we obtain t 1 = 7 . 155, t 2 = -. 445, t 3 = -. 105.
- (b) Using t i = c ′ i y / √ c ′ i Sc i / n , we obtain t 1 = 332 . 358, t 2 = 54 . 589, t 3 = . 056, t 4 = 7 . 637, t 5 = 4 . 344, t 6 = 1 . 968.
- (b) t 1 = . 951, t 2 = 1 . 606, t 3 = . 127 [Since the T 2 -test in part (a) did not reject H 0, these would ordinarily not be calculated.]
- 6.39 (a) Using T 2 = N ( Cy .. ) ′ ( CS pl C ′ ) -1 ( Cy .. ) in (6.122), we obtain T 2 = 17 . 582 &lt; T 2 . 05 , 3 , 9 = 27 . 202.

- (c) Using /Lambda1 = | CEC ′ | / | C ( E + H ) C ′ | in (6.124), we obtain /Lambda1 = . 3107 &gt; /Lambda1 . 05 , 3 , 2 , 9 = . 203.
- (d) To compare groups using each row of C , we use /Lambda1 i = c ′ i Ec i / c i ( E + H ) c i to obtain /Lambda1 1 = . 833, /Lambda1 2 = . 988, /Lambda1 3 = . 650. [Since the /Lambda1 -test in part (c) did not reject H 0, we would ordinarily not have calculated these.]
- 6.40 (a) Using T 2 = N ( Cy .. ) ( CS pl C ) 1 ( Cy .. ) in (6.122), we obtain T 2 = 33 . 802 &gt; T 2 12 . 983.
- (b) Using t 2 i = N ( c ′ i y ) 2 / c ′ i S pl c i , we obtain t 2 1 = . 675, t 2 2 = . 393, t 2 3 = 32 . 626. Only the cubic effect is significant.
- (c)
- ′ ′ -. 05 , 4 , 24 =
- For an overall test comparing groups, we use (6.124),

<!-- formula-not-decoded -->

- (d) To compare groups using each row of C , we use

<!-- formula-not-decoded -->

- 6.41 (a) Using T 2 = N ( Cy .. ) ( CS pl C ) 1 ( Cy .. ) in (6.122), we obtain T 2 =
- ′ ′ -45 . 500.
- (b) Using t 2 i = N ( c ′ i y ) 2 / c ′ i S pl c i , we obtain t 2 1 = 18 . 410, t 2 2 = 8 . 385, t 2 3 = 3 . 446, t 2 . 011, t 2 . 098, t 2 2 . 900.
- (c) For an overall test comparing groups, we use (6.124),
- 4 = 5 = 6 =

<!-- formula-not-decoded -->

- (d) To compare groups using each row of C , we use

<!-- formula-not-decoded -->

- 6.42 (a) Combined groups (pooled covariance matrix). Using t = number of minutes -30, we obtain, by (6.115),

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

- By (6.116), we obtain T 2 = . 216. By (6.118), we have

<!-- formula-not-decoded -->

- (b) Group 1: ˆ 𝛃 ′ 1 = ( 100 . 7 , . 819 , . 040 , -. 00085 , -. 000038 ) , T 2 = . 0113, ˆ 𝛍 ′ 1 = ( 105 . 2 , 104 . 4 , 101 . 5 , 98 . 6 , 100 . 6 , 108 . 1 )
- (c) Groups 2-4: 𝛃 2 = ( 97 . 4 , 1 . 010 , . 0403 , -. 00103 , -. 000049 ) , T 2 = . 2554, 𝛍 ( 92 . 6 , 94 . 4 , 93 . 8 , 92 . 4 , 97 . 4 , 96 . 6 )
- ˆ ′ ˆ ′ 2 =

- 6.43 (a) For the control group, the overall test is

<!-- formula-not-decoded -->

For each row of C (linear, quadratic, etc.), we have

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

- (b) For the obese group, we obtain T 2 = n 2 ( Cy 2 . ) ′ ( CS 2 C ′ ) -1 ( Cy 2 . ) = 128 . 552. For the five rows of C , we obtain t 2 1 = 4 . 978, t 2 2 = 107 . 129, t 2 3 = 5 . 225, t 2 4 = 10 . 750, t 2 5 = 3 . 572.
- 6.44 Control group: By (6.115),
- (c) For the combined groups ( S pl = pooled covariance matrix), we use T 2 = N ( Cy .. ) ′ ( CS pl C ′ ) -1 ( Cy .. ) in (6.122) to obtain T 2 = 247 . 0079. We test for linear, quadratic, etc., trends using the rows of C in t 2 i = N ( c ′ i y .. ) 2 / c ′ i S pl c i : t 2 1 = 1 . 162, t 2 2 = 155 . 017, t 2 3 = 30 . 540, t 2 4 = 1 . 319, t 2 5 = . 506. To compare groups, we use /Lambda1 = | CEC ′ | / | C ( E + H ) C ′ | in (6.124) and /Lambda1 i = c ′ i Ec i / c ′ i ( E + H ) c i : /Lambda1 = . 4902, /Lambda1 1 = . 7947, /Lambda1 2 = . 9940, /Lambda1 3 = . 7987, /Lambda1 4 = . 6228, /Lambda1 5 = . 9172.

<!-- formula-not-decoded -->

By (6.116), T 2 = . 7633. By (6.118),

<!-- formula-not-decoded -->

Obese group: ˆ 𝛃 ′ 2 = ( 3 . 207 , -. 187 , . 463 , . 056 , -. 102 , -. 010 , . 010 ) , T 2 = . 3943, ˆ 𝛍 ′ 2 = ( 4 . 51 , 4 . 12 , 3 . 81 , 3 . 48 , 3 . 24 , 3 . 37 , 3 . 70 , 4 . 02 )

Combined groups (pooled covariance matrix): ˆ 𝛃 ′ = ( 3 . 15 , . 162 , . 183 , -. 115 , . 012 , . 010 , -. 002 ) , T 2 = . 0158, ˆ 𝛍 ′ = ( 4 . 36 , 3 . 80 , 3 . 36 , 3 . 15 , 3 . 13 , 3 . 37 , 3 . 63 3 . 98 )

- 6.45 A = activator, T = time, C = group. In (6.101), (6.102), and (6.103), we use

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

,

T 2 A = 5072 . 579, T 2 T = 268 . 185, T 2 AT = 143 . 491. The same within-sample factors and interaction can be tested with Wilks' /Lambda1 using (6.105) and the other three MANOVA tests:

| Source   |   /Lambda1 |   V ( s ) |   U ( s ) |     θ | Significant?   |
|----------|------------|-----------|-----------|-------|----------------|
| A        |      0.003 |     0.997 |    317.04 | 0.997 | Yes            |
| T        |      0.056 |     0.944 |     16.76 | 0.944 | Yes            |
| AT       |      0.1   |     0.9   |      8.97 | 0.9   | Yes            |

The interactions of the within factors with the between factor G are tested with Wilks' /Lambda1 (Section 6.9.5) and with the other three MANOVA tests:

| Source   |   /Lambda1 |   V ( s ) |   U ( s ) |     θ | Significant?   |
|----------|------------|-----------|-----------|-------|----------------|
| AC       |      0.884 |     0.116 |     0.131 | 0.116 | No             |
| TC       |      0.889 |     0.111 |     0.125 | 0.111 | No             |
| ATC      |      0.795 |     0.205 |     0.258 | 0.205 | No             |

The between-subjects factor C is tested with an ANOVA F -test: F = . 47, p -value = . 504.

## CHAPTER 7

- 7.1 If 𝚺 0 is substituted for S in (7.1), we have

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

- 7.3 -ln (∏ p i = 1 λ i ) + ∑ p i = 1 λ i = -∑ p i = 1 ln λ i + ∑ p i = 1 λ i = ∑ p i = 1 (λ i -ln λ i ) 7.4 As noted in Section 7.1, the likelihood ratio in this case involves the ratio of the determinants of the sample covariance matrices under H 0 and H 1. Under H 1, which is essentially unrestricted, the maximum likelihood estimate of 𝚺 (corrected for bias) is given by (4.12) as S . Under H 0 it is assumed that each of the p yi 's in y has variance σ 2 and that all yi 's are independent. Thus we estimate σ 2 (unbiasedly) in each of the p columns of the Y matrix [see (3.17) and (3.23)] and pool the p estimates to obtain

<!-- formula-not-decoded -->

Show that by (3.22) and (3.23) this is equal to

<!-- formula-not-decoded -->

Thus the likelihood ratio is

<!-- formula-not-decoded -->

Show that by (2.85) this becomes

<!-- formula-not-decoded -->

- 7.5 If λ 1 = λ 2 = · · · = λ p = λ , say, then by (7.5),

<!-- formula-not-decoded -->

- 7.7 (a) Substitute J = jj ′ and x = j into Jx = λ x to obtain jj ′ j = λ j , which gives p j = λ j .

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

- (c) By (2.85) and (2.108), we have

<!-- formula-not-decoded -->

7.8 M = | S 1 | ν 1 / 2 | S 2 | ν 2 / 2 · · · | S k | ν k / 2 | S | ∑ i ν i / 2 = | S 1 | ν 1 / 2 | S 2 | ν 2 / 2 · · · | S k | ν k / 2 | S | ν 1 / 2 | S | ν 2 / 2 · · · | S | ν k / 2 7.9 (a) M = . 7015 (b) M = . 0797 7.10 /Lambda1 = | S | | S yy || S xx | = | S xx || S yy -S yx S -1 xx S xy | | S yy || S xx | = | S -1 yy || S yy -S yx S -1 xx S xy | [by (2.91)] = | S -1 yy ( S yy -S yx S -1 xx S xy ) | [by (2.89)] = | I -S -1 yy S yx S -1 xx S xy | = ∏ s i = 1 ( 1 -r 2 i ) [by (2.108)] , where the r 2 i 's are the nonzero eigenvalues of S -1 yy S yx S -1 xx S xy . It was shown in Section 2.11.2 that 1 -λ i is an eigenvalue of I -A , where λ i is an eigenvalue of A .

- 7.11 When all pi = 1, we have k = p , and the submatrices in the denominators of (7.33) and (7.34) reduce to S j j = s j j , j = 1, 2 , . . . , p , and R j j = 1, j = 1, 2 , . . . , p .
- 7.12 When all pi = 1, we have k = p and

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

- 7.13 As noted below (7.6), the degrees of freedom for the χ 2 -approximation is the total number of parameters minus the number estimated under H 0. The number of distinct parameters in 𝚺 is p + p 2 = 1 2 p ( p + 1 ) . The number of parameters estimated under H 0 is p . The difference is 1 p ( p 1 ) p 1 p ( p 1 ) .
- 7.14 By (7.1) and (7.2), u = 11 . 094 and u ′ = 10 . 668.
- ( ) 2 + -= 2 -
- 7.15 By (7.7), u = . 0000594. By (7.9), u ′ = 23 . 519. For H 0 : C 𝚺 C ′ = σ 2 I , u = . 471 and u ′ = 2 . 050.
- 7.16 For H 0 : 𝚺 = σ 2 I , u = . 00513 and u ′ = 131 . 922. For H 0 : C 𝚺 C ′ = σ 2 I , u = . 129 and u ′ = 36 . 278.

- 7.17 For H 0 : 𝚺 = σ 2 I , u = . 00471 and u ′ = 136 . 190. For H 0 : C 𝚺 C ′ = σ 2 I ,
- 7.18 By (7.16), u 6 . 3323 with 13 degrees of freedom. The F -approximation is F . 4802 with 13 and 1147 degrees of freedom.
- u = . 747 and u ′ = 7 . 486.
- 7.19 u ′ = 21 . 488, F = 2 . 511 with 8 and 217 degrees of freedom
- ′ = =
- 7.20 u ′ = 35 . 795, F = 4 . 466 with 8 and 4905 degrees of freedom
- 7.22 | S 1 | = 2 . 620 × 10 14 , | S 2 | = 2 . 410 × 10 14 , | S pl | = 4 . 368 × 10 14 , u = 17 . 502,
- 7.21 u = 8 . 7457, F = . 8730 with 10 and 6502 degrees of freedom
- F = . 829
- 7.24 ln M 7 . 082, u 10 . 565, a 1 10, a 2 1340, F 1 . 046
- 7.23 ln M = -85 . 965, u = 156 . 434, a 1 = 21, a 2 = 17 , 797, F = 7 . 4396
- 7.25 ln M = -8 . 6062, u = 14 . 222, a 1 = 20, a 2 = 3909, F = . 707
- = -= = = =
- 7.26 ln M = -28 . 917, u = 44 . 018, a 1 = 50, a 2 = 3238, F = . 8625
- 7.27 ln M 142 . 435, u 174 . 285, a 1 110, a 2 2084, F 1 . 448
- 7.28 | S | = 1 , 207, 109.5, | S yy | = 2385 . 1, | S xx | = 1341 . 9, /Lambda1 = . 3772
- = -= = = =
- 7.29 | S | = 4 . 237 × 10 13 , | S yy | = 484, 926.6, | S xx | = 131, 406, 938, /Lambda1 = . 6650
- 7.31 | S | = 1 . 7148 × 10 16 , | S 11 | = 11, 284.967, | S 22 | = 11 , 891 . 15, | S 33 | = 25 , 951 . 605, s 44 = 22 , 227 . 158, s 55 = 214 . 06, u = . 00103, u ′ = 274 . 787, ν = 46
- 7.30 | S | = 9 . 676 × 10 -8 , | S yy | = . 02097, | S xx | = 9 . 94 × 10 -6 , /Lambda1 = . 4642
- 7.32 | S | = 459 . 96, s 11 = 140 . 54, s 22 = 72 . 25, s 33 = . 250, u = . 1811, u ′ = 12 . 246, f = 3
- 7.34 u = . 0005176, u ′ = 127 . 367
- 7.33 u = . 0001379, u ′ = 16 . 297
- 7.35 u = . 005071, u ′ = 131 . 226

## CHAPTER 8

- 8.1 Using a = S -1 pl ( y 1 -y 2 ) , we obtain

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

- 8.2 You may wish to use the following steps:
- (i) In Section 5.6.2 the grouping variable w is defined as n 2 /( n 1 + n 2 ) for each observation in group 1 and -n 1 /( n 1 + n 2 ) for group 2. Show that with this formulation, w = 0.

- (ii) Because w = 0, there is no intercept, and the fitted model becomes

<!-- formula-not-decoded -->

Denote the resulting matrix of y values corrected for their means as Y c and the vector of w 's as w . Then the least squares estimate b = ( b 1 , b 2 , . . . , bp ) ′ is obtained as

<!-- formula-not-decoded -->

Using (2.51), show that

<!-- formula-not-decoded -->

where y = ( n 1 y 1 + n 2 y 2 )/( n 1 + n 2 ) . It will be helpful to write the first sum above as

<!-- formula-not-decoded -->

and add and subtract y 1 in the first term and y 2 in the second. (iii) Show that

<!-- formula-not-decoded -->

Again it will be helpful to sum separately over the two groups.

(iv) From steps (ii) and (iii) we have

<!-- formula-not-decoded -->

where S = ∑ i j ( y i j -y i )( y i j -y i ) ′ /( n 1 + n 2 -2 ) , ν = n 1 + n 2 -2, k = n 1 n 2 /( n 1 + n 2 ) , and d = y 1 -y 2 . Use (2.77) for the inverse of a patterned matrix of the type ν S + k dd ′ to obtain (8.4).

- 8.3 You may want to use the following steps:
- (i) R 2 is defined as [see (10.30)]

<!-- formula-not-decoded -->

In this case the expression simplifies because w = 0. Using Y ′ c w in Problem 8.2(iii), show that R 2 = b ′ ( y 1 -y 2 ) .

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

- 8.6 Substituting a ∗ r = sr ar , r = 1, 2 , . . . , p , into (8.15), we obtain
- (ii) Show that

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

- 8.7 (a) a ∗ ′ = ( 1 . 366 , -. 810 , 2 . 525 , -1 . 463 )
- (c) The standardized coefficients rank the variables in the order y 3, y 4, y 1, y 2. The t -tests rank them in the order y 3, y 1, y 2, y 4.
- (b) t 1 = 5 . 417, t 2 = 2 . 007, t 3 = 7 . 775, t 4 = . 688
- (d) The partial F 's calculated by (8.26) are F ( y 1 | y 2 , y 3 , y 4 ) = 7 . 844, F ( y 2 | y 1 , y 3 , y 4 ) = 2 . 612, F ( y 3 | y 1 , y 2 , y 4 ) = 40 . 513, and F ( y 4 | y 1 , y 2 , y 3 ) = 9 . 938.
- (b) a ∗ ′ = ( 4 . 137 , -2 . 501 , -1 . 158 , -2 . 068 )
- 8.8 (a) a ′ = (. 345 , -. 130 , -. 106 , -. 143 )
- (c) t 1 = 3 . 888 , t 2 = -3 . 865 , t 3 = -5 . 691 , t 4 = -5 . 043
- (e) F ( y 1 | y 2 , y 3 , y 4 ) = 35 . 934, F ( y 2 | y 1 , y 3 , y 4 ) = 5 . 799 , F ( y 3 | y 1 , y 2 , y 4 ) = 1 . 775, F ( y 4 | y 1 , y 2 , y 3 ) = 8 . 259
- 8.9 (a) a ′ = ( -. 145 , . 052 , -. 005 , -. 089 , -. 007 , -. 022 ) (b) a ∗ ′ = ( -1 . 016 , . 147 , -. 542 , -1 . 035 , -. 107 , -1 . 200 )

- (c) t 1 = -4 . 655, t 2 = . 592, t 3 = -4 . 354, t 4 = -5 . 257, t 5 = -4 . 032, t 6 = -6 . 439

<!-- formula-not-decoded -->

- 8.10 (a) a ′ = (. 057 , . 010 , . 242 , . 071 )
- (c) t 1 = -3 . 713, t 2 = . 549, t 3 = -3 . 262, t 4 = -. 724
- (b) a ∗ ′ = ( 1 . 390 , . 083 , 1 . 025 , . 032 )
- (e) F ( y 1 | y 2 , y 3 , y 4 ) = 3 . 332, F ( y 2 | y 1 , y 3 , y 4 ) = . 010, F ( y 3 | y 1 , y 2 , y 4 ) = 1 . 482, F ( y 4 | y 1 , y 2 , y 3 ) = . 001
- 8.11 (a) a ′ 1 = (. 021 , . 533 , -. 347 , -. 135 ), a ′ 2 = ( -. 317 , . 298 , . 243 , -. 026 )
- (b) λ 1 /(λ 1 + λ 2 ) = . 958, λ 2 /(λ 1 + λ 2 ) = . 042. Using the methods of Section 8.6.2, we have two tests, the first for significance of λ 1 and λ 2 and the second for significance of λ 2:
- (c) a ∗ ′ 1 = (. 076 , 1 . 553 , -1 . 182 , -. 439 ) , a ∗ ′ 2 = ( -1 . 162 , . 869 , . 828 , -. 085 )

|   Test |   /Lambda1 |      F | p -Value for F   |
|--------|------------|--------|------------------|
|      1 |     0.2245 | 8.3294 | < .0001          |
|      2 |     0.8871 | 1.3157 | .2869            |

F

- (d) F ( y 1 | y 2 , y 3 , y 4 ) = 1 . 067, F ( y 2 | y 1 , y 3 , y 4 ) = 20 . 975,

(

y

3

y

1

,

y

2

,

y

4

)

9

.

630,

F

(

y

4

y

1

,

y

2

,

y

3

)

1

.

228

- (e) In the plot, the first discriminant function separates groups 1 and 2 from group 3, but the second is ineffective in separating group 1 from group 2.

|

=

|

=

| 8.12   |   (a) λ i | λ i / ∑ 4 j = 1 λ j   | Eigenvector                                                                                                                               |
|--------|-----------|-----------------------|-------------------------------------------------------------------------------------------------------------------------------------------|
|        |    1.8757 | .6421 .2707           | a ′ 1 = (. 470 , - . 263 , . 653 , - . 074 ) a ′ 2 = (. 176 , . 188 , - 1 . 058 , 1 . 778 ) a ′ 3 = ( - . 155 , . 258 , . 470 , - . 850 ) |
|        |    0.7907 |                       |                                                                                                                                           |
|        |    0.229  | .0784                 |                                                                                                                                           |
|        |    0.026  | .0089                 | a ′ ( 3 . 614 , . 475 , . 310 , . 479 )                                                                                                   |

4

=

-

-

- (b) Test of significance of each eigenvalue and those that follow it:

|   Test |   /Lambda1 |   Approximate F | p -Value for F   |
|--------|------------|-----------------|------------------|
|      1 |     0.154  |           4.937 | < .0001          |
|      2 |     0.4429 |           3.188 | .0006            |
|      3 |     0.7931 |           1.68  | .1363            |
|      4 |     0.9747 |           0.545 | .5839            |

<!-- formula-not-decoded -->

- (d) F ( y 1 | y 2 , y 3 , y 4 ) = . 299, F ( y 2 | y 1 , y 3 , y 4 ) = 1 . 931, F ( y 3 | y 1 , y 2 , y 4 ) = 6 . 085, F ( y 4 | y 1 , y 2 , y 3 ) = 4 . 659

<!-- formula-not-decoded -->

- (e) In the plot, the first discriminant function separates groups 1, 4, and 6 from groups 2, 3, and 5. The second function achieves some separation of group 6 from groups 1 and 4 and some separation of group 3 from groups 2 and 5.
- 8.13 Three variables entered the model in the stepwise selection. The summary table is as follows:

|   Step | Variable Entered   |   Overall /Lambda1 | p -Value   |   Partial /Lambda1 |   Partial F | p -Value   |
|--------|--------------------|--------------------|------------|--------------------|-------------|------------|
|      1 | y 4                |             0.4086 | < .0001    |             0.4086 |      12.158 | < .0001    |
|      2 | y 3                |             0.2655 | < .0001    |             0.6499 |       4.418 | .0026      |
|      3 | y                  |             0.1599 | < .0001    |             0.6022 |       5.284 | .0008      |

2

## 8.14 Summary table:

|   Step | Variable Entered   |   Overall /Lambda1 | p -Value   |   Partial /Lambda1 |   Partial F | p -Value   |
|--------|--------------------|--------------------|------------|--------------------|-------------|------------|
|      1 | y 4                |             0.6392 | < .0001    |             0.6392 |      21.451 | < .0001    |
|      2 | y 3                |             0.543  | < .0001    |             0.8495 |       6.554 | .0147      |
|      3 | y 6                |             0.4594 | < .0001    |             0.8461 |       6.549 | .0148      |
|      4 | y 2                |             0.4063 | < .0001    |             0.8843 |       4.578 | .0394      |
|      5 | y 5                |             0.3639 | < .0001    |             0.8957 |       3.959 | .0547      |

In this case, the fifth variable to enter, y 5, would not ordinarily be included in the subset. The p -value of .0547 is large in this setting, where several tests are run at each step and the variable with smallest p -value is selected.

## 8.15 Summary table:

|   Step | Variable Entered   |   Overall /Lambda1 | p -Value   |   Partial /Lambda1 |   Partial F | p -Value   |
|--------|--------------------|--------------------|------------|--------------------|-------------|------------|
|      1 | y 2                |             0.6347 | .0006      |             0.6347 |       9.495 | .0006      |
|      2 | y 3                |             0.2606 | < .0001    |             0.4106 |      22.975 | < .0001    |

## CHAPTER 9

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

and substitute f ( y | Gi ) = Np ( 𝛍 i , 𝚺 ) from (4.2) to obtain

<!-- formula-not-decoded -->

Substitute estimates for 𝛍 1, 𝛍 2, and 𝚺 , and take the logarithm of both sides to obtain (9.8). Note that if a &gt; b , then ln a &gt; ln b .

- 9.4 Maximizing pi f ( y , Gi ) is equivalent to maximizing ln [ pi f ( y | Gi ) ] . Use f ( y | Gi ) = Np ( 𝛍 i , 𝚺 ) from (4.2) and take the logarithm to obtain

<!-- formula-not-decoded -->

Expand the last term, delete terms common to all groups (terms that do not involve i ), and substitute estimators of 𝛍 i and 𝚺 to obtain (9.11).

- 9.5 Use f ( y | Gi ) = Np ( 𝛍 i , 𝚺 i ) in ln [ pi f ( y | Gi ) ] , delete -( p / 2 ) ln ( 2 π) , and substitute y i and S i for 𝛍 i and 𝚺 i .

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Error rate = 1 39 = . 0256

<!-- formula-not-decoded -->

- (c) Using the k nearest neighbor method with k = 5, we obtain the same classification table as in part (b). With k = 4, two observations are misclassified, and the error rate becomes 2 / 39 = . 0513.

<!-- formula-not-decoded -->

- (b) Linear Classification

| Actual Group   | Number of Observations   | Predicted Group   |
|----------------|--------------------------|-------------------|
|                | 39                       | 1 2               |
| 1              |                          | 37 2              |
| 2              | 34                       | 8 26              |

Error rate = ( 2 + 8 )/ 73 = . 1370

- (c) p 1 and p 2 Proportional to Sample Sizes

| Actual Group   | Number of Observations   | Predicted Group 1 2   |
|----------------|--------------------------|-----------------------|
|                | 39                       | 37 2                  |
| 1              |                          |                       |
| 2              | 34                       | 8 26                  |

Error rate = ( 2 + 8 )/ 73 = . 1370

<!-- formula-not-decoded -->

(b)

(c)

(d)

## 9.9 (a)

Linear Classification

| Actual Group   | Number of Observations   | Predicted Group   |
|----------------|--------------------------|-------------------|
|                | 9                        | 1 2               |
| 1              |                          | 8 1               |
| 2              | 10                       | 1 9               |

Error rate

=

=

1053

Holdout Method

| Actual Group   | Number of Observations   | Predicted Group 1 2   |
|----------------|--------------------------|-----------------------|
| 1              | 9                        | 6 3                   |
| 2              |                          |                       |
|                | 10                       | 3 7                   |

Error rate = ( 3 + 3 )/ 19 = . 3158

Kernel Density Estimator with

h

2

| Actual   | Number of    | Predicted Group   |
|----------|--------------|-------------------|
| Group    | Observations | 1 2               |
| 1        | 9            | 9 0               |
| 2        | 10           | 1 9               |

Error rate = 1 19 = . 0526

| Actual   | Number of    | Predicted Group   |
|----------|--------------|-------------------|
| Group    | Observations | 1 2               |
| 1        | 20           | 18 2              |
| 2        | 20           | 2 18              |

Error rate = ( 2 + 2 )/ 40 = . 100

- (b) Four variables were selected by the stepwise discriminant analysis: y 2, y 3, y 4, and y 6 (see Problem 8.14). With these four variables we obtain the classification table in part (c).

=

2

19

.

| Actual Group   | Number of Observations   | Predicted Group 1 2   |
|----------------|--------------------------|-----------------------|
|                | 20                       |                       |
| 1              |                          | 18 2                  |
| 2              | 20                       | 2 18                  |

Error rate = ( 2 + 2 )/ 40 = . 100. The four variables classified the sample as well as did all six variables in part (a).

<!-- formula-not-decoded -->

(b)

(c)

| Group 1   | Group 2               | Group 3   |
|-----------|-----------------------|-----------|
| - 72.77   | - 65.18               | - 68.57   |
| .81       | 2.12                  | .68       |
| 15.15     | 10.11                 | 2.79      |
| - 1.03    | - .24                 | 6.54      |
| 10.02     | 11.06                 | 13.09     |
|           | Linear Classification |           |

| Actual   | Number of    |   Predicted Group |   Predicted Group |
|----------|--------------|-------------------|-------------------|
| Group    | Observations |                 1 |                 3 |
| 1        | 12           |                 9 |                 0 |
| 2        | 12           |                 3 |                 2 |
| 3        | 12           |                 0 |                11 |

Error rate = ( 3 + 3 + 2 + 1 )/ 36 = . 250

Quadratic Classification

| Actual   | Number of    |   Predicted Group |   Predicted Group |   Predicted Group |
|----------|--------------|-------------------|-------------------|-------------------|
| Group    | Observations |                 1 |                 2 |                 3 |
| 1        | 12           |                10 |                 2 |                 0 |
| 2        | 12           |                 2 |                 8 |                 2 |
| 3        | 12           |                 0 |                 1 |                11 |

Error rate = ( 2 + 2 + 2 + 1 )/ 36 = . 194

## (d) Linear Classification-Holdout Method

| Actual Group   | Number of Observations   | Predicted Group   |
|----------------|--------------------------|-------------------|
| 1              | 12                       | 1 2 3             |
|                |                          | 7 5 0             |
| 2              | 12                       | 4 5 3             |
| 3              | 12                       | 0 1 11            |

Error rate = ( 5 + 4 + 3 + 1 )/ 36 = . 361

(e)

(b)

(c)

k Nearest Neighbor with k = 5

| Actual   | Number of    |   Predicted Group |   Predicted Group |   Predicted Group |
|----------|--------------|-------------------|-------------------|-------------------|
| Group    | Observations |                 1 |                 2 |                 3 |
| 1        | 11           |                 9 |                 2 |                 0 |
| 2        | 11           |                 2 |                 7 |                 2 |
| 3        | 12           |                 0 |                 1 |                11 |

Error rate = ( 2 + 2 + 2 + 1 )/ 34 = . 206

9.11 (a) By (9.10), Li ( y ) = y ′ i S -1 pl y -1 2 y ′ i S -1 pl y i = c ′ i y + c 0 i . The vectors ( c 0 i c i ) , i = 1, 2 , . . . , 6, are

| Group 1   | Group 2   | Group 3   | Group 4   | Group 5   | Group 6   |
|-----------|-----------|-----------|-----------|-----------|-----------|
| - 300.0   | - 353.2   | - 328.5   | - 291.8   | - 347.5   | - 315.8   |
| 314.6     | 317.1     | 324.6     | 307.3     | 316.8     | 311.3     |
| - 59.4    | - 64.0    | - 65.2    | - 59.4    | - 65.8    | - 63.1    |
| 149.6     | 168.2     | 154.9     | 147.7     | 168.2     | 160.6     |
| 161.2     | 172.6     | 150.4     | 153.4     | 172.9     | 175.5     |

-

-

-

-

Linear Classification

| Actual   | Number of    |   Predicted Group |   Predicted Group |   Predicted Group |   Predicted Group |   Predicted Group |   Predicted Group |
|----------|--------------|-------------------|-------------------|-------------------|-------------------|-------------------|-------------------|
| Group    | Observations |                 1 |                 2 |                 3 |                 4 |                 5 |                 6 |
| 1        | 8            |                 5 |                 0 |                 0 |                 1 |                 0 |                 2 |
| 2        | 8            |                 0 |                 3 |                 2 |                 1 |                 2 |                 0 |
| 3        | 8            |                 0 |                 0 |                 6 |                 1 |                 1 |                 0 |
| 4        | 8            |                 3 |                 0 |                 1 |                 4 |                 0 |                 0 |
| 5        | 8            |                 0 |                 3 |                 1 |                 0 |                 3 |                 1 |
| 6        | 8            |                 2 |                 0 |                 0 |                 0 |                 2 |                 4 |

Quadratic Classification

Correct classification rate = ( 5 + 3 + 6 + 4 + 3 + 4 )/ 48 = . 521 Error rate = 1 -. 521 = . 479

| Actual   | Number of    |   Predicted Group |   Predicted Group |   Predicted Group |   Predicted Group |   Predicted Group |   Predicted Group |
|----------|--------------|-------------------|-------------------|-------------------|-------------------|-------------------|-------------------|
| Group    | Observations |                 1 |                 2 |                 3 |                 4 |                 5 |                 6 |
| 1        | 8            |                 8 |                 0 |                 0 |                 0 |                 0 |                 0 |
| 2        | 8            |                 0 |                 7 |                 0 |                 1 |                 0 |                 0 |
| 3        | 8            |                 1 |                 0 |                 6 |                 0 |                 1 |                 0 |
| 4        | 8            |                 0 |                 0 |                 1 |                 7 |                 0 |                 0 |
| 5        | 8            |                 0 |                 3 |                 0 |                 0 |                 4 |                 1 |
| 6        | 8            |                 2 |                 0 |                 0 |                 0 |                 1 |                 5 |

Correct classification rate = ( 8 + 7 + 6 + 7 + 4 + 5 )/ 48 = . 771 Error rate = -. 771 = . 229

-

-

(d)

(e)

k Nearest Neighbor with k = 3

| Actual   | Number of    |   Predicted Group |   Predicted Group |   Predicted Group |   Predicted Group |   Predicted Group |   Predicted Group |      |
|----------|--------------|-------------------|-------------------|-------------------|-------------------|-------------------|-------------------|------|
| Group    | Observations |                 1 |                 2 |                 3 |                 4 |                 5 |                 6 | Ties |
| 1        | 8            |                 5 |                 0 |                 0 |                 2 |                 0 |                 0 | 1    |
| 2        | 8            |                 0 |                 4 |                 0 |                 0 |                 1 |                 0 | 3    |
| 3        | 8            |                 1 |                 0 |                 6 |                 0 |                 1 |                 0 | 0    |
| 4        | 8            |                 0 |                 0 |                 0 |                 5 |                 0 |                 0 | 3    |
| 5        | 8            |                 0 |                 1 |                 0 |                 0 |                 6 |                 1 | 0    |
| 6        | 8            |                 2 |                 0 |                 0 |                 0 |                 0 |                 5 | 1    |

Correct classification rate = ( 5 + 4 + 6 + 5 + 6 + 5 )/ 40 = . 775 Error rate = 1 -. 775 = . 225

Normal Kernel with h = 1 (For this data set, larger values of h do much worse.)

| Actual   | Number of    |   Predicted Group |   Predicted Group |   Predicted Group |   Predicted Group |   Predicted Group |   Predicted Group |
|----------|--------------|-------------------|-------------------|-------------------|-------------------|-------------------|-------------------|
| Group    | Observations |                 1 |                 2 |                 3 |                 4 |                 5 |                 6 |
| 1        | 8            |                 8 |                 0 |                 0 |                 0 |                 0 |                 0 |
| 2        | 8            |                 0 |                 8 |                 0 |                 0 |                 0 |                 0 |
| 3        | 8            |                 1 |                 0 |                 6 |                 0 |                 1 |                 0 |
| 4        | 8            |                 1 |                 0 |                 0 |                 7 |                 0 |                 0 |
| 5        | 8            |                 0 |                 0 |                 0 |                 0 |                 7 |                 1 |
| 6        | 8            |                 2 |                 0 |                 0 |                 0 |                 0 |                 6 |

Correct classification rate = ( 8 + 8 + 6 + 7 + 7 + 6 )/ 48 = . 875 Error rate = 1 -. 875 = . 125

## CHAPTER 10

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

- 10.5 First show that cov ( ˆ 𝛃 p ) = σ 2 ( X ′ p X p ) -1 . This can be done by noting that ˆ 𝛃 p = ( X ′ p X p ) -1 X ′ p y = Ay , say. Then, by (3.74), cov ( Ay ) = A cov ( y ) A ′ = A (σ 2 I ) A ′ = σ 2 AA ′ . By substituting A = ( X ′ p X p ) -1 X ′ p , this becomes cov ( ˆ 𝛃 p ) = σ 2 ( X ′ p X p ) -1 . Then, by (3.70), var ( x ′ pi ˆ 𝛃 p ) = x ′ pi cov ( ˆ 𝛃 p ) x pi and the remaining steps follow as indicated.

The second term on the right vanishes because E ( yi ) E ( yi ) is constant and E [ ˆ yi -E ( ˆ yi ) ] = E ( ˆ yi ) -E ( ˆ yi ) = 0. For the third term, we have E [ E ( ˆ yi ) -E ( yi ) 2 E ( yi ) E ( yi ) 2 , because E ( yi ) E ( yi ) 2 is constant.

[ ˆ -] ] = [ ˆ -] [ ˆ -]

- 10.6 By (10.36), s 2 p = SSE p /( n -p ) . Then by (10.44),

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

- 10.7 ( Y -X ˆ B ) ′ ( Y -X ˆ B ) = Y ′ Y -Y ′ X ˆ B - ˆ B ′ X ′ Y - ˆ B ′ X ′ X ˆ B . Transpose ˆ B = ( X ′ X ) -1 X ′ Y from (10.46) and substitute into ˆ B ′ X ′ X ˆ B .

The second and third terms are equal to O because [ E ( ˆ y i ) -E ( y i ) ] is a constant vector and E [ˆ y i -E ( ˆ y i ) ] = E ( ˆ y i ) -E ( ˆ y i ) = 0 . The fourth term is a constant matrix and the first E can be deleted.

<!-- formula-not-decoded -->

- 10.9 As in Problem 10.5, we have cov ( ˆ 𝛃 p ( j )) = σ j j ( X ′ p X p ) -1 , where σ j j = var ( y j ) is the j th diagonal element of 𝚺 = cov ( y ) . Similarly, cov ( ˆ 𝛃 p ( j ) , ˆ 𝛃 p ( k )) = σ j k ( X ′ p X p ) -1 , where σ j k = cov ( y j , yk ) is the ( j k ) th element of 𝚺 .

The notation cov ( ˆ 𝛃 p ( j ) , ˆ 𝛃 p ( k )) indicates a matrix containing the covariance of each element of ˆ 𝛃 p ( j ) and each element of ˆ 𝛃 p ( k ) . Now for the covariance matrix, cov ( ˆ y ′ i ) = cov ( x ′ pi ˆ 𝛃 p ( 1 ) , . . . , x ′ pi ˆ 𝛃 p ( m )) , we need the variance of each of the m random variables and the covariance of each pair. By Problem 10.5 and (3.70), var ( x ′ pi ˆ 𝛃 p ( 1 )) = x ′ pi cov ( ˆ 𝛃 p ( 1 )) x pi = σ 11 x ′ pi ( X ′ p X p ) -1 x pi . Similarly, cov ( x ′ pi ˆ 𝛃 p ( 1 ) , x ′ pi ˆ 𝛃 p ( 2 )) = σ 12 x ′ pi ( X ′ p X p ) -1 x pi . The other variances and covariances can be obtained in an analogous manner.

10.10 By (10.77), S p = E p /( n -p ) . Then by (10.83),

<!-- formula-not-decoded -->

- 10.11 E -1 E p E -1 E p &gt; 0, because both E -1 and E p are positive definite.
- 10.12 By (10.84), C p S 1 E p ( 2 p n ) I . Using S k E k /( n k ) , we obtain

| k | = | k || | k = -k + -= -

<!-- formula-not-decoded -->

10.13 If C p is replaced by p I in (10.86), we obtain

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

- (c) λ 1 = . 3594, λ 2 = . 0160. The essential rank of ˆ B 1 is 1, and the power ranking is θ &gt; U ( s ) &gt; /Lambda1 &gt; V ( s ) .
- (b) /Lambda1 = . 724, V ( s ) = . 280, U ( s ) = . 375, θ = . 264
- (d) The Wilks' /Lambda1 test of x 2 adjusted for x 1 and x 3, for example, is given by
- (10.65) as

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

which is distributed as /Lambda1 p , 1 , n -4 and has an exact F -transformation. The tests for x 1 and x 3 are similar. For the three tests we obtain the following:

|                 |   /Lambda1 |     F |   p -Value |
|-----------------|------------|-------|------------|
| x 1 | x 2 , x 3 |      0.931 | 1.519 |      0.231 |
| x 2 | x 1 , x 3 |      0.887 | 2.606 |      0.086 |
| x 3 x 1 , x 2   |      0.762 | 6.417 |      0.004 |

<!-- formula-not-decoded -->

- (b) /Lambda1 = . 377, V ( s ) = . 625, U ( s ) = 1 . 647, θ = . 622

<!-- formula-not-decoded -->

- (c) λ 1 = 1 . 644, λ 2 = . 0029. The essential rank of ˆ B 1 is 1, and the power ranking is θ &gt; U ( s ) &gt; /Lambda1 &gt; V ( s ) .

<!-- formula-not-decoded -->

- (b) /Lambda1 = . 665, V ( s ) = . 365, U ( s ) = . 458, θ = . 240

<!-- formula-not-decoded -->

- (c) λ 1 = . 3159, λ 2 = . 1385, λ 3 = . 0037. The essential rank of ˆ B 1 is 2, and the power ranking is V ( s ) &gt; /Lambda1 &gt; U ( s ) &gt; θ .

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Test of overall regression of ( y 1 , y 2 ) on ( x 1 , x 2 , . . . , x 8 ) : /Lambda1 = . 4642 (with p = 2, exact F = 1 . 169, p -value = . 332). Tests on subsets (the F 's are exact because p = 2):

<!-- formula-not-decoded -->

|                                                     |   /Lambda1 |     F |   p -Value |
|-----------------------------------------------------|------------|-------|------------|
| ( b ) x 7 , x 8 | x 1 , x 2 , . . . , x 6           |      0.856 | 0.808 |      0.527 |
| ( c ) x 4 , x 5 , x 6 | x 1 , x 2 , x 3 , x 7 , x 8 |      0.674 | 1.457 |      0.218 |
| ( d ) x 1 , x 2 , x 3 x 4 , x 5 , . . . , x 8       |      0.569 | 2.17  |      0.066 |

|

10.18 (a) The overall test of ( y 1 , y 2 ) on ( x 1 , x 2 , . . . , x 8 ) gives /Lambda1 = . 4642, with (exact) F = 1 . 169 ( p -value = . 332 ) . Even though this test result is not significant, we give the results of a backward elimination for illustrative purposes:

|      |      | Partial /Lambda1 -Test on Each x i Using (10.72)   | Partial /Lambda1 -Test on Each x i Using (10.72)   | Partial /Lambda1 -Test on Each x i Using (10.72)   | Partial /Lambda1 -Test on Each x i Using (10.72)   | Partial /Lambda1 -Test on Each x i Using (10.72)   |      |      |
|------|------|----------------------------------------------------|----------------------------------------------------|----------------------------------------------------|----------------------------------------------------|----------------------------------------------------|------|------|
| Step | x 1  | x 2                                                | x 3                                                | x 4                                                | x 5                                                | x 6                                                | x 7  | x 8  |
| 1    | .723 | .969                                               | .817                                               | .859                                               | .821                                               | .945                                               | .924 | .943 |
| 2    | .741 |                                                    | .801                                               | .851                                               | .839                                               | .948                                               | .927 | .940 |
| 3    | .737 |                                                    | .837                                               | .798                                               | .757                                               |                                                    | .949 | .938 |
| 4    | .675 |                                                    | .852                                               | .821                                               | .794                                               |                                                    |      | .925 |
| 5    | .680 |                                                    | .861                                               | .835                                               | .817                                               |                                                    |      |      |
| 6    | .701 |                                                    |                                                    | .805                                               | .806                                               |                                                    |      |      |
| 7    | .855 |                                                    |                                                    | .930                                               |                                                    |                                                    |      |      |
| 8    | .891 |                                                    |                                                    |                                                    |                                                    |                                                    |      |      |

At each step, the variable deleted was not significant. In fact, the variable remaining at the last step, x 1, is not a significant predictor of y 1 and y 2.

- (b) There were no significant x 's, but to illustrate, we will use the three x 's at step 6 and test for each y :

|           |   /Lambda1 |     F |   p -Value |
|-----------|------------|-------|------------|
| y 1 | y 2 |      0.701 | 3.548 |      0.029 |
| y 2 y 1   |      0.808 | 1.984 |      0.142 |

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

10.20 Using a backward elimination based on (10.72), we obtain the following partial /Lambda1 -values:

|   Step | x 1   | x 2   |   x 3 | x 4   |   x 5 |   x 6 | x 7   | x 8   |   x 9 |
|--------|-------|-------|-------|-------|-------|-------|-------|-------|-------|
|      1 | .993  | .962  | 0.916 | .958  | 0.919 | 0.879 | .981  | .999  | 0.797 |
|      2 | .994  | .962  | 0.916 | .956  | 0.909 | 0.874 | .980  |       | 0.626 |
|      3 |       | .951  | 0.883 | .954  | 0.912 | 0.873 | .981  |       | 0.626 |
|      4 |       | .948  | 0.884 | .955  | 0.861 | 0.867 |       |       | 0.561 |
|      5 |       | .953  | 0.862 |       | 0.84  | 0.803 |       |       | 0.561 |
|      6 |       |       | 0.83  |       | 0.781 | 0.783 |       |       | 0.535 |

At step 6, we stop and retain all four x 's because each /Lambda1 has a p -value less than .05.

## CHAPTER 11

- 11.1 By (3.38), S yy = D y R yy D y and S xx = D x R xx D x , where D y and D x are defined below (11.14). Similarly, S yx = D y R yx D x and S xy = D x R xy D y . Substitute these into (11.7), replace I by D -1 y D y , and factor out D y on the right.
- 11.2 Multiply (11.7) by S -1 xx S xy on the left to obtain ( S -1 xx S xy S -1 yy S yx S -1 xx S xy -r 2 S -1 xx S xy ) a = 0 . Factor out S -1 xx S xy on the right to write this in the form ( S -1 xx S xy S -1 yy S yx -r 2 I ) S -1 xx S xy a = 0 . Upon comparing this to (11.8), we see that b = S -1 xx S xy a .
- 11.3 When p = 1 , s is also 1, and there is only one canonical correlation, which is equal to R 2 from multiple regression [see comments between (11.28) and (11.29)]. Thus

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

11.5 By (11.39),

<!-- formula-not-decoded -->

11.6 Substitute E = ( n -1 )( S yy -S yx S -1 xx S xy ) and H = ( n -1 ) S yx S -1 xx S xy from (11.44) and (11.45) into (11.41):

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

11.7 By (11.42), S yx S -1 xx S xy a = r 2 S yy a . Subtracting r 2 S yx S -1 xx S xy a from both sides gives

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

11.9 (a) r 1 = . 7885, r 2 = . 0537

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

11.10

11.11

r

<!-- formula-not-decoded -->

3

.

0609

<!-- formula-not-decoded -->

4135

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

## 11.12 (b) By (11.34),

where r 2 1 and r 2 2 are the squared canonical correlations from the full model, and c 2 1 and c 2 2 are the squared canonical correlations from the reduced model:

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

|   /Lambda1 |   Approximate F | p -Value   |
|------------|-----------------|------------|
|     0.0925 |         17.9776 | < .0001    |
|     0.6651 |          4.6366 | .0020      |
|     0.9725 |          1.1898 | .2816      |

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

| k       |   /Lambda1 Approximate | p -Value   |
|---------|------------------------|------------|
| 1 .0561 |                  4.992 | < .0001    |
| 2 .3037 |                  2.601 | .0007      |
| 3 .7747 |                  0.829 | .6210      |
| 4 .8898 |                  0.761 | .6030      |
| 5 .9937 |                  0.124 | .8840      |

<!-- formula-not-decoded -->

## CHAPTER 12

- 12.1 From λ = a ′ Sa / a ′ a in (12.7), we obtain λ a ′ a = aSa , which can be factored as a ′ ( Sa -λ a ) = 0. Since a = 0 is not a solution to λ = a ′ Sa / a ′ a , we have Sa -λ a = 0 .

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

which gives a 11 = a 12 for any r . Normalizing to a ′ 1 a 1 = 1, yields a 11 = 1 / √ 2.

- 12.3 (a) By (4.14) and a comment in Section 7.1, the likelihood ratio is given by LR = ( | S | / | S 0 | ) n / 2 , where S 0 is the estimate of 𝚺 under H 0. By (2.108) and (7.6), the test statistic is

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

- In (12.15), the coefficient n is modified to give an improved chi-square approximation.
- 12.4 If S is diagonal, then λ i = sii , as in (12.17). Thus

<!-- formula-not-decoded -->

- 12.5 By (10.34) and (12.2),

From the first element, we obtain s 11 ai 1 = sii ai 1 or ( s 11 -sii ) ai 1 = 0. Since s 11 -sii /negationslash= 0 (unless i = 1), we must have ai 1 = 0. Thus, a i = ( 0 , . . . , 0 , aii , 0 , . . . , 0 ) ′ , and normalizing a i leads to aii = 1.

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Show that this is equal to

<!-- formula-not-decoded -->

- 12.6 The variances of y 1, y 2, x 1, x 2, and x 3 on the diagonal of S are .016, 70.6, 1106.4, 2381.9, and 2136.4. The eigenvalues of S and R are as follows:

<!-- formula-not-decoded -->

| S       | S             | S          | R    | R             | R          |
|---------|---------------|------------|------|---------------|------------|
| λ i     | λ i / ∑ j λ j | Cumulative | λ i  | λ i / ∑ j λ j | Cumulative |
| 3466.18 | .608607       | .60861     | 1.72 | .34           | .34        |
| 1264.47 | .222021       | .83063     | 1.23 | .25           | .59        |
| 895.27  | .157195       | .98782     | .96  | .19           | .78        |
| 69.34   | .012174       | .99999     | .79  | .16           | .94        |
| .01     | .000002       | 1.00000    | .30  | .06           | 1.00       |

Two principal components of S account for 83% of the variance, but it requires three principal components of R to reach 78%. For most purposes we would use two components of S , although with three we could account for 99% of the variance. However, we show all five eigenvectors below because of the interesting pattern they exhibit. The first principal component is largely a weighted average of the last two variables, x 2 and x 3, which have the largest variances. The second and third components represent contrasts in the last three variables and could be described as 'shape' components. The fourth and fifth components are associated uniquely with y 2 and y 1, respectively. These components are 'variable specific,' as described in the discussion of method 1 in Section 12.6. As expected, the principal components of R show an entirely different pattern. All five variables contribute to the first three components of R , whereas in S , y 1 and y 2 have small variances and contribute almost nothing to the first three components. The eigenvectors of S and R are as follows:

|     | S       | S       | S       | S       | S       | R   | R     | R     | R     | R     |
|-----|---------|---------|---------|---------|---------|-----|-------|-------|-------|-------|
|     | a 1     | a 2     | a 3     | a 4     | a 5     | a 1 | a 2   | a 3   | a 4   | a 5   |
| y 1 | .0004   | - .0008 | .0018   | .0029   | .9999   | .42 | .53   | - .42 | - .40 | .46   |
| y 2 | - .0080 | .0166   | .0286   | .9994   | - .0029 | .07 | .68   | .16   | .70   | - .10 |
| x 1 | .1547   | .6382   | .7535   | - .0309 | - .0008 | .36 | .20   | .76   | - .44 | - .24 |
| x 2 | .7430   | .4279   | - .5145 | .0136   | .0009   | .54 | - .43 | .25   | .39   | .56   |
| x 3 | .6511   | - .6397 | .4083   | .0042   | - .0015 | .63 | - .18 | - .40 | .10   | - .64 |

<!-- formula-not-decoded -->

The eigenvalues of S and R are as follows:

| S     | S             | S          | R    | R             | R          |
|-------|---------------|------------|------|---------------|------------|
| λ i   | λ i / ∑ j λ j | Cumulative | λ i  | λ i / ∑ j λ i | Cumulative |
| 200.4 | .684          | .684       | 3.42 | .683          | .683       |
| 36.1  | .123          | .807       | .61  | .123          | .806       |
| 34.1  | .116          | .924       | .57  | .114          | .921       |
| 15.0  | .051          | .975       | .27  | .054          | .975       |
| 7.4   | .025          | 1.000      | .13  | .025          | 1.000      |

The first three eigenvectors of S and R are as follows:

| S   | S     | S     | R   | R     | R     |
|-----|-------|-------|-----|-------|-------|
| a 1 | a 2   | a 3   | a 1 | a 2   | a 3   |
| .47 | - .58 | - .42 | .44 | - .20 | - .68 |
| .39 | - .11 | .45   | .45 | - .43 | .35   |
| .49 | .10   | - .48 | .47 | .37   | - .38 |
| .47 | - .12 | .62   | .45 | - .39 | .33   |
| .41 | .80   | - .09 | .41 | .70   | .41   |

The variances in S are nearly identical, and the covariances are likewise similar in magnitude. Consequently, the percent of variance explained by the eigenvalues of S and R are indistinguishable. The interpretation of the second principal component from S is slightly different from that of the second one from R , but otherwise there is little to choose between them.

12.8 The variances on the diagonal of S are 95.5, 73.2, 76.2, 808.6, 505.9, and 508.7. The eigenvalues of S and R are as follows:

| S      | S             | S          | R    | R             | R          |
|--------|---------------|------------|------|---------------|------------|
| λ i    | λ i / ∑ j λ j | Cumulative | λ i  | λ i / ∑ j λ j | Cumulative |
| 1152.0 | .557          | .557       | 2.17 | .363          | .363       |
| 394.1  | .191          | .748       | 1.08 | .180          | .543       |
| 310.8  | .150          | .898       | .98  | .163          | .706       |
| 97.8   | .047          | .945       | .87  | .144          | .850       |
| 68.8   | .033          | .978       | .55  | .092          | .942       |
| 44.6   | .022          | 1.000      | .35  | .058          | 1.000      |

We could keep either two or three components from S . The first three components of S account for a larger percent of variance than do the first three from R . The first three eigenvectors of S and R are as follows:

| S    | S      | S      | R    | R      | R      |
|------|--------|--------|------|--------|--------|
| a 1  | a 2    | a 3    | a 1  | a 2    | a 3    |
| .080 | .092   | - .069 | .336 | .176   | .497   |
| .034 | - .018 | .202   | .258 | .843   | - .093 |
| .076 | .122   | - .011 | .370 | .049   | .466   |
| .758 | - .446 | - .469 | .475 | - .329 | - .358 |
| .493 | - .081 | .844   | .486 | .079   | - .567 |
| .412 | .878   | - .147 | .471 | - .376 | .278   |

As expected, the first three principal components from S are heavily influenced by the last three variables because of their relatively large variances.

12.9 The variances on the diagonal of S are .69; 5.4; 2,006, 682.4; 90.3; 56.4; 18.1. With the large variance of y 3, we would expect the first principal component from S to account for most of the variance, and y 3 would essentially constitute that single component. This is indeed the pattern that emerges in the eigenvalues and eigenvectors of S . The principal components from R , on the other hand, are not dominated by y 3. The eigenvalues of S and R are as follows:

| S         | S             | R    | R             | R          |
|-----------|---------------|------|---------------|------------|
| λ i       | λ i / ∑ j λ j | λ i  | λ i / ∑ j λ j | Cumulative |
| 2,006,760 | .999954       | 2.42 | .404          | .404       |
| 65        | .000033       | 1.40 | .234          | .638       |
| 18        | .000009       | 1.03 | .171          | .809       |
| 7         | .000003       | .92  | .153          | .963       |
| 3         | .000001       | .20  | .033          | .996       |
| 0         | .000000       | .02  | .004          | 1.000      |

Most of the correlations in R are small (only three exceed .3), and its first three principal components account for only 72% of the variance. The first three eigenvectors of S and R are as follows:

| S      | S       | S        | R    | R       | R       |
|--------|---------|----------|------|---------|---------|
| a 1    | a 2     | a 3      | a 1  | a 2     | a 3     |
| .00016 | .005    | - . 0136 | .424 | - . 561 | - . 150 |
| .00051 | .017    | .0787    | .446 | - . 528 | .087    |
| .99998 | - . 001 | - . 0002 | .563 | .387    | - . 051 |
| .00529 | .698    | .0174    | .454 | .267    | .166    |
| .00322 | - . 716 | .0195    | .303 | .425    | - . 296 |
| .00020 | .025    | .9965    | .073 | .069    | .923    |

## 12.10 Covariance matrix for males:

<!-- formula-not-decoded -->

Covariance matrix for females:

<!-- formula-not-decoded -->

The eigenvalues are as follows:

| Males   | Males         | Males      | Females   | Females       | Females    |
|---------|---------------|------------|-----------|---------------|------------|
| λ i     | λ i / ∑ j λ j | Cumulative | λ i       | λ i / ∑ j λ j | Cumulative |
| 43.56   | .684          | .684       | 48.96     | .571          | .571       |
| 11.14   | .175          | .858       | 18.46     | .215          | .786       |
| 6.47    | .102          | .960       | 13.54     | .158          | .944       |
| 2.52    | .040          | 1.000      | 4.82      | .056          | 1.000      |

The first two eigenvectors are as follows:

| Males   | Males   | Females   | Females   |
|---------|---------|-----------|-----------|
| a 1     | a 2     | a 1       | a 2       |
| .24     | .21     | .22       | .27       |
| .31     | .85     | .39       | .62       |
| .76     | - . 48  | .68       | .17       |
| .52     | .09     | .58       | - . 72    |

The variances in S M have a slightly wider range (5.19-28.67) than those in S F (9.14-30.04), and this is reflected in the eigenvalues. The first two components account for 86% of the variance from S M , whereas the first two account for 79% from S F .

## 12.11 Covariance matrix for species 1:

<!-- formula-not-decoded -->

Covariance matrix for species 2:

<!-- formula-not-decoded -->

The eigenvalues are as follows:

| Species 1   | Species 1     | Species 1   | Species 2   | Species 2     | Species 2   |
|-------------|---------------|-------------|-------------|---------------|-------------|
| λ i         | λ i / ∑ j λ j | Cumulative  | λ i         | λ i / ∑ j λ j | Cumulative  |
| 561.3       | .669          | .669        | 555.7       | .664          | .664        |
| 169.0       | .201          | .870        | 145.4       | .174          | .838        |
| 65.3        | .078          | .948        | 93.5        | .112          | .950        |
| 43.7        | .057          | 1.000       | 41.7        | .050          | 1.000       |

The first two eigenvectors are as follows:

| Species 1   | Species 1   | Species 2   | Species 2   |
|-------------|-------------|-------------|-------------|
| a 1         | a 2         | a 1         | a 2         |
| .50         | .01         | .28         | - . 20      |
| .72         | - . 48      | .81         | - . 34      |
| .17         | - . 22      | .42         | .14         |
| .45         | .85         | .30         | .91         |

The variances in S 1 have a wider range than those in S 2, and the first two components of S 1 account for a higher percent of variance.

## 12.12 The variances on the diagonal of S in each case are:

- (a) Pooled: 536.0, 59.9, 116.0, 896.4, 248.1, 862.0,

- (b)

- Unpooled: 528.2, 68.9, 145.2, 1366.4, 264.4, 1069.1. The eigenvalues are as follows:

| Pooled   | Pooled        | Pooled     | Unpooled   | Unpooled      | Unpooled   |
|----------|---------------|------------|------------|---------------|------------|
| λ i      | λ i / ∑ j λ j | Cumulative | λ i        | λ i / ∑ j λ j | Cumulative |
| 1050.6   | .386          | .386       | 1722.0     | .500          | .500       |
| 858.3    | .316          | .702       | 878.4      | .255          | .755       |
| 398.9    | .147          | .849       | 401.4      | .117          | .872       |
| 259.2    | .095          | .944       | 261.1      | .076          | .948       |
| 108.1    | .040          | .984       | 128.9      | .037          | .985       |
| 43.4     | .016          | 1.000      | 50.4       | .015          | 1.000      |

The first three eigenvectors are as follows:

| Pooled   | Pooled   | Pooled   | Unpooled   | Unpooled   | Unpooled   |
|----------|----------|----------|------------|------------|------------|
| a 1      | a 2      | a 3      | a 1        | a 2        | a 3        |
| .441     | - . 190  | .864     | .212       | .389       | .888       |
| .041     | - . 038  | .082     | - . 039    | .064       | .096       |
| - . 039  | .031     | .143     | .080       | - . 066    | .081       |
| .450     | .892     | - . 033  | .776       | - . 608    | .081       |
| - . 019  | - . 001  | - . 054  | - . 096    | .010       | .015       |
| .774     | - . 407  | - . 471  | .580       | .686       | - . 434    |

- (c) The pattern in eigenvalues as well as eigenvectors is similar for the pooled and unpooled cases. The first three principal components account for 87.2% of the variance in the unpooled case compared to 84.9% for the pooled case.
- 12.13 The variances on the diagonal of S in each case are:
- (a) Pooled: 49.1, 8.1, 12140.8, 136.2, 210.8, 2983.9,
- (b) Unpooled: 63.2, 8.0, 15168.9, 186.6, 255.4, 4660.7.

The eigenvalues are as follows:

| Pooled   | Pooled        | Pooled     | Unpooled   | Unpooled      | Unpooled   |
|----------|---------------|------------|------------|---------------|------------|
| λ i      | λ i / ∑ j λ j | Cumulative | λ i        | λ i / ∑ j λ j | Cumulative |
| 12,809.0 | .8249         | .8249      | 17,087.0   | .8400         | .8400      |
| 2,455.9  | .1582         | .9830      | 2,958.0    | .1454         | .9854      |
| 137.1    | .0088         | .9918      | 168.6      | .0083         | .9937      |
| 77.2     | .0050         | .9968      | 77.1       | .0038         | .9974      |
| 42.2     | .0027         | .9995      | 44.7       | .0022         | .9996      |
| 7.4      | .0005         | 1.0000     | 7.3        | .0004         | 1.0000     |

The eigenvectors are as follows:

| Pooled   | Pooled   | Unpooled   | Unpooled   |
|----------|----------|------------|------------|
| a 1      | a 2      | a 1        | a 2        |
| - . 004  | - . 000  | .013       | .027       |
| - . 005  | .004     | - . 004    | .004       |
| .968     | - . 233  | .931       | - . 355    |
| - . 002  | .023     | .028       | .069       |
| .103     | .041     | .103       | .021       |
| .228     | .971     | .350       | .932       |

- 12.14 The variances on the diagonal of S are all less than 1, except s 2 x 4 = 5 . 02 and s 2 x 8 = 1541 . 08. We therefore expect the last variable, x 8, to dominate the principal components of S . This is the case for S but not for R . The eigenvalues of S and R are as follows:

| S       | S             | R     | R             | R          |
|---------|---------------|-------|---------------|------------|
| λ i     | λ i / ∑ j λ j | λ i   | λ i / ∑ j λ j | Cumulative |
| 1541.55 | .996273       | 3.174 | .317          | .317       |
| 4.83    | .003123       | 2.565 | .256          | .574       |
| .44     | .000286       | 1.432 | .143          | .717       |
| .27     | .000174       | 1.277 | .128          | .845       |
| .10     | .000066       | .542  | .054          | .899       |
| .07     | .000043       | .473  | .047          | .946       |
| .02     | .000014       | .251  | .025          | .971       |
| .02     | .000011       | .118  | .012          | .983       |
| .01     | .000005       | .104  | .010          | .994       |
| .00     | .000003       | .064  | .006          | 1.000      |

The eigenvectors of S and R are as follows:

| S        | S       | R      | R      | R      | R      |
|----------|---------|--------|--------|--------|--------|
| a 1      | a 2     | a 1    | a 2    | a 3    | a 4    |
| .0009    | - . 005 | .12    | .19    | .69    | .10    |
| .0007    | - . 034 | .06    | .32    | .54    | .26    |
| .0029    | - . 007 | .46    | - . 06 | .07    | - . 38 |
| .0014    | .004    | .29    | .17    | - . 18 | .49    |
| .0059    | - . 009 | .52    | .14    | - . 04 | - . 01 |
| - . 0150 | .982    | - . 09 | - . 42 | .07    | .55    |
| - . 0028 | - . 092 | - . 31 | .45    | - . 01 | - . 14 |
| - . 0022 | - . 158 | - . 23 | .54    | - . 14 | - . 10 |
| .0044    | - . 011 | .09    | .36    | - . 38 | .44    |
| .9998    | .014    | .50    | .11    | . 13   | . 09   |

-

-

12.15 The variances in the diagonal of S are: 55.7, 10.9, 402.7, 25.7, 13.4, 438.3, 1.5, 106.2, 885.6, 22227.2, 214.1.

The eigenvalues of S and R are as follows:

| S        | S             | R          | R     | R             | R          |
|----------|---------------|------------|-------|---------------|------------|
| λ i      | λ i / ∑ j λ j | Cumulative | λ i   | λ i / ∑ j λ j | Cumulative |
| 22,303.5 | .91479        | .91479     | 6.020 | .54730        | .54730     |
| 1590.7   | .06524        | .98003     | 2.119 | .19267        | .73996     |
| 358.0    | .01469        | .99471     | 1.130 | .10275        | .84272     |
| 63.4     | .00260        | .99731     | .760  | .06909        | .91181     |
| 29.3     | .00120        | .99852     | .355  | .03231        | .94411     |
| 17.1     | .00070        | .99922     | .259  | .02358        | .96769     |
| 12.7     | .00052        | .99974     | .122  | .01110        | .97879     |
| 2.8      | .00012        | .99986     | .110  | .01004        | .98883     |
| 1.9      | .00008        | .99994     | .060  | .00544        | .99427     |
| .9       | .00004        | .99997     | .042  | .00384        | .99810     |
| .7       | .00003        | 1.00000    | .021  | .00190        | 1.00000    |

The eigenvectors of S and R are as follows:

|      | S        | S        | R        | R        | R        | R        |
|------|----------|----------|----------|----------|----------|----------|
|      | a 1      | a 2      | a 1      | a 2      | a 3      | a 4      |
| y 1  | - . 0097 | .1331    | .3304    | - . 0787 | .0880    | - . 2807 |
| y 2  | .0006    | .0608    | .3542    | .1928    | .1071    | - . 2301 |
| y 3  | - . 0141 | .4397    | .3923    | .0518    | .1105    | - . 1413 |
| y 4  | - . 0033 | .1078    | .3820    | .0474    | .1334    | - . 0104 |
| y 5  | .0101    | .0398    | .2323    | .5303    | .0154    | - . 0710 |
| y 6  | .0167    | .4290    | .3621    | .2361    | .1198    | .1350    |
| y 7  | - . 0012 | - . 0072 | - . 0884 | .0213    | .7946    | .5414    |
| y 8  | .0275    | - . 1844 | - . 2501 | .5023    | .0826    | - . 1506 |
| y 9  | .0456    | - . 6657 | - . 3111 | .3595    | .2136    | - . 2278 |
| y 10 | .9982    | .0346    | - . 0243 | .4685    | - . 4669 | .5001    |
| y 11 | .0034    | .3311    | .3357    | - . 1153 | - . 1853 | .4550    |

For most purposes, one or two principal components would suffice for S , with 91% or 98% of the variance explained. For R , on the other hand, three components are required to explain 84% of the variance, and seven components are necessary to reach 98%. The reduction to one or two components for S is due in part to the relatively large variances of y 3, y 6, y 9, and y 10. In the eigenvectors of S , we see that these four variables figure prominently in the first two principal components.

## CHAPTER 13

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

By (2.98),

Sum of squared elements of S - ˆ 𝚲 ˆ 𝚲 ′ = tr ( S - ˆ 𝚲 ˆ 𝚲 ′ ) ′ ( S - ˆ 𝚲 ˆ 𝚲 ′ ).

Since S 𝚲 𝚲 is symmetric, we have by (13.20), (13.23), and (13.24),

<!-- formula-not-decoded -->

where C = ( c 1 , c 2 , . . . , c p ) contains normalized eigenvectors of S , D = diag (θ 1 , θ 2 , . . . , θ p ) contains eigenvalues of S , C 1 = ( c 1 , c 2 , . . . , c m ) , and D 1 = diag (θ 1 , θ 2 , . . . , θ m ) .

Using the partitioned forms C = ( C 1 , C 2 ) and D = ( D 1 O O D 2 ) , show

I

m

,

C

′

1

C

2

O

,

C

′

C

1

I

m

O

,

D

I

m

O

D

1

O

,

<!-- formula-not-decoded -->

that C ′ 1 C 1 = = = ( ) ( ) = ( ) C ( D 1 O ) = C 1 D 1, and CDC ′ C 1 D 1 C ′ 1 = C 1 D 2 1 C ′ 1 . Show similarly that C 1 D 1 C ′ 1 CDC ′ = C 1 D 2 1 C ′ 1 and C 1 D 1 C ′ 1 C 1 D 1 C ′ 1 = C 1 D 2 1 C ′ 1 . Now by (2.97) tr ( CD 2 C ′ ) = tr ( C ′ CD 2 ) = tr ( D 2 ) = ∑ p i = 1 θ 2 i . Similarly, tr ( C 1 D 2 1 C ′ 1 ) = ∑ m i = 1 θ 2 i . Then

<!-- formula-not-decoded -->

- ˆ ˆ ′

13.5 ∑ p i = 1 ∑ m j = 1 ˆ λ 2 i j = ∑ p i = 1 [ ∑ m j = 1 ˆ λ 2 i j ] = ∑ p i = 1 ˆ h 2 i [by (13.28)] By interchanging the order of summation, we have

<!-- formula-not-decoded -->

- 13.6 We use the covariance matrix to avoid working with standardized variables. The eigenvalues of S are 39.16, 8.78, .66, .30, and 0. The eigenvector corresponding to λ 5 = 0 is

<!-- formula-not-decoded -->

As noted in Section 12.7, s 2 z 5 = 0 implies z 5 = 0. Thus

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

## 13.7 Words data of Table 5.9:

|                | Principal Component Loadings   | Principal Component Loadings   | Varimax Rotated Loadings   | Varimax Rotated Loadings   | Communalities,   |
|----------------|--------------------------------|--------------------------------|----------------------------|----------------------------|------------------|
|                | f 1                            | f 2                            | f 1                        | f 2                        | ˆ h 2 i          |
| Variables      |                                |                                |                            |                            |                  |
| Informal words | .802                           | - . 535                        | .956                       | .129                       | .930             |
| Informal verbs | .856                           | - . 326                        | .858                       | .321                       | .839             |
| Formal words   | .883                           | .270                           | .484                       | .786                       | .853             |
| Formal verbs   | .714                           | .658                           | .101                       | .966                       | .943             |
| Variance       | 2.666                          | .899                           | 1.894                      | 1.671                      | 3.565            |
| Proportion     | .666                           | .225                           | .474                       | .418                       | .891             |

The orthogonal matrix T for the varimax rotation as given by (13.49) is

<!-- formula-not-decoded -->

Thus sin φ = -. 661 and φ = -41 . 4 ◦ . A graphical rotation of -40 ◦ would produce results very close to the varimax rotation.

13.8 Ramus bone data of Table 3.6:

|             | Principal Component Loadings   | Principal Component Loadings   | Varimax Rotated Loadings   | Varimax Rotated Loadings   | Communalities,   | Orthoblique Pattern Loadings   | Orthoblique Pattern Loadings   |
|-------------|--------------------------------|--------------------------------|----------------------------|----------------------------|------------------|--------------------------------|--------------------------------|
|             | f 1                            | f 2                            | f 1                        | f 2                        | ˆ h 2 i          | f 1                            | f 2                            |
| Variables   |                                |                                |                            |                            |                  |                                |                                |
| 8 years     | .949                           | - . 295                        | .884                       | .455                       | .988             | - . 108                        | 1.087                          |
| 8 1 2 years | .974                           | - . 193                        | .830                       | .545                       | .986             | .106                           | .900                           |
| 9 years     | .978                           | .171                           | .578                       | .808                       | .986             | .825                           | .188                           |
| 9 1 2 years | .943                           | .319                           | .449                       | .888                       | .991             | 1.099                          | - . 121                        |
| Variance    | 3.695                          | .255                           | 2.005                      | 1.946                      | 3.951            |                                |                                |
| Proportion  | .924                           | .064                           | .501                       | .486                       | .988             |                                |                                |

The Harris-Kaiser orthoblique rotation produced loadings for which the variables have a complexity of 1. These oblique loadings provide a much cleaner simple structure than that given by the varimax loadings. For interpretation, we see that one factor represents variables 1 and 2, and the other factor represents variables 3 and 4. This same clustering of variables can be deduced from the varimax loadings if we simply use the larger of the two loadings for each variable.

The correlation between the two oblique factors is .87. The angle between the oblique axes is cos -1 (. 87 ) = 29 . 5 ◦ . With such a small angle between the axes and a large correlation between the factors, it is clear that a single factor would better represent the variables. This is also borne out by the eigenvalues of the correlation matrix: 3.695, .255, .033, and .017. The first accounts for 92% of the variance and the second for only 6%.

## 13.9 Rootstock data of Table 6.2:

|                   | Principal Component Loadings   | Principal Component Loadings   | Varimax Rotated Loadings   | Varimax Rotated Loadings   | Communalities,   |
|-------------------|--------------------------------|--------------------------------|----------------------------|----------------------------|------------------|
|                   | f 1                            | f 2                            | f 1                        | f 2                        | ˆ h 2 i          |
| Variables         |                                |                                |                            |                            |                  |
| Trunk 4 years     | .787                           | .575                           | .167                       | .960                       | .949             |
| Extension 4 years | .849                           | .467                           | .287                       | .925                       | .939             |
| Trunk 15 years    | .875                           | - . 455                        | .946                       | .280                       | .973             |
| Weight 15 years   | .824                           | - . 547                        | .973                       | .179                       | .978             |
| Variance          | 2.785                          | 1.054                          | 1.951                      | 1.888                      | 3.839            |
| Proportion        | .696                           | .264                           | .488                       | .472                       | .960             |

The rotation was successful in producing variables with a complexity of 1, that is, partitioning the variables into two groups, each with two variables. 13.10 (a) Fish data of Table 6.17:

|            | Principal Component Loadings   | Principal Component Loadings   | Varimax Rotated Loadings   | Varimax Rotated Loadings   | Communalities,   |
|------------|--------------------------------|--------------------------------|----------------------------|----------------------------|------------------|
|            | f 1                            | f 2                            | f 1                        | f 2                        | ˆ h 2 i          |
| Variables  |                                |                                |                            |                            |                  |
| y 1        | .830                           | - . 403                        | .874                       | .294                       | .851             |
| y 2        | .783                           | - . 504                        | .911                       | .189                       | .866             |
| y 3        | .803                           | .432                           | .270                       | .871                       | .831             |
| y 4        | .769                           | .497                           | .200                       | .893                       | .838             |
| Variance   | 2.537                          | .850                           | 1.709                      | 1.678                      | 3.386            |
| Proportion | .634                           | .213                           | .427                       | .420                       | .847             |

- (b) The loadings for y 1 and y 2 are similar. In R we see some indication of the reason for this; y 1 and y 2 are more highly correlated than any other pair of variables, and their correlations with y 3 and y 4 are similar:

<!-- formula-not-decoded -->

- (c) By (13.58), the factor score coefficient matrix is

<!-- formula-not-decoded -->

where ˆ 𝚲 is the matrix of rotated factor loadings given in part (a). The factor scores are given by (13.59) as follows:

| Method 1   | Method 1   | Method 2   | Method 2   | Method 3   | Method 3   |
|------------|------------|------------|------------|------------|------------|
| ˆ f 1      | ˆ f 2      | ˆ f 1      | ˆ f 2      | ˆ f 1      | ˆ f 2      |
| .544       | 1.151      | - . 254    | .309       | - 1 . 156  | 2.104      |
| 1.250      | - . 254    | - . 309    | - 1 . 534  | - . 321    | .878       |
| 1.017      | 1.120      | - 1 . 865  | - 1 . 558  | - . 671    | .947       |
| - . 147    | - 1 . 583  | - . 999    | - . 690    | .067       | 1.130      |
| .219       | - . 103    | .520       | - . 343    | - 1 . 610  | - . 458    |
| 1.007      | .679       | .919       | - . 111    | .557       | .491       |
| 1.413      | - . 186    | - . 443    | - . 018    | - . 454    | 1.157      |
| - . 666    | - 2 . 279  | - . 265    | .676       | - . 961    | .063       |
| 1.057      | - 1 . 870  | 1.449      | - . 295    | - . 230    | 1.721      |
| .388       | - . 440    | 1.371      | .295       | - 1 . 309  | .054       |
| 1.328      | - . 298    | 1.260      | - . 027    | - 1 . 766  | - . 111    |
| .694       | . 033      | . 000      | 1 . 452    | 1 . 636    | . 048      |

-

-

-

-

-

- (d) Aone-way MANOVA on the two factor scores comparing the three methods yielded the following values for E and H :

<!-- formula-not-decoded -->

The four MANOVA test statistics are /Lambda1 = . 3631, V ( s ) = . 6552, U ( s ) = 1 . 7035, and θ = . 6259. All are highly significant.

- 13.11 (a) For the flea data of Table 5.5, the eigenvalues of R are 2.273, 1.081, .450, and .196. There is a noticeable gap between 1.081 and .450, and the first two factors account for 83.9% of the variance. Thus m = 2 factors seem to be indicated for this set of data.

(b)

|            | Principal Component Loadings   | Principal Component Loadings   | Varimax Rotated Loadings   | Varimax Rotated Loadings   | Communalities,   | Orthoblique Pattern Loadings   | Orthoblique Pattern Loadings   |
|------------|--------------------------------|--------------------------------|----------------------------|----------------------------|------------------|--------------------------------|--------------------------------|
|            | f 1                            | f 2                            | f 1                        | f 2                        | ˆ h 2 i          | f 1                            | f 2                            |
| Variables  |                                |                                |                            |                            |                  |                                |                                |
| y 1        | - . 038                        | .989                           | - . 025                    | .990                       | .980             | - . 003                        | .990                           |
| y 2        | .889                           | .269                           | .892                       | .256                       | .862             | .898                           | .253                           |
| y 3        | .893                           | - . 157                        | .891                       | - . 170                    | .823             | .887                           | - . 173                        |
| y 4        | .827                           | - . 073                        | .823                       | - . 084                    | .689             | .824                           | - . 087                        |
| Variance   | 2.273                          | 1.081                          | 2.273                      | 1.081                      | 3.354            |                                |                                |
| Proportion | .568                           | .270                           | .568                       | .270                       | .839             |                                |                                |

(The variance explained by the varimax rotated factors remains the same as for the initial factors when rounded to three decimal places.)

- (c) In this case, neither of the rotations changes the initial loadings appreciably. The reason for this unusual outcome can be seen in the correlation matrix:

<!-- formula-not-decoded -->

There are clearly two clusters of variables: { y 1 } and { y 2 , y 3 , y 4 } . We would expect two factors corresponding to these groupings to emerge after rotation. That the same pattern surfaces in the initial factor loadings (based on eigenvectors) is due to their affiliation with principal components. As noted in Section 12.8.1, if a variable has small correlations with all other variables, the variable itself will essentially constitute a principal component. In this case, y 1 has this property and makes up most of the second principal component. The first component is comprised of the other three variables.

- 13.12 (a) For the engineer data of Table 5.6, the number of eigenvalues greater than 1 is three, but the three account for only 70% of the variance. It requires four eigenvalues to reach 84%. The scree plot also indicates four eigenvalues.

(b)

|            | Principal Component Loadings   | Principal Component Loadings   | Principal Component Loadings   | Varimax Rotated Loadings   | Varimax Rotated Loadings   | Varimax Rotated Loadings   | Communalities,   |
|------------|--------------------------------|--------------------------------|--------------------------------|----------------------------|----------------------------|----------------------------|------------------|
|            | f 1                            | f 2                            | f 3                            | f 1                        | f 2                        | f 3                        | ˆ h 2 i          |
| Variables  |                                |                                |                                |                            |                            |                            |                  |
| y 1        | .536                           | .461                           | .478                           | - . 063                    | .834                       | .170                       | .729             |
| y 2        | - . 129                        | .870                           | - . 182                        | - . 357                    | .100                       | .818                       | .806             |
| y 3        | .514                           | - . 254                        | - . 448                        | .724                       | - . 026                    | .068                       | .529             |
| y 4        | .724                           | - . 366                        | - . 110                        | .739                       | .295                       | - . 193                    | .670             |
| y 5        | - . 416                        | - . 414                        | .649                           | - . 484                    | - . 013                    | - .729                     | .766             |
| y 6        | .715                           | .124                           | .420                           | .239                       | .800                       | - . 069                    | .702             |
| Variance   | 1.775                          | 1.354                          | 1.073                          | 1.493                      | 1.435                      | 1.275                      | 4.202            |
| Proportion | .296                           | .226                           | .179                           | .249                       | .239                       | .212                       | .700             |

- (c) The initial communality estimates for the six variables are given by (13.36) as .215, .225, .113, .255, .161, .248. With these substituted for the diagonal of R , the eigenvalues of R - ˆ 𝚿 are

| Eigenvalue   | . 994   | . 569   | . 255   | - . 025   | - . 237   | - . 339   |
|--------------|---------|---------|---------|-----------|-----------|-----------|
| Proportion   | . 816   | . 468   | . 209   | - . 020   | - . 195   | - . 278   |
| Cumulative   | . 816   | 1 . 284 | 1 . 493 | 1 . 473   | 1 . 278   | 1 . 000   |

The principal factor loadings and varimax rotation are as follows:

|           | Principal Component Loadings   | Principal Component Loadings   | Principal Component Loadings   | Varimax Rotated Loadings   | Varimax Rotated Loadings   | Varimax Rotated Loadings   | Communalities,   |
|-----------|--------------------------------|--------------------------------|--------------------------------|----------------------------|----------------------------|----------------------------|------------------|
|           | f 1                            | f 2                            | f 3                            | f 1                        | f 2                        | f 3                        | ˆ h 2 i          |
| Variables |                                |                                |                                |                            |                            |                            |                  |
| y 1       | .403                           | .312                           | .227                           | .030                       | .536                       | .151                       | .311             |
| y 2       | - . 106                        | .569                           | - . 100                        | - . 288                    | .083                       | .505                       | .345             |
| y 3       | .343                           | - . 139                        | - . 197                        | .413                       | .060                       | .037                       | .176             |
| y 4       | .559                           | - . 247                        | - . 090                        | .564                       | .233                       | - . 094                    | .381             |
| y 5       | - . 286                        | - . 246                        | .328                           | - . 262                    | - . 088                    | - .417                     | .250             |
| y 6       | .556                           | .089                           | .197                           | .258                       | .537                       | .003                       | .356             |

- (d) The pattern of loadings is similar in parts (b) and (c), and the interpretation of the three factors would be the same.

## 13.13 Probe word data of Table 3.5:

|            | Principal Component Loadings   | Principal Component Loadings   | Varimax Rotated Loadings   | Varimax Rotated Loadings   | Communalities,   | Orthoblique Pattern Loadings   | Orthoblique Pattern Loadings   |
|------------|--------------------------------|--------------------------------|----------------------------|----------------------------|------------------|--------------------------------|--------------------------------|
|            | f 1                            | f 2                            | f 1                        | f 2                        | ˆ h 2 i          | f 1                            | f 2                            |
| Variables  |                                |                                |                            |                            |                  |                                |                                |
| y 1        | .817                           | - . 157                        | .732                       | .395                       | .692             | .737                           | .131                           |
| y 2        | .838                           | - . 336                        | .861                       | .271                       | .815             | .963                           | - . 092                        |
| y 3        | .874                           | .288                           | .494                       | .776                       | .847             | .248                           | .734                           |
| y 4        | .838                           | - . 308                        | .844                       | .292                       | .798             | .931                           | - . 057                        |
| y 5        | .762                           | .547                           | .244                       | .905                       | .879             | - . 134                        | 1.023                          |
| Variance   | 3.416                          | .614                           | 2.294                      | 1.736                      | 4.031            |                                |                                |
| Proportion | .683                           | .123                           | .459                       | .347                       | .806             |                                |                                |

The loadings for y 2 are similar to those for y 4 in all three sets of loadings. The reason for this can be seen in the correlation matrix

<!-- formula-not-decoded -->

The correlations of y 2 with y 1, y 3, and y 5 are very similar to the correlations of y 4 with y 1, y 3, and y 5.

## CHAPTER 14

- 14.1 Adding and subtracting x and y in (14.2) (squared), we obtain

<!-- formula-not-decoded -->

The other two terms vanish because ∑ j ( x j -x ) = ∑ j ( y j -y ) = 0. Substituting v 2 x = ∑ p j = 1 ( x j -x ) 2 and v 2 y = ∑ p j = 1 ( y j -y ) 2 and adding and subtracting -2 √ v 2 x v 2 y = -2 v x v y , we obtain

<!-- formula-not-decoded -->

14.2 (a) Since y AB = ∑ n AB i = 1 y i / nAB , we have by (14.16),

<!-- formula-not-decoded -->

Similarly, SSE A = ∑ n A i = 1 y ′ i y i -nA y ′ A y A and SSE B = ∑ nB i = 1 y ′ i y i -nB y ′ B y B . Now

<!-- formula-not-decoded -->

Thus

<!-- formula-not-decoded -->

Show that when the right side of (14.16) is expanded, it reduces to this same expression [see Problem 14.3(b)].

- (b) Multiplying out the right side of (14.16), we have

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

- 14.3 (a) Complete linkage. From Table 14.2, we have

<!-- formula-not-decoded -->

If D ( C , A ) &gt; D ( C , B ) , then | D ( C , A ) -D ( C , B ) | = D ( C , A ) -D ( C , B ) , and equation (1) becomes D ( C , AB ) = D ( C , A ) . If D ( C , A ) &gt; D ( C , B ) , then | D ( C , A ) -D ( C , B ) | = D ( C , B ) -D ( C , A ) and equation (1) becomes D ( C , AB ) = D ( C , B ) . Thus equation (1) can be written as D ( C , AB ) = max [ D ( C , A ), D ( C , B ) ] , which is equivalent to (14.9), the definition of distance for the complete linkage method.

- (b) Average linkage. From Table 14.2, we have

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

By (14.10) equation (2) can be written as

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

which, by (14.10), is the definition of distance for the average linkage method.

- (c) Substitute y AB = ( nA y A + nB y B )/( nA + nB ) in the left side of (14.40) in the statement of Problem 14.3(c) and multiply to obtain

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Similarly, multiply on the right side of (14.40) to obtain the same result.

- (d) Using nA = nB in y AB = ( nA y A + nB y B )/( nA + nB ) in (14.12), we obtain m AB = 1 2 ( y A + y B ) in (14.13). Then (14.40) [see part (c)] becomes

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

which matches the parameter values for the median method in Table 14.2. (e) By (14.19),

<!-- formula-not-decoded -->

and we have analogous expressions for ( y C -y AB ) ′ ( y C -y AB ) , ( y C -y A ) ′ ( y C -y A ) , and ( y C -y B ) ′ ( y C -y B ) . Then (14.40) in part (c) becomes

<!-- formula-not-decoded -->

Solve for IC ( AB ) .

14.4 If γ = 0, then (14.20) becomes

<!-- formula-not-decoded -->

By (14.25), we have D ( A , C ) &gt; D ( A , B ) and D ( B , C ) &gt; D ( A , B ) . Thus, replacing D ( C , A ) and D ( C , B ) in equation (1) by D ( A , B ) , we obtain

<!-- formula-not-decoded -->

which is equivalent to (14.26).

<!-- formula-not-decoded -->

Show similarly that v i . = Ay i . + b . Then by (6.9), we have

<!-- formula-not-decoded -->

Show similarly that E v = AE y A ′ .

- (b) tr ( E v ) = tr ( AE y A ′ ) = tr ( A ′ AE y ) /negationslash= tr ( E y )
- (c) E v AE y A A E y A A E y c E y , where c &gt; 0. Thus minimizing E is equivalent to minimizing E y .

<!-- formula-not-decoded -->

- | | = | ′ | = | || || ′ | = | | 2 | | = | | | v | | |
- 14.6 There are p parameters in each 𝛍 i , 2 p ( p 1 ) unique parameters in each 𝚺 i , and g 1 unique parameters α . Thus the total number is

<!-- formula-not-decoded -->

- 1 + -i

<!-- formula-not-decoded -->

- 14.7 (a) The two-cluster solution from single linkage puts boy No. 20 in one cluster
- and the other 19 boys in the other cluster.
- (b) , (c) , and (d) . Based on the change in distance, average linkage and the other cluster solutions in parts (c) and (d) clearly indicate two clusters. These solutions generally agree and also correspond to a division into two groups seen in the first principal component in Figure 12.5. The separation of the three apparent outliers from the other 17 observations is less pronounced in the cluster analyses than in Figure 12.5. Note that the scale of the second component in Figure 12.5 is much larger than that of the first component, so the separation of points 9, 12, and 20 from the rest is not as large as it appears in the figure. Of the methods in parts (b), (c), and (d), only flexible beta with β = -. 50 and -. 75 place points 9, 12, and 20 together in one cluster. All others place 9 and 12 in one of the clusters and
- 20 in the other.
- (b) From the dendrogram produced by the average linkage method, the largest change in distance corresponds to a two-cluster solution.
- 14.8 (a) The distance between centroids of the two clusters is √ 2994 . 9 = 54 . 7.
- (c) The discriminant function completely separates the two clusters, with no overlap.
- 14.9 (a) Observation 22 seems to be an outlier, because it forms its own cluster in both the single linkage and average linkage methods. The cluster consisting of observations 2, 21, 24, 26, and 30 is the same in all six methods.
- (b) The discriminant function completely separates the two clusters, with no overlap.

14.10 (a) The following five clusters were found using as seeds the five observations that are mutually farthest apart.

| Cluster        | 1          | 2        | 3           | 4       | 5   |
|----------------|------------|----------|-------------|---------|-----|
| Observation(s) | 9, 15, 16, | 1, 2, 3, | 6, 7, 8, 20 | 10, 11, | 14  |
| Observation(s) | 18, 19     | 4, 5, 17 |             | 12, 13  |     |

In the plot of the first two discriminant functions, observation 14 is relatively far removed from the rest. Clusters 1, 2, and 3 are somewhat closer to each other.

- (b) The following five clusters were found using as seeds the first five observations.

| Cluster        | 1       | 2   | 3      | 4          | 5          |
|----------------|---------|-----|--------|------------|------------|
| Observation(s) | 1, 3, 4 | 2   | 5, 17, | 6, 7, 8,   | 9, 10, 11  |
|                |         |     | 18, 19 | 15, 16, 20 | 12, 13, 14 |

The plot of the first two discriminant functions shows a pattern different from that in part (a).

- (c) The following five clusters were found using as seeds the centroids of the five-cluster solution resulting from Ward's method.

| Cluster        | 1            | 2         | 3       | 4     | 5   |
|----------------|--------------|-----------|---------|-------|-----|
| Observation(s) | 6, 7, 8, 15, | 5, 9, 17, | 10, 11, | 1, 2, | 14  |
| Observation(s) | 16, 20       | 18, 19    | 12, 13  | 3, 4  |     |

The plot of the first two discriminant functions shows a pattern similar to that found in part (a), with observation 14 isolated.The dendrogram shows that Ward's method gives the same five-cluster solution as the k -means result.

- (d) The following five clusters were found using the k -means method with seeds equal to the centroids of the five clusters from average linkage.

| Cluster        | 1                   | 2                         | 3              | 4   | 5   |
|----------------|---------------------|---------------------------|----------------|-----|-----|
| Observation(s) | 6, 7, 8, 15, 16, 20 | 1, 2, 3, 4, 5, 17, 18, 19 | 10, 11, 12, 13 | 9   | 14  |
| Observation(s) |                     |                           |                |     |     |

The plot of the first two discriminant functions shows a pattern somewhat similar to that in part (a).

In the dendrogram for average linkage, observations 9 and 14 are isolated clusters in the five-cluster solution, which is identical to the fivecluster solution using k -means clustering with these seeds.

- (e) Observation 14 does not appear as an outlier in the plot of the first two principal compoments, but it does show up as an outlier in the plot of the second and third components. The solutions found in parts (a) and (c) seem to agree most with the principal component plots. This suggests that a different number of initial cluster seeds be used.
- (f) The two clustering solutions are identical. The results are given next.
- (g) The clustering solution is identical to that found in part (f), which indicates that the three-cluster solution is appropriate.
- 14.11 The number of clusters obtained from the indicated combinations of k and r are shown in the following table. Note that for each pair of values of k and r , the value of r was increased if necessary for each point until k points were included in the sphere.

| Cluster      | 1            | 2              | 3              |
|--------------|--------------|----------------|----------------|
| Observations | 6, 7, 8, 15, | 9, 10, 11, 12, | 1, 2, 3, 4, 5, |
| Observations | 16, 20       | 13, 14         | 17, 18, 19     |

|   k / r |   .2 |   .4 |   .6 |   .8 |   1.0 |   1.2 |   1.4 |   1.6 |   1.8 |   2.0 |
|---------|------|------|------|------|-------|-------|-------|-------|-------|-------|
|       2 |   10 |   10 |   10 |   10 |     8 |     6 |     4 |     3 |     3 |     2 |
|       3 |    5 |    5 |    5 |    5 |     5 |     3 |     2 |     2 |     2 |     2 |
|       4 |    2 |    2 |    2 |    2 |     2 |     2 |     2 |     2 |     2 |     2 |
|       5 |    1 |    1 |    1 |    1 |     1 |     1 |     1 |     1 |     1 |     1 |

The maximum value of k that yields a two-cluster solution is 4.

- 14.12 (a) The number of clusters obtained from the initial combinations of k and r are shown in the following table. The value of r was variable, as noted in Problem 14.11.
- (b) The plot of the first two discriminant functions for k = 2 and r = 1 shows the three clusters to be well separated.
- (c) The plot of the first two principal components shows the same groupings as in the plot in part (b).

|   k / r |   .2 |   .4 |   .6 |   .8 |   1.0 |   1.2 |   1.4 |   1.6 |   1.8 |   2.0 |
|---------|------|------|------|------|-------|-------|-------|-------|-------|-------|
|       2 |    3 |    3 |    3 |    3 |     3 |     3 |     3 |     3 |     3 |     3 |
|       3 |    2 |    2 |    2 |    2 |     2 |     2 |     2 |     2 |     2 |     2 |
|       4 |    1 |    1 |    1 |    1 |     1 |     1 |     1 |     1 |     1 |     1 |

- (d) The plot of the discriminant function shows wide separation of the two clusters, which do not overlap. The three-cluster solution found in part (b) is given next

| Cluster 1                                                | Cluster 2                                | Cluster 3             |
|----------------------------------------------------------|------------------------------------------|-----------------------|
| Harpers Morley Myerscough Sparsholt Sutton Bonington Wye | Rosemaund Terrington Headley Seale-Hayne | Cambridge Cockle Park |

The two-cluster solution found in part (d) merges clusters 2 and 3 of part (b).

## CHAPTER 15

15.1

By (2.38),

Hence,

Show that

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Using equation (2), we obtain

<!-- formula-not-decoded -->

By (3.63),

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Hence the i j th element of equation (1) is bi j = ai j -ai . -a . j + a .. . 15.2 (a) (Seber 1984, pp. 236-237) The elements of B = ( bi j ) are defined as bi j = ai j ai . a . j a .. , where ai j 1 δ 2 . Thus

<!-- formula-not-decoded -->

Then

<!-- formula-not-decoded -->

Similarly, show that

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Solve for ai j , ai . , a . j , and a .. and substitute into bi j = ai j -ai . -a . j + a .. to obtain bi j = z ′ i z j -z ′ i z -z ′ z j + z ′ z , which can be factored as bi j = ( z i -z ) ′ ( z j -z ) . Hence

<!-- formula-not-decoded -->

Thus B is positive semidefinite (see Section 2.7).

- (b) If B is positive semidefinite of rank q , then by (2.109) and Section 2.11.4, B can be expressed in the form B = V 𝚲 V ′ , where V = ( v 1 , v 2 , . . . , v n ) is an orthogonal matrix of eigenvectors of B , and 𝚲 is a diagonal matrix of eigenvalues, q of which are positive, with the rest equal to zero. Letting 𝚲 1 be the q × q upper-left-hand block of 𝚲 with positive eigenvalues and V 1 = ( v 1 , v 2 , . . . , v q ) be the n × q matrix with the corresponding eigenvectors, we can write B = V 𝚲 V ′ as

<!-- formula-not-decoded -->

where the n × q matrix Z is

<!-- formula-not-decoded -->

To show that ( z i -z j ) ′ ( z i -z j ) is equal to δ 2 i j , we can proceed as follows:

<!-- formula-not-decoded -->

By equation (1), we have

<!-- formula-not-decoded -->

Hence equation (2) becomes

<!-- formula-not-decoded -->

Show that substituting bi j = ai j -ai . -a . j + a .. into equation (3) leads to

<!-- formula-not-decoded -->

Show that the symmetry of A implies ai . = a . i and a . j = a j . . Hence,

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

- 15.5 By (15.8), (15.9), and (15.10), pi j = nij / n , pi . = ni ./ n , and p . j = n . j / n . Substituting these into (15.25), we obtain

<!-- formula-not-decoded -->

- 15.6 (a) Multiplying numerator and denominator of (15.25) by pi . , we obtain

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

15.7 (a) By (15.29), (15.10), (15.12), and (15.18), we obtain

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

- 15.8 (a) By (15.9), r = Pj . Then D -1 r r = D -1 r Pj = Rj by (15.15). By (15.13), r ′ i j = 1, and therefore Rj = j . Now

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

- 15.9 By (15.49), z ′ i = y ′ i A (ignoring the centering on y i ). Thus the squared Euclidean distance can be written as

<!-- formula-not-decoded -->

since A is orthogonal.

15.10 (a) From Y c V = U 𝚲 in (15.55), we have Y c V 𝚲 -1 = U . Then

<!-- formula-not-decoded -->

Since ( 𝚲 -1 ) 2 = diag ( 1 /λ 2 1 , 1 /λ 2 2 , . . . , 1 /λ 2 p ) , where the λ 2 i 's are eigenvalues of Y ′ c Y c , the matrix ( 𝚲 -1 ) 2 contains eigenvalues of ( Y ′ c Y c ) -1 = [ ( n -1 ) S ] -1 = S -1 /( n -1 ) [see (2.115) and (2.116)]. The matrix V contains eigenvectors of Y ′ c Y c and thereby of ( Y ′ c Y c ) -1 (see Section 2.11.9). Hence we recognize V ( 𝚲 -1 ) 2 V ′ as the spectral decomposition of ( Y ′ c Y c ) -1 [see (2.109), (2.115), and (2.116)]. Therefore, equation (1) can be written as

<!-- formula-not-decoded -->

- (b) If H = V 𝚲 , then HH ′ = V 𝚲𝚲 V ′ = V 𝚲 2 V ′ . The diagonal matrix 𝚲 2 contains the eigenvalues λ 2 i of the matrix Y ′ c Y c . Thus by (2.115), V 𝚲 2 V ′ is the spectral decomposition of Y ′ c Y c , and

<!-- formula-not-decoded -->

- 15.11 By (15.64), (3.63), and (3.64) (ignoring n -1 and assuming the y i 's are centered),

<!-- formula-not-decoded -->

- 15.12 (a) The first ten rows and columns of the matrix B are as follows.

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

- (b) The first two columns of the matrix Z are given by

| City   | z 1       | z 2       | City   | z 1       | z 2       |
|--------|-----------|-----------|--------|-----------|-----------|
| A      | 354.1     | - 10 . 2  | M      | 391.6     | 47.5      |
| B      | - 77 . 1  | 25.0      | N      | 21.0      | - 44 . 7  |
| C      | - 238 . 2 | - 75 . 7  | O      | 9.8       | 30.9      |
| D      | - 154 . 9 | 65.9      | P      | - 173 . 8 | - 78 . 5  |
| E      | - 163 . 2 | 72.2      | Q      | 6.3       | 17.1      |
| F      | 126.0     | 24.9      | R      | 117.0     | - 48 . 0  |
| G      | - 228 . 8 | - 149 . 4 | S      | - 102 . 3 | - 170 . 2 |
| H      | 223.9     | 1.5       | T      | - 53 . 2  | - 27 . 3  |
| I      | 337.7     | 44.8      | U      | - 315 . 2 | 190.9     |
| J      | 226.7     | 34.7      | V      | - 255 . 7 | 140.2     |
| K      | - 33 . 4  | 22.3      | W      | - 19 . 3  | - 34 . 3  |
| L      | 1.1       | 79 . 4    |        |           |           |

-

- (c) The metric multidimensional scaling plot shows the relative positions of the cities.
- 15.13 (a) The multidimensional scaling plot shows two clusters, one for positive values of the first dimension and one for negative values. The two clusters can be interpreted as comfort (positive values) and discomfort (negative values). Hence, the axis of the first dimension can be interpreted as the level of comfort.
- (b) The dendrogram for Ward's method clearly shows two clusters, the same as in part (a).
- 15.14 (a) The initial configuration of points will vary. One example is as follows:
- (b) Answers will vary. For the seeds given in part (a), STRESS = . 0266.
- (c) Answers will vary. The plot of STRESS versus k for one solution showed that two dimensions should be retained. The nonmetric MDS plot showed that Franco, Mussolini, and Hitler were close together, as well as Churchill and DeGaulle, and Eisenhower and Truman.
- (d) Answers will vary. One solution gave results similar to part (c).

| y 1       | y 2       | y 3       | y 4     | y 5       | y 6       |
|-----------|-----------|-----------|---------|-----------|-----------|
| 1.458     | .769      | - 1 . 350 | .456    | - 1 . 610 | 1.827     |
| - . 598   | - 1 . 069 | - 2 . 667 | .458    | .416      | 1.094     |
| - 1 . 777 | - . 409   | .369      | .655    | - . 058   | 1.177     |
| .071      | .361      | 1.157     | - . 154 | .343      | - . 417   |
| - . 060   | 1.361     | .743      | 1.436   | .332      | - . 894   |
| - . 757   | - . 432   | - . 545   | .233    | .646      | - . 102   |
| - 1 . 971 | - . 492   | - . 461   | .078    | 1.441     | .039      |
| - 1 . 560 | - . 173   | .657      | -.528   | 1.001     | 1.030     |
| - . 597   | .814      | - . 898   | .283    | - . 355   | - 1 . 115 |
| 1.449     | -.942     | .867      | - . 922 | .833      | 1.196     |
| - 1 . 809 | - . 093   | - 1 . 762 | - . 533 | - 1 . 136 | - . 226   |
| 1.067     | .199      | .978      | .884    | - 1 . 060 | - . 800   |

- (e) Answers will vary. One solution showed three dimensions. A plot of two dimensions showed Mussolini and Franco together in the center with the others forming a circle around them of almost equally spaced points.
- (f) Answers will vary. One solution was similar to that in part (c).
- 15.15 (a) The correspondence matrix P is found by dividing each element of Table 15.16 by n = 1281 to obtain the following:

| Death Birth   |   Jan. |   Feb. |   Mar. |   Apr. |   May |   Jun. |   Jul. |   Aug. |   Sep. |   Oct. |   Nov. |   Dec. |   Total |
|---------------|--------|--------|--------|--------|-------|--------|--------|--------|--------|--------|--------|--------|---------|
| Jan.          |  0.007 |  0.011 |  0.009 |  0.011 | 0.007 |  0.009 |  0.008 |  0.012 |  0.007 |  0.009 |  0.009 |  0.01  |   0.108 |
| Feb.          |  0.01  |  0.005 |  0.005 |  0.006 | 0.007 |  0.004 |  0.003 |  0.004 |  0.005 |  0.009 |  0.001 |  0.01  |   0.069 |
| Mar.          |  0.009 |  0.011 |  0.007 |  0.005 | 0.013 |  0.008 |  0.007 |  0.008 |  0.007 |  0.002 |  0.01  |  0.007 |   0.094 |
| Apr.          |  0.005 |  0.009 |  0.008 |  0.005 | 0.007 |  0.009 |  0.003 |  0.009 |  0.003 |  0.007 |  0.006 |  0.009 |   0.08  |
| May           |  0.006 |  0.005 |  0.009 |  0.005 | 0.003 |  0.009 |  0.007 |  0.007 |  0.009 |  0.005 |  0.007 |  0.003 |   0.075 |
| Jun.          |  0.011 |  0.004 |  0.004 |  0.005 | 0.01  |  0.004 |  0.005 |  0.003 |  0.006 |  0.007 |  0.005 |  0.004 |   0.069 |
| Jul.          |  0.009 |  0.008 |  0.01  |  0.003 | 0.004 |  0.009 |  0.005 |  0.005 |  0.003 |  0.008 |  0.003 |  0.006 |   0.073 |
| Aug.          |  0.005 |  0.005 |  0.009 |  0.01  | 0.008 |  0.007 |  0.002 |  0.006 |  0.006 |  0.006 |  0.006 |  0.009 |   0.081 |
| Sep.          |  0.005 |  0.009 |  0.009 |  0.008 | 0.008 |  0.009 |  0.003 |  0.006 |  0.009 |  0.005 |  0.006 |  0.005 |   0.083 |
| Oct.          |  0.012 |  0.006 |  0.009 |  0.007 | 0.005 |  0.008 |  0.009 |  0.006 |  0.007 |  0.006 |  0.005 |  0.005 |   0.087 |
| Nov.          |  0.005 |  0.007 |  0.012 |  0.008 | 0.009 |  0.008 |  0.005 |  0.008 |  0.005 |  0.008 |  0.007 |  0.005 |   0.087 |
| Dec.          |  0.005 |  0.014 |  0.007 |  0.009 | 0.011 |  0.006 |  0.007 |  0.007 |  0.008 |  0.005 |  0.008 |  0.006 |   0.092 |
| Total         |  0.092 |  0.094 |  0.096 |  0.084 | 0.092 |  0.088 |  0.066 |  0.08  |  0.077 |  0.075 |  0.074 |  0.081 |   1     |

## (b) The R matrix is given by

<!-- formula-not-decoded -->

and the C matrix is given by

<!-- formula-not-decoded -->

,

.

- (c) The chi-square statistic is 117.7742 with 121 degrees of freedom, which gives a p -value of .5660. The two variables appear to be independent.
- (d) In the correspondence plot, the following associations are seen: { November births, June deaths } , { March deaths, April deaths, January births } , { September births, February deaths } , { August births, April births } , { May deaths, September deaths, May births } .
- 15.16 (a) The correspondence matrix P is found by dividing each element of Table 15.17 by 8193 to obtain the following:

| Part of Country   |   Burglary |   Fraud |   Vandalism |   Total |
|-------------------|------------|---------|-------------|---------|
| Oslo area         |      0.048 |   0.3   |       0.215 |   0.563 |
| Mid Norway        |      0.018 |   0.019 |       0.112 |   0.148 |
| North Norway      |      0.085 |   0.04  |       0.164 |   0.289 |
| Total             |      0.151 |   0.358 |       0.491 |   1     |

## (b) The R matrix is given by

| Part of Country   |   Burglary |   Fraud |   Vandalism |
|-------------------|------------|---------|-------------|
| Oslo area         |      0.086 |   0.533 |       0.381 |
| Mid Norway        |      0.121 |   0.126 |       0.753 |
| North Norway      |      0.293 |   0.138 |       0.569 |

## and the C matrix is

| Part of Country   |   Burglary |   Fraud |   Vandalism |
|-------------------|------------|---------|-------------|
| Oslo area         |      0.32  |   0.837 |       0.437 |
| Mid Norway        |      0.119 |   0.052 |       0.228 |
| North Norway      |      0.561 |   0.111 |       0.335 |

- (c) The chi-square statistic is 1662.6 with 4 degrees of freedom, which gives a p -value less than .0001. The two variables are dependent.
- (d) In the correspondence plot, North Norway is associated with burglaries, Oslo is associated with fraud, and Mid Norway is associated with vandalism.

15.17 (a) The Burt matrix is as follows.

| No          |   5254 |   0 |   564 |   3408 |   1282 |   1830 |   3424 |   2466 |   2788 |   2190 |   3064 |   686 |   2666 |   1902 |
|-------------|--------|-----|-------|--------|--------|--------|--------|--------|--------|--------|--------|-------|--------|--------|
| Yes         |      0 | 165 |   105 |     42 |     18 |     73 |     92 |     37 |    128 |      4 |    125 |    26 |     63 |     76 |
| High dust   |    564 | 105 |   669 |      0 |      0 |    402 |    267 |     62 |    607 |    218 |    451 |    87 |    359 |    223 |
| Low dust    |   3408 |  42 |     0 |   3450 |      0 |   1056 |   2394 |   1642 |   1808 |   1446 |   2004 |   480 |   1684 |   1286 |
| Medium dust |   1282 |  18 |     0 |      0 |   1300 |    445 |    855 |    799 |    501 |    566 |    734 |   145 |    686 |    469 |
| Race- Other |   1830 |  73 |   402 |   1056 |    445 |   1930 |      0 |    932 |    971 |    799 |   1104 |   108 |   1658 |    137 |
| White       |   3424 |  92 |   267 |   2394 |    855 |      0 |   3516 |   1571 |   1945 |   1431 |   2085 |   604 |   1071 |   1841 |
| Female      |   2466 |  37 |    62 |   1642 |    799 |    932 |   1571 |   2503 |      0 |   1373 |   1130 |   266 |   1421 |    816 |
| Male        |   2788 | 128 |   607 |   1808 |    501 |    971 |   1945 |      0 |   2916 |    857 |   2059 |   446 |   1308 |   1162 |
| Nonsmoker   |   2190 |  40 |   218 |   1446 |    566 |    799 |   1431 |   1373 |    857 |   2230 |      0 |   231 |   1142 |    857 |
| Smoker      |   3064 | 125 |   451 |   2004 |    734 |   1104 |   2085 |   1130 |   2059 |      0 |   3189 |   481 |   1587 |   1121 |
| 10-20       |    686 |  26 |    87 |    480 |    145 |    108 |    604 |    266 |    446 |    231 |    481 |   712 |      0 |      0 |
| ≤ 10        |   2666 |  63 |   359 |   1684 |    686 |   1658 |   1071 |   1421 |   1308 |   1142 |   1587 |     0 |   2729 |      0 |
| ≥ 20        |   1902 |  76 |   223 |   1286 |    469 |    137 |   1841 |    816 |   1162 |    857 |   1121 |     0 |      0 |   1978 |

- (b) The column coordinates for the plot are given by
- (c) Some associations seen in the plot are { byssinosis-yes, high dust } , { female, nonsmoker, medium dust } , { smoker, male } .

| Variables   | y 1     | y 2     |
|-------------|---------|---------|
| No          | - . 032 | - . 087 |
| Yes         | 1.013   | 2.761   |
| High dust   | 1.072   | 1.648   |
| Low dust    | - . 209 | - . 107 |
| Medium dust | .003    | - . 564 |
| Race-Other  | 1.184   | - . 153 |
| White       | - . 641 | .083    |
| Female      | .007    | - . 791 |
| Male        | - . 006 | .679    |
| Nonsmoker   | - . 036 | - . 592 |
| Smoker      | .025    | .414    |
| 10-20       | - . 605 | .535    |
| ≤ 10        | .789    | - . 300 |
| ≥ 20        | - . 871 | .221    |

15.18 (a) The two-dimensional coordinates of the observation points and variable points are

## Observation Points

| Name        | Coordinate 1   | Coordinate 2   |
|-------------|----------------|----------------|
| Albania     | 14.102         | - 1 . 322      |
| Austria     | - 5 . 461      | 1.548          |
| Belgium     | - 6 . 077      | - 1 . 479      |
| Bulgaria    | 26.116         | 3.319          |
| Czech.      | 3.317          | - 2 . 092      |
| Denmark     | - 13 . 861     | 1.374          |
| E. Germany  | - 4 . 902      | - 8 . 360      |
| Finland     | - 12 . 262     | 11.290         |
| France      | - 6 . 345      | .672           |
| Greece      | 9.036          | 3.033          |
| Hungary     | 10.805         | - 2 . 363      |
| Ireland     | - 11 . 857     | 5.312          |
| Italy       | 6.309          | - 1 . 314      |
| Netherlands | - 11 . 809     | 2.133          |
| Norway      | - 11 . 005     | - . 077        |
| Poland      | 2.526          | 2.999          |
| Portugal    | .784           | - 16 . 753     |
| Romania     | 19.067         | 2.591          |
| Spain       | 1.923          | - 10 . 483     |
| Sweden      | - 14 . 842     | .726           |
| Switzerland | - 9 . 068      | 4.000          |
| UK          | - 9 . 311      | .698           |
| USSR        | 10.586         | 4.355          |
| W. Germany  | - 13 . 514     | - 3 . 353      |
| Yugoslavia  | 25.742         | 3.548          |

## Variable Points

| Name     | Coordinate 1   | Coordinate 2   |
|----------|----------------|----------------|
| R MEAT   | - . 151        | .133           |
| W MEAT   | - . 129        | .043           |
| EGGS     | - . 067        | .021           |
| MILK     | - . 425        | .831           |
| FISH     | - . 127        | - . 292        |
| CEREALS  | .861           | .406           |
| STARCHY  | - . 067        | - . 076        |
| NUTS     | .114           | - . 070        |
| FRUT VEG | .020           | - . 169        |

In the biplot, the arrows for variables are too short to pass through the points for observations.

- (b) The two-dimensional coordinates of the observation points and variable points are given next.

## Observation Points

| Name        | Coordinate 1   | Coordinate 2   |
|-------------|----------------|----------------|
| Albania     | .231           | - . 049        |
| Austria     | - . 089        | .057           |
| Belgium     | - . 100        | - . 055        |
| Bulgaria    | .428           | .122           |
| Czech.      | .054           | - . 077        |
| Denmark     | - . 227        | .051           |
| E. Germany  | - . 080        | - . 308        |
| Finland     | - . 201        | .416           |
| France      | - . 104        | .025           |
| Greece      | .148           | .112           |
| Hungary     | .177           | - . 087        |
| Ireland     | - . 194        | .196           |
| Italy       | .103           | - . 048        |
| Netherlands | - . 193        | .079           |
| Norway      | - . 180        | - . 003        |
| Poland      | .041           | .110           |
| Portugal    | .013           | - . 617        |
| Romania     | .312           | .095           |
| Spain       | .032           | - . 386        |
| Sweden      | - . 243        | .027           |
| Switzerland | - . 149        | .147           |
| UK          | - . 153        | .026           |
| USSR        | .173           | .160           |
| W. Germany  | - . 221        | - . 124        |
| Yugoslavia  | .422           | .131           |

## Variable Points

| Name     | Coordinate 1   | Coordinate 2   |
|----------|----------------|----------------|
| R MEAT   | - 9 . 196      | 3.602          |
| W MEAT   | - 7 . 904      | 1.179          |
| EGGS     | - 4 . 106      | .569           |
| MILK     | - 25 . 964     | 22.552         |
| FISH     | - 7 . 750      | - 7 . 934      |
| CEREALS  | 52.545         | 11.025         |
| STARCHY  | - 4 . 080      | - 2 . 064      |
| NUTS     | 6.953          | - 1 . 902      |
| FRUT VEG | 1.235          | - 4 . 593      |

In the biplot, the observation points are tightly clustered around the point ( 0 , 0 ) , making them difficult to distinguish,whereas variable points are easily discerned. Red meats, white meats, and milk are highly posi-

tively correlated. These three variables are negatively correlated with nuts and frut veg.

- (c) The two-dimensional coordinates of the observation points and variable points are as follows.

Observation Points

| Name        | Coordinate 1   | Coordinate 2   |
|-------------|----------------|----------------|
| Albania     | 1.805          | - . 254        |
| Austria     | - . 699        | .297           |
| Belgium     | - . 778        | - . 284        |
| Bulgaria    | 3.343          | .637           |
| Czech.      | .425           | - . 402        |
| Denmark     | - 1 . 774      | .264           |
| E. Germany  | - . 627        | - 1 . 605      |
| Finland     | - 1 . 570      | 2.167          |
| France      | - . 812        | .129           |
| Greece      | 1.157          | .582           |
| Hungary     | 1.383          | - . 454        |
| Ireland     | - 1 . 518      | 1.020          |
| Italy       | .808           | - . 252        |
| Netherlands | - 1 . 511      | .409           |
| Norway      | - 1 . 409      | - . 015        |
| Poland      | .323           | .576           |
| Portugal    | .100           | - 3 . 216      |
| Romania     | 2.441          | .497           |
| Spain       | .246           | - 2 . 012      |
| Sweden      | - 1 . 900      | .139           |
| Switzerland | - 1 . 161      | .768           |
| UK          | - 1 . 192      | .134           |
| USSR        | 1.355          | .836           |
| W. Germany  | - 1 . 730      | - . 644        |
| Yugoslavia  | 3.295          | .681           |

## Variable Points

| Name      | Coordinate 1   | Coordinate 2   |
|-----------|----------------|----------------|
| R MEAT    | - 1 . 177      | .691           |
| W MEAT    | - 1 . 012      | .226           |
| EGGS      | - . 526        | .109           |
| MILK      | - 3 . 323      | 4.329          |
| FISH      | - . 992        | - 1 . 523      |
| CEREALS   | 6.726          | 2.116          |
| STARCHY   | - . 522        | - . 396        |
| NUTS      | .890           | - . 365        |
| FRUIT VEG | .158           | - . 882        |

In the biplot, the variable points and observation points are both well spaced. Finland scored high on the milk variable. Yugoslavia and Bulgaria scored high on the cereal variable. Spain and Portugal scored highest on the fish and frut veg variables.

- (d) The biplot from part (c) seems better because the scales on the variables and points are more evenly matched.
- 15.19 (a) The two-dimensional coordinates of the observation points and variable points are as follows.

Observation Points

| Name    | Coordinate 1   | Coordinate 2   |
|---------|----------------|----------------|
| FSM1    | - 9 . 535      | - 4 . 752      |
| Sister  | 2.705          | .796           |
| FSM2    | 4.043          | - . 584        |
| Father  | 4.392          | .614           |
| Teacher | - 8 . 708      | 5.008          |
| MSM     | 3.409          | .701           |
| FSM3    | 3.694          | - 1 . 782      |

Variable Points

| Name   |   Coordinate 1 | Coordinate 2   |
|--------|----------------|----------------|
| KIND   |          0.61  | - . 054        |
| INTEL  |          0.085 | .413           |
| HAPPY  |          0.407 | - . 456        |
| LIKE   |          0.621 | - . 039        |
| JUST   |          0.264 | .785           |

In the biplot, the arrows for the variables are too short to pass through the points for observations.

- (b) The two-dimensional coordinates of the observation points and variable points are as follows.

Observation Points

| Name    | Coordinate 1   | Coordinate 2   |
|---------|----------------|----------------|
| FSM1    | - . 622        | - . 655        |
| Sister  | .176           | .110           |
| FSM2    | .264           | - . 080        |
| Father  | .287           | .085           |
| Teacher | - . 568        | .690           |
| MSM     | .222           | .097           |
| FSM3    | .241           | - . 246        |

Variable Points

| Name   |   Coordinate 1 | Coordinate 2   |
|--------|----------------|----------------|
| KIND   |          9.345 | - . 391        |
| INTEL  |          1.298 | 2.997          |
| HAPPY  |          6.235 | - 3 . 313      |
| LIKE   |          9.521 | - . 282        |
| JUST   |          4.054 | 5.700          |

In the biplot, the observation points are tightly clustered around the point ( 0 , 0 ) , making them difficult to distinguish, whereas variable points are well spaced. Just and intelligent are highly positively correlated, as are kind and likeable.

- (c) The two-dimensional coordinates of the observation points and variable points are as follows.

## Observation Points

| Name    | Coordinate 1   | Coordinate 2   |
|---------|----------------|----------------|
| FSM1    | - 2 . 435      | - 1 . 764      |
| Sister  | .691           | .295           |
| FSM2    | 1.033          | - . 217        |
| Father  | 1.122          | .228           |
| Teacher | - 2 . 224      | 1.859          |
| MSM     | .871           | .260           |
| FSM3    | .943           | - . 662        |

Variable Points

| Name   |   Coordinate 1 | Coordinate 2   |
|--------|----------------|----------------|
| KIND   |          2.387 | - . 145        |
| INTEL  |          0.331 | 1.113          |
| HAPPY  |          1.593 | - 1 . 230      |
| LIKE   |          2.432 | - . 105        |
| JUST   |          1.036 | 2.116          |

In the biplot, the variable points and observation points are both well spaced. Father, sister, FSM2, and FSM3 all scored high on the kind, likeable, and happy variables, whereas teacher and FSM1 scored negatively on those variables.

- (d) The biplot from part (c) seems better because the scales on the variables and points are more evenly matched.

15.20 (a) The two-dimensional coordinates of the observation points and variable points are as follows:

## Observation Points

|   Name | Coordinate 1   | Coordinate 2   |
|--------|----------------|----------------|
|      1 | 49.410         | - 5 . 832      |
|      2 | 25.407         | - 7 . 658      |
|      3 | 21.600         | - 2 . 340      |
|      4 | - 23 . 545     | - 6 . 367      |
|      5 | - 28 . 477     | - 4 . 773      |
|      6 | - 33 . 341     | 2.315          |
|      7 | - 28 . 176     | 7.992          |
|      8 | - 25 . 786     | 12.655         |
|      9 | - 29 . 703     | 9.275          |
|     10 | - 33 . 868     | - 3 . 776      |
|     11 | - 33 . 529     | - 1 . 977      |
|     12 | 28.186         | - 16 . 031     |
|     13 | 10.804         | - 6 . 608      |
|     14 | .566           | 3.021          |
|     15 | 77.970         | .109           |
|     16 | 12.859         | 16.294         |
|     17 | 41.960         | 5.103          |
|     18 | 46.930         | 19.064         |
|     19 | 34.958         | - 1 . 018      |
|     20 | - 16 . 477     | 1.148          |
|     21 | - 23 . 634     | - 1 . 055      |
|     22 | - 34 . 036     | - 2 . 424      |
|     23 | 20.632         | - 5 . 882      |
|     24 | - 15 . 873     | - 6 . 731      |
|     25 | - 23 . 023     | .745           |
|     26 | - 15 . 183     | - 1 . 942      |
|     27 | - 11 . 903     | - 6 . 917      |
|     28 | 5.273          | 3.610          |

## Variable Points

| Name   |   Coordinate 1 | Coordinate 2   |
|--------|----------------|----------------|
| North  |          0.526 | .225           |
| East   |          0.429 | .752           |
| South  |          0.579 | - . 379        |
| West   |          0.452 | - . 490        |

In the biplot, the variable points are tightly grouped and the corresponding arrows do not pass through the points for observations.

- (b) The two-dimensional coordinates of the observation points and variable points are as follows:

## Observation Points

|   Name | Coordinate 1   | Coordinate 2   |
|--------|----------------|----------------|
|      1 | .303           | - . 145        |
|      2 | .156           | - . 191        |
|      3 | .132           | - . 058        |
|      4 | - . 144        | - . 158        |
|      5 | - . 175        | - . 119        |
|      6 | - . 205        | .058           |
|      7 | - . 173        | .199           |
|      8 | - . 158        | .315           |
|      9 | - . 182        | .231           |
|     10 | - . 208        | - . 094        |
|     11 | - . 206        | - . 049        |
|     12 | .173           | - . 399        |
|     13 | .066           | - . 164        |
|     14 | .003           | .075           |
|     15 | .478           | .003           |
|     16 | .079           | .406           |
|     17 | .257           | .127           |
|     18 | .288           | .474           |
|     19 | .214           | - . 025        |
|     20 | - . 101        | .029           |
|     21 | - . 145        | - . 026        |
|     22 | - . 209        | - . 060        |
|     23 | .127           | - . 146        |
|     24 | - . 097        | - . 168        |
|     25 | - . 141        | .019           |
|     26 | - . 093        | - . 048        |
|     27 | - . 073        | - . 172        |
|     28 | .032           | .090           |

## Variable Points

| Name   |   Coordinate 1 | Coordinate 2   |
|--------|----------------|----------------|
| North  |         85.779 | 9.026          |
| East   |         69.899 | 30.223         |
| South  |         94.377 | - 15 . 213     |
| West   |         73.682 | - 19 . 694     |

In the biplot, the observation points are tightly clustered around the point ( 0 , 0 ) , making them difficult to distinguish, whereas variable points are well spaced. All the variables are positively correlated, with south and west showing the closest relationship.

- (c) The two-dimensional coordinates of the observation points and variable points are as follows:

## Observation Points

|   Name | Coordinate 1   | Coordinate 2   |
|--------|----------------|----------------|
|      1 | 3.870          | - . 920        |
|      2 | 1.990          | - 1 . 208      |
|      3 | 1.692          | - . 369        |
|      4 | - 1 . 844      | - 1 . 004      |
|      5 | - 2 . 230      | - . 753        |
|      6 | - 2 . 611      | .365           |
|      7 | - 2 . 207      | 1.261          |
|      8 | - 2 . 020      | 1.996          |
|      9 | - 2 . 326      | 1.463          |
|     10 | - 2 . 652      | . 596          |
|     11 | - 2 . 626      | - - . 312      |
|     12 | 2.207          | - 2 . 529      |
|     13 | .846           | - 1 . 042      |
|     14 | .044           | .477           |
|     15 | 6.106          | .017           |
|     16 | 1.007          | 2.571          |
|     17 | 3.286          | .805           |
|     18 | 3.675          | 3.008          |
|     19 | 2.738          | - . 161        |
|     20 | - 1 . 290      | .181           |
|     21 | - 1 . 851      | - . 166        |
|     22 | - 2 . 666      | - . 382        |
|     23 | 1.616          | - . 928        |
|     24 | - 1 . 243      | - 1 . 062      |
|     25 | - 1 . 803      | .118           |
|     26 | - 1 . 189      | - . 306        |
|     27 | - . 932        | - 1 . 091      |
|     28 | .413           | .569           |

## Variable Points

| Name   |   Coordinate 1 | Coordinate 2   |
|--------|----------------|----------------|
| North  |          6.718 | 1.424          |
| East   |          5.474 | 4.768          |
| South  |          7.391 | - 2 . 400      |
| West   |          5.771 | - 3 . 107      |

In the biplot, the variable points and observation points are both well spaced. Tree 18 is associated with east, 17 with north, 1 and 3 with south, and 2 and 23 with west.

- (d) The biplot from part (c) seems better because the scales on the variables and points are more evenly matched.

## A P P E N D I X C

## Data Sets and SAS Files

Two sets of files are located on the ftp server of John Wiley &amp; Sons STM (Scientific, Technical, and Medical) Division.

All data sets in the book,

SAS command files for all numerical examples.

## DOWNLOADING DATA SET FILES FROM FTP SERVER

Please read the message in the multivariate analysis area before downloading the files. The message has information about the files and discusses how to access them.

The files can be accessed through either a standard ftp program or a Web browser using the ftp protocol. To gain ftp access, type the following at your ftp command prompt:

ftp://ftp.wiley.com

The files are located in the multivariate analysis area of the public/sci tech med directory. If you need further information about downloading the files, you can reach Wiley's technical support line at 212-850-6194.

## References

- Anderberg, M. R. (1973), Cluster Analysis for Applications , New York: Academic Press.
- Anderson, E. (1960), 'A Semi-Graphical Method for Analysis of Complex Problems,' Technometrics , 2 , 287-292.
- Andrews, D. F., and Herzberg, A. M. (1985), Data , New York: Springer-Verlag.
- Bailey, B. J. R. (1977), 'Tables of the Bonferroni t Statistic,' Journal of the American Statistical Association , 72 , 469-479.
- Barcikowski, R. S. (1981), 'Statistical Power with Group Means as the Unit of Analysis,' Journal of Educational Statistics , 6 , 267-285.
- Barnett, V., and Lewis, T. (1978), Outliers in Statistical Data , New York: Wiley.
- Bartlett, M. S. (1937), 'Properties of Sufficiency and Statistical Tests,' Proceedings of the Royal Society of London, Ser. A , 160 , 268-282.
- Baten, W. D., Tack, P. I., and Baeder, H. A. (1958), 'Testing for Differences between Methods of Preparing Fish by Use of Discriminant Function,' Industrial Quality Control , 14 , 6-10.
- Bayne, C. K., Beauchamp, J. J., Kane, V. E., and McCabe, G. P. (1983), 'Assessment of Fisher and Logistic Linear and Quadratic Discrimination Models,' Computational Statistics and Data Analysis , 1 , 257-273.
- Beale, E. M. L. (1969), 'Euclidean Cluster Analysis,' Bulletin of the International Statistical Institute , 43 (2), 92-4.
- Beall, G. (1945), 'Approximate Methods in Calculating Discriminant Functions,' Psychometrika , 10 (3), 205-217.
- Beauchamp, J. J., and Hoel, D. G. (1974), 'Some Investigation and Simulation Studies of Canonical Analysis,' Journal of Statistical Computation and Simulation .
- Beckman, R. J., and Cook, R. D. (1983), 'Outliers' (with comments), Technometrics , 25 , 119-163.
- Benz· ecri, J.-P . (1992), Correspondence Analysis Handbook , New York: Marcel Dekker.
- Bock, R. D. (1963), 'Multivariate Analysis of Variance of Repeated Measurements,' in Problems of Measuring Change , C. W. Harris (ed.), Madison, Wis.: University of Wisconsin Press, pp. 85-103.
- Bock, R. D. (1975), Multivariate Statistical Methods in Behavioral Research , New York: McGraw-Hill.
- Boik, R. J. (1981), 'A Priori Tests in Repeated Measures Designs: Effects of Non-sphericity,' Psychometrika , 46 , 241-255.

- Bonferroni, C. E. (1936), 'Il Calcolo delle Assicurazioni su Gruppi di Teste,' in Studii in Onore del Profesor S. O. Carboni Roma .
- Box, G. E. P. (1949), 'A General Distribution Theory for a Class of Likelihood Criteria,' Biometrika , 36 , 317-346.
- Box, G. E. P. (1950), 'Problems in the Analysis of Growth and Linear Curves,' Biometrics , 6 , 362-389.
- Box, G. E. P. (1954), 'Some Theorems on Quadratic Forms Applied in the Study of Analysis of Variance Problems: II. The Effect of Inequality of Variance and of Correlation between Errors in the Two-Way Classification,' Annals of Mathematical Statistics , 25 , 484-498.
- Box, G. E. P., and Youle, P. V . (1955), 'The Exploration of Response Surfaces: An Example of the Link between the Fitted Surface and the Basic Mechanism of the System,' Biometrics , 11 , 287-323.
- Brown, B. L., Strong, W. J., and Rencher, A. C. (1973), 'Perceptions of Personality from Speech: Effects of Manipulations of Acoustical Parameters,' The Journal of the Acoustical Society of America , 54 , 29-35.
- Brown, B. L., Williams, R. N., and Barlow, C. D. (1984), 'PRIFAC: A Pascal Factor Analysis Program,' Journal of Pascal, Ada, and Modula , 2 , 18-24.
- Brown, T. A., and Koplowitz, J. (1979), 'The Weighted Nearest Neighbor Rule for Class Dependent Sample Sizes,' IEEE Transactions on Information Theory , IT-25 , 617-619.
- Bryce, G. R. (1980), 'Some Observations on the Analysis of Growth Curves,' Paper No. SD025-R, Brigham Young University, Department of Statistics.
- Buck, S. F. A. (1960), 'A Method of Estimation of Missing Values in Multivariate Data Suitable for Use With an Electronic Computer,' Journal of the Royal Statistical Society, Series B , 22 , 302-307.
- Burdick, R. K. (1979), 'On the Use of Canonical Analysis to Test MANOVA Hypotheses,' Presented at the Annual Meeting of the American Statistical Association, Washington, D.C., August 1979.
- Cacoullos, T. (1966), 'Estimation of a Multivariate Density,' Annals of the Institute of Statistical Mathematics , 18 , 179-189.
- Cameron, E., and Pauling, L. (1978), 'Supplemental Ascorbate in the Supportive Treatment of Cancer: Reevaluation of Prolongation of Survival Times in Terminal Human Cancer,' Proceedings of the National Academy of Science, U.S.A. , 75 , 4538-4552.
- Campbell, N. A. (1980), 'Robust Procedures in Multivariate Analysis: I. Robust Covariance Estimation,' Applied Statistics , 29 , 231-237.
- Chambers, J. M., and Kleiner, B. (1982), 'Graphical Techniques for Multivariate Data and for Clustering,' in Handbook of Statistics , Vol. 2, P . R. Krishnaiah and L. N. Kanal (eds.), New York: North-Holland, pp. 209-244.
- Chernoff, H. (1973), 'The Use of Faces to Represent Points in k -dimensional Space Graphically,' Journal of the American Statistical Association , 68 , 361-368.
- Chidananda Gowda, K., and Krishna, G. (1979), 'The Condensed Nearest Neighbor Rule Using the Concept of Mutual Nearest Neighborhood,' IEEE Transactions on Information Theory , IT-25 , 488-490.
- Clausen, Sten-Erik (1998), Applied Correspondence Analysis , Thousand Oaks, Calif.: Sage Publications.
- Cochran, W. G., and Cox, G. M. (1957), Experimental Designs , 2nd ed. New York: Wiley.
- Collier, R. O., Baker, F. B., Mandeville, G. K., and Hayes, T. F. (1967), 'Estimates of Test Size for Several Procedures Based on Conventional Ratios in the Repeated Measure Design,' Psychometrika , 32 , 339-353.

- Cramer, E. M., and Nicewander, W. A. (1979), 'Some Symmetric, Invariant Measures of Multivariate Association,' Psychometrika , 44 , 43-54.
- Crepeau, H., Koziol, J., Reid, N., and York, Y . S. (1985), 'Analysis of Incomplete Multivariate Data from Repeated Measurement Experiments,' Biometrics , 41 , 505-514.
- Critchley, F. (1985), 'Influence in Principal Components Analysis,' Biometrika , 72 , 627-636.
- Crowder, M. J., and Hand, D. J. (1990), Analysis of Repeated Measures , New York: Chapman &amp;Hall.
- D'Agostino, R. B. (1971), 'An Omnibus Test of Normality for Moderate and Large Size Samples,' Biometrika , 58 , 341-348.
- D'Agostino, R. B. (1972), 'Small Sample Probability Points for the D Test of Normality,' Biometrika , 59 , 219-221.
- D'Agostino, R. B., and Pearson, E. S. (1973), 'Tests for Departure from Normality. Empirical Results for the Distributions of b 2 and √ b 1 ,' Biometrika , 60 , 613-622; correction, 61 , 647.
- D'Agostino, R. B., and Tietjen, G. L. (1971), 'Simulated Probability Points of b 2 for Small Samples,' Biometrika , 58 , 669-672.
- Davidson, M. L. (1972), 'Univariate versus Multivariate Tests in Repeated Measures Experiments,' Psychological Bulletin , 77 , 446-452.
- Davidson, M. L. (1983), Multidimensional Scaling , New York: Wiley.
- Davis, A. W. (1970a), 'Exact Distributions of Hotelling's Generalized T 2 0 -Test,' Biometrika , 57 , 187-191.
- Davis, A. W. (1970b), 'Further Applications of a Differential Equation for Hotelling's Generalized T 2 0 -Test,' Annals of the Institute of Statistical Mathematics , 22 , 77-87.
- Davis, A. W. (1980), 'Further Tabulation of Hotelling's Generalized T 2 0 -Test,' Communications in Statistics-Part B Simulation and Computation , 9 , 321-336.
- Dawkins, B. (1989), 'Multivariate Analysis of National Track Records,' American Statistician , 43 , 110-115.
- Dempster, A. P., Laird, N. M., and Rubin, D. B. (1977), 'Maximum Likelihood from Incomplete Data via the EM Algorithm,' Journal of the Royal Statistical Society, Series B , 39 , 1-38.
- Devlin, S. J., Gnanadesikan, R., and Kettenring, J. R. (1981), 'Robust Estimation of Dispersion Matrices and Principal Components,' Journal of the American Statistical Association , 76 , 354-362.
- Dubien, J. L. and Warde, W. D. (1979), 'A Mathematical Comparison of the Members of an Infinite Family of Agglomerative Clustering Algorithms,' Canadian Journal of Statistics , 7 , 29-38.
- Duran, B. S., and Odell, P. L. (1974), 'Cluster Analysis: a Survey,' Lecture Notes in Economics and Mathematical Systems , New York: Springer-Verlag.
- Edwards, D., and Kreiner, S. (1983), 'The Analysis of Contingency Tables by Graphical Models,' Biometrika , 70 (3), 553-565.
- Ehrenberg, A. S. C. (1977), 'Rudiments of Numeracy,' Journal of the Royal Statistical Society, Series A , 140 , 277-297.
- Elston, R. C., and Grizzle, J. E. (1962), 'Estimation of Time-response Curves and Their Confidence Bands,' Biometrics , 18 , 148-159.
- Everitt, B. S. (1987), Introduction to Optimization Methods and Their Application in Statistics , London: Chapman &amp; Hall.
- Everitt, B. S. (1993), Cluster Analysis , 3rd ed., London: Edward Arnold.

- Fearn, T. (1975), 'A Bayesian Approach to Growth Curves,' Biometrika , 62 , 89-100.
- Fearn, T. (1977), 'A Two-Stage Model for Growth Curves which Leads to Rao's Covariance Adjusted Estimators,' Biometrika , 64 , 141-143.
- Federer, W. T. (1986), 'On Planning Repeated Measures Experiments,' Unit Report BU-909M, Cornell University, Biometrics Unit.
- Fehlberg, W. T. (1980), 'Repeated Measures: The Effect of a Preliminary Test for the Structure of the Covariance Matrix upon the Alpha Levels of the Tests for Experimental Design Factors,' M.A. Thesis, Brigham Young University, Department of Statistics.
- Ferguson, T. S. (1961), 'On the Rejection of Outliers,' Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability , Vol. 1, Los Angeles: University of California Press, pp. 253-287.
- Fisher, R. A. (1936), 'The Use of Multiple Measurement in Taxonomic Problems,' Annals of Eugenics , 7 , 179-188.
- Fix, E., and Hodges, J. L. (1951), 'Discriminatory Analysis, Nonparametric Discrimination: Consistency Properties,' Report No. 4, Project No. 21-49-004, Brooks Air Force Base, USAF School of Aviation Medicine.
- Flack, V. F., and Chang, P. C. (1987), 'Frequency of Selecting Noise Variables in Subset Regression Analysis: A Simulation Study,' American Statistician , 41 , 84-86.
- Flury, B., and Riedwyl, H. (1981), 'Graphical Representation of Multivariate Data by Means of Asymmetric Faces,' Journal of the American Statistical Association , 76 , 757-765.
- Flury, B., and Riedwyl, H. (1985), ' T 2 Tests, the Linear Two-Group Discriminant Function, and Their Computation by Linear Regression,' American Statistician , 39 , 20-25.
- Frets, G. P. (1921), 'Heredity of Head Form in Man,' Genetica , 3 , 193-384.
- Friedman, J. H., and Tukey, J. W. (1974), 'A Projection Pursuit Algorithm for Exploratory Data Analysis,' IEEE Transactions on Component Parts , 9 , 881-890.
- Furnival, G. M., and Wilson, R. W. (1974), 'Regression by Leaps and Bounds,' Technometrics , 16 , 499-511.
- Gabriel, K. R. (1968), 'Simultaneous Test Procedures in Multivariate Analysis of Variance,' Biometrika , 55 , 489-504.
- Gabriel, K. R. (1969), 'Simultaneous Test Procedures-Some Theory of Multiple Comparisons,' Annals of Mathematics and Statistics , 40 , 224-250.
- Gabriel, K. R. (1971), 'The Biplot Graphical Display of Matrices with Application to Principal Component Analysis,' Biometrika , 58 , 453-467.
- Gates, G. W. (1972), 'The Reduced Nearest Neighbor Rule,' IEEE Transactions on Information Theory , IT-18 , 431.
- Geisser, S. (1980), 'Growth Curve Analysis,' in Handbook of Statistics , Vol. 1, P. R. Krishnaiah (ed.), Amsterdam: North-Holland, pp. 89-115.
- Gilbert, E. S. (1968), 'On Discrimination Using Qualitative Variables,' Journal of the American Statistical Association , 63 , 1399-1412.
- Gnanadesikan, R. (1997), Methods for Statistical Data Analysis of Multivariate Observations , 2nd ed., New York: Wiley.
- Gnanadesikan, R., and Kettenring, J. R. (1972), 'Robust Estimates, Residuals, and Outlier Detection with Multiresponse Data,' Biometrics , 28 , 81-124.
- Gordon, A. D. (1999), Classification , 2nd ed., London: Chapman &amp; Hall/CRC.
- Gower, J. C. (1966), 'Some Distance Properties of Latent Root and Vector Methods used in Multivariate Analysis,' Biometrika , 53 , 325-338.

- Gower, J. C., and Hand, D. J. (1996), Biplots , London: Chapman &amp; Hall.
- Graybill, F. A. (1969), Introduction to Matrices with Applications in Statistics , Belmont, Calif.: Wadsworth.
- Greenacre, M. J. (1984), Theory and Applications of Correspondence Analysis , London: Academic Press.
- Greenhouse, S. W., and Geisser, S. (1959), 'On Methods in the Analysis of Profile Data,' Psychometrika , 24 , 95-112.
- Grizzle, J. E., and Allen, D. M. (1969), 'Analysis of Growth and Dose Response Curves,' Biometrics , 25 , 357-381.
- Guttman, I. (1982), Linear Models: An Introduction , New York: Wiley.
- Habbema, J. D. F., Hermans, J., and Remme, J. (1978), 'Variable Kernel Density Estimation in Discriminant Analysis,' in Compstat 1978: Proceedings in Computational Statistics , G. Bruckman (ed.), Vienna: Physica-Verlag, pp. 178-185.
- Habbema, J. D. F., Hermans, J., and van den Broek, K. (1974), 'A Stepwise Discriminant Analysis Program Using Density Estimation,' in Comstat , G. Bruckmann, F. Ferschl, and L. Schmetterer (eds.), Vienna: Physica-Verlag, pp. 101-110.
- Hammond, J., Smith, D., and Gill, D. S. (1981), 'Selection of Adequate Subsets in Multivariate Multiple Regression,' Presented at the Annual Meeting of the American Statistical Association, August 10-13, Detroit, Mich.
- Hand, D. J., and Batchelor, B. G. (1978), 'An Edited Condensed Nearest Neighbor Rule,' Information Sciences , 14 , 171-180.
- Hand, D. J., Daly, F., Lunn, A. D., McConway, K. J., and Ostrowski, E. (eds.) (1994), A Handbook of Small Data Sets , London: Chapman &amp; Hall.
- Harman, H. H. (1976), Modern Factor Analysis , 3rd rev. ed., Chicago: University of Chicago Press.
- Hart, P. E. (1968), 'The Condensed Nearest Neighbor Rule,' IEEE Transactions on Information Theory , IT-14 , 515-516.
- Hartigan, J. A. (1975a), Clustering Algorithms , New York: Wiley.
- Hartigan, J. A. (1975b), 'Printer Graphics for Clustering,' Journal of Statistical Computation and Simulation , 4 , 187-213.
- Harville, D. A. (1997), Matrix Algebra from a Statistician's Perspective , New York: SpringerVerlag.
- Hawkins, D. M. (1980), Identification of Outliers , London: Chapman &amp; Hall.
- Healy, M. J. R. (1969), 'Rao's Paradox Concerning Multivariate Tests of Significance,' Biometrics , 25 , 411-413.
- Heywood, H. B. (1931), 'On Finite Sequences of Real Numbers,' Proceedings of the Royal Society, Series A , 134 , 486-501.
- Hilton, D. K. (1983), 'A Simulation and Comparison of Three Criterion Functions Used for Subset Selection in Linear Regression,' M.A. Thesis, Brigham Young University, Department of Statistics.
- Hocking, R. R. (1976), 'The Analysis and Selection of Variables in Linear Regression,' Biometrics , 32 , 1-51.
- Hotelling, H. (1931), 'The Generalization of Student's Ratio,' Annals of Mathematical Statistics , 2 , 360-378.
- Hotelling, H. (1951), 'A Generalized T Test and Measure of Multivariate Dispersion,' Proceedings of the Second Berkeley Symposium on Mathematical Statistics and Probability , 1 , 23-41.

- Hsu, P. L. (1938), 'Notes on Hotelling's Generalized T 2 ,' Annals of Mathematical Statistics , 9 , 231-243.
- Huber, P. J. (1985), 'Projection Pursuit,' Annals of Statistics , 13 , 435-525.
- Huberty, C. J. (1975), 'The Stability of Three Indices of Relative Variable Contribution in Discriminant Analysis,' Journal of Experimental Education , 44 , 59-64.
- Hummel, T. J., and Sligo, J. (1971), 'Empirical Comparison of Univariate and Multivariate Analysis of Variance Procedures,' Psychological Bulletin , 76 , 49-57.
- Huynh, H. (1978), 'Some Approximate Tests in Repeated Measures Designs,' Psychometrika , 43 , 1582-1589.
- Huynh, H., and Feldt, L. S. (1970), 'Conditions Under Which Mean Square Ratios in Repeated Measurement Designs Have Exact F Distributions,' Journal of the American Statistical Association , 65 , 1582-1589.
- Huynh, H., and Feldt, L. S. (1976), 'Estimation of the Box Correction for Degrees of Freedom from Sample Data in Randomized Block and Split-Plot Designs,' Journal of Educational Statistics , 1 (1), 69-82.
- Izenman, A. J., and Williams, J. S. (1989), 'A Class of Linear Spectral Models and Analyses for the Study of Longitudinal Data,' Biometrics , 45 , 831-849.
- Jackson, D., and Bryce, G. R. (1981), 'A Univariate-Like Method of Analyzing Growth Curve Data with Nonuniform Variance-Covariance Matrices,' Paper No. SD-028-R, Brigham Young University, Department of Statistics.
- Jackson, J. E. (1980), 'Principal Components and Factor Analysis: Part I-Principal Components,' Journal of Quality Technology , 12 , 201-213.
- Jacoby, W. G. (1998), Statistical Graphics for Visualizing Multivariate Data , Thousand Oaks, Calif.: Sage Publications.
- Jardine, N., and Sibson, R. (1968), 'The Construction of Hierarchic and Non-hierarchic Classifications,' Computer Journal , 11 , 117-184.
- Jardine, N., and Sibson, R. (1971), Mathematical Taxonomy , London: Wiley.
- Jeffers, J. N. R. (1967), 'Two Case Studies in the Application of Principal Component Analysis,' Applied Statistics , 16 , 225-236.
- Jensen, D. R. (1982), 'Efficiency and Robustness in the Use of Repeated Measurements,' Biometrics , 38 , 813-825.
- Jensen, R. E. (1969), 'A Dynamic Programming Algorithm for Cluster Analysis,' Operations Research , 12 , 1034-1057.
- Jobson, J. D. (1992), Applied Multivariate Data Analysis, Volume II: Cateogorical and Multivariate Methods , New York: Springer-Verlag.
- Jolliffe, I. T. (1972), 'Discarding Variables in a Principal Component Analysis, I: Artificial Data,' Applied Statistics , 21 , 160-173.
- Jolliffe, I. T. (1973), 'Discarding Variables in a Principal Component Analysis, II: Real Data,' Applied Statistics , 22 , 21-31.
- Jones, M. C., and Sibson, R. (1987), 'What is Projection Pursuit?' Journal of the Royal Statistical Society , 150 , 1-36.
- Kabe, D. G. (1985), 'On Some Multivariate Statistical Methodology with Applications to Statistics, Psychology, and Mathematical Programming,' Journal of the Industrial Mathematics Society , 35 , 1-18.
- Kaiser, H. F. (1970), 'A Second Generation Little Jiffy,' Psychometrika , 35 , 401-415.

- Kaiser, H. F., and Rice, J. (1974), 'Little Jiffy, Mark IV,' Educational and Psychological Measurement , 34 , 111-117.
- Kaufman, L., and Rousseeuw, P. J. (1990), Finding Groups in Data: An Introduction to Cluster Analysis , New York: Wiley.
- Keuls, M., Martakis, G. F. P., and Magid, A. H. A. (1984), 'The Relationship between Pod Yield and Specific Leaf Area in Snapbeans; an Example of Stepwise Multivariate Analysis of Variance,' Scientia Horticulturae , 23 , 231-246.
- Khattree, R., and Naik, D. N. (1999), Applied Multivariate Statistics with SAS Software , 2nd ed, New York: Wiley.
- Khattree, R., and Naik, D. N. (2000), Multivariate Data Reduction and Discrimination with SAS Software , New York: Wiley.
- Khuri, A. I., Mathew, T., and Nel, D. G. (1994), 'A Test to Determine Closeness of Multivariate Satterthwaite's Approximation,' Journal of Multivariate Analysis , 51 , 201-209.
- Kleinbaum, D. G., Kupper, L. L., and Muller, K. E. (1988), Applied Regression Analysis and Other Multivariable Methods , Boston: PWS-KENT.
- Kleiner, B., and Hartigan, J. A. (1981), 'Representing Points in Many Dimensions by Trees and Castles,' Journal of the American Statistical Association , 76 , 260-269.
- Kramer, C. Y. (1972), A First Course in Multivariate Analysis , Published by the author, Blacksburg, VA.
- Kramer, C. Y., and Jensen, D. R. (1969a), 'Fundamentals of Multivariate Analysis, Part I. Inference about Means,' Journal of Quality Technology , 1 (2), 120-133.
- Kramer, C. Y., and Jensen, D. R. (1969b), 'Fundamentals of Multivariate Analysis, Part II. Inference about Two Treatments,' Journal of Quality Technology , 1 (3), 189-204.
- Kramer, C. Y., and Jensen, D. R. (1970), 'Fundamentals of Multivariate Analysis, Part IV. Analysis of Variance for Balanced Experiments,' Journal of Quality Technology , 2 , 32-40.
- Kruskal, J. B. (1964a), 'Multidimensional Scaling by Optimizing Goodness of Fit to a Nonmetric Hypothesis,' Psychometrika , 29 , 1-27.
- Kruskal, J. B. (1964b), 'Nonmetric Multidimensional Scaling: A Numerical Method,' Psychometrika , 29 , 115-129.
- Kruskal, J. B., and Wish, M. (1978), Multidimensional Scaling , Beverly Hills: Sage Publications.
- Krzanowski, W. J. (1975), 'Discrimination and Classification Using Both Binary and Continuous Variables,' Journal of the American Statistical Association , 70 , 782-790.
- Krzanowski, W. J. (1976), 'Canonical Representation of the Location Model for Discrimination or Classification,' Journal of the American Statistical Association , 71 , 845-848.
- Krzanowski, W. J. (1977), 'The Performance of Fisher's Linear Discriminant Function Under Non-Optimal Conditions,' Technometrics , 19 , 191-200.
- Krzanowski, W. J. (1979), 'Some Linear Transformations for Mixtures of Binary and Continuous Variables, with Particular Reference to Linear Discriminant Analysis,' Biometrika , 66 , 33-39.
- Krzanowski, W. J. (1980), 'Mixtures of Continuous and Categorical Variables in Discriminant Analysis,' Biometrics , 36 , 493-499.
- Lachenbruch, P. A. (1975), Discriminant Analysis , New York: Hafner.
- Lachenbruch, P. A., and Goldstein, M. (1979), 'Discriminant Analysis,' Biometrics , 35 , 6985.

- Lance, G. N., and Williams, W. T. (1966), 'Computer Programs for Hierarchical Polythetic Classification,' Computer Journal , 9 , 60-64.
- Lance, G. N., and Williams, W. T. (1967), 'A General Theory of Classificatory Sorting Strategies: I. Hierarchical Systems,' Computer Journal , 9 , 373-380.
- Lavine, B., and Carlson, D. (1987), 'European Bee or Africanized Bee?: Species Identification through Chemical Analysis,' Analytical Chemistry , 59 (6), 468-470.
- Lawley, D. N. (1938), 'A Generalization of Fisher's Z-Test,' Biometrika , 30 , 180-187.
- Lee, J. C. (1988), 'Prediction and Estimation of Growth Curves with Special Covariance Structures,' Journal of the American Statistical Association , 83 , 432-440.
- Lee, J. C., Chang, T. C., and Krishnaiah, P. R. (1977), 'Approximations to the Distributions of the Likelihood Ratio Statistics for Testing Certain Structures on the Covariance Matrices of Real Multivariate Normal Populations,' in Multivariate Analysis , Vol. 4, P. R. Krishnaiah (ed.), Amsterdam: North-Holland, pp. 105-118.
- Lin, C. C., and Mudholkar, G. S. (1980), 'A Simple Test for Normality Against Asymmetric Alternatives,' Biometrika , 67 , 455-461.
- Lindsey, H., Webster, J. T., and Halpern, H. (1985), 'Canonical Correlation as a Discriminant Tool in a Periodontal Problem,' Biometrika , 72 , 257-264.
- Loftsgaarden, D. O., and Quesenberry, C. P. (1965), 'A Nonparametric Estimate of a Multivariate Density Function,' Annals of Mathematical Statistics , 36 , 1049-1051.
- Lubischew, A. A. (1962), 'On the Use of Discriminant Functions in Taxonomy,' Biometrics , 18 , 455-477.
- MacNaughton-Smith, P., Williams, W. T., Dale, M. B., and Mockett, L. G. (1964), 'Dissimilarity Analysis,' Nature , 202 , 1034-1035.
- Mahalanobis, P. C. (1936), 'On the Generalized Distance in Statistics,' Proceedings of the National Institute of Sciences of India , 12 , 49-55.
- Mallows, C. L. (1964), 'Choosing Variables in a Linear Regression: A Graphical Aid,' Paper presented at the Central Regional Meeting of the Institute of Mathematical Statistics, Manhatten, Kans.
- Mallows, C. L. (1973), 'Some Comments on Cp ,' Technometrics , 15 , 661-675.
- Mardia, K. V. (1970), 'Measures of Multivariate Skewness and Kurtosis with Applications,' Biometrika , 57 , 519-530.
- Mardia, K. V. (1974), 'Applications of Some Measures of Multivariate Skewness and Kurtosis for Testing Normality and Robustness Studies,' Sankhya, Series B , 36 , 115-128.
- Mardia, K. V., Kent, J. T., and Bibby, J. M. (1979), Multivariate Analysis , New York: Academic Press.
- Marks, R. G., and Rao, P. V. (1978), 'A Modified Tiao-Guttman Rule for Multiple Outliers,' Communications in Statistics , A7 , 113-126.
- Mathai, A. M., and Katiyar, R. S. (1979), 'Exact Percentage Points for Testing Independence,' Biometrika , 66 , 353-356.
- Mauchly, J. W. (1940), 'Significance Test for Sphericity of a Normal N -Variate Distribution,' Annals of Mathematical Statistics , 11 , 204-209.
- Maxwell, S. E., and Avery, R. D. (1982), 'Small Sample Profile Analysis with Many Variables,' Psychological Bulletin , 92 , 778-785.
- McCabe, G. P. (1984), 'Principal Variables,' Technometrics , 26 , 137-144.
- McKay, R. J. (1977), 'Variable Selection in Multivariate Regression: An Application of Simultaneous Test Procedures,' Journal of the Royal Statistical Society, Series B , 39 , 371-380.

- McKay, R. J. (1979), 'The Adequacy of Variable Subsets in Multivariate Regression,' Technometrics , 21 , 475-480.
- Milligan, G. W. (1996), 'Clustering Validation: Results and Implications for Applied Analyses,' Clustering and Classification (eds P. Arabie, L. J. Hubert and G. De Soete), World Scientific, Singapore, 341-375.
- Milligan, G. W., and Cooper, M. C. (1985), 'An Examination of Procedures for Determining the Number of Clusters in a Data Set,' Psychometrika , 50 , 159-179.
- Mojena, R. (1977), 'Hierarchical Grouping Methods and Stopping Rules: An Evaluation,' Computer Journal , 20 , 359-363.
- Morrison, D. F. (1972), 'The Analysis of a Single Sample of Repeated Measurements,' Biometrics , 28 , 55-71.
- Morrison, D. F. (1983), Applied Linear Statistical Methods , Englewood Cliffs, N.J.: Prentice Hall.
- Morrison, D. F. (1990), Multivariate Statistical Methods , 3rd ed, New York: McGraw-Hill.
- Mosteller, F., and Wallace, D. L. (1984), Applied Bayesian and Classical Inference: The Case of the Federalist Papers , New York: Springer-Verlag.
- Mulholland, H. P. (1977), 'On the Null Distribution of √ b 1 for Samples of Size at Most 25, with Tables,' Biometrika , 64 , 401-409.
- Muller, K. E. (1982), 'Understanding Canonical Correlation through the General Linear Model and Principal Components,' American Statistician , 36 , 342-354.
- Muller, K. E., and Peterson, B. L. (1984), 'Practical Methods for Computing Power in Testing the Multivariate General Linear Hypothesis,' Computational Statistics and Data Analysis , 2 , 143-158.
- Myers, R. H. (1990), Classical and Modern Regression with Applications , 2nd ed, Boston: PWS-Kent.
- Nagarsenker, B. N., and Pillai, K. C. S. (1973), 'The Distribution of the Sphericity Test Criterion,' Journal of Multivariate Analysis , 3 , 226-235.
- Nason, G. (1995), 'Three-dimensional Projection Pursuit,' Journal of Applied Statistics , 44 (4), 411-430.
- Olson, C. L. (1974), 'Comparative Robustness of Six Tests in Multivariate Analysis of Variance,' Journal of the American Statistical Association , 69 , 894-908.
- O'Sullivan, J. B., and Mahan, C. M. (1966), 'Glucose Tolerance Test: Variability in Pregnant and Non-pregnant Women,' American Journal of Clinical Nutrition , 19 , 345-351.
- Parzen, E. (1962), 'On Estimation of a Probability Density Function and Mode,' Annals of Mathematical Statistics , 33 , 1065-1076.
- Patel, H. I. (1986), 'Analysis of Repeated Measures Designs with Changing Covariates in Clinical Trials,' Biometrika , 73 , 707-715.
- Pearson, E. S., and Hartley, H. O. (eds.) (1972), Biometrika Tables for Statisticians , Volume 2, Cambridge: Cambridge University Press.
- Pfieffer, K. P. (1985), 'Stepwise Variable Selection and Maximum Likelihood Estimation of Smoothing Factors of Kernel Functions for Nonparametric Discriminant Functions Evaluated by Different Criteria,' Computers and Biomedical Research , 18 , 46-61.
- Pillai, K. C. S. (1964), 'On the Distribution of the Largest of Seven Roots of a Matrix in Multivariate Analysis,' Biometrika , 51 , 270-275.
- Pillai, K. C. S. (1965), 'On the Distribution of the Largest Characteristic Root of a Matrix in Multivariate Analysis,' Biometrika , 52 , 405-414.

- Posse, C. (1990), 'An Effective Two-dimensional Projection Pursuit Algorithm,' Communications in Statistics: Simulation and Computation , 19 , 1143-1164.
- Posten, H. O. (1962), 'Analysis of Variance and Analysis of Regression with More than One Response,' Proceedings of a Symposium on Applications of Statistics and Computers to Fuel and Lubricant Problems , 91-109.
- Potthoff, R. F., and Roy, S. N. (1964), 'A Generalized Multivariate Analysis of Variance Model Useful Especially for Growth Curve Problems,' Biometrika , 51 , 313-326.
- Rao, C. R. (1948), 'Tests of Significance in Multivariate Analysis,' Biometrika , 35 , 58-79.
- Rao, C. R. (1959), 'Some Problems Involving Linear Hypotheses in Multivariate Analysis,' Biometrika , 46 , 49-58.
- Rao, C. R. (1966), 'Covariance Adjustment and Related Problems in Multivariate Analysis,' in Multivariate Analysis , P. Krishnaiah (ed.), New York: Academic Press, pp. 87-103.
- Rao, C. R. (1973), Linear Statistical Inference and Its Applications , 2nd ed, New York: Wiley.
- Rao, C. R. (1984), 'Prediction of Future Observations in Polynomial Growth Curve Models,' in Proceedings of the Indian Statistical Institute Golden Jubilee International Conference on Statistics: Applications and New Directions , Calcutta: Indian Statistical Institute, pp. 512-520.
- Rao, C. R. (1987), 'Prediction of Future Observations in Growth Curve Models,' Statistical Science , 2 , 434-471.
- Reaven, G. M., and Miller, R. G. (1979), 'An Attempt to Define the Nature of Chemical Diabetes Using a Multidimensional Analysis,' Diabetologia , 16 , 17-24.
- Reinsel, G. (1982), 'Multivariate Repeated-Measurement or Growth Curve Models with Random-Effects Covariance Structure,' Journal of the American Statistical Association , 77 , 190-195.
- Remme, J., Habbema, J. D. F., and Hermans, J. (1980), 'A Simulative Comparison of Linear, Quadratic, and Kernel Discrimination,' Journal of Statistical Computation and Simulation , 11 , 87-106.
- Rencher, A. C. (1988), 'On the Use of Correlations to Interpret Canonical Functions,' Biometrika , 75 , 363-365.
- Rencher, A. C. (1992a), 'Bias in Apparent Classification Rates in Stepwise Discriminant Analysis,' Communications in Statistics-Simulation and Computation , 21 , 373-389.
- Rencher, A. C. (1992b), 'Interpretation of Canonical Discriminant Functions, Canonical Variates, and Principal Components,' American Statistician , 46 , 217-225.
- Rencher, A. C. (1993), 'The Contribution of Individual Variables to Hotelling's T 2 , Wilks' /Lambda1 , and R 2 ,' Biometrics , 49 , 479-489.
- Rencher, A. C. (1998), Multivariate Statistical Inference and Applications , New York: Wiley.
- Rencher, A. C. (2000), Linear Models in Statistics , New York: Wiley.
- Rencher, A. C., and Larson, S. F. (1980), 'Bias in Wilks' /Lambda1 in Stepwise Discriminant Analysis,' Technometrics , 22 , 349-356.
- Rencher, A. C., and Pun, F. C. (1980), 'Inflation of R 2 in Best Subset Regression,' Technometrics , 22 , 49-53.
- Rencher, A. C., and Scott, D. T. (1990), 'Assessing the Contribution of Individual Variables Following Rejection of a Multivariate Hypothesis,' Communications in Statistics: Simulation and Computation , 19 (2), 535-553.
- Rencher, A. C., Wadham, R. A., and Young, J. R. (1978), 'A Discriminant Analysis of Four Levels of Teacher Competence,' Journal of Experimental Education , 46 , 46-51.

- Ripley, B. D. (1996), Pattern Recognition and Neural Networks , Cambridge: Cambridge University Press.
- Robert, P., and Escoufier, Y. (1976), 'A Unifying Tool for Linear Multivariate Analysis: The RV-Coefficient,' Journal of the Royal Statistical Society, Series C , 25 , 257-265.
- Roebruck, P. (1982), 'Canonical Forms and Tests of Hypotheses; Part II: Multivariate Mixed Linear Models,' Statistica Neerlandica , 36 , 75-80.
- Rogan, J. C., Keselman, H. J., and Mendoza, J. L. (1979), 'Analysis of Repeated Measurements,' British Journal of Mathematical and Statistical Psychology , 32 , 269-286.
- Rogers, W. H., and Wagner, T. J. (1978), 'A Finite Sample Distribution Free Performance Bound for Local Discrimination Rules,' Annals of Statistics , 6 , 506-514.
- Romney, A. K., Shepard, R. N., and Nerlove, S. B. (eds.) (1972), Multidimensional Scaling: Theory and Applications in the Behavioral Sciences, Volume II: Applications , New York: Seminar Press.
- Rosenblatt, M. (1956), 'Remarks on Some Nonparametric Estimates of a Density Function,' Annals of Mathematical Statistics , 27 , 832-837.
- Royston, J. P. (1983), 'Some Techniques for Assessing Multivariate Normality Based on the Shapiro-Wilk W ,' Applied Statistics , 32 , 121-133.
- Ruymgaart, F. H. (1981), 'A Robust Principal Component Analysis,' Journal of Multivariate Analysis , 11 , 485-497.
- Satterthwaite, F. E. (1941), 'Synthesis of Variances,' Psychometrika , 6 , 309-316.
- Schoenberg, I. J. (1935), 'Remarks to Maurice Fr· echet's article 'Sur la definition axiomatique d'une classe d'espace distanci· es vectoriellement applicable sur l'espace de Hilbert,'' Annals of Mathematics , 36 , 724-732.
- Schott, J. R., and Saw, J. G. (1984), 'A Multivariate One-Way Classification Model with Random Effects,' Journal of Multivariate Analysis , 15 , 1-12.
- Schuurmann, F. J., Krishnaiah, P. R., and Chattopadhyay, A. K. (1975), 'Exact Percentage Points of the Distribution of the Trace of a Multivariate Beta Matrix,' Journal of Statistical Computation and Simulation , 3 , 331-343.
- Schwager, S. J., and Margolin, B. H. (1982), 'Detection of Multivariate Normal Outliers,' Annals of Statistics , 10 , 943-954.
- Scott, David W. (1992), Multivariate Density Estimation Theory, Practice and Visualization , New York: Wiley.
- Searle, S. R., (1982), Matrix Algebra Useful for Statistics , New York: Wiley.
- Seber, G. A. F. (1984), Multivariate Observations , New York: Wiley.
- Shapiro, S. S., and Wilk, M. B. (1965), 'An Analysis of Variance Test for Normality (Complete Samples),' Biometrika , 52 , 591-611.
- Shepard, R. N., Romney, A. K., and Nerlove, S. B. (eds.) (1972), Multidimensional Scaling: Theory and Applications in the Behavioral Sciences, Volume I: Theory , New York: Seminar Press.
- Sibson, R. (1984), 'Present Position and Potential Developments: Some Personal Views. Multivariate analysis (with discussion),' Journal of the Royal Statistical Society, Series A , 147 , 198-207.
- Silverman, B. W. (1986), Density Estimation for Statistics and Data Analysis , New York: Chapman &amp; Hall.
- Sinha, B. K. (1984), 'Detection of Multivariate Outliers in Elliptically Symmetric Distributions,' Annals of Statistics , 12 , 1558-1565.

- Siotani, M., Hayakawa, T., and Fujikoshi, Y. (1985), Modern Multivariate Statistical Analysis , Columbus, Ohio: American Sciences.
- Siotani, M., Yoshida, K., Kawakami, H., Nojiro, K., Kawashima, K., and Sato, M. (1963), 'Statistical Research on the Taste Judgement: Analysis of the Preliminary Experiment on Sensory and Chemical Characters of SEISHU,' Proceedings of the Institute of Statistical Mathematics , 10 , 99-118.
- Small, N. J. H. (1978), 'Plotting Squared Radii,' Biometrika , 65 , 657-658.
- Smith, D. W., Gill, D. S., and Hammond, J. J. (1985), 'Variable Selection in Multivariate Multiple Regression,' Journal of Statistical Computation and Simulation , 22 , 217-227.
- Snee, R. D. (1972), 'On the Analysis of Response Curve Data,' Technometrics , 14 , 47-62.
- Snee, R. D., Acuff, S. J., and Gibson, J. R. (1979), 'A Useful Method for the Analysis of Growth Studies,' Biometrics , 35 , 835-848.
- Sokal, R. R., and Rohlf, F. J. (1981), Biometry: The Principles and Practices of Statistics in Biological Research , 2nd ed., San Francisco: W. H. Freeman and Co.
- Sparks, R. S., Coutsourides, D., and Troskie, L. (1983), 'The Multivariate Cp ,' Communications in Statistics-Part A Theory and Methods , 12 (15), 1775-1793.
- Strauss, J. S., Bartko, J. J., and Carpenter, W. T. (1973), 'The Use of Clustering Techniques for the Classification of Psychiatric Patients,' British Journal of Psychiatry , 122 , 531-540.
- Thomas, D. R. (1983), 'Univariate Repeated Measures Techniques Applied to Multivariate Data,' Psychometrika , 48 , 451-464.
- Thomson, G. H. (1951), The Factorial Analysis of Human Ability , London: London University Press.
- Timm, N. H. (1975), Multivariate Analysis: With Applications in Education and Psychology , Monterey, Calif.: Brooks/Cole.
- Timm, N. H. (1980), 'Multivariate Analysis of Variance of Repeated Measures,' in Handbook of Statistics , Vol. 1, P. R. Krishnaiah (ed.), Amsterdam: North-Holland, pp. 41-87.
- Tinter, G. (1946), 'Some Applications of Multivariate Analysis to Economic Data,' Journal of the American Statistical Association , 41 , 472-500.
- Titterington, D. M., Murray, G. D., Murray, L. S., Speigelhalter, D. J., Skene, A. M., Habbema, J. D. F., and Gelpke, D. J. (1981), 'Comparison of Discrimination Techniques Applied to a Complex Data Set of Head Injured Patients,' Journal of the Royal Statistical Society, Series A , 144 , 145-175.
- Travers, R. M. W. (1939), 'The Use of a Discriminant Function in the Treatment of Psychological Group Differences,' Psychometrika , 4 (1), 25-32.
- Tu, C. T., and Han, C. P. (1982), 'Discriminant Analysis Based on Binary and Continuous Variables,' Journal of the American Statistical Association , 77 , 447-454.
- Venables, W. N. (1976), 'Some Implications of the Union Intersection Principle for Test of Sphericity,' Journal of Multivariate Analysis , 6 , 185-190.
- Vonesh, E. F. (1986), 'Sample Sizes in the Multivariate Analysis of Repeated Measurements,' Biometrics , 42 , 601-610.
- Wainer, H. (1981), 'Comments on a Paper by Kleiner and Hartigan,' Journal of the American Statistical Association , 76 , 270-272.
- Wall, F. J. (1967), The Generalized Variance Ratio or U-Statistic , Albuquerque, N.M.
- Wang, C. M. (1983), 'On the Analysis of Multivariate Repeated Measures Designs,' Communications in Statistics-Part A Theory and Methods , 12 , 1647-1659.

- Ward, J. H. (1963), 'Hierarchical Grouping to Optimize an Objective Function,' Journal of the American Statistical Association , 58 , 236-244.
- Wegman, E. J. (1972), 'Nonparametric Probability Density Estimation: I. A Summary of Available Methods,' Technometrics , 14 , 533-546.
- Welch, B. L. (1939), 'Note on Discriminant Functions,' Biometrika , 31 , 218-220.
- Wilks, S. S. (1932), 'Certain Generalizations in the Analysis of Variance,' Biometrika , 24 , 471-494.
- Wilks, S. S. (1946), 'Sample Criteria for Testing Equality of Means, Equality of Variances and Equality of Covariances in a Normal Multivariate Distribution,' Annals of Mathematical Statistics , 17 , 257-281.
- Wilks, S. S. (1963), 'Multivariate Statistical Outliers,' Sankhya, Series A , 25 , 407-426.
- Williams, E. J. (1970), 'Comparing Means of Correlated Variates,' Biometrika , 57 , 459-461.
- Williams, J. S., and Izenman, A. J. (1981), 'A Class of Linear Spectral Models and Analysis for the Study of Longitudinal Data,' Technical Report, Colorado State University, Department of Statistics.
- Wishart, D. (1969a), 'An Algorithm for Hierarchical Classifications,' Biometrics , 25 , 165170.
- Wishart, D. (1969b), 'Numerical Classification Method for Deriving Natural Classes,' Nature , 221 , 97-98.
- Wishart, D. (1971), A Generalised Approach to Cluster Analysis . Part of Ph.D. Thesis, University of St. Andrews.
- Yang, S. S., and Lee, Y. (1987), 'Identification of a Multivariate Outlier,' Presented at the Annual Meeting of the American Statistical Association, San Francisco, August 1987.
- Yenyukov, I. S. (1988), 'Detecting Structures by Means of Projection Pursuit,' Compstat , 88 , 47-58.
- Young, F. W. (1987), Multidimensional Scaling: History, Theory and Applications , R. M. Hamer (ed.), Hillsdale, NJ: Lawrence Erlbaum.
- Young, G. and Householder, A. S. (1938), 'Discussion of a Set of Points in Terms of Their Mutual Distances,' Psychometrika , 3 , 19-22.
- Zerbe, G. O. (1979a), 'Randomization Analysis of the Completely Randomized Design Extended to Growth Curves,' Journal of the American Statistical Association , 74 , 215221.
- Zerbe, G. O. (1979b), 'Randomization Analysis of Randomized Block Design Extended to Growth and Response Curves,' Communications in Statistics-Part A Theory and Methods , 8 , 191-205.
- Zhang, L., Helander, M. G., and Drury, C. G. (1996), 'Identifying Factors of Comfort and Discomfort in Sitting,' Human Factors , 38 (3), 377-380.

## Index

| Additional information, test for, 136-139, 231-233 Air pollution data, 502 Airline distance data, 508 Algebra, matrix, see Matrix, algebra Analysis of variance: multivariate (MANOVA): additional information, test for, association, measures of, 173-176 assumptions, checking on, 198-199 and canonical correlation, 376-377 contrasts, 180-183 discriminant function, 165, 184-185, 191 growth curves, 221-230. See also curves; Repeated measures designs H and E matrices, 160-161 higher order models, 195-196 individual variables, tests on, 183-186 discriminant function, 184-185, experimentwise error rate, 183-185 protected tests, 184 Lawley-Hotelling test, 167 table of critical values, 524-528 likelihood ratio test, 164 mixed models, 196-198 expected mean squares, 196-197 multivariate association, measures of, 173-176 one-way, 158-161 contrasts, 180-183 orthogonal, 181 model, 159 unbalanced, 168 Pillai's test, 166   | profile analysis, 199-201 repeated measures, 204-221. See also Repeated measures designs; Growth curves Roy's test (union-intersection), 164-166 table of critical values, 517-520 stepwise discriminant analysis, 233 stepwise selection of variables, 233 test for additional information, 231-233 test statistics, 161-173 comparison of, 169-170, 176-178 and eigenvalues, 168 power of, 176-178 and T 2 , 169 tests on individual variables, 163-174, 183-186, 191 discriminant function, 165, 184-185, 191 experimentwise error rate, 183-185 protected tests, 184 test on a subvector, 231-233 two-way, 188-195 contrasts, 190-191 discriminant function, 191 interactions, 189-190 main effects, 189-190 model, 189 tests on individual variables, 191 test statistics, 190 unbalanced one-way, 168 union-intersection test, 164 Wilks' /Lambda1 (likelihood ratio) test, 161-164 chi-square approximation, 162 F approximation, 162 partial /Lambda1 -statistic, 232 properties of, 162-164 table of critical values, 501-516 transformations to exact F , 162-163   |
|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 231-233 Growth 163-164, 191                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |

ISBN: 0-471-41889-7

| Analysis of variance ( cont. )                                                                                                                   | subset selection, 376                                                                                                                |
|--------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------|
| univariate (ANOVA):                                                                                                                              | with test for independence of two subvectors,                                                                                        |
| one-way, 156-158                                                                                                                                 | 260, 367-368                                                                                                                         |
| contrasts, 178-180                                                                                                                               | tests of significance, 367-371                                                                                                       |
| orthogonal, 179-180                                                                                                                              | all canonical correlations, 367-369                                                                                                  |
| SSH, SSE, F -statistic, 158                                                                                                                      | comparison of tests, 368-369                                                                                                         |
| two-way, 186-188                                                                                                                                 | and test of independence, 367-368                                                                                                    |
| contrasts, 188                                                                                                                                   | and test of overall regression, 367-368,                                                                                             |
| F -test, 188                                                                                                                                     | 375                                                                                                                                  |
| interaction, 187                                                                                                                                 | subset of canonical correlations, 369-371                                                                                            |
| main effects, 187-188                                                                                                                            | subset selection, 376                                                                                                                |
| model, 186                                                                                                                                       | test of a subset in regression, 375-376                                                                                              |
| ANOVA, see Analysis of variance, univariate                                                                                                      | Canonical variates:                                                                                                                  |
| Association, measures of, 173-176, 349-351                                                                                                       | correlations among, 364                                                                                                              |
| Athletic record data, 480                                                                                                                        | definition of, 363                                                                                                                   |
|                                                                                                                                                  | interpretation, 371-374                                                                                                              |
| Bar steel data, 192                                                                                                                              | by correlations (structure coefficients),                                                                                            |
| Beetles data, 150                                                                                                                                | 373                                                                                                                                  |
| Bilinear form, 19-20 Biplots, 531-539                                                                                                            | by rotation, 373 by standardized coefficients, 371-373                                                                               |
| coordinates of points, 533-534                                                                                                                   | redundancy analysis, 373-374                                                                                                         |
| correlation, 534                                                                                                                                 | and regression, 374-376                                                                                                              |
| cosine, 534, 537                                                                                                                                 | standardized coefficients, 365, 371-373                                                                                              |
| points for observations, 531-534                                                                                                                 | Categorical variables, see Dummy variables                                                                                           |
| points for variables, 531-534                                                                                                                    | Central Limit Theorem (Multivariate), 91                                                                                             |
| principal component approach, 531-532, 535 singular value decomposition, 532-533, 535                                                            | Characteristic form:                                                                                                                 |
| 543                                                                                                                                              | of t -statistic, 117, 122 2                                                                                                          |
| Birth and death data,                                                                                                                            | of T -statistic, 118, 123                                                                                                            |
| Bivariate normal distribution, 46, 84, 88-89, 133                                                                                                | Characteristic roots, see Eigenvalues                                                                                                |
| Blood data, 237                                                                                                                                  | Chemical data, 340                                                                                                                   |
| Blood pressure data, 245                                                                                                                         | Chi-square distribution, 86, 91-92, 114 Cholesky decomposition, 25-26                                                                |
| Bonferroni critical values, 127 table, 562-565                                                                                                   | City crime data, 456                                                                                                                 |
| Box's M -test, 257-259 table of exact critical values, 588-589                                                                                   | Classification analysis (allocation), 299-321 299                                                                                    |
| Bronchus data, 154                                                                                                                               | assigning a sampling unit to a group, asymptotic optimality, 302                                                                     |
| Burt matrix, 526-529                                                                                                                             | correct classification rates, 307-309                                                                                                |
| Byssinosis data, 545-546                                                                                                                         | error rates, 307-313 See also Error rates                                                                                            |
|                                                                                                                                                  | estimates of, 307-313                                                                                                                |
| Calcium data, 56                                                                                                                                 | as a stopping rule, 311-313                                                                                                          |
| Calculator speed data, 210                                                                                                                       | k -nearest neighbor rule, 318-319                                                                                                    |
| Canonical correlation(s), 174, 260, 361-378 canonical variates, see Canonical variates definition of, 362-364 and discriminant analysis, 376-378 | nonparametric classification procedures, 302, 314-320 density estimators (kernel), 315-317 multinomial data (categorical variables), |
| and eigenvalues, 362-363, 377-378                                                                                                                | 314-315                                                                                                                              |
| with grouping variables, 174                                                                                                                     | dummy variables, 315                                                                                                                 |
| and MANOVA, 376-378                                                                                                                              | neighbor rule,                                                                                                                       |
| dummy variables, 376-377                                                                                                                         | nearest 318-320 k -nearest neighbor rule, 318-319                                                                                    |
| and measures of association, 362,                                                                                                                | groups, 304-307                                                                                                                      |
| 373-374 and multiple correlation, 361-362, 366, 376                                                                                              | several linear classification functions,                                                                                             |
| properties of, 366-367                                                                                                                           | 304-306 equal covariance matrices, 304-305                                                                                           |
| redundancy analysis, 373-374                                                                                                                     | optimal classification rule (Welch), 305                                                                                             |
| and regression, 368-369, 374-376                                                                                                                 | prior probabilities, 305-307                                                                                                         |

| quadratic classification functions, 306-307                         |
|---------------------------------------------------------------------|
| unequal covariance matrices, 306                                    |
| subset selection, 311-313                                           |
| stepwise discriminant analysis, 311-313 311-313                     |
| error rate as a stopping rule,                                      |
| two groups, 300-303                                                 |
| Fisher's classification function, 300-302                           |
| linear classification function, 301-302 (Welch), 302                |
| optimal classification rule prior probabilities, 302                |
| Cluster analysis, 451-503                                           |
| average linkage method, 463                                         |
| centroid method, 463-465                                            |
| choosing the number of clusters, 494-496                            |
| and classification, 451                                             |
| clustering observations, 451-496                                    |
| clustering variables, 451, 497-499                                  |
| comparison of methods, 478-479                                      |
| complete linkage method, 459-462                                    |
| definition, 451                                                     |
| dendrogram, 456                                                     |
| examples of, 458-459, 461-462, 464-465, 467, 469, 472-473, 476-477  |
| inversion, 471 reversal, 471                                        |
| 452                                                                 |
| dissimilarity,                                                      |
| distance, 451-454 distance matrix, 453                              |
| Euclidean distance,                                                 |
| 452                                                                 |
| Minkowski metric, 453                                               |
| profile of observation vector: level, 454                           |
| shape, 454                                                          |
| variation, 454                                                      |
| scale of measurement,                                               |
| 453-454 statistical distance, 452-453                               |
| farthest neighbor method, see                                       |
| complete linkage method                                             |
| flexible 468-471                                                    |
| hierarchical clustering, 452, 455-481 agglomerative method, 455-479 |
| average linkage, 463                                                |
| centroid, 463-465                                                   |
| mean vectors, 463                                                   |
| complete linkage, flexible beta, 468-471                            |
| median, 466 single linkage,                                         |
| Ward's method, 466-468                                              |
| of methods, 478-479 456                                             |
| method, 455,                                                        |
| dendrogram,                                                         |
| 479-481                                                             |
| monothetic, 479                                                     |
| divisive                                                            |
| comparison                                                          |
| beta method,                                                        |
| 459-462                                                             |
| 456-459                                                             |

| polythetic, 479-480                                                                         |
|---------------------------------------------------------------------------------------------|
| properties, 471-479 chaining, 474 contraction, 474 dilation, 474 471                        |
| monotonicity,                                                                               |
| outliers, 478-479                                                                           |
| space contracting, 474                                                                      |
| space dilating, 474 ultrametric, 471                                                        |
| incremental sum of squares method, see                                                      |
| Ward's method                                                                               |
| median method, 466                                                                          |
| nearest neighbor method, see single linkage                                                 |
| method                                                                                      |
| nonhierarchical methods, 481-494 density estimation, 493                                    |
| dense point, 493 modes, 493                                                                 |
| mixtures of distributions, 490-492 partitioning, 481-490                                    |
| k -means, 482-488                                                                           |
| seeds, 482-487                                                                              |
| methods based on E and H , 488-490 number of clusters:                                      |
| choosing the number of clusters, 494-496 cutting the dendrogram, 494-495                    |
| methods based on E and H , 495-496                                                          |
| total possible number, 455                                                                  |
| methods, see methods, partitioning                                                          |
| optimization nonhierarchical partitioning, 452, 481-490                                     |
| plotting of clusters:                                                                       |
| discriminant functions, 486-488, 494 principal components, 451, 484 projection pursuit, 451 |
| of observation level, 454                                                                   |
| shape, 454                                                                                  |
| variation, 454                                                                              |
| 451-455                                                                                     |
| single linkage method, 456-459 tree diagram, see dendrogram                                 |
| validity of a cluster solution, 496                                                         |
| cross validation, 496 hypothesis test, 496                                                  |
| clustering of, 497-499                                                                      |
| 451, correlations, 497 and factor analysis, 498                                             |
| Ward's method, 466-468                                                                      |
| pipe data, 135                                                                              |
| of determination,                                                                           |
| see                                                                                         |
| R 2                                                                                         |
| variables, see                                                                              |
| Variables,                                                                                  |
| commensurate                                                                                |
| Commensurate                                                                                |
| Coefficient                                                                                 |
| profile                                                                                     |
| vector:                                                                                     |
| variables,                                                                                  |
| similarity,                                                                                 |

| Communality, see Factor analysis Confidence interval (reference), 119, 127 Contingency table: graphical analysis of, see Correspondence analysis higher-way table, 526, 528-529 two-way table, 514-516, 519, 521 Contour plots, 84-85 Contrast(s): contrast matrices in growth curves, 227-230 contrast matrices in repeated measures, 208-221 one-sample profile analysis, 141-142 one-way ANOVA, 178-180 one-way MANOVA, 180-183 orthonormal, 206 two-way ANOVA, 188 two-way MANOVA, 190-191 Cork data, 239 Correct classification rate, 307-309 Correlation: canonical, see Canonical correlation(s) and cosine of angle between two vectors, 49-50 intra-class correlation, 198-199 and law of cosines, 49-50 multiple, see Multiple correlation and orthogonality of two vectors, 50 population correlation ( ρ ), 49 sample correlation ( r ), 49 of two linear combinations, 67, 72-73   | definition (graph of contingency table), 514-515 independence of rows and columns, testing, 519-521 chi-square, 515, 520-521 in terms of frequencies, 520 in terms of inertia, 521 in terms of relative frequencies, 520 in terms of row and column profiles, 520-521 inertia, 515, 524 multiple correspondence analysis, 526-530 Burt matrix, 526-529 indicator matrix, 526-527 profiles of rows and columns, 515-519 rows and columns, 514-525 association, 514 inertia, 515, 524 interaction, 514-515   |         |
|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------|
| 222-225,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |         |
| 206,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |         |
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | points for plotting, 521-525                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |         |
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | profiles, 515-519                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |         |
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | singular value decomposition, 522, 524 generalized singular value decomposition,                                                                                                                                                                                                                                                                                                                                                                                                                           |         |
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 522 Covariance:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |         |
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | and independence, 46-47 and orthogonality, 47-48 population covariance ( σ xy ), sample covariance ( s xy ), 46-48                                                                                                                                                                                                                                                                                                                                                                                         |         |
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 46-47                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |         |
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | expected value of, 47                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |         |
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | and linear relationships, 47                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |         |
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | of two linear combinations,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |         |
| matrix:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |         |
| Correlation and covariance matrix, 61                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | 67-68, 72 Covariance matrix: compound symmetry, 206                                                                                                                                                                                                                                                                                                                                                                                                                                                        |         |
| factor analysis on, 418-419 partitioned, 365 population correlation matrix,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | and correlation matrix, 61 of linear combinations of variables, partitioned, 62-66, 362                                                                                                                                                                                                                                                                                                                                                                                                                    |         |
| 61 principal components from,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | 69-73 dependence of y and x and cov ( y , x )                                                                                                                                                                                                                                                                                                                                                                                                                                                              |         |
| 383-384, 393-397                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | , 63                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |         |
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | y                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |         |
| sample correlation matrix, 60-61 from covariance matrix, 61                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | difference between cov ( x ) and                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |         |
| from data, 60                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | cov ( y , x ) , 63                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |         |
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | or more subsets,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |         |
| analysis, 514-530                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | three 64-66 covariance matrix,                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |         |
| Correspondence contingency table:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | pooled 122-123                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |         |
| higher-way table, 526, 528-529 two-way table, 514-516, 519,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | population covariance matrix ( 𝚺 ), 58-59 sample covariance matrix ( S ), 57-60                                                                                                                                                                                                                                                                                                                                                                                                                            |         |
| 521 coordinates for row and column                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | from data matrix, 58                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |         |
| points, 521-525                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | distribution of, 91-92 Wishart distribution,                                                                                                                                                                                                                                                                                                                                                                                                                                                               |         |
| distances between column points,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |         |
| 523-524 distances between row points,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | 91-92 from observations, 57-58                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |         |
| singular value decomposition, 522                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | positive definiteness of, 67                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |         |
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | and sample mean vector,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |         |
| generalized singular value                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | 92                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |         |
| decomposition, 522                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |         |
| matrix,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |         |
| correspondence                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |         |
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | sphericity, 206, 250-252                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |         |
| 515-516                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |         |
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | independence                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |         |
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | of,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |         |
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | 523-524 |

tests on, 248-268. See also Tests of hypotheses, covariance matrices

unbiasedness of, 59

uniformity, 206, 252-254

Cross validation, 310-311

Cyclical data, 153

Data matrix ( Y ), 55

Data sets:

air pollution data, 502

airline distance data, 508

athletic record data, 480

bar steel data, 192

beetles data, 150

birth and death data, 543

blood data, 237

blood pressure data, 245

bronchus data, 154

byssinosis data, 545-546

calcium data, 56

calculator speed data, 210

chemical data, 340

city crime data, 456

coated pipe data, 135

cork data, 239

cyclical data, 153

dental data, 227

diabetes data, 65

dogs data, 243-244

do-it-yourself data, 529

dystrophy data, 152

engineer data, 151

fabric wear data, 238

fish data, 235

football data, 280-281

glucose data, 80-81

guinea pig data, 201

height-weight data, 45

hematology data, 109-110

mandible data, 247

mice data, 241

Norway crime data, 544

people data, 526

perception data, 419

plasma data, 246

piston ring data, 518

politics data, 542

probe word data, 70

protein data, 483

psychological data, 125

ramus bone data, 78

repeated data, 218

Republican vote data, 53

road distance data, 541

rootstock data, 171

Seishu data, 263

snapbean data, 236

sons data, 79

steel data, 273

survival data, 239-241

temperature data, 269

trout data, 242

voting data, 512

weight gain data, 243

wheat data, 503

words data, 154

Data, types of, 3-4. See also Multivariate data

Density function, 43

Dental data, 227

Descriptive statistics, 2

Determinant, 26-29

definition of, 26-27

of diagonal matrix, 27

of inverse, 29

of nonsingular matrix, 28

of partitioned matrix, 29

of positive definite matrix, 28

of product, 28

as product of eigenvalues, 34

of scalar multiple of a matrix, 28

of singular matrix, 28

Diabetes data, 65

Diagonal matrix, 8

Discriminant analysis (descriptive), 270-296

and canonical correlation, 282, 376-378

and classification analysis, 270

discriminant functions:

for several groups, 165, 184-185, 191, 277-279

measures of association for, 282

for two groups, 126-132, 271-275

and distance, 272

and eigenvalues, 278-279

interpretation of discriminant functions, 288-291

correlations (structure coefficients), 291

partial F -values, 290

rotation, 291

standardized coefficients, 289

purposes of, 277

scatter plots, 291-293

selection of variables, 233, 293-296

several groups, 277-279

standardized discriminant functions, 282-284

stepwise discriminant analysis, 233, 293-296

tests of significance, 284-288

two groups, 271-275

and multiple regression, 130-132, 275-276

| Discriminant analysis (predictive), see                                                                        | F -test(s):                                                                 |
|----------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------|
| Classification analysis                                                                                        | ANOVA, 158, 188                                                             |
| Dispersion matrix, see Covariance matrix                                                                       | between-subjects tests in repeated measures,                                |
| Distance between vectors, 76-77, 83, 115, 118,                                                                 | 212, 216, 221 two variances, 254-255                                        |
| 123, 271-272                                                                                                   | comparing contrasts, 179                                                    |
| Distribution: beta, 97                                                                                         | equivalent to T 2 , 119, 124, 137-138                                       |
| bivariate normal, 46, 84, 88-89                                                                                | in multiple regression, 138, 330-332                                        |
| chi-square, 86                                                                                                 | partial F -test, 127, 138, 232, 293-296                                     |
| elliptically symmetric, 103                                                                                    | stepwise selection, 233, 293-296, 336                                       |
| F , 119, 138, 158, 162-163, 179, 254-255                                                                       | test for additional information, 137                                        |
| multivariate normal, see Multivariate normal distribution; Multivariate                                        | test for individual variables in MANOVA, 183-186                            |
| normality, tests for                                                                                           | /Lambda1 :                                                                  |
|                                                                                                                | Wilks'                                                                      |
| univariate normal, 82-83, 86 tests for, see Univariate normality, tests for                                    | exact F transformation for, 162-163 F approximation for, 162-163            |
| Wishart, 91-92                                                                                                 | Fabric wear data, 238                                                       |
| Dogs data,                                                                                                     | Factor                                                                      |
| 243-244                                                                                                        | analysis, 408-450                                                           |
| Do-it-yourself data, 529 Dummy variables, 173-174, 282, 315,                                                   | assumptions, 410-412 failure of assumptions,                                |
| 376-377                                                                                                        | consequences of, 414, 444-445                                               |
| Dystrophy data, 152                                                                                            | common factors, 409                                                         |
| E matrix, 160-161, 339, 342-344                                                                                | communalities, 413, 418, 422-423, 427-428 estimation of, 418, 422, 424, 428 |
| 397-398, 416-419, 422-423 32-35, 363-365, 382-384,                                                             | 446 416-418,                                                                |
| Eigenvectors, 397-398, 416-418, 420-422                                                                        | eigenvectors, 420, 422 factor scores, 438-443                               |
| Elliptically symmetric distribution, 103                                                                       | averaging method, 440                                                       |
| EMalgorithm, 75, 491                                                                                           | regression method, 439-440                                                  |
|                                                                                                                | factors, 408-414                                                            |
| Engineer data, 151 Error rate(s), 307-313                                                                      | common, 409                                                                 |
| actual error rate, 308                                                                                         | definition of, 408-409                                                      |
| apparent error rate, 307                                                                                       | interpretation of, 409, 438                                                 |
| bias in, 308, 309-311                                                                                          | number of, 426-430                                                          |
|                                                                                                                | Heywood case, 424-425                                                       |
| classification table, 307-308 cross validation, 310-311                                                        | loadings:                                                                   |
| experimentwise error rate, 1-2, 128-129, 183-185                                                               | definition of, 409 estimation of,                                           |
|                                                                                                                | 415-426                                                                     |
| holdout method, 310-311, 318 leaving-one-out method, 310-311, 318                                              | comparison of methods, 424 fit of the model, 419                            |
| partitioning the sample, 310 resubstitution, 307-308                                                           | iterated principal factor method, 424-425                                   |
|                                                                                                                | Heywood case, 424-425                                                       |
| Expected value: of random matrix, 59                                                                           | maximum likelihood method, 425-426 principal component method, 415-421      |
| of random vector [ E ( y ) ], 55-56 of sample covariance matrix [ E ( S ) ], 59 of sample mean [ E ( y ) ], 44 | principal factor method, 421-424 from S or R , 418-419, 421-422             |
| of sample mean vector [ E ( y ) ], 56                                                                          | 409-414                                                                     |
|                                                                                                                | model,                                                                      |
| of sample variance [ E ( s 2 ) ], 44                                                                           | modeling covariances or correlations, 408, 410, 412, 414, 417               |
| of sum or product of random                                                                                    | 426-430                                                                     |
| variables, 46                                                                                                  | of factors to retain,                                                       |
| of univariate random variable [ E ( y ) ],                                                                     | number average eigenvalue, 427-428                                          |
| 43                                                                                                             |                                                                             |
| Experimental units, 1                                                                                          | comparison of methods, 428-430                                              |

| hypothesis test, 427-428                                                         | several samples, 229-230                                         |
|----------------------------------------------------------------------------------|------------------------------------------------------------------|
| indeterminacy of for certain data sets,                                          | unequally spaced time points, 225-227                            |
| 428-429                                                                          | Guinea pig data, 201                                             |
| scree plot, 427-428 variance accounted for, 427-428                              | H matrix, 160-161, 343-344                                       |
| orthogonal factors, 409-415, 431-435                                             | Height-weight data, 45                                           |
| and principal components, 408-409, 447-448                                       | Hematology data, 109-110                                         |
| and regression, 410, 439-440                                                     | Hierarchical clustering, see Cluster analysis,                   |
|                                                                                  | hierarchical clustering                                          |
| rotation, 414-415, 417, 430-437 complexity of the variables, 431                 | Hotelling-Lawley test statistic, see                             |
| interpretation of factors, 409, 438                                              | Lawley-Hotelling test statistic 2 -statistic                     |
| oblique rotation, 431, 435-437                                                   | Hotelling's T 2 -statistic, see T                                |
| and orthogonality, 437                                                           | Hyperellipsoid, 73                                               |
| pattern matrix, 436                                                              | Hypothesis tests, see Tests of                                   |
|                                                                                  | hypotheses                                                       |
| orthogonal rotation, 431-435 analytical, 434 communalities, 415, 431             | Identity matrix, 8 Imputation, 74                                |
| graphical, 431-433                                                               | Independence of variables, test for,                             |
| varimax, 434-435                                                                 | 265-266                                                          |
| simple structure, 431                                                            | table of exact critical values, 590                              |
| scree plot, 427-428                                                              | Indicator variables, see Dummy variables                         |
| simple structure, 431                                                            | Inferential statistics, 2                                        |
| singular matrix and, 422                                                         | Intra-class correlation, 198-199                                 |
| specific variance, 410, 417 specificity, see specific variance                   | Kernel density estimators, 315-317                               |
| total variance, 418-419, 427 validity of factor analysis model,                  | Kurtosis, 94-95, 98-99, 103-104                                  |
| 443-447 how well model fits the data, 419, 444 measure of sampling adequacy, 445 | Largest root test, see Roy's test statistic                      |
| variance due to a factor, 418-419                                                | Latent roots, see Eigenvalues                                    |
| Fish data, 235                                                                   | Lawley-Hotelling test statistic: definition of, 167              |
| Fisher's classification function,                                                | table of critical values, 582-586                                |
| 300-302 Football data, 280-281                                                   | Length of vector, 14                                             |
|                                                                                  | Likelihood function, 90                                          |
| Gauss-Markov theorem, 341                                                        | ratio                                                            |
| Generalized population variance, 83-85, 105                                      | Likelihood test(s): for covariance matrices, 248-250, 253, 256,  |
| Generalized sample variance, 73 total sample variance, 73, 383, 409, 418,        | 260, 262, 265 in factor analysis, 428 for mean vectors, 126, 164 |
| 427                                                                              |                                                                  |
| Generalized singular value decomposition, 522                                    | Linear classification functions, 301-306                         |
| Geometric mean, 174                                                              | Linear combination of matrices, 19                               |
| Glucose data, 80                                                                 | combination(s) of                                                |
|                                                                                  | Linear variables, 2, 67-73, 113                                  |
| Graphical display of multivariate data, 52-53 Graphical procedures, 504-547      | correlation matrix for several linear                            |
| biplots, see Biplots correspondence analysis, see Correspondence                 | correlation of two linear combinations, 67, 71-73                |
|                                                                                  | combinations, 72                                                 |
| analysis                                                                         |                                                                  |
| multidimensional scaling, see Multidimensional                                   | covariance matrix for several linear combinations, 69-70,        |
| scaling                                                                          | 72-73 covariance of two linear combinations, 67-68,              |
| Growth curves, 221-230 contrast matrices, 222-225, 227-230                       | 71-72                                                            |
|                                                                                  | of, 86                                                           |
| one sample, 221-229                                                              | distribution mean of a single linear                             |
| orthogonal polynomials, 222-225                                                  | 67,                                                              |
| polynomial function of t , 225-227                                               | combination,                                                     |
|                                                                                  | 71-72                                                            |

Linear combination(s) of variables ( cont. )

mean vector for several linear combinations, 69

variance of a single linear combination, 67,

71-72

Linear combination of vectors, 19

Linear hypotheses, 141-142, 199-201,

208-225

Mahalanobis distance, 76-77, 83

Mandible data, 247

MANOVA, 130, 158. See also Analysis of variance, multivariate

Matrix (matrices):

algebra of, 5-37

bilinear form, 19-20

Burt matrix, 526-529

Cholesky decomposition, 25-26

conformable, 11

covariance matrix, 57-59

definition, 5-6

determinant, 26-29, 34. See also Determinant of diagonal matrix, 27

of inverse matrix, 29

of partitioned matrix, 29

of positive definite matrix, 28

of product, 28

of scalar multiple of a matrix, 28

of singular matrix, 28

of transpose, 29

diagonal, 8

eigenvalues, 32-37. See also Eigenvalues characteristic equation, 32

and determinant, 34

of I + A , 33

of inverse matrix, 36

of positive definite matrix, 34

Perron-Frobenius theorem, 34

square root matrix, 36

of product, 35

singular value decomposition, 36

of square matrix, 36

of symmetric matrix, 35

spectral decomposition, 35

and trace, 34

eigenvectors, 32-37. See also Eigenvectors equality of, 7

identity, 8

indicator matrix, 526-527

inverse, 23-25

of partitioned matrix, 25

of product, 24

of transpose, 24

j vector, 9

J matrix, 9

linear combination of, 19

nonsingular matrix, 23

notation for matrix and vector, 5-6

O (zero matrix), 9

operations with, 9-20

distributive law, 12

factoring, 12-13, 15

product, 11-20, 23-25

conformable, 11

with diagonal matrix, 18

distributive over addition, 12

and eigenvalues, 34-35

of matrix and scalar, 19

of matrix and transpose, 16-18

of matrix and vector, 12-13, 16, 21

as linear combination, 21

noncommutativity of, 11

product equal zero, 23

transpose of, 12

triple product, 13

of vectors, 14

sum, 10

commutativity of, 10

orthogonal, 31

rotation of axes, 31-32

partitioned matrices, 20-22

determinant of, 29

inverse of, 25

product of, 20-21

transpose of, 22

Perron-Frobenius theorem, 34, 402

positive definite, 25, 34

positive semidefinite, 25, 34

quadratic form, 19

rank, 22-23

full rank, 22

scalar, 6

product of scalar and matrix, 19

singular matrix, 24

singular value decomposition, 36

size of a matrix, 6

spectral decomposition, 35

square root matrix, 36

sum of products in vector notation, 14

sum of squares in vector notation, 14

symmetric, 7, 35

trace, 30, 34, 69

and eigenvalues, 34

of product, 30

of sum, 30

transpose, 6-7

of product, 12

of sum, 10

| triangular, 8                                          | column coordinates, 527, 5290530                                 |
|--------------------------------------------------------|------------------------------------------------------------------|
| vectors, see Vector(s)                                 | indicator matrix, 526-527                                        |
| zero matrix ( O ) and zero vector ( 0 ), 9             | Multiple regression, see Regression, multiple                    |
| Maximum likelihood estimation, 90-91                   | Multivariate analysis, 1                                         |
| of correlation matrix, 91                              | descriptive statistics, 1-2                                      |
| of covariance matrix, 90                               | inferential statistics, 2                                        |
| likelihood function, 90                                | Multivariate analysis of variance (MANOVA),                      |
| of mean vector, 90-91                                  | see Analysis of variance,                                        |
| multivariate normal, 90                                | multivariate                                                     |
| Mean:                                                  | Multivariate data:                                               |
| geometric, 174                                         | basic types of, 4                                                |
| of linear function, 67, 72                             | plotting of, 52-53                                               |
| population mean ( µ ), 43                              | sparceness of, 97                                                |
| of product, 46                                         | Multivariate inference, 2                                        |
| sample mean ( y ), 43-44                               | Multivariate normal distribution, 82-105                         |
| of sum, 46                                             | applicability of, 85                                             |
| Mean vector, 54-56, 83, 90-92                          | conditional distribution, 88                                     |
| notation, 54                                           | contour plots, 84-85                                             |
| population mean vector ( 𝛍 ), 55-56                    | density function, 83                                             |
| sample mean vector ( y ), 54-56                        | distribution of y and S , 91-92                                  |
| from data matrix, 55                                   | features of, 82                                                  |
| distribution of, 91                                    | independence of y and S , 92                                     |
| and sample covariance matrix,                          | linear combinations of, 86                                       |
| independence of, 92                                    | marginal distribution, 87                                        |
| Measurement scale, 2                                   | maximum likelihood estimates, 90-91. See                         |
| interval scale, 2 ordinal scale, 2                     | also Maximum likelihood estimation                               |
| ratio scale, 2                                         | properties of, 85-90                                             |
| Mice data, 241                                         | quadratic form and chi-square distribution,                      |
| Misclassification rates, see Error rate(s)             | standardized variables, 86                                       |
| Missing values, 74-76                                  | zero covariance matrix implies independence                      |
| Multicollinearity, 74, 84 Multidimensional             | of subvectors, 87 Multivariate normality, tests for, 92, 96-99 2 |
| scaling, 504-514 classical solution, see metric        | D i , 97-98, 102-103                                             |
| multidimensional scaling                               | and chi-square, 98                                               |
| definition, 504-505                                    | table of critical values, 557                                    |
| distances, 504-505                                     | dynamic plot, 98                                                 |
| seriation (ranking), 504                               | scatter plots, 98, 105                                           |
| metric multidimensional scaling, 504-508               | skewness and kurtosis, multivariate, 98-99,                      |
| algorithm for finding the points,                      | 103-104, 106                                                     |
| 505-508 and principal component                        | table of critical values, 553-556                                |
| analysis, 506 nonmetric multidimensional scaling, 505, | Multivariate regression, see Regression, multivariate            |
| 508-514                                                |                                                                  |
| monotonic regression, 509-510                          | Nonsingular matrix, 23                                           |
| ranked dissimilarities, 508-509                        | Normal distribution:                                             |
| STRESS, 510-512 metric                                 | bivariate normal, 46,                                            |
| principal coordinate analysis, see                     | 84, 88-89, 133 multivariate normal, see Multivariate normal      |
| multidimensional scaling decomposition, 505-506        | distribution normal,                                             |
| spectral                                               | univariate 82-83, 86 Normality, tests for, see Multivariate      |
| Multiple correlation, 332, 361-362, 423. See also R 2  | normality; Univariate normality                                  |
| Multiple correspondence analysis, 526-530              | Norway crime data, 544                                           |
| Burt matrix, 526-529                                   | Numerical taxonomy, see Cluster                                  |
|                                                        | analysis                                                         |

| Objectives of this book, 3                                                        | major axis, 384, 388                                                    |
|-----------------------------------------------------------------------------------|-------------------------------------------------------------------------|
| Observations, 1                                                                   | and factor analysis, 403, 408-409, 447-448                              |
| One-sample test for a mean vector, 117-121                                        | geometry of, 381-385                                                    |
| Orthogonal matrix, 31                                                             | interpretation, 401-404                                                 |
| Orthogonal polynomials, 222-225                                                   | correlations, 403-404                                                   |
| table of, 587                                                                     | rotation, 403                                                           |
| Orthogonal vectors, 50                                                            | special patterns in S or R , 401-403                                    |
| Outliers:                                                                         | size and shape, 402-403                                                 |
| multivariate:                                                                     | large variance of a variable, effect of,                                |
| kurtosis, 103-104                                                                 | 383-384, 402 last few principal components, 382, 389, 401               |
| elliptically symmetric distributions, 103                                         |                                                                         |
| principal components, 389-392                                                     | maximum variance, 380, 385                                              |
| slippage in mean, variance, and correlation,                                      | minimum perpendicular distances to line,                                |
| 101 Wilks' statistic, 102-103                                                     | 387-388 number of components to retain, 397-401                         |
| univariate:                                                                       | orthogonality of, 380, 383-384                                          |
| accommodation, 100                                                                | percent of variance, 383, 397                                           |
| block test, 101                                                                   | and perpendicular regression, 385, 387-389                              |
| identification, 100                                                               | plotting of, 389-393                                                    |
| masking, 101                                                                      | assessing normality, 389-390                                            |
| maximum studentized residual, 100-101                                             | detection of outliers, 389-391                                          |
| skewness and kurtosis, 101                                                        | properties of, 381-386                                                  |
| slippage in mean and variance, 100                                                | proportion of variance, 383                                             |
| swamping, 101                                                                     | robust, 389                                                             |
| Overall variability, 73-74                                                        | as rotation of axes, 381-382, 384-385                                   |
| Paired observation test, 132-136                                                  | from S or R , 383-384, 393-397 nonuniqueness of components from R , 397 |
| Partial F -tests, 127, 138, 232, 293-296                                          | sample specific components, 398                                         |
| Partitioned matrices, see Matrix (matrices),                                      | scale invariance, lack of, 383                                          |
| partitioned matrices                                                              | scree graph, 397-399                                                    |
| Partitioning, see Cluster analysis, partitioning                                  | selection of variables, 404-406                                         |
| Pattern recognition, see Cluster analysis                                         | singular matrix and, 385-386                                            |
| People data, 526                                                                  | size and shape, 402-403                                                 |
| Perception data, 419                                                              | smaller principal components, 382, 389, 401                             |
| Perron-Frobenius theorem, 34, 402                                                 | tests of significance for, 397, 399-400                                 |
| Pillai's test statistic: definition of, 166                                       | variable specific components, 398 variances of, 382-383                 |
| 578-581                                                                           |                                                                         |
| table of critical values,                                                         | Probe word data, 70                                                     |
|                                                                                   | ∏                                                                       |
| Plasma data, 246                                                                  | Profile, 139-140                                                        |
| Politics data, 542                                                                |                                                                         |
| Positive definite                                                                 | Profile analysis:                                                       |
| matrix,                                                                           | and contrasts, 141-142                                                  |
| 25 positive definite sample covariance matrix, 67                                 | one-sample, 139-141                                                     |
| Prerequisites for this book, 3                                                    | and one-way ANOVA, 140 profile, definition of, 139-140                  |
| Principal components, 380-407                                                     | and repeated measures, 139                                              |
| algebra of, 385-387 and biplots, 531-532                                          |                                                                         |
| and cluster analysis, 390-393, 395, 482-484,                                      | several-sample, 199-204                                                 |
| 487                                                                               | two-sample, 141-148 hypotheses:                                         |
| scores, 386                                                                       | flatness, 145-146,                                                      |
| component                                                                         | 199-201                                                                 |
| definition of, 380, 382, 385                                                      | levels, 143-145, 199-200 parallelism, 141-143,                          |
| dimension reduction, 381-384, 385-387, 389 eigenvalues and eigenvectors, 382-385, | 199-200 and two-way ANOVA, 143-145                                      |
|                                                                                   | Projection pursuit, 451                                                 |
| 397-398                                                                           |                                                                         |

| Protein data, 483                                                           | model, 323-324                                                                 |
|-----------------------------------------------------------------------------|--------------------------------------------------------------------------------|
| Psychological data, 125                                                     | assumptions, 323-324 corrected for means (centered), 327                       |
| Q - Q plot, 92-94                                                           | multiple correlation, 332                                                      |
| Quadratic classification functions, 306-307                                 | R 2 (squared multiple correlation), 332-333,                                   |
| Quadratic form, 19                                                          | 337, 349, 355. See also R 2                                                    |
| Quantiles, 92-94, 97                                                        | random x 's, 322-323, 337 regression coefficients, 323                         |
| R 2 (squared multiple correlation), 332-333,                                | SSE, 325-326, 330-331, 333-336, 456                                            |
| 337, 349, 355, 361-362, 365,                                                | SSR, 330-331                                                                   |
| 375-376, 422-423                                                            | subset selection, 333-337                                                      |
| Ramus bone data, 78                                                         | all possible subsets, 333-335                                                  |
| Random variable(s):                                                         | criteria for selection ( R 2 p , s 2 p , C p ),                                |
| bivariate, 45                                                               | 333-335                                                                        |
| bivariate normal distribution, 46, 84, 88-89                                | comparison of criteria, 335 stepwise selection, 335-337                        |
| correlation of, 49-50                                                       | tests of hypotheses, 329-332                                                   |
| as cosine, 49-50                                                            | full and reduced model, 330-332                                                |
| covariance of, 46-48                                                        | partial F -test, 331-332                                                       |
| linear relationships, 47                                                    | overall regression test, 329-330                                               |
| independent, 46                                                             | subset of the β 's, 330-332                                                    |
| test for independence, 265-266                                              | variables:                                                                     |
| table of exact critical values, 590                                         | dependent ( y ), 322                                                           |
| orthogonal, 47-48                                                           | independent ( x ), 322                                                         |
| scatter plot, 50-51                                                         | predictor ( x ), 322                                                           |
| linear combinations, see Linear                                             | response ( y ), 322                                                            |
| univariate, 43                                                              | x 's), 322-323, 337-358                                                        |
| 43                                                                          | association, measures of, 349-351                                              |
| expected value of, mean of, 43                                              | centered x 's, 342-343                                                         |
| variance of, 44                                                             | estimation of B (matrix of regression                                          |
| vector, 53-56                                                               | coefficients):                                                                 |
|                                                                             | centered x 's, 342-343                                                         |
| Random vector(s), 52-56 distance between, 76-77, 83, 115, 118, 123, 271-272 | covariances, 343                                                               |
| linear functions of, 66-73. See also Linear combination(s) of variables     | least squares, 339-341 properties of estimators, 341-342 estimation of 𝚺 , 342 |
| mean of, 54-56                                                              | fixed x 's, 337-349                                                            |
| partitioned random vector, 62-66                                            | Gauss-Markov theorem, 341                                                      |
| standardized, 86                                                            | model,                                                                         |
| subvectors, 62-66                                                           | 337-339 assumptions, 339                                                       |
| Rank of a matrix, 22-23                                                     | corrected for means (centered), 342-343                                        |
| Rao's paradox, 116                                                          | random x 's, 358                                                               |
| Redundancy analysis, 373-374                                                | regression coefficients, matrix of ( B ), 88,                                  |
| Regression, monotonic, 509-510                                              | 338                                                                            |
| Regression, multiple (one y and several x 's),                              | subset selection, 351-358                                                      |
| 130-132, 323-337. See also                                                  | all possible subsets, 355-358 2 S                                              |
| Regression, multivariate x 's, 327-329                                      | criteria for selection ( R p , p , C p ) , 355-358                             |
| centered                                                                    | stepwise procedures,                                                           |
| estimation of 𝛃 :                                                           | 351-355                                                                        |
| centered x 's, 327-328                                                      | partial Wilks' /Lambda1 , 352-354 subset of the x 's, 351-353                  |
| covariances, 328-329 least squares, 325-326                                 | subset of the y 's, 353-355                                                    |
| of σ 2 , 326-327                                                            | tests of hypotheses, 343-349                                                   |
| estimation                                                                  | E matrix, 339, 342-344                                                         |
| fixed x 's, 323-333                                                         |                                                                                |

Regression, multivariate ( cont. )

full and reduced model:

on the x 's, 347-349

on the y 's, 353-355

with canonical correlations, 375-376

H matrix, 343-344

overall regression test, 343-347

with canonical correlations, 375

comparison of test statistics, 345

Lawley-Hotelling test, 345

Pillai's test, 345

rank of B , 345

Roy's test (union-intersection), 344-345

Wilks' /Lambda1 test (likelihood ratio), 344

subset of the x 's, 351-353

with canonical correlations, 375-376

subset of the y 's, 353-355

Repeated data set, 218

Repeated measures designs, 204-221. See also

Growth curves assumptions, 204-207

computation of test statistics, 212-213

contrast matrices, 206, 208-221

doubly multivariate data, 221

higher order designs, 213-221

multivariate approach, advantages of,

205-207

one sample, 208-211

likelihood ratio test, 209-210

and randomized block designs, 208

and profile analysis, 139

several samples, 211-212

univariate approach, 204-207

Republican vote data, 53

Research units, 1

Road distance data, 541

Rootstock data, 171

Rotation, see Factor analysis

Roy's test statistic:

definition of, 164-165

table of critical values, 574-577

Sampling units, 1

Scalar, 6

Scale of measurement, 1

Scatter plot, 50-51, 98, 105

Seishu data, 263

Selection of variables, 233, 333-337, 351-358

Singular value decomposition, 36, 522, 524,

532-533

generalized singular value decomposition, 522

Size and shape, 402-403

Skewness, 94-95, 98-99, 104

Snapbean data, 236

Sons data, 79

Specific variance, see Factor analysis

Spectral decomposition, 35, 382, 416-418,

505-506

Squared multiple correlation, see R 2

Standard deviation, 44

Standardized vector, 86

Steel data, 273

Stepwise selection of variables, 233, 335-337,

351-355

STRESS, 510-512

Subvectors, 62-66

conditional distribution of, 88

covariance matrix of, 62-66

distribution of sum of, 88

independence of, 63, 87

mean vector, 62-64, 66

tests of, 136-139, 231-233, 347-349,

353-359

Summation notation ( ∑ ), 9 Survival data, 239-241

t -tests:

characteristic form, 117, 122

contrasts, 179

equal levels in profile analysis, 145

growth curves, 224, 228

matched pairs, 132-133

one sample, 117

paired observations, 132-133

repeated measures, 210-211

two samples, 121-122, 127

T 2 -statistic:

additional information, test for, 136-139

assumptions for, 122

characteristic form, 118, 123

chi-square approximation for, 120

computation of, 130-132

by MANOVA, 130

by regression, 130-132

and F -distribution, 119, 124, 137-138

full and reduced model test, 137

likelihood ratio test, 126

matched pairs, 134-136

one-sample, 117-121

paired observations, 134-136

and profile analysis, 139-148

one sample, 139-141

two samples, 141-148

properties of, 119-120, 123-124

for a subvector, 136-139

table of critical values for T 2 , 558-561

two-sample, 122-126

Taxonomy, numerical, see Cluster analysis

Temperature data, 269

Tests of hypotheses:

accepting H 0, 118

for additional information, 136-139,

231-233, 347-349, 353-359

partial F -tests, 127, 138, 232

covariance matrices, 248-268

one covariance matrix, 248-254

independence:

individual variables, 265-266

table of exact critical values, 590

several subvectors, 261-264

two subvectors, 259-261

and canonical correlations, 260

a specified matrix 𝚺 0, 248-249

sphericity, 250-252

uniformity, compound symmetry, 206,

252-254

several covariance matrices, 254-259

Box's M -test, 257-259

table of exact critical values, 588-589

on individual variables, 126-130

Bonferroni critical values for, 127

tables, 562-565

discriminant functions, 126-132

experimentwise error rate, 128-129

partial F -tests, 127, 232

protected tests, 128-129

likelihood ratio test, 126. See also Likelihood ratio tests for linear combinations:

one sample ( H 0: C 𝛍 = 0 ) , 117, 140-141, 208-211

two samples ( H 0: C 𝛍 1 = C 𝛍 2 ) ,

142-143

mean vectors:

likelihood ratio tests, 126

one sample, 𝚺 known, 114-117

one sample, 𝚺 unknown, 117-121

several samples, 158-173

two-sample T 2 -test, 122-126

multivariate vs. univariate testing, 1-2,

112-113, 115-117, 127-130

paired observations (matched pairs),

132-136

multivariate, 134-136

univariate, 132-133

partial F -tests, 127, 138, 232

power of a test, 113, protected tests, 128-129

on regression coefficients, 329-332,

343-349

on a subvector, 136-139, 231-233, 347-349, 353-359

univariate tests:

ANOVA F -test, 156-158, 186-188

one-sample test on a mean, σ known,

113

one-sample test on a mean, σ unknown,

117

paired observation test, 132-133

tests on variances, 254-255

two-sample t -test, 121-122, 127

variances, equality of, 254-255

Total sample variance, 74, 383, 409, 418-419,

427

Trace of a matrix, 30, 34, 69

Trout data, 242

Two-sample test for equal mean vectors, 122-126

Union-intersection test, 164-165

Unit:

experimental, 1

research, 1

sampling, 1

Univariate normal distribution, 82-83, 86

Univariate normality, tests for, 92-96

D'Agostino's D -statistic, 96

table of critical values, 552

goodness-of-fit test, 96-97

normal probability paper, 94

Q -Q plot, 92-94

quantiles, 92-94, 97

skewness and kurtosis, 94-95

tables of critical values, 549-551

transformation of correlation, 96

Variables, 1. See also Random variables commensurate, 1

dummy variables, 173-174, 282, 315, 376-377

linear combinations of, 66-73

Variance:

generalized sample variance, 73

pooled variance, 121

population variance ( σ 2 ), 44

sample variance ( s 2 ), 44

total sample variance, 74

Variance-covariance matrix, see Covariance matrix

Variance matrix, see Covariance matrix

Varimax rotation, 434-435

Vector(s):

0 vector, 9

definition of, 6

linear independence and dependence of,

Vector(s) ( cont. ) distance: Mahalanobis, 76-77 from origin to a point, 14 between two vectors, 76-77 geometry of, 6 j vector, 9 length of, 14 linear combination of, 19 22 normalized, 31 notation for vector, 6 observation vector, 53-54

orthogonal, 31, 50

perpendicular, 50

product of, 14-16

dot product, 14

rows and columns of a matrix, 15-16 standardized, 86 subvectors, 62-66 sum of products, 14 sum of squares, 14 transpose of, 6-7 zero vector, 9 Voting data, 512 Weight gain data, 243 Wheat data, 503 Wilks' /Lambda1 test statistic: definition of, 161-164 partial /Lambda1 -statistic, 232 table of critical values, 566-573 Wishart distribution, 91-92 Words data, 154

## WILEY SERIES IN PROBABILITY AND STATISTICS ESTABLISHED BY WALTER A. SHEWHART AND SAMUEL S. WILKS

Editors: David J. Balding, Peter Bloomfield, Noel A. C. Cressie,

Nicholas I. Fisher, Iain M. Johnstone, J. B. Kadane, Louise M. Ryan,

David W. Scott, Adrian F. M. Smith, Jozef L. Teugels

Editors Emeriti:

Vic Barnett, J. Stuart Hunter, David G. Kendall

The Wiley Series in Probability and Statistics is well established and authoritative. It covers many topics of current research interest in both pure and applied statistics and probability theory. Written by leading statisticians and institutions, the titles span both state-of-the-art developments in the field and classical methods.

Reflecting the wide range of current research in statistics, the series encompasses applied, methodological and theoretical statistics, ranging from applications and new techniques made possible by advances in computerized practice to rigorous treatment of theoretical approaches.

This series provides essential and invaluable reading for all statisticians, whether in academia, industry, government, or research.

ABRAHAM and LEDOLTER · Statistical Methods for Forecasting

AGRESTI · Analysis of Ordinal Categorical Data

AGRESTI · An Introduction to Categorical Data Analysis

AGRESTI · Categorical Data Analysis, Second Edition

ANDE  L · Mathematics of Chance

ANDERSON · An Introduction to Multivariate Statistical Analysis, Second Edition

*ANDERSON · The Statistical Analysis of Time Series

ANDERSON, AUQUIER, HAUCK, OAKES, VANDAELE, and WEISBERG ·

Statistical Methods for Comparative Studies

ANDERSON and LOYNES · The Teaching of Practical Statistics

ARMITAGE and DAVID (editors) · Advances in Biometry

ARNOLD, BALAKRISHNAN, and NAGARAJA · Records

- *ARTHANARI and DODGE · Mathematical Programming in Statistics
- *BAILEY · The Elements of Stochastic Processes with Applications to the Natural Sciences

BALAKRISHNAN and KOUTRAS · Runs and Scans with Applications

BARNETT · Comparative Statistical Inference, Third Edition

BARNETT and LEWIS · Outliers in Statistical Data, Third Edition

BARTOSZYNSKI and NIEWIADOMSKA-BUGAJ · Probability and Statistical Inference

BASILEVSKY · Statistical Factor Analysis and Related Methods: Theory and Applications

BASU and RIGDON · Statistical Methods for the Reliability of Repairable Systems

BATES and WATTS · Nonlinear Regression Analysis and Its Applications

BECHHOFER, SANTNER, and GOLDSMAN · Design and Analysis of Experiments for Statistical Selection, Screening, and Multiple Comparisons

BELSLEY · Conditioning Diagnostics: Collinearity and Weak Data in Regression

BELSLEY, KUH, and WELSCH · Regression Diagnostics: Identifying Influential Data and Sources of Collinearity

BENDAT and PIERSOL · Random Data: Analysis and Measurement Procedures, Third Edition

*Now available in a lower priced paperback edition in the Wiley Classics Library.

BERRY, CHALONER, and GEWEKE · Bayesian Analysis in Statistics and Econometrics: Essays in Honor of Arnold Zellner BERNARDO and SMITH · Bayesian Theory BHAT and MILLER · Elements of Applied Stochastic Processes, Third Edition BHATTACHARYA and JOHNSON · Statistical Concepts and Methods BHATTACHARYA and WAYMIRE · Stochastic Processes with Applications BILLINGSLEY · Convergence of Probability Measures, Second Edition BILLINGSLEY · Probability and Measure, Third Edition BIRKES and DODGE · Alternative Methods of Regression BLISCHKE AND MURTHY (editors) · Case Studies in Reliability and Maintenance BLISCHKE AND MURTHY · Reliability: Modeling, Prediction, and Optimization BLOOMFIELD · Fourier Analysis of Time Series: An Introduction, Second Edition BOLLEN · Structural Equations with Latent Variables BOROVKOV · Ergodicity and Stability of Stochastic Processes BOULEAU · Numerical Methods for Stochastic Processes BOX · Bayesian Inference in Statistical Analysis BOX · R. A. Fisher, the Life of a Scientist BOX and DRAPER · Empirical Model-Building and Response Surfaces *BOX and DRAPER · Evolutionary Operation: A Statistical Method for Process Improvement BOX, HUNTER, and HUNTER · Statistics for Experimenters: An Introduction to Design, Data Analysis, and Model Building BOX and LUCEÑO · Statistical Control by Monitoring and Feedback Adjustment BRANDIMARTE · Numerical Methods in Finance: A MATLAB-Based Introduction BROWN and HOLLANDER · Statistics: A Biomedical Introduction BRUNNER, DOMHOF, and LANGER · Nonparametric Analysis of Longitudinal Data in Factorial Experiments BUCKLEW · Large Deviation Techniques in Decision, Simulation, and Estimation CAIROLI  and DALANG · Sequential Stochastic Optimization CHAN · Time Series: Applications to Finance CHATTERJEE and HADI · Sensitivity Analysis in Linear Regression CHATTERJEE and PRICE · Regression Analysis by Example, Third Edition CHERNICK · Bootstrap Methods: A Practitioner's Guide CHERNICK and FRIIS · Introductory Biostatistics for the Health Sciences CHILÈS and DELFINER · Geostatistics: Modeling Spatial Uncertainty CHOW and LIU · Design and Analysis of Clinical Trials: Concepts and Methodologies CLARKE and DISNEY · Probability and Random Processes: A First Course with Applications, Second Edition *COCHRAN and COX · Experimental Designs, Second Edition CONGDON · Bayesian Statistical Modelling CONOVER · Practical Nonparametric Statistics, Second Edition COOK · Regression Graphics COOK and WEISBERG · Applied Regression Including Computing and Graphics COOK and WEISBERG · An Introduction to Regression Graphics CORNELL · Experiments with Mixtures, Designs, Models, and the Analysis of Mixture Data, Third Edition COVER and THOMAS · Elements of Information Theory COX · A Handbook of Introductory Statistical Methods *COX · Planning of Experiments CRESSIE · Statistics for Spatial Data, Revised Edition CSÖRGO ´´ and HORVÁTH · Limit Theorems in Change Point Analysis DANIEL · Applications of Statistics to Industrial Experimentation DANIEL · Biostatistics: A Foundation for Analysis in the Health Sciences, Sixth Edition

*Now available in a lower priced paperback edition in the Wiley Classics Library.

*DANIEL · Fitting Equations to Data: Computer Analysis of Multifactor Data, Second Edition DAVID · Order Statistics, Second Edition *DEGROOT, FIENBERG, and KADANE · Statistics and the Law DEL CASTILLO · Statistical Process Adjustment for Quality Control DETTE and STUDDEN · The Theory of Canonical Moments with Applications in Statistics, Probability, and Analysis DEY and MUKERJEE · Fractional Factorial Plans DILLON and GOLDSTEIN · Multivariate Analysis: Methods and Applications DODGE · Alternative Methods of Regression *DODGE and ROMIG · Sampling Inspection Tables, Second Edition *DOOB · Stochastic Processes DOWDY and WEARDEN · Statistics for Research, Second Edition DRAPER and SMITH · Applied Regression Analysis, Third Edition DRYDEN and MARDIA · Statistical Shape Analysis DUDEWICZ and MISHRA · Modern Mathematical Statistics DUNN and CLARK · Applied Statistics: Analysis of Variance and Regression, Second Edition DUNN and CLARK · Basic Statistics: A Primer for the Biomedical Sciences, Third Edition DUPUIS and ELLIS · A Weak Convergence Approach to the Theory of Large Deviations *ELANDT-JOHNSON and JOHNSON · Survival Models and Data Analysis ETHIER and KURTZ · Markov Processes: Characterization and Convergence EVANS, HASTINGS, and PEACOCK · Statistical Distributions, Third Edition FELLER · An Introduction to Probability Theory and Its Applications, Volume I, Third Edition, Revised; Volume II, Second Edition FISHER and VAN BELLE · Biostatistics: A Methodology for the Health Sciences *FLEISS · The Design and Analysis of Clinical Experiments FLEISS · Statistical Methods for Rates and Proportions, Second Edition FLEMING and HARRINGTON · Counting Processes and Survival Analysis FULLER · Introduction to Statistical Time Series, Second Edition FULLER · Measurement Error Models GALLANT · Nonlinear Statistical Models GHOSH, MUKHOPADHYAY, and SEN · Sequential Estimation GIFI · Nonlinear Multivariate Analysis GLASSERMAN and YAO · Monotone Structure in Discrete-Event Systems GNANADESIKAN · Methods for Statistical Data Analysis of Multivariate Observations, Second Edition GOLDSTEIN and LEWIS · Assessment: Problems, Development, and Statistical Issues GREENWOOD and NIKULIN · A Guide to Chi-Squared Testing GROSS and HARRIS · Fundamentals of Queueing Theory, Third Edition *HAHN and SHAPIRO · Statistical Models in Engineering HAHN and MEEKER · Statistical Intervals: A Guide for Practitioners HALD · A History of Probability and Statistics and their Applications Before 1750 HALD · A History of Mathematical Statistics from 1750 to 1930 HAMPEL · Robust Statistics: The Approach Based on Influence Functions HANNAN and DEISTLER · The Statistical Theory of Linear Systems HEIBERGER · Computation for the Analysis of Designed Experiments HEDAYAT and SINHA · Design and Inference in Finite Population Sampling HELLER · MACSYMA for Statisticians HINKELMAN and KEMPTHORNE: · Design and Analysis of Experiments, Volume 1: Introduction to Experimental Design HOAGLIN, MOSTELLER, and TUKEY · Exploratory Approach to Analysis of Variance

*Now available in a lower priced paperback edition in the Wiley Classics Library.

- HOAGLIN, MOSTELLER, and TUKEY · Exploring Data Tables, Trends and Shapes *HOAGLIN, MOSTELLER, and TUKEY · Understanding Robust and Exploratory
- Data Analysis
- HOCHBERG and TAMHANE · Multiple Comparison Procedures
- HOCKING · Methods and Applications of Linear Models: Regression and the Analysis of Variance, Second Edition
- HOEL · Introduction to Mathematical Statistics, Fifth Edition
- HOGG and KLUGMAN · Loss Distributions HOLLANDER and WOLFE · Nonparametric Statistical Methods, Second Edition HOSMER and LEMESHOW · Applied Logistic Regression, Second Edition HOSMER and LEMESHOW · Applied Survival Analysis: Regression Modeling of Time to Event Data HØYLAND and RAUSAND · System Reliability Theory: Models and Statistical Methods HUBER · Robust Statistics HUBERTY · Applied Discriminant Analysis HUNT and KENNEDY · Financial Derivatives in Theory and Practice HUSKOVA, BERAN, and DUPAC · Collected Works of Jaroslav Hajekwith Commentary IMAN and CONOVER · A Modern Approach to Statistics JACKSON · A User's Guide to Principle Components JOHN · Statistical Methods in Engineering and Quality Assurance JOHNSON · Multivariate Statistical Simulation JOHNSON and BALAKRISHNAN · Advances in the Theory and Practice of Statistics: A Volume in Honor of Samuel Kotz JUDGE, GRIFFITHS, HILL, LÜTKEPOHL, and LEE · The Theory and Practice of Econometrics, Second Edition JOHNSON and KOTZ · Distributions in Statistics JOHNSON and KOTZ (editors) · Leading Personalities in Statistical Sciences: From the Seventeenth Century to the Present JOHNSON, KOTZ, and BALAKRISHNAN · Continuous Univariate Distributions, Volume 1, Second Edition JOHNSON, KOTZ, and BALAKRISHNAN · Continuous Univariate Distributions, Volume 2, Second Edition JOHNSON, KOTZ, and BALAKRISHNAN · Discrete Multivariate Distributions JOHNSON, KOTZ, and KEMP · Univariate Discrete Distributions, Second Edition JUREC  KOVÁ and SEN · Robust Statistical Procedures: Aymptotics and Interrelations JUREK and MASON · Operator-Limit Distributions in Probability Theory KADANE · Bayesian Methods and Ethics in a Clinical Trial Design KADANE AND SCHUM · A Probabilistic Analysis of the Sacco and Vanzetti Evidence KALBFLEISCH and PRENTICE · The Statistical Analysis of Failure Time Data, Second Edition KASS and VOS · Geometrical Foundations of Asymptotic Inference KAUFMAN and ROUSSEEUW · Finding Groups in Data: An Introduction to Cluster Analysis KEDEM and FOKIANOS · Regression Models for Time Series Analysis KENDALL, BARDEN, CARNE, and LE · Shape and Shape Theory KHURI · Advanced Calculus with Applications in Statistics, Second Edition KHURI, MATHEW, and SINHA · Statistical Tests for Mixed Linear Models KLUGMAN, PANJER, and WILLMOT · Loss Models: From Data to Decisions KLUGMAN, PANJER, and WILLMOT · Solutions Manual to Accompany Loss Models: From Data to Decisions KOTZ, BALAKRISHNAN, and JOHNSON · Continuous Multivariate Distributions, Volume 1, Second Edition KOTZ and JOHNSON (editors) · Encyclopedia of Statistical Sciences: Volumes 1 to 9 with Index
- *Now available in a lower priced paperback edition in the Wiley Classics Library.

KOTZ and JOHNSON (editors) · Encyclopedia of Statistical Sciences: Supplement Volume KOTZ, READ, and BANKS (editors) · Encyclopedia of Statistical Sciences: Update Volume 1 KOTZ, READ, and BANKS (editors) · Encyclopedia of Statistical Sciences: Update Volume 2 KOVALENKO, KUZNETZOV, and PEGG · Mathematical Theory of Reliability of Time-Dependent Systems with Practical Applications LACHIN · Biostatistical Methods: The Assessment of Relative Risks LAD · Operational Subjective Statistical Methods: A Mathematical, Philosophical, and Historical Introduction LAMPERTI · Probability: A Survey of the Mathematical Theory, Second Edition LANGE, RYAN, BILLARD, BRILLINGER, CONQUEST, and GREENHOUSE · Case Studies in Biometry LARSON · Introduction to Probability Theory and Statistical Inference, Third Edition LAWLESS · Statistical Models and Methods for Lifetime Data, Second Edition LAWSON · Statistical Methods in Spatial Epidemiology LE · Applied Categorical Data Analysis LE · Applied Survival Analysis LEE and WANG · Statistical Methods for Survival Data Analysis, Third Edition LEPAGE and BILLARD · Exploring the Limits of Bootstrap LEYLAND and GOLDSTEIN (editors) · Multilevel Modelling of Health Statistics LIAO · Statistical Group Comparison LINDVALL · Lectures on the Coupling Method LINHART and ZUCCHINI · Model Selection LITTLE and RUBIN · Statistical Analysis with Missing Data, Second Edition LLOYD · The Statistical Analysis of Categorical Data MAGNUS and NEUDECKER · Matrix Differential Calculus with Applications in Statistics and Econometrics, Revised Edition MALLER and ZHOU · Survival Analysis with Long Term Survivors MALLOWS · Design, Data, and Analysis by Some Friends of Cuthbert Daniel MANN, SCHAFER, and SINGPURWALLA · Methods for Statistical Analysis of Reliability and Life Data MANTON, WOODBURY, and TOLLEY · Statistical Applications Using Fuzzy Sets MARDIA and JUPP · Directional Statistics MASON, GUNST, and HESS · Statistical Design and Analysis of Experiments with Applications to Engineering and Science, Second Edition McCULLOCH and SEARLE · Generalized, Linear, and Mixed Models McFADDEN · Management of Data in Clinical Trials McLACHLAN · Discriminant Analysis and Statistical Pattern Recognition McLACHLAN and KRISHNAN · The EM Algorithm and Extensions McLACHLAN and PEEL · Finite Mixture Models McNEIL · Epidemiological Research Methods MEEKER and ESCOBAR · Statistical Methods for Reliability Data MEERSCHAERT and SCHEFFLER · Limit Distributions for Sums of Independent Random Vectors: Heavy Tails in Theory and Practice *MILLER · Survival Analysis, Second Edition MONTGOMERY, PECK, and VINING · Introduction to Linear Regression Analysis, Third Edition MORGENTHALER and TUKEY · Configural Polysampling: A Route to Practical Robustness MUIRHEAD · Aspects of Multivariate Statistical Theory MURRAY · X-STAT 2.0 Statistical Experimentation, Design Data Analysis, and Nonlinear Optimization

*Now available in a lower priced paperback edition in the Wiley Classics Library.

MYERS and MONTGOMERY · Response Surface Methodology: Process and Product Optimization Using Designed Experiments, Second Edition MYERS, MONTGOMERY, and VINING · Generalized Linear Models. With Applications in Engineering and the Sciences NELSON · Accelerated Testing, Statistical Models, Test Plans, and Data Analyses NELSON · Applied Life Data Analysis NEWMAN · Biostatistical Methods in Epidemiology OCHI · Applied Probability and Stochastic Processes in Engineering and Physical Sciences OKABE, BOOTS, SUGIHARA, and CHIU · Spatial Tesselations: Concepts and Applications of Voronoi Diagrams, Second Edition OLIVER and SMITH · Influence Diagrams, Belief Nets and Decision Analysis PANKRATZ · Forecasting with Dynamic Regression Models PANKRATZ · Forecasting with Univariate Box-Jenkins Models: Concepts and Cases *PARZEN · Modern Probability Theory and Its Applications PEÑA, TIAO, and TSAY · A Course in Time Series Analysis PIANTADOSI · Clinical Trials: A Methodologic Perspective PORT · Theoretical Probability for Applications POURAHMADI · Foundations of Time Series Analysis and Prediction Theory PRESS · Bayesian Statistics: Principles, Models, and Applications PRESS · Subjective and Objective Bayesian Statistics, Second Edition PRESS and TANUR · The Subjectivity of Scientists and the Bayesian Approach PUKELSHEIM · Optimal Experimental Design PURI, VILAPLANA, and WERTZ · New Perspectives in Theoretical and Applied Statistics PUTERMAN · Markov Decision Processes: Discrete Stochastic Dynamic Programming *RAO · Linear Statistical Inference and Its Applications, Second Edition RENCHER · Linear Models in Statistics RENCHER · Methods of Multivariate Analysis, Second Edition RENCHER · Multivariate Statistical Inference with Applications RIPLEY · Spatial Statistics RIPLEY · Stochastic Simulation ROBINSON · Practical Strategies for Experimenting ROHATGI and SALEH · An Introduction to Probability and Statistics, Second Edition ROLSKI, SCHMIDLI, SCHMIDT, and TEUGELS · Stochastic Processes for Insurance and Finance ROSENBERGER and LACHIN · Randomization in Clinical Trials: Theory and Practice ROSS · Introduction to Probability and Statistics for Engineers and Scientists ROUSSEEUW and LEROY · Robust Regression and Outlier Detection RUBIN · Multiple Imputation for Nonresponse in Surveys RUBINSTEIN · Simulation and the Monte Carlo Method RUBINSTEIN and MELAMED · Modern Simulation and Modeling RYAN · Modern Regression Methods RYAN · Statistical Methods for Quality Improvement, Second Edition SALTELLI, CHAN, and SCOTT (editors) · Sensitivity Analysis *SCHEFFE · The Analysis of Variance SCHIMEK · Smoothing and Regression: Approaches, Computation, and Application SCHOTT · Matrix Analysis for Statistics SCHUSS · Theory and Applications of Stochastic Differential Equations SCOTT · Multivariate Density Estimation: Theory, Practice, and Visualization *SEARLE · Linear Models SEARLE · Linear Models for Unbalanced Data SEARLE · Matrix Algebra Useful for Statistics SEARLE, CASELLA, and McCULLOCH · Variance Components SEARLE and WILLETT · Matrix Algebra for Applied Economics

*Now available in a lower priced paperback edition in the Wiley Classics Library.

- SEBER · Linear Regression Analysis

SEBER · Multivariate Observations SEBER and WILD · Nonlinear Regression SENNOTT · Stochastic Dynamic Programming and the Control of Queueing Systems *SERFLING · Approximation Theorems of Mathematical Statistics SHAFER and VOVK · Probability and Finance: It's Only a Game! SMALL and MCLEISH · Hilbert Space Methods in Probability and Statistical Inference SRIVASTAVA · Methods of Multivariate Statistics STAPLETON · Linear Statistical Models STAUDTE and SHEATHER · Robust Estimation and Testing STOYAN, KENDALL, and MECKE · Stochastic Geometry and Its Applications, Second Edition STOYAN and STOYAN · Fractals, Random Shapes and Point Fields: Methods of Geometrical Statistics STYAN · The Collected Papers of T. W. Anderson: 1943-1985 SUTTON, ABRAMS, JONES, SHELDON, and SONG · Methods for Meta-Analysis in Medical Research TANAKA · Time Series Analysis: Nonstationary and Noninvertible Distribution Theory THOMPSON · Empirical Model Building THOMPSON · Sampling, Second Edition THOMPSON · Simulation: A Modeler's Approach THOMPSON and SEBER · Adaptive Sampling THOMPSON, WILLIAMS, and FINDLAY · Models for Investors in Real World Markets TIAO, BISGAARD, HILL, PEÑA, and STIGLER (editors) · Box on Quality and Discovery: with Design, Control, and Robustness TIERNEY · LISP-STAT: An Object-Oriented Environment for Statistical Computing and Dynamic Graphics TSAY · Analysis of Financial Time Series UPTON and FINGLETON · Spatial Data Analysis by Example, Volume II: Categorical and Directional Data VAN BELLE · Statistical Rules of Thumb VIDAKOVIC · Statistical Modeling by Wavelets WEISBERG · Applied Linear Regression, Second Edition WELSH · Aspects of Statistical Inference WESTFALL and YOUNG · Resampling-Based Multiple Testing: Examples and Methods for p -Value Adjustment WHITTAKER · Graphical Models in Applied Multivariate Statistics WINKER · Optimization Heuristics in Economics: Applications of Threshold Accepting WONNACOTT and WONNACOTT · Econometrics, Second Edition WOODING · Planning Pharmaceutical Clinical Trials: Basic Statistical Principles WOOLSON and CLARKE · Statistical Methods for the Analysis of Biomedical Data, Second Edition WU and HAMADA · Experiments: Planning, Analysis, and Parameter Design Optimization YANG · The Construction Theory of Denumerable Markov Processes *ZELLNER · An Introduction to Bayesian Inference in Econometrics ZHOU, OBUCHOWSKI, and MCCLISH · Statistical Methods in Diagnostic Medicine

*Now available in a lower priced paperback edition in the Wiley Classics Library.

## A on the analysis of multiple variables for students and scientists alike primer

This book strikes a nice balance between meeting the needs of statistics majors and students in other fields. The discussion of each multivariate technique is straightforward and comprehensive  This textbook is likely to become a useful reference for students in their future work. quite

In this  well-written and interesting book, Rencher has done great job in presenting intuitive and innovative explanations of some of the otherwise difficult concepts;'

"This book is excellent for an introductory course in multivariate analysis for students with minimal background in mathematics and statistics '

'Excellent introduction to standard topics in multivariate analysis.

When measuringseveral  variables on a complex experimental  unit, it is often to analyze the variables simultaneously; rather than isolate them and consider them individ ually. Multivariate analysis enables researchers to the joint performance of such ables and to determine the effect of variable in the presence of the others. The Second Edition of Alvin Rencher's Methods of Multivariate Analysis provides students of all statistical backgrounds with both the fundamental and more sophisticated skills necessary to master the discipline. necessary explore varieach

To illustrate multivariate applications, the author provides examples and exercises based on fifty-nine real data sets from a wide variety of scientific fields. Rencher takes a 'methods approach to his subject, with an emphasis on how students and practitioners can employ multivariate analysis in real-life situations. The Second Edition contains revised and chapters from the critically acclaimed First Edition as well as brand-new chapters on: updated

- Cluster analysis
- Correspondence analysis
- Multidimensional scaling
- Biplots

Each chapter contains exercises, with corresponding answers and hints in the appendix; providing students the opportunity to test and extend their understanding of the subject. Methods of Multivariate Analysis provides an authoritative reference for statistics students as well as for practicing scientists and clinicians.

Fellow of the American Statistical Association. He is the author of Linear Models in Statistics and Multivariate Statistical Inference and Applications; both available from Wiley

<!-- image -->

<!-- image -->